{
  "project": "Research Data/HypochondriAI",
  "repo": "Mihai-Tirtara/HypochondriAI",
  "prior_commit": "3b23faa83b3007490569ac2951887fe622c0cdcc",
  "researched_commit": "c24b8d2c2fc40913415a7883c87a5c8185a17a37",
  "compare_url": "https://github.com/Mihai-Tirtara/HypochondriAI/compare/3b23faa83b3007490569ac2951887fe622c0cdcc...c24b8d2c2fc40913415a7883c87a5c8185a17a37",
  "ahead_by": 1,
  "behind_by": 0,
  "changed_files": [
    {
      "path": ".pre-commit-config.yaml",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "patch": "@@ -13,7 +13,7 @@ repos:\n     rev: 25.1.0\n     hooks:\n     -   id: black\n-        language_version: python3.11\n+        language_version: python3.12\n \n # Ruff for linting\n -   repo: https://github.com/astral-sh/ruff-pre-commit",
      "patch_lines": [
        "@@ -13,7 +13,7 @@ repos:\n",
        "     rev: 25.1.0\n",
        "     hooks:\n",
        "     -   id: black\n",
        "-        language_version: python3.11\n",
        "+        language_version: python3.12\n",
        " \n",
        " # Ruff for linting\n",
        " -   repo: https://github.com/astral-sh/ruff-pre-commit\n"
      ]
    },
    {
      "path": "backend/app/services/llm.py",
      "status": "modified",
      "additions": 89,
      "deletions": 139,
      "patch": "@@ -34,65 +34,34 @@ class LangchainService:\n     _initialized: bool = False\n \n     def __init__(self, model_id: str | None = None, model_provider: str | None = None):\n-        \"\"\"\n-        Initialize Langchain service with optional model and provider\n-\n-        Args:\n-            model_id: The model ID to use. If None, uses the default from settings.\n-            model_provider: The provider of the model. If None, uses the default from settings.\n-        \"\"\"\n-        logger.debug(\"LangchainSerivice created\")\n         LangchainService.initialize_bedrock_client()\n \n-    @staticmethod\n-    def initialize_bedrock_client():\n-        \"\"\"Create and return an Amazon Bedrock client\"\"\"\n-        try:\n-            boto3.setup_default_session(\n-                region_name=settings.AWS_REGION,\n-                aws_access_key_id=settings.AWS_ACCESS_KEY_ID,\n-                aws_secret_access_key=settings.AWS_SECRET_ACCESS_KEY,\n-            )\n-        except Exception as e:\n-            logging.error(f\"Error initializing Bedrock client: {e!s}\")\n-            raise\n-\n-    @staticmethod\n-    def call_model(state: State):\n-        \"\"\"\n-        Call the model with the given state.\n-\n-        Args:\n-            state: The state to pass to the model.\n-\n-        Returns:\n-            The response from the model.\n+    async def conversation(\n+        self, conversation_id: str, user_input: str, user_context: str | None = None\n+    ):\n \n-        \"\"\"\n+        if not self.__class__._initialized or self.__class__.graph is None:\n+            raise Exception(\"Graph not initialized. Call initialize_graph() first.\")\n \n-        # Get the components from the state\n-        messages = state[\"messages\"]\n-        user_context = state.get(\"user_context\", None)\n-        prompt_template = generate_health_anxiety_prompt(user_context)\n-        # Invoke the prompt template with the state to get a formatted prompt\n-        formatted_prompt = prompt_template.invoke({\"messages\": messages})\n+        # Check if checkpointer exists and log its type for debugging\n+        if self.__class__.checkpointer:\n+            logger.debug(f\"Using checkpointer: {type(self.__class__.checkpointer)}\")\n+        else:\n+            logger.error(\"Checkpointer is None during conversation call.\")\n+            raise Exception(\"Checkpointer not available.\")\n \n-        # Now pass the formatted prompt to the model\n-        response = LangchainService.model.invoke(formatted_prompt)\n-        logger.info(f\"Model response: {response}\")\n-        return {\"messages\": [response]}\n+        config = {\"configurable\": {\"thread_id\": conversation_id}}\n+        input_messages = [HumanMessage(content=user_input)]\n+        response = await self.__class__.graph.ainvoke(\n+            {\"messages\": input_messages, \"user_context\": user_context}, config=config\n+        )\n+        return response[\"messages\"][-1]\n \n     @classmethod\n     async def initialize_langchain_components(\n         cls, model_id: str | None = None, model_provider: str | None = None\n     ):\n-        \"\"\"\n-        Set up all the necessary componnents  for the Langchain service.\n \n-        Args:\n-            model_id: The model ID to use. If None, uses the default from settings.\n-            model_provider: The provider of the model. If None, uses the default from settings.\n-        \"\"\"\n         if cls._initialized:\n             logger.info(\"Graph already initialized.\")\n             return cls.graph\n@@ -104,43 +73,40 @@ async def initialize_langchain_components(\n         except Exception as e:\n             logger.error(f\"Error initializing Langchain components: {e!s}\")\n             raise\n+        logger.info(\"Langchain components initialized successfully.\")\n \n     @classmethod\n-    def _initialize_model(\n-        cls, model_id: str | None = None, model_provider: str | None = None\n-    ):\n+    def _initialize_graph(cls):\n         \"\"\"\n-        Initialize the model with the given ID and provider.\n-\n-        Args:\n-            model_id: The ID of the model to use.\n-            model_provider: The provider of the model.\n-\n-        Returns:\n-            The initialized model.\n+        model and checkpointer need to be initialized before this method is called.\n         \"\"\"\n-        # Initialize the model\n-        cls._model_id = model_id or settings.MODEL_ID\n-        cls._model_provider = model_provider or settings.MODEL_PROVIDER\n-        try:\n-            # Initialize the Bedrock client one extra time for safety\n-            # This is a bit redundant but ensures the client is set up before model initialization\n-            cls.initialize_bedrock_client()\n-            cls.model = init_chat_model(\n-                model=cls._model_id, model_provider=cls._model_provider\n-            )\n-            logger.info(\n-                f\"Model initialized: {cls._model_id} with provider: {cls._model_provider}\"\n-            )\n-        except Exception as e:\n-            logger.error(f\"Error initializing model: {e!s}\")\n-            raise\n+        if cls.graph is None:\n+            logger.info(\"Initializing graph...\")\n+            workflow = StateGraph(state_schema=State)\n+            workflow.add_edge(START, \"model\")\n+            workflow.add_node(\"model\", cls.call_model)\n+            workflow.add_edge(\"model\", END)\n+            cls.graph = workflow.compile(checkpointer=cls.checkpointer)\n+            cls._initialized = True\n+            logger.info(\"Graph initialized.\")\n+        else:\n+            logger.info(\"Graph already initialized.\")\n+\n+    @classmethod\n+    async def _initialize_checkpointer(cls):\n+        if cls.checkpointer is None and cls.db_pool is not None:\n+            logger.info(\"Initializing checkpointer...\")\n+            try:\n+                cls.checkpointer = AsyncPostgresSaver(cls.db_pool)\n+                await cls.checkpointer.setup()\n+                logger.info(\"Checkpointer initialized.\")\n+            except Exception as e:\n+                logger.error(f\"Error initializing checkpointer: {e!s}\")\n+                cls.checkpointer = None\n+                raise\n \n     @classmethod\n     async def _initialize_pool(cls):\n-        \"\"\"\n-        Initialize the database pool\n-        \"\"\"\n         if cls.db_pool is None:\n             logger.info(\"Initializing database pool...\")\n             try:\n@@ -169,27 +135,8 @@ async def _initialize_pool(cls):\n                 cls.db_pool = None\n                 raise\n \n-    @classmethod\n-    async def _initialize_checkpointer(cls):\n-        \"\"\"\n-        Initialize the checkpointer for the graph.\n-        \"\"\"\n-        if cls.checkpointer is None and cls.db_pool is not None:\n-            logger.info(\"Initializing checkpointer...\")\n-            try:\n-                cls.checkpointer = AsyncPostgresSaver(cls.db_pool)\n-                await cls.checkpointer.setup()\n-                logger.info(\"Checkpointer initialized.\")\n-            except Exception as e:\n-                logger.error(f\"Error initializing checkpointer: {e!s}\")\n-                cls.checkpointer = None\n-                raise\n-\n     @classmethod\n     async def close_pool(cls):\n-        \"\"\"\n-        Close the pool and any resources it holds.\n-        \"\"\"\n         if cls.db_pool:\n             try:\n                 await cls.db_pool.close()\n@@ -200,49 +147,52 @@ async def close_pool(cls):\n         else:\n             logger.info(\"No database pool to close.\")\n \n-    @classmethod\n-    def _initialize_graph(cls):\n-        \"\"\"\n-        Initialize the graph with the model and checkpointer.\n-        \"\"\"\n-        if cls.graph is None:\n-            logger.info(\"Initializing graph...\")\n-            workflow = StateGraph(state_schema=State)\n-            workflow.add_edge(START, \"model\")\n-            workflow.add_node(\"model\", cls.call_model)\n-            workflow.add_edge(\"model\", END)\n-            cls.graph = workflow.compile(checkpointer=cls.checkpointer)\n-            cls._initialized = True\n-            logger.info(\"Graph initialized.\")\n-        else:\n-            logger.info(\"Graph already initialized.\")\n+    @staticmethod\n+    def call_model(state: State):\n \n-    async def conversation(\n-        self, conversation_id: str, user_input: str, user_context: str | None = None\n+        # Get the components from the state\n+        messages = state[\"messages\"]\n+        user_context = state.get(\"user_context\", None)\n+        prompt_template = generate_health_anxiety_prompt(user_context)\n+\n+        # Invoke the prompt template with the state to get a formatted prompt\n+        formatted_prompt = prompt_template.invoke({\"messages\": messages})\n+\n+        # Now pass the formatted prompt to the model\n+        response = LangchainService.model.invoke(formatted_prompt)\n+        logger.info(f\"Model response: {response}\")\n+        return {\"messages\": [response]}\n+\n+    @classmethod\n+    def _initialize_model(\n+        cls, model_id: str | None = None, model_provider: str | None = None\n     ):\n-        \"\"\"\n-        Create a conversation with the chat model or continue an existing one.\n-        Args:\n-            conversation_id: The ID of the conversation.\n-            user_input: The user's query.\n-            user_context: Additional context provided by the user.\n-\n-        Returns:\n-            The response from the AI model\n-        \"\"\"\n-        if not self.__class__._initialized or self.__class__.graph is None:\n-            raise Exception(\"Graph not initialized. Call initialize_graph() first.\")\n \n-        # Check if checkpointer exists and maybe log its type for debugging\n-        if self.__class__.checkpointer:\n-            logger.debug(f\"Using checkpointer: {type(self.__class__.checkpointer)}\")\n-        else:\n-            logger.error(\"Checkpointer is None during conversation call.\")\n-            raise Exception(\"Checkpointer not available.\")\n+        cls._model_id = model_id or settings.MODEL_ID\n+        cls._model_provider = model_provider or settings.MODEL_PROVIDER\n+        try:\n+            # Initialize the Bedrock client one extra time for safety\n+            # This is a bit redundant but ensures the client is set up before model initialization\n+            cls.initialize_bedrock_client()\n+            cls.model = init_chat_model(\n+                model=cls._model_id, model_provider=cls._model_provider\n+            )\n+            logger.info(\n+                f\"Model initialized: {cls._model_id} with provider: {cls._model_provider}\"\n+            )\n+        except Exception as e:\n+            logger.error(f\"Error initializing model: {e!s}\")\n+            raise\n \n-        config = {\"configurable\": {\"thread_id\": conversation_id}}\n-        input_messages = [HumanMessage(content=user_input)]\n-        response = await self.__class__.graph.ainvoke(\n-            {\"messages\": input_messages, \"user_context\": user_context}, config=config\n-        )\n-        return response[\"messages\"][-1]\n+    @staticmethod\n+    def initialize_bedrock_client():\n+        \"\"\"Create and return an Amazon Bedrock client\"\"\"\n+        try:\n+            boto3.setup_default_session(\n+                region_name=settings.AWS_REGION,\n+                aws_access_key_id=settings.AWS_ACCESS_KEY_ID,\n+                aws_secret_access_key=settings.AWS_SECRET_ACCESS_KEY,\n+            )\n+        except Exception as e:\n+            logging.error(f\"Error initializing Bedrock client: {e!s}\")\n+            raise",
      "patch_lines": [
        "@@ -34,65 +34,34 @@ class LangchainService:\n",
        "     _initialized: bool = False\n",
        " \n",
        "     def __init__(self, model_id: str | None = None, model_provider: str | None = None):\n",
        "-        \"\"\"\n",
        "-        Initialize Langchain service with optional model and provider\n",
        "-\n",
        "-        Args:\n",
        "-            model_id: The model ID to use. If None, uses the default from settings.\n",
        "-            model_provider: The provider of the model. If None, uses the default from settings.\n",
        "-        \"\"\"\n",
        "-        logger.debug(\"LangchainSerivice created\")\n",
        "         LangchainService.initialize_bedrock_client()\n",
        " \n",
        "-    @staticmethod\n",
        "-    def initialize_bedrock_client():\n",
        "-        \"\"\"Create and return an Amazon Bedrock client\"\"\"\n",
        "-        try:\n",
        "-            boto3.setup_default_session(\n",
        "-                region_name=settings.AWS_REGION,\n",
        "-                aws_access_key_id=settings.AWS_ACCESS_KEY_ID,\n",
        "-                aws_secret_access_key=settings.AWS_SECRET_ACCESS_KEY,\n",
        "-            )\n",
        "-        except Exception as e:\n",
        "-            logging.error(f\"Error initializing Bedrock client: {e!s}\")\n",
        "-            raise\n",
        "-\n",
        "-    @staticmethod\n",
        "-    def call_model(state: State):\n",
        "-        \"\"\"\n",
        "-        Call the model with the given state.\n",
        "-\n",
        "-        Args:\n",
        "-            state: The state to pass to the model.\n",
        "-\n",
        "-        Returns:\n",
        "-            The response from the model.\n",
        "+    async def conversation(\n",
        "+        self, conversation_id: str, user_input: str, user_context: str | None = None\n",
        "+    ):\n",
        " \n",
        "-        \"\"\"\n",
        "+        if not self.__class__._initialized or self.__class__.graph is None:\n",
        "+            raise Exception(\"Graph not initialized. Call initialize_graph() first.\")\n",
        " \n",
        "-        # Get the components from the state\n",
        "-        messages = state[\"messages\"]\n",
        "-        user_context = state.get(\"user_context\", None)\n",
        "-        prompt_template = generate_health_anxiety_prompt(user_context)\n",
        "-        # Invoke the prompt template with the state to get a formatted prompt\n",
        "-        formatted_prompt = prompt_template.invoke({\"messages\": messages})\n",
        "+        # Check if checkpointer exists and log its type for debugging\n",
        "+        if self.__class__.checkpointer:\n",
        "+            logger.debug(f\"Using checkpointer: {type(self.__class__.checkpointer)}\")\n",
        "+        else:\n",
        "+            logger.error(\"Checkpointer is None during conversation call.\")\n",
        "+            raise Exception(\"Checkpointer not available.\")\n",
        " \n",
        "-        # Now pass the formatted prompt to the model\n",
        "-        response = LangchainService.model.invoke(formatted_prompt)\n",
        "-        logger.info(f\"Model response: {response}\")\n",
        "-        return {\"messages\": [response]}\n",
        "+        config = {\"configurable\": {\"thread_id\": conversation_id}}\n",
        "+        input_messages = [HumanMessage(content=user_input)]\n",
        "+        response = await self.__class__.graph.ainvoke(\n",
        "+            {\"messages\": input_messages, \"user_context\": user_context}, config=config\n",
        "+        )\n",
        "+        return response[\"messages\"][-1]\n",
        " \n",
        "     @classmethod\n",
        "     async def initialize_langchain_components(\n",
        "         cls, model_id: str | None = None, model_provider: str | None = None\n",
        "     ):\n",
        "-        \"\"\"\n",
        "-        Set up all the necessary componnents  for the Langchain service.\n",
        " \n",
        "-        Args:\n",
        "-            model_id: The model ID to use. If None, uses the default from settings.\n",
        "-            model_provider: The provider of the model. If None, uses the default from settings.\n",
        "-        \"\"\"\n",
        "         if cls._initialized:\n",
        "             logger.info(\"Graph already initialized.\")\n",
        "             return cls.graph\n",
        "@@ -104,43 +73,40 @@ async def initialize_langchain_components(\n",
        "         except Exception as e:\n",
        "             logger.error(f\"Error initializing Langchain components: {e!s}\")\n",
        "             raise\n",
        "+        logger.info(\"Langchain components initialized successfully.\")\n",
        " \n",
        "     @classmethod\n",
        "-    def _initialize_model(\n",
        "-        cls, model_id: str | None = None, model_provider: str | None = None\n",
        "-    ):\n",
        "+    def _initialize_graph(cls):\n",
        "         \"\"\"\n",
        "-        Initialize the model with the given ID and provider.\n",
        "-\n",
        "-        Args:\n",
        "-            model_id: The ID of the model to use.\n",
        "-            model_provider: The provider of the model.\n",
        "-\n",
        "-        Returns:\n",
        "-            The initialized model.\n",
        "+        model and checkpointer need to be initialized before this method is called.\n",
        "         \"\"\"\n",
        "-        # Initialize the model\n",
        "-        cls._model_id = model_id or settings.MODEL_ID\n",
        "-        cls._model_provider = model_provider or settings.MODEL_PROVIDER\n",
        "-        try:\n",
        "-            # Initialize the Bedrock client one extra time for safety\n",
        "-            # This is a bit redundant but ensures the client is set up before model initialization\n",
        "-            cls.initialize_bedrock_client()\n",
        "-            cls.model = init_chat_model(\n",
        "-                model=cls._model_id, model_provider=cls._model_provider\n",
        "-            )\n",
        "-            logger.info(\n",
        "-                f\"Model initialized: {cls._model_id} with provider: {cls._model_provider}\"\n",
        "-            )\n",
        "-        except Exception as e:\n",
        "-            logger.error(f\"Error initializing model: {e!s}\")\n",
        "-            raise\n",
        "+        if cls.graph is None:\n",
        "+            logger.info(\"Initializing graph...\")\n",
        "+            workflow = StateGraph(state_schema=State)\n",
        "+            workflow.add_edge(START, \"model\")\n",
        "+            workflow.add_node(\"model\", cls.call_model)\n",
        "+            workflow.add_edge(\"model\", END)\n",
        "+            cls.graph = workflow.compile(checkpointer=cls.checkpointer)\n",
        "+            cls._initialized = True\n",
        "+            logger.info(\"Graph initialized.\")\n",
        "+        else:\n",
        "+            logger.info(\"Graph already initialized.\")\n",
        "+\n",
        "+    @classmethod\n",
        "+    async def _initialize_checkpointer(cls):\n",
        "+        if cls.checkpointer is None and cls.db_pool is not None:\n",
        "+            logger.info(\"Initializing checkpointer...\")\n",
        "+            try:\n",
        "+                cls.checkpointer = AsyncPostgresSaver(cls.db_pool)\n",
        "+                await cls.checkpointer.setup()\n",
        "+                logger.info(\"Checkpointer initialized.\")\n",
        "+            except Exception as e:\n",
        "+                logger.error(f\"Error initializing checkpointer: {e!s}\")\n",
        "+                cls.checkpointer = None\n",
        "+                raise\n",
        " \n",
        "     @classmethod\n",
        "     async def _initialize_pool(cls):\n",
        "-        \"\"\"\n",
        "-        Initialize the database pool\n",
        "-        \"\"\"\n",
        "         if cls.db_pool is None:\n",
        "             logger.info(\"Initializing database pool...\")\n",
        "             try:\n",
        "@@ -169,27 +135,8 @@ async def _initialize_pool(cls):\n",
        "                 cls.db_pool = None\n",
        "                 raise\n",
        " \n",
        "-    @classmethod\n",
        "-    async def _initialize_checkpointer(cls):\n",
        "-        \"\"\"\n",
        "-        Initialize the checkpointer for the graph.\n",
        "-        \"\"\"\n",
        "-        if cls.checkpointer is None and cls.db_pool is not None:\n",
        "-            logger.info(\"Initializing checkpointer...\")\n",
        "-            try:\n",
        "-                cls.checkpointer = AsyncPostgresSaver(cls.db_pool)\n",
        "-                await cls.checkpointer.setup()\n",
        "-                logger.info(\"Checkpointer initialized.\")\n",
        "-            except Exception as e:\n",
        "-                logger.error(f\"Error initializing checkpointer: {e!s}\")\n",
        "-                cls.checkpointer = None\n",
        "-                raise\n",
        "-\n",
        "     @classmethod\n",
        "     async def close_pool(cls):\n",
        "-        \"\"\"\n",
        "-        Close the pool and any resources it holds.\n",
        "-        \"\"\"\n",
        "         if cls.db_pool:\n",
        "             try:\n",
        "                 await cls.db_pool.close()\n",
        "@@ -200,49 +147,52 @@ async def close_pool(cls):\n",
        "         else:\n",
        "             logger.info(\"No database pool to close.\")\n",
        " \n",
        "-    @classmethod\n",
        "-    def _initialize_graph(cls):\n",
        "-        \"\"\"\n",
        "-        Initialize the graph with the model and checkpointer.\n",
        "-        \"\"\"\n",
        "-        if cls.graph is None:\n",
        "-            logger.info(\"Initializing graph...\")\n",
        "-            workflow = StateGraph(state_schema=State)\n",
        "-            workflow.add_edge(START, \"model\")\n",
        "-            workflow.add_node(\"model\", cls.call_model)\n",
        "-            workflow.add_edge(\"model\", END)\n",
        "-            cls.graph = workflow.compile(checkpointer=cls.checkpointer)\n",
        "-            cls._initialized = True\n",
        "-            logger.info(\"Graph initialized.\")\n",
        "-        else:\n",
        "-            logger.info(\"Graph already initialized.\")\n",
        "+    @staticmethod\n",
        "+    def call_model(state: State):\n",
        " \n",
        "-    async def conversation(\n",
        "-        self, conversation_id: str, user_input: str, user_context: str | None = None\n",
        "+        # Get the components from the state\n",
        "+        messages = state[\"messages\"]\n",
        "+        user_context = state.get(\"user_context\", None)\n",
        "+        prompt_template = generate_health_anxiety_prompt(user_context)\n",
        "+\n",
        "+        # Invoke the prompt template with the state to get a formatted prompt\n",
        "+        formatted_prompt = prompt_template.invoke({\"messages\": messages})\n",
        "+\n",
        "+        # Now pass the formatted prompt to the model\n",
        "+        response = LangchainService.model.invoke(formatted_prompt)\n",
        "+        logger.info(f\"Model response: {response}\")\n",
        "+        return {\"messages\": [response]}\n",
        "+\n",
        "+    @classmethod\n",
        "+    def _initialize_model(\n",
        "+        cls, model_id: str | None = None, model_provider: str | None = None\n",
        "     ):\n",
        "-        \"\"\"\n",
        "-        Create a conversation with the chat model or continue an existing one.\n",
        "-        Args:\n",
        "-            conversation_id: The ID of the conversation.\n",
        "-            user_input: The user's query.\n",
        "-            user_context: Additional context provided by the user.\n",
        "-\n",
        "-        Returns:\n",
        "-            The response from the AI model\n",
        "-        \"\"\"\n",
        "-        if not self.__class__._initialized or self.__class__.graph is None:\n",
        "-            raise Exception(\"Graph not initialized. Call initialize_graph() first.\")\n",
        " \n",
        "-        # Check if checkpointer exists and maybe log its type for debugging\n",
        "-        if self.__class__.checkpointer:\n",
        "-            logger.debug(f\"Using checkpointer: {type(self.__class__.checkpointer)}\")\n",
        "-        else:\n",
        "-            logger.error(\"Checkpointer is None during conversation call.\")\n",
        "-            raise Exception(\"Checkpointer not available.\")\n",
        "+        cls._model_id = model_id or settings.MODEL_ID\n",
        "+        cls._model_provider = model_provider or settings.MODEL_PROVIDER\n",
        "+        try:\n",
        "+            # Initialize the Bedrock client one extra time for safety\n",
        "+            # This is a bit redundant but ensures the client is set up before model initialization\n",
        "+            cls.initialize_bedrock_client()\n",
        "+            cls.model = init_chat_model(\n",
        "+                model=cls._model_id, model_provider=cls._model_provider\n",
        "+            )\n",
        "+            logger.info(\n",
        "+                f\"Model initialized: {cls._model_id} with provider: {cls._model_provider}\"\n",
        "+            )\n",
        "+        except Exception as e:\n",
        "+            logger.error(f\"Error initializing model: {e!s}\")\n",
        "+            raise\n",
        " \n",
        "-        config = {\"configurable\": {\"thread_id\": conversation_id}}\n",
        "-        input_messages = [HumanMessage(content=user_input)]\n",
        "-        response = await self.__class__.graph.ainvoke(\n",
        "-            {\"messages\": input_messages, \"user_context\": user_context}, config=config\n",
        "-        )\n",
        "-        return response[\"messages\"][-1]\n",
        "+    @staticmethod\n",
        "+    def initialize_bedrock_client():\n",
        "+        \"\"\"Create and return an Amazon Bedrock client\"\"\"\n",
        "+        try:\n",
        "+            boto3.setup_default_session(\n",
        "+                region_name=settings.AWS_REGION,\n",
        "+                aws_access_key_id=settings.AWS_ACCESS_KEY_ID,\n",
        "+                aws_secret_access_key=settings.AWS_SECRET_ACCESS_KEY,\n",
        "+            )\n",
        "+        except Exception as e:\n",
        "+            logging.error(f\"Error initializing Bedrock client: {e!s}\")\n",
        "+            raise\n"
      ]
    }
  ]
}
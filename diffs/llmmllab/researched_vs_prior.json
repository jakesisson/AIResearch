{
  "project": "Research Data/llmmllab",
  "repo": "LongStoryMedia/llmmllab",
  "prior_commit": "17cf68d744762046d273005c753725853549772f",
  "researched_commit": "70e006ef3c47e51254a9f85f00ae806278e5da0d",
  "compare_url": "https://github.com/LongStoryMedia/llmmllab/compare/17cf68d744762046d273005c753725853549772f...70e006ef3c47e51254a9f85f00ae806278e5da0d",
  "ahead_by": 1,
  "behind_by": 0,
  "changed_files": [
    {
      "path": "inference/.sync_state/debug_out.manifest.last",
      "status": "modified",
      "additions": 3,
      "deletions": 0,
      "patch": "@@ -21,6 +21,7 @@\n ./composer_llm_output_qwen3_30b_a3b_q4_k_m_20251021_163804.txt\n ./composer_llm_output_qwen3_30b_a3b_q4_k_m_20251021_172832.txt\n ./composer_llm_output_qwen3_30b_a3b_q4_k_m_20251021_174620.txt\n+./composer_llm_output_qwen3_30b_a3b_q4_k_m_20251021_175249.txt\n ./composer_test_20251021_024209.json\n ./composer_test_20251021_024651.json\n ./composer_test_20251021_024904.json\n@@ -41,6 +42,7 @@\n ./composer_test_20251021_164040.json\n ./composer_test_20251021_173022.json\n ./composer_test_20251021_174720.json\n+./composer_test_20251021_175506.json\n ./workflow_graph_20251021_024131.md\n ./workflow_graph_20251021_024614.md\n ./workflow_graph_20251021_024827.md\n@@ -64,3 +66,4 @@\n ./workflow_graph_20251021_163810.md\n ./workflow_graph_20251021_172838.md\n ./workflow_graph_20251021_174626.md\n+./workflow_graph_20251021_175256.md",
      "patch_lines": [
        "@@ -21,6 +21,7 @@\n",
        " ./composer_llm_output_qwen3_30b_a3b_q4_k_m_20251021_163804.txt\n",
        " ./composer_llm_output_qwen3_30b_a3b_q4_k_m_20251021_172832.txt\n",
        " ./composer_llm_output_qwen3_30b_a3b_q4_k_m_20251021_174620.txt\n",
        "+./composer_llm_output_qwen3_30b_a3b_q4_k_m_20251021_175249.txt\n",
        " ./composer_test_20251021_024209.json\n",
        " ./composer_test_20251021_024651.json\n",
        " ./composer_test_20251021_024904.json\n",
        "@@ -41,6 +42,7 @@\n",
        " ./composer_test_20251021_164040.json\n",
        " ./composer_test_20251021_173022.json\n",
        " ./composer_test_20251021_174720.json\n",
        "+./composer_test_20251021_175506.json\n",
        " ./workflow_graph_20251021_024131.md\n",
        " ./workflow_graph_20251021_024614.md\n",
        " ./workflow_graph_20251021_024827.md\n",
        "@@ -64,3 +66,4 @@\n",
        " ./workflow_graph_20251021_163810.md\n",
        " ./workflow_graph_20251021_172838.md\n",
        " ./workflow_graph_20251021_174626.md\n",
        "+./workflow_graph_20251021_175256.md\n"
      ]
    },
    {
      "path": "inference/composer/graph/subgraphs/tools_agent.py",
      "status": "modified",
      "additions": 289,
      "deletions": 397,
      "patch": "@@ -1,489 +1,381 @@\n \"\"\"\n-Tools Agent Subgraph for efficient tool execution with minimal state.\n+Tools Agent Subgraph - Complete agent workflow with chat_agent + tool_node cycling.\n \n-This subgraph provides a lightweight environment for executing tools with a simplified\n-state structure to minimize context window usage and prevent schema bloat. It uses\n-the new ToolRuntime pattern instead of InjectedState to access necessary context.\n+This subgraph implements the proper LangGraph agent pattern with both chat_agent and \n+tool_node, allowing the full agent workflow to execute internally with minimal state.\n+The agent cycles between LLM calls and tool execution until completion, then returns\n+results to the main workflow via middleware-controlled ingress/egress.\n \n Key Benefits:\n-1. Minimal state - only essential data for tool execution\n-2. State isolation - tool execution doesn't affect main workflow state directly  \n-3. Clean tool schemas - no massive state injection in tool definitions\n-4. Easy integration - returns results to main workflow via Command pattern\n+1. Complete agent workflow - chat_agent <-> tool_node cycling within subgraph\n+2. Minimal state - ToolsState with only essential fields to minimize context usage\n+3. Proper tool integration - uses ToolNode with ToolRuntime pattern\n+4. Middleware control - clean ingress/egress boundaries with main workflow\n+5. State isolation - agent operations don't bloat main workflow state\n \n Architecture:\n-- ToolsState: Minimal state with only required fields\n-- Tool execution node: Handles tool calls with ToolRuntime access\n-- Result aggregation: Collects tool outputs for return to main workflow\n-- State transformation: Converts between main WorkflowState and ToolsState\n+- ToolsState: Minimal state optimized for agent operations\n+- chat_agent: LLM node that can make tool calls using available tools\n+- tool_node: ToolNode that executes tools with ToolRuntime[ToolsState] access\n+- Conditional routing: should_continue logic for agent cycling\n+- Middleware boundaries: controlled data flow to/from main workflow\n \"\"\"\n \n-import asyncio\n-import inspect\n-from typing import Dict, List, Any, Optional, Sequence\n-from typing_extensions import TypedDict\n-from dataclasses import dataclass\n+from typing import Dict, List, Any, Optional, Literal\n+from typing_extensions import TypedDict, Annotated\n \n from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n from langchain_core.tools import BaseTool\n+from langchain_core.language_models import BaseChatModel\n from langgraph.graph import StateGraph, START, END\n+from langgraph.prebuilt import ToolNode, create_react_agent\n from langgraph.types import Command\n-from langgraph.prebuilt import ToolNode\n+from langgraph.graph.message import add_messages\n \n-from models import MessageRole, MessageContent, MessageContentType, Tool\n+from models import MessageRole, MessageContent, MessageContentType, NodeMetadata, PipelinePriority\n from composer.graph.state import WorkflowState\n+from composer.agents.chat_agent import ChatAgent\n+from composer.tools.registry import ToolRegistry\n+from runner import PipelineFactory\n from utils.logging import llmmllogger\n \n logger = llmmllogger.bind(component=\"ToolsAgentSubgraph\")\n \n \n class ToolsState(TypedDict):\n     \"\"\"\n-    Minimal state for tool execution subgraph.\n+    Minimal state for agent subgraph with chat_agent + tool_node workflow.\n     \n-    Contains only the essential data needed for tool execution to minimize\n-    context window usage and prevent schema bloat.\n+    Contains only essential data for the agent to operate efficiently while\n+    minimizing context window usage. The agent cycles between chat_agent and\n+    tool_node until completion, then returns results via Command.\n     \"\"\"\n-    # Essential tool execution data\n-    messages: List[BaseMessage]  # Only recent messages needed for tool context\n-    user_id: str  # User identifier for tool operations\n-    conversation_id: int  # Conversation context\n+    # Message thread for agent conversation (using LangChain core messages for proper serialization)\n+    messages: Annotated[List[BaseMessage], add_messages]\n     \n-    # User configuration subset (only tool-relevant settings)\n-    web_search_config: Optional[Dict[str, Any]]  # Web search preferences\n-    memory_config: Optional[Dict[str, Any]]  # Memory retrieval settings\n-    \n-    # Tool execution results\n-    tool_results: List[Dict[str, Any]]  # Collected tool outputs\n-    \n-\n-@dataclass\n-class ToolExecutionContext:\n-    \"\"\"Context data for tool execution that doesn't need to be in state.\"\"\"\n+    # Essential context for tool operations\n     user_id: str\n     conversation_id: int\n-    web_search_config: Dict[str, Any]\n-    memory_config: Dict[str, Any]\n+    \n+    # User configuration (serialized for minimal overhead)\n+    user_config: Optional[Dict[str, Any]]\n+    system_config: Optional[Dict[str, Any]]\n+    \n+    # Current operation tracking\n+    current_date: str\n+    tool_call_count: int\n \n \n class ToolsAgentSubgraph:\n     \"\"\"\n-    Subgraph for efficient tool execution with minimal state overhead.\n+    Complete agent subgraph with chat_agent + tool_node cycling workflow.\n     \n-    This subgraph handles tool execution in an isolated environment with a\n-    simplified state structure to prevent context window bloat while maintaining\n-    full tool functionality via ToolRuntime access patterns.\n+    Uses proper dependency injection pattern like the main graph builder,\n+    importing ChatAgent and ToolExecutorNode with their required dependencies.\n     \"\"\"\n     \n-    def __init__(self):\n+    def __init__(\n+        self,\n+        pipeline_factory: PipelineFactory,\n+        tool_registry: ToolRegistry,\n+    ):\n+        \"\"\"Initialize subgraph with dependency injection.\"\"\"\n+        self.pipeline_factory = pipeline_factory\n+        self.tool_registry = tool_registry\n         self.graph = None\n+        \n+        # Create node metadata for the subgraph agents\n+        self.subgraph_metadata = NodeMetadata(\n+            node_name=\"tools_agent_subgraph\",\n+            node_id=\"tools_agent_subgraph\",\n+            node_type=\"subgraph\",\n+            user_id=\"system\",  # Will be updated at runtime\n+            conversation_id=0   # Will be updated at runtime\n+        )\n+        \n         self._build_graph()\n     \n-    def _build_graph(self) -> None:\n-        \"\"\"Build the tools execution subgraph.\"\"\"\n-        builder = StateGraph(ToolsState)\n+    def _create_chat_agent(self, user_id: str, conversation_id: int) -> ChatAgent:\n+        \"\"\"Create ChatAgent instance with proper dependency injection.\"\"\"\n+        from models.default_model_profiles import DEFAULT_PRIMARY_PROFILE\n         \n-        # Add the tool execution node\n-        builder.add_node(\"execute_tools\", self._execute_tools_node)\n-        \n-        # Simple linear flow\n-        builder.add_edge(START, \"execute_tools\")\n-        builder.add_edge(\"execute_tools\", END)\n+        # Update metadata with runtime context\n+        runtime_metadata = NodeMetadata(\n+            node_name=\"subgraph_chat_agent\",\n+            node_id=\"subgraph_chat_agent\", \n+            node_type=\"agent\",\n+            user_id=user_id,\n+            conversation_id=conversation_id\n+        )\n         \n-        # Compile the subgraph\n-        self.graph = builder.compile()\n-        logger.info(\"Tools agent subgraph compiled successfully\")\n+        return ChatAgent(\n+            pipeline_factory=self.pipeline_factory,\n+            profile=DEFAULT_PRIMARY_PROFILE,\n+            node_metadata=runtime_metadata,\n+            priority=PipelinePriority.MEDIUM\n+        )\n     \n-    async def _execute_tools_node(self, state: ToolsState) -> Dict[str, Any]:\n-        \"\"\"\n-        Execute tools using the existing tool pattern with state injection.\n-        \n-        This node works with the current InjectedState pattern by creating a\n-        compatible execution environment for tools.\n-        \"\"\"\n+    async def _tool_executor_wrapper(self, state: ToolsState) -> Dict[str, Any]:\n+        \"\"\"Tool executor wrapper using LangGraph's ToolNode.\"\"\"\n         try:\n-            # Get the tools that need to be executed from the messages\n-            # Handle both LangChain core messages and LangChainMessage format\n-            tool_messages = []\n-            for msg in state[\"messages\"]:\n-                has_tool_calls = False\n-                if isinstance(msg, AIMessage) and msg.tool_calls:\n-                    has_tool_calls = True\n-                elif hasattr(msg, 'type') and msg.type == 'ai' and hasattr(msg, 'tool_calls') and msg.tool_calls:\n-                    has_tool_calls = True\n-                \n-                if has_tool_calls:\n-                    tool_messages.append(msg)\n+            # Get executable tools from registry\n+            executable_tools = self.tool_registry.get_all_executable_tools()\n+            tools_list = list(executable_tools.values()) if executable_tools else []\n             \n-            if not tool_messages:\n-                logger.warning(\"No tool calls found in messages\")\n-                return {\"tool_results\": []}\n+            if not tools_list:\n+                logger.warning(\"No tools available for execution\")\n+                return state\n             \n-            # Extract tool calls from the latest AI message\n-            latest_ai_message = tool_messages[-1]\n-            tool_calls = latest_ai_message.tool_calls\n+            # Create ToolNode with available tools\n+            tool_node = ToolNode(tools_list)\n             \n-            if not tool_calls:\n-                logger.warning(\"No tool calls in latest AI message\")\n-                return {\"tool_results\": []}\n+            # Execute tools using ToolNode\n+            result = await tool_node.ainvoke(state)\n             \n-            logger.info(f\"Executing {len(tool_calls)} tool calls\", tool_names=[call[\"name\"] for call in tool_calls])\n+            return result\n             \n-            # Get available tools from the registry\n-            from composer.tools.registry import ToolRegistry\n-            from runner.pipeline_factory import pipeline_factory\n+        except Exception as e:\n+            logger.error(f\"Tool executor wrapper failed: {e}\")\n+            # Return current state on error\n+            return state\n+    \n+    def _build_graph(self) -> None:\n+        \"\"\"Build the complete agent subgraph using proper dependency injection.\"\"\"\n+        try:\n+            # Build graph with StateGraph pattern like main builder\n+            builder = StateGraph(ToolsState)\n             \n-            registry = ToolRegistry(pipeline_factory)\n-            executable_tools = registry.get_all_executable_tools()\n+            # Add chat agent node - will be created at runtime with proper context\n+            builder.add_node(\"chat_agent\", self._chat_agent_wrapper)\n             \n-            if not executable_tools:\n-                logger.error(\"No executable tools available\")\n-                return {\"tool_results\": []}\n+            # Add tool executor node - using wrapper for ToolNode\n+            builder.add_node(\"tool_executor\", self._tool_executor_wrapper)\n             \n-            # Execute each tool call\n-            tool_results = []\n-            new_messages = list(state[\"messages\"])  # Copy existing messages\n+            # Add conditional routing between chat agent and tool executor\n+            builder.add_conditional_edges(\n+                \"chat_agent\",\n+                self._should_continue,\n+                {\n+                    \"continue\": \"tool_executor\", \n+                    \"end\": END\n+                }\n+            )\n             \n-            for call in tool_calls:\n-                tool_name = call.get(\"name\")\n-                args = call.get(\"args\") or call.get(\"arguments\") or {}\n-                tool_call_id = call.get(\"id\", f\"call_{tool_name}\")\n-                \n-                if tool_name not in executable_tools:\n-                    logger.error(f\"Tool '{tool_name}' not found in available tools\")\n-                    tool_results.append({\n-                        \"tool_call_id\": tool_call_id,\n-                        \"name\": tool_name,\n-                        \"content\": f\"Error: Tool '{tool_name}' not available\",\n-                        \"status\": \"failed\"\n-                    })\n-                    continue\n-                \n-                tool = executable_tools[tool_name]\n-                \n-                try:\n-                    # Check tool function signature to determine execution method\n-                    import inspect\n-                    \n-                    # Get the actual function to inspect its signature\n-                    # Check both 'func' and 'coroutine' attributes for LangGraph tools\n-                    tool_func = getattr(tool, 'func', None) or getattr(tool, 'coroutine', None)\n-                    if tool_func and (inspect.isfunction(tool_func) or inspect.iscoroutinefunction(tool_func)):\n-                        sig = inspect.signature(tool_func)\n-                        param_names = list(sig.parameters.keys())\n-                        \n-                        # Check if this tool expects injected parameters\n-                        if 'tool_call_id' in param_names and 'state' in param_names:\n-                            # This is a LangGraph tool with injection - call directly with injected params\n-                            minimal_state = self._create_minimal_workflow_state(state)\n-                            \n-                            # Call the function directly with injected parameters\n-                            if inspect.iscoroutinefunction(tool_func):\n-                                # Async function - call directly\n-                                result = await tool_func(\n-                                    tool_call_id=tool_call_id,\n-                                    state=minimal_state,\n-                                    **args\n-                                )\n-                            else:\n-                                # Sync function - use thread\n-                                result = await asyncio.create_task(\n-                                    asyncio.to_thread(\n-                                        tool_func,\n-                                        tool_call_id=tool_call_id,\n-                                        state=minimal_state,\n-                                        **args\n-                                    )\n-                                )\n-                            \n-                            # Handle Command returns (like web_search)\n-                            if hasattr(result, 'update') and result.update:\n-                                # Apply command updates to our minimal state\n-                                for key, value in result.update.items():\n-                                    if hasattr(minimal_state, key):\n-                                        # Special handling for messages - convert LangChain messages to LangChainMessage schema\n-                                        if key == 'messages' and isinstance(value, list):\n-                                            from composer.utils.langchain_compat import _coerce_to_langchain_message_dict\n-                                            converted_messages = []\n-                                            for msg in value:\n-                                                converted_messages.append(_coerce_to_langchain_message_dict(msg))\n-                                            setattr(minimal_state, key, converted_messages)\n-                                        else:\n-                                            setattr(minimal_state, key, value)\n-                                        \n-                                # Extract the actual result content\n-                                result_content = \"Tool completed and results added to state\"\n-                            else:\n-                                result_content = str(result)\n-                        else:\n-                            # Regular LangChain tool - use the standard execution pattern\n-                            from langchain_core.runnables import RunnableConfig\n-                            \n-                            tool_config = RunnableConfig()\n-                            \n-                            if hasattr(tool, \"_arun\"):\n-                                result_content = await tool._arun(config=tool_config, **args)\n-                            else:\n-                                # Fallback to sync _run\n-                                run_fn = getattr(tool, \"_run\", None) or getattr(tool, \"run\", None)\n-                                if run_fn is None:\n-                                    raise RuntimeError(f\"Tool '{tool_name}' has no runnable method\")\n-                                result_content = run_fn(**args)\n-                    else:\n-                        # Fallback: try different execution methods\n-                        if hasattr(tool, \"_arun\"):\n-                            # Try with injection parameters first\n-                            try:\n-                                minimal_state = self._create_minimal_workflow_state(state)\n-                                result_content = await tool._arun(\n-                                    tool_call_id=tool_call_id,\n-                                    state=minimal_state,\n-                                    **args\n-                                )\n-                            except TypeError:\n-                                # If that fails, try without injection\n-                                from langchain_core.runnables import RunnableConfig\n-                                tool_config = RunnableConfig()\n-                                result_content = await tool._arun(config=tool_config, **args)\n-                        else:\n-                            run_fn = getattr(tool, \"_run\", None) or getattr(tool, \"run\", None)\n-                            if run_fn is None:\n-                                raise RuntimeError(f\"Tool '{tool_name}' has no runnable method\")\n-                            result_content = run_fn(**args)\n-                    \n-                    # Create tool message\n-                    tool_message = ToolMessage(\n-                        content=str(result_content),\n-                        tool_call_id=tool_call_id,\n-                        name=tool_name\n-                    )\n-                    new_messages.append(tool_message)\n-                    \n-                    tool_results.append({\n-                        \"tool_call_id\": tool_call_id,\n-                        \"name\": tool_name,\n-                        \"content\": str(result_content),\n-                        \"status\": \"success\"\n-                    })\n-                    \n-                    logger.info(f\"Successfully executed tool '{tool_name}'\")\n-                    \n-                except Exception as e:\n-                    logger.error(f\"Tool '{tool_name}' execution failed: {e}\", exc_info=True)\n-                    \n-                    error_message = ToolMessage(\n-                        content=f\"Error executing {tool_name}: {str(e)}\",\n-                        tool_call_id=tool_call_id,\n-                        name=tool_name\n+            # Tool executor always goes back to chat agent for potential follow-up\n+            builder.add_edge(\"tool_executor\", \"chat_agent\")\n+            \n+            # Start with chat agent\n+            builder.add_edge(START, \"chat_agent\")\n+            \n+            # Compile the graph\n+            self.graph = builder.compile()\n+            logger.info(\"Agent subgraph built with proper dependency injection\")\n+            \n+        except Exception as e:\n+            logger.error(f\"Failed to build agent subgraph: {e}\")\n+            raise\n+    \n+    async def _chat_agent_wrapper(self, state: ToolsState) -> Dict[str, Any]:\n+        \"\"\"Wrapper that creates ChatAgent at runtime and executes it.\"\"\"\n+        try:\n+            # Extract user context from state\n+            user_id = state.get(\"user_id\", \"system\")\n+            conversation_id = state.get(\"conversation_id\", 0)\n+            \n+            # Create ChatAgent with runtime context\n+            chat_agent = self._create_chat_agent(user_id, conversation_id)\n+            \n+            # Convert BaseMessage list to LangChainMessage format for ChatAgent\n+            from models import LangChainMessage\n+            \n+            messages = state[\"messages\"]\n+            langchain_messages = []\n+            \n+            for msg in messages:\n+                if isinstance(msg, (HumanMessage, AIMessage, ToolMessage)):\n+                    # Convert to LangChainMessage format\n+                    langchain_msg = LangChainMessage(\n+                        content=msg.content,\n+                        type=msg.type,\n+                        additional_kwargs=getattr(msg, 'additional_kwargs', {}),\n+                        response_metadata=getattr(msg, 'response_metadata', {})\n                     )\n-                    new_messages.append(error_message)\n-                    \n-                    tool_results.append({\n-                        \"tool_call_id\": tool_call_id,\n-                        \"name\": tool_name,\n-                        \"content\": f\"Error: {str(e)}\",\n-                        \"status\": \"failed\"\n-                    })\n+                    langchain_messages.append(langchain_msg)\n+                else:\n+                    # Already in correct format or compatible\n+                    langchain_messages.append(msg)\n             \n-            logger.info(f\"Successfully executed {len(tool_results)} tools\")\n-            return {\"tool_results\": tool_results, \"messages\": new_messages}\n+            # Get tools from registry for the agent\n+            executable_tools = self.tool_registry.get_all_executable_tools()\n+            tools_list = list(executable_tools.values()) if executable_tools else None\n+            \n+            # Execute chat completion with tools\n+            response_msg = await chat_agent.chat_completion_with_conversion(\n+                messages=langchain_messages,\n+                tools=tools_list\n+            )\n+            \n+            # Return new message in state update format\n+            return {\"messages\": [response_msg]}\n             \n         except Exception as e:\n-            logger.error(f\"Tool execution failed: {e}\", exc_info=True)\n-            return {\n-                \"tool_results\": [{\n-                    \"error\": str(e),\n-                    \"status\": \"failed\"\n-                }]\n-            }\n+            logger.error(f\"Chat agent wrapper failed: {e}\")\n+            # Return error message\n+            error_msg = AIMessage(content=f\"Agent error: {str(e)}\")\n+            return {\"messages\": [error_msg]}\n     \n-    def _create_minimal_workflow_state(self, tools_state: ToolsState) -> 'WorkflowState':\n-        \"\"\"\n-        Create a minimal WorkflowState for tool injection from ToolsState.\n-        \n-        This allows tools that expect InjectedState to work with our subgraph.\n-        \"\"\"\n-        # Import WorkflowState here to avoid circular imports\n-        from composer.graph.state import WorkflowState\n-        from models import LangChainMessage, UserConfig\n-        from models.default_configs import (\n-            DEFAULT_SUMMARIZATION_CONFIG,\n-            DEFAULT_MEMORY_CONFIG, \n-            DEFAULT_WEB_SEARCH_CONFIG,\n-            DEFAULT_PREFERENCES_CONFIG,\n-            DEFAULT_MODEL_PROFILE_CONFIG,\n-            DEFAULT_REFINEMENT_CONFIG,\n-            DEFAULT_IMAGE_GENERATION_CONFIG,\n-            DEFAULT_CIRCUIT_BREAKER_CONFIG,\n-            DEFAULT_GPU_CONFIG,\n-            DEFAULT_WORKFLOW_CONFIG,\n-            DEFAULT_TOOL_CONFIG,\n-            DEFAULT_CONTEXT_WINDOW_CONFIG\n-        )\n-        \n-        # Create a minimal state object with required fields\n-        minimal_state = WorkflowState()\n-        \n-        # Convert LangChain core messages to LangChainMessage format\n-        converted_messages = []\n-        for msg in tools_state[\"messages\"]:\n-            if isinstance(msg, (HumanMessage, AIMessage, ToolMessage)):\n-                # Convert to LangChainMessage format\n-                lang_chain_msg = LangChainMessage(\n-                    content=msg.content,\n-                    type=msg.type,\n-                    additional_kwargs=getattr(msg, 'additional_kwargs', {}),\n-                    response_metadata=getattr(msg, 'response_metadata', {})\n-                )\n-                converted_messages.append(lang_chain_msg)\n-            else:\n-                # Already in correct format\n-                converted_messages.append(msg)\n-        \n-        # Set essential fields from tools_state\n-        minimal_state.messages = converted_messages\n-        minimal_state.user_id = tools_state[\"user_id\"]\n-        minimal_state.conversation_id = tools_state[\"conversation_id\"]\n-        \n-        # Create a minimal but valid UserConfig using defaults and subsets\n-        web_search_config = tools_state.get(\"web_search_config\", {})\n-        memory_config = tools_state.get(\"memory_config\", {})\n-        \n-        # Merge with defaults\n-        merged_web_search = {**DEFAULT_WEB_SEARCH_CONFIG.model_dump(), **web_search_config}\n-        merged_memory = {**DEFAULT_MEMORY_CONFIG.model_dump(), **memory_config}\n-        \n-        minimal_user_config = UserConfig(\n-            user_id=tools_state[\"user_id\"],\n-            summarization=DEFAULT_SUMMARIZATION_CONFIG,\n-            memory=type(DEFAULT_MEMORY_CONFIG)(**merged_memory),\n-            web_search=type(DEFAULT_WEB_SEARCH_CONFIG)(**merged_web_search),\n-            preferences=DEFAULT_PREFERENCES_CONFIG,\n-            model_profiles=DEFAULT_MODEL_PROFILE_CONFIG,\n-            refinement=DEFAULT_REFINEMENT_CONFIG,\n-            image_generation=DEFAULT_IMAGE_GENERATION_CONFIG,\n-            circuit_breaker=DEFAULT_CIRCUIT_BREAKER_CONFIG,\n-            gpu_config=DEFAULT_GPU_CONFIG,\n-            workflow=DEFAULT_WORKFLOW_CONFIG,\n-            tool=DEFAULT_TOOL_CONFIG,\n-            context_window=DEFAULT_CONTEXT_WINDOW_CONFIG\n-        )\n-        \n-        minimal_state.user_config = minimal_user_config\n+    def _should_continue(self, state: ToolsState) -> Literal[\"continue\", \"end\"]:\n+        \"\"\"Determine if agent should continue to tools or end.\"\"\"\n+        messages = state[\"messages\"]\n+        if not messages:\n+            return \"end\"\n+            \n+        last_message = messages[-1]\n         \n-        # Set other required fields to defaults\n-        minimal_state.current_date = \"\"\n-        minimal_state.things_to_remember = []\n-        minimal_state.web_search_results = []\n-        minimal_state.tool_calls = []\n+        # Check if last message has tool calls\n+        if isinstance(last_message, AIMessage) and last_message.tool_calls:\n+            return \"continue\"\n         \n-        return minimal_state\n+        return \"end\"\n+    \n+\n+    \n+\n+    \n+\n     \n     def transform_to_tools_state(self, main_state: WorkflowState) -> ToolsState:\n-        \"\"\"\n-        Transform main WorkflowState to minimal ToolsState.\n-        \n-        Extracts only the essential data needed for tool execution to minimize\n-        context window usage.\n-        \"\"\"\n-        # Get only recent messages (last 10 to keep context minimal)\n+        \"\"\"Transform main WorkflowState to minimal ToolsState for agent subgraph.\"\"\"\n+        # Get recent messages for agent context and convert to LangChain core messages\n         recent_messages = getattr(main_state, \"messages\", [])[-10:]\n+        langchain_messages = []\n         \n-        # Extract user config subsets\n-        user_config = getattr(main_state, \"user_config\", None)\n-        web_search_config = {}\n-        memory_config = {}\n+        for msg in recent_messages:\n+            if hasattr(msg, 'type') and hasattr(msg, 'content'):\n+                # Convert custom LangChainMessage to proper LangChain core message\n+                if msg.type == \"human\":\n+                    langchain_messages.append(HumanMessage(content=msg.content))\n+                elif msg.type == \"ai\":\n+                    # Check if this AI message has tool calls\n+                    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n+                        langchain_messages.append(AIMessage(\n+                            content=msg.content,\n+                            tool_calls=msg.tool_calls\n+                        ))\n+                    else:\n+                        langchain_messages.append(AIMessage(content=msg.content))\n+                elif msg.type == \"tool\":\n+                    langchain_messages.append(ToolMessage(\n+                        content=msg.content,\n+                        tool_call_id=getattr(msg, 'id', None) or \"unknown\"\n+                    ))\n+                else:\n+                    # Default to human message for unknown types\n+                    langchain_messages.append(HumanMessage(content=str(msg.content)))\n+            else:\n+                # Already a proper LangChain message, use as-is\n+                langchain_messages.append(msg)\n         \n-        if user_config:\n-            web_search_config = getattr(user_config, \"web_search\", {})\n-            memory_config = getattr(user_config, \"memory\", {})\n+        # Serialize configs to dict format for minimal state\n+        user_config_dict = None\n+        if hasattr(main_state, \"user_config\") and main_state.user_config:\n+            try:\n+                user_config_dict = main_state.user_config.model_dump()\n+            except:\n+                user_config_dict = None\n         \n-        return ToolsState(\n-            messages=recent_messages,\n-            user_id=getattr(main_state, \"user_id\", \"\"),\n-            conversation_id=getattr(main_state, \"conversation_id\", 0),\n-            web_search_config=web_search_config,\n-            memory_config=memory_config,\n-            tool_results=[]\n-        )\n+        return {\n+            \"messages\": langchain_messages,\n+            \"user_id\": getattr(main_state, \"user_id\", \"\"),\n+            \"conversation_id\": getattr(main_state, \"conversation_id\", 0),\n+            \"user_config\": user_config_dict,\n+            \"system_config\": None,  # Not available in WorkflowState\n+            \"current_date\": getattr(main_state, \"current_date\", \"\"),\n+            \"tool_call_count\": 0\n+        }\n     \n-    def transform_to_main_state(self, tools_state: ToolsState, main_state: WorkflowState) -> Dict[str, Any]:\n-        \"\"\"\n-        Transform ToolsState results back to main WorkflowState updates.\n-        \n-        Returns a state update dictionary that can be applied to the main workflow.\n-        \"\"\"\n+    def transform_to_main_state(self, agent_result: Dict[str, Any], main_state: WorkflowState) -> Dict[str, Any]:\n+        \"\"\"Transform agent subgraph results back to main WorkflowState updates.\"\"\"\n         from models import LangChainMessage\n         \n         updates = {}\n         \n-        # Add tool messages to main state\n-        if tools_state.get(\"messages\"):\n-            # Only add new tool messages\n+        # Add new messages from agent execution\n+        if agent_result.get(\"messages\"):\n             main_messages = getattr(main_state, \"messages\", [])\n-            tools_messages = tools_state[\"messages\"]\n+            agent_messages = agent_result[\"messages\"]\n             \n-            # Find new messages (tool responses) and convert to LangChainMessage format\n+            # Find messages that weren't in the original main state\n+            original_count = len(main_messages)\n             new_messages = []\n-            for msg in tools_messages:\n-                if isinstance(msg, ToolMessage) and msg not in main_messages:\n-                    # Convert ToolMessage to LangChainMessage format\n-                    lang_chain_msg = LangChainMessage(\n-                        content=msg.content,\n-                        type=\"tool\",\n-                        name=getattr(msg, 'name', None),\n-                        id=getattr(msg, 'tool_call_id', None),\n-                        additional_kwargs=getattr(msg, 'additional_kwargs', {}),\n-                        response_metadata=getattr(msg, 'response_metadata', {})\n-                    )\n-                    new_messages.append(lang_chain_msg)\n-                elif not isinstance(msg, ToolMessage) and msg not in main_messages:\n-                    # For non-tool messages, add as-is if they're already LangChainMessage\n-                    new_messages.append(msg)\n+            \n+            for i, msg in enumerate(agent_messages):\n+                if i >= original_count:  # This is a new message from agent\n+                    if isinstance(msg, (AIMessage, ToolMessage)):\n+                        # Convert to LangChainMessage format for main state\n+                        lang_chain_msg = LangChainMessage(\n+                            content=msg.content,\n+                            type=msg.type,\n+                            name=getattr(msg, 'name', None),\n+                            id=getattr(msg, 'id', None) or getattr(msg, 'tool_call_id', None),\n+                            additional_kwargs=getattr(msg, 'additional_kwargs', {}),\n+                            response_metadata=getattr(msg, 'response_metadata', {})\n+                        )\n+                        new_messages.append(lang_chain_msg)\n             \n             if new_messages:\n                 updates[\"messages\"] = main_messages + new_messages\n         \n-        # Add tool results to things_to_remember if they have useful content\n-        tool_results = tools_state.get(\"tool_results\", [])\n-        if tool_results:\n-            things_to_remember = getattr(main_state, \"things_to_remember\", [])\n-            for result in tool_results:\n-                if result.get(\"status\") == \"success\" and result.get(\"content\"):\n-                    things_to_remember.append({\n-                        \"type\": \"tool_result\",\n-                        \"tool_name\": result.get(\"name\", \"unknown\"),\n-                        \"content\": result[\"content\"],\n-                        \"timestamp\": getattr(main_state, \"current_date\", \"\")\n-                    })\n-            updates[\"things_to_remember\"] = things_to_remember\n-        \n         return updates\n     \n     async def execute(self, main_state: WorkflowState) -> Command:\n-        \"\"\"\n-        Execute the tools subgraph and return a Command with state updates.\n-        \n-        This is the main entry point for using the subgraph from the main workflow.\n-        \"\"\"\n+        \"\"\"Execute the agent subgraph and return Command with state updates.\"\"\"\n         try:\n-            # Transform to minimal tools state\n+            if not self.graph:\n+                logger.error(\"Agent subgraph not initialized\")\n+                return Command(update={})\n+            \n+            # Transform to agent state\n             tools_state = self.transform_to_tools_state(main_state)\n             \n-            # Execute the subgraph\n+            # Execute the agent subgraph\n             result = await self.graph.ainvoke(tools_state)\n             \n             # Transform results back to main state updates\n             updates = self.transform_to_main_state(result, main_state)\n             \n-            logger.info(f\"Tools subgraph completed with {len(updates)} state updates\")\n+            logger.info(f\"Agent subgraph completed with {len(updates)} state updates\")\n             return Command(update=updates)\n             \n         except Exception as e:\n-            logger.error(f\"Tools subgraph execution failed: {e}\", exc_info=True)\n-            # Return empty update on failure\n+            logger.error(f\"Agent subgraph execution failed: {e}\", exc_info=True)\n             return Command(update={})\n \n \n-# Global instance for use in main workflow\n-tools_agent_subgraph = ToolsAgentSubgraph()\n\\ No newline at end of file\n+class _LazyToolsAgentSubgraph:\n+    \"\"\"Lazy initializer for tools agent subgraph with dependency injection.\"\"\"\n+    \n+    def __init__(self):\n+        self._subgraph = None\n+    \n+    def _ensure_initialized(self):\n+        \"\"\"Initialize the subgraph if not already done.\"\"\"\n+        if self._subgraph is None:\n+            # Import here to avoid circular imports\n+            from runner.pipeline_factory import pipeline_factory\n+            from composer.tools.registry import ToolRegistry\n+            \n+            # Create registry - this should be improved to use proper DI in the future\n+            tool_registry = ToolRegistry(pipeline_factory)\n+            \n+            self._subgraph = ToolsAgentSubgraph(pipeline_factory, tool_registry)\n+        return self._subgraph\n+    \n+    async def execute(self, main_state: WorkflowState):\n+        \"\"\"Execute the subgraph (lazy initialization).\"\"\"\n+        subgraph = self._ensure_initialized()\n+        return await subgraph.execute(main_state)\n+\n+\n+# Global instance for backward compatibility\n+tools_agent_subgraph = _LazyToolsAgentSubgraph()\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -1,489 +1,381 @@\n",
        " \"\"\"\n",
        "-Tools Agent Subgraph for efficient tool execution with minimal state.\n",
        "+Tools Agent Subgraph - Complete agent workflow with chat_agent + tool_node cycling.\n",
        " \n",
        "-This subgraph provides a lightweight environment for executing tools with a simplified\n",
        "-state structure to minimize context window usage and prevent schema bloat. It uses\n",
        "-the new ToolRuntime pattern instead of InjectedState to access necessary context.\n",
        "+This subgraph implements the proper LangGraph agent pattern with both chat_agent and \n",
        "+tool_node, allowing the full agent workflow to execute internally with minimal state.\n",
        "+The agent cycles between LLM calls and tool execution until completion, then returns\n",
        "+results to the main workflow via middleware-controlled ingress/egress.\n",
        " \n",
        " Key Benefits:\n",
        "-1. Minimal state - only essential data for tool execution\n",
        "-2. State isolation - tool execution doesn't affect main workflow state directly  \n",
        "-3. Clean tool schemas - no massive state injection in tool definitions\n",
        "-4. Easy integration - returns results to main workflow via Command pattern\n",
        "+1. Complete agent workflow - chat_agent <-> tool_node cycling within subgraph\n",
        "+2. Minimal state - ToolsState with only essential fields to minimize context usage\n",
        "+3. Proper tool integration - uses ToolNode with ToolRuntime pattern\n",
        "+4. Middleware control - clean ingress/egress boundaries with main workflow\n",
        "+5. State isolation - agent operations don't bloat main workflow state\n",
        " \n",
        " Architecture:\n",
        "-- ToolsState: Minimal state with only required fields\n",
        "-- Tool execution node: Handles tool calls with ToolRuntime access\n",
        "-- Result aggregation: Collects tool outputs for return to main workflow\n",
        "-- State transformation: Converts between main WorkflowState and ToolsState\n",
        "+- ToolsState: Minimal state optimized for agent operations\n",
        "+- chat_agent: LLM node that can make tool calls using available tools\n",
        "+- tool_node: ToolNode that executes tools with ToolRuntime[ToolsState] access\n",
        "+- Conditional routing: should_continue logic for agent cycling\n",
        "+- Middleware boundaries: controlled data flow to/from main workflow\n",
        " \"\"\"\n",
        " \n",
        "-import asyncio\n",
        "-import inspect\n",
        "-from typing import Dict, List, Any, Optional, Sequence\n",
        "-from typing_extensions import TypedDict\n",
        "-from dataclasses import dataclass\n",
        "+from typing import Dict, List, Any, Optional, Literal\n",
        "+from typing_extensions import TypedDict, Annotated\n",
        " \n",
        " from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
        " from langchain_core.tools import BaseTool\n",
        "+from langchain_core.language_models import BaseChatModel\n",
        " from langgraph.graph import StateGraph, START, END\n",
        "+from langgraph.prebuilt import ToolNode, create_react_agent\n",
        " from langgraph.types import Command\n",
        "-from langgraph.prebuilt import ToolNode\n",
        "+from langgraph.graph.message import add_messages\n",
        " \n",
        "-from models import MessageRole, MessageContent, MessageContentType, Tool\n",
        "+from models import MessageRole, MessageContent, MessageContentType, NodeMetadata, PipelinePriority\n",
        " from composer.graph.state import WorkflowState\n",
        "+from composer.agents.chat_agent import ChatAgent\n",
        "+from composer.tools.registry import ToolRegistry\n",
        "+from runner import PipelineFactory\n",
        " from utils.logging import llmmllogger\n",
        " \n",
        " logger = llmmllogger.bind(component=\"ToolsAgentSubgraph\")\n",
        " \n",
        " \n",
        " class ToolsState(TypedDict):\n",
        "     \"\"\"\n",
        "-    Minimal state for tool execution subgraph.\n",
        "+    Minimal state for agent subgraph with chat_agent + tool_node workflow.\n",
        "     \n",
        "-    Contains only the essential data needed for tool execution to minimize\n",
        "-    context window usage and prevent schema bloat.\n",
        "+    Contains only essential data for the agent to operate efficiently while\n",
        "+    minimizing context window usage. The agent cycles between chat_agent and\n",
        "+    tool_node until completion, then returns results via Command.\n",
        "     \"\"\"\n",
        "-    # Essential tool execution data\n",
        "-    messages: List[BaseMessage]  # Only recent messages needed for tool context\n",
        "-    user_id: str  # User identifier for tool operations\n",
        "-    conversation_id: int  # Conversation context\n",
        "+    # Message thread for agent conversation (using LangChain core messages for proper serialization)\n",
        "+    messages: Annotated[List[BaseMessage], add_messages]\n",
        "     \n",
        "-    # User configuration subset (only tool-relevant settings)\n",
        "-    web_search_config: Optional[Dict[str, Any]]  # Web search preferences\n",
        "-    memory_config: Optional[Dict[str, Any]]  # Memory retrieval settings\n",
        "-    \n",
        "-    # Tool execution results\n",
        "-    tool_results: List[Dict[str, Any]]  # Collected tool outputs\n",
        "-    \n",
        "-\n",
        "-@dataclass\n",
        "-class ToolExecutionContext:\n",
        "-    \"\"\"Context data for tool execution that doesn't need to be in state.\"\"\"\n",
        "+    # Essential context for tool operations\n",
        "     user_id: str\n",
        "     conversation_id: int\n",
        "-    web_search_config: Dict[str, Any]\n",
        "-    memory_config: Dict[str, Any]\n",
        "+    \n",
        "+    # User configuration (serialized for minimal overhead)\n",
        "+    user_config: Optional[Dict[str, Any]]\n",
        "+    system_config: Optional[Dict[str, Any]]\n",
        "+    \n",
        "+    # Current operation tracking\n",
        "+    current_date: str\n",
        "+    tool_call_count: int\n",
        " \n",
        " \n",
        " class ToolsAgentSubgraph:\n",
        "     \"\"\"\n",
        "-    Subgraph for efficient tool execution with minimal state overhead.\n",
        "+    Complete agent subgraph with chat_agent + tool_node cycling workflow.\n",
        "     \n",
        "-    This subgraph handles tool execution in an isolated environment with a\n",
        "-    simplified state structure to prevent context window bloat while maintaining\n",
        "-    full tool functionality via ToolRuntime access patterns.\n",
        "+    Uses proper dependency injection pattern like the main graph builder,\n",
        "+    importing ChatAgent and ToolExecutorNode with their required dependencies.\n",
        "     \"\"\"\n",
        "     \n",
        "-    def __init__(self):\n",
        "+    def __init__(\n",
        "+        self,\n",
        "+        pipeline_factory: PipelineFactory,\n",
        "+        tool_registry: ToolRegistry,\n",
        "+    ):\n",
        "+        \"\"\"Initialize subgraph with dependency injection.\"\"\"\n",
        "+        self.pipeline_factory = pipeline_factory\n",
        "+        self.tool_registry = tool_registry\n",
        "         self.graph = None\n",
        "+        \n",
        "+        # Create node metadata for the subgraph agents\n",
        "+        self.subgraph_metadata = NodeMetadata(\n",
        "+            node_name=\"tools_agent_subgraph\",\n",
        "+            node_id=\"tools_agent_subgraph\",\n",
        "+            node_type=\"subgraph\",\n",
        "+            user_id=\"system\",  # Will be updated at runtime\n",
        "+            conversation_id=0   # Will be updated at runtime\n",
        "+        )\n",
        "+        \n",
        "         self._build_graph()\n",
        "     \n",
        "-    def _build_graph(self) -> None:\n",
        "-        \"\"\"Build the tools execution subgraph.\"\"\"\n",
        "-        builder = StateGraph(ToolsState)\n",
        "+    def _create_chat_agent(self, user_id: str, conversation_id: int) -> ChatAgent:\n",
        "+        \"\"\"Create ChatAgent instance with proper dependency injection.\"\"\"\n",
        "+        from models.default_model_profiles import DEFAULT_PRIMARY_PROFILE\n",
        "         \n",
        "-        # Add the tool execution node\n",
        "-        builder.add_node(\"execute_tools\", self._execute_tools_node)\n",
        "-        \n",
        "-        # Simple linear flow\n",
        "-        builder.add_edge(START, \"execute_tools\")\n",
        "-        builder.add_edge(\"execute_tools\", END)\n",
        "+        # Update metadata with runtime context\n",
        "+        runtime_metadata = NodeMetadata(\n",
        "+            node_name=\"subgraph_chat_agent\",\n",
        "+            node_id=\"subgraph_chat_agent\", \n",
        "+            node_type=\"agent\",\n",
        "+            user_id=user_id,\n",
        "+            conversation_id=conversation_id\n",
        "+        )\n",
        "         \n",
        "-        # Compile the subgraph\n",
        "-        self.graph = builder.compile()\n",
        "-        logger.info(\"Tools agent subgraph compiled successfully\")\n",
        "+        return ChatAgent(\n",
        "+            pipeline_factory=self.pipeline_factory,\n",
        "+            profile=DEFAULT_PRIMARY_PROFILE,\n",
        "+            node_metadata=runtime_metadata,\n",
        "+            priority=PipelinePriority.MEDIUM\n",
        "+        )\n",
        "     \n",
        "-    async def _execute_tools_node(self, state: ToolsState) -> Dict[str, Any]:\n",
        "-        \"\"\"\n",
        "-        Execute tools using the existing tool pattern with state injection.\n",
        "-        \n",
        "-        This node works with the current InjectedState pattern by creating a\n",
        "-        compatible execution environment for tools.\n",
        "-        \"\"\"\n",
        "+    async def _tool_executor_wrapper(self, state: ToolsState) -> Dict[str, Any]:\n",
        "+        \"\"\"Tool executor wrapper using LangGraph's ToolNode.\"\"\"\n",
        "         try:\n",
        "-            # Get the tools that need to be executed from the messages\n",
        "-            # Handle both LangChain core messages and LangChainMessage format\n",
        "-            tool_messages = []\n",
        "-            for msg in state[\"messages\"]:\n",
        "-                has_tool_calls = False\n",
        "-                if isinstance(msg, AIMessage) and msg.tool_calls:\n",
        "-                    has_tool_calls = True\n",
        "-                elif hasattr(msg, 'type') and msg.type == 'ai' and hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
        "-                    has_tool_calls = True\n",
        "-                \n",
        "-                if has_tool_calls:\n",
        "-                    tool_messages.append(msg)\n",
        "+            # Get executable tools from registry\n",
        "+            executable_tools = self.tool_registry.get_all_executable_tools()\n",
        "+            tools_list = list(executable_tools.values()) if executable_tools else []\n",
        "             \n",
        "-            if not tool_messages:\n",
        "-                logger.warning(\"No tool calls found in messages\")\n",
        "-                return {\"tool_results\": []}\n",
        "+            if not tools_list:\n",
        "+                logger.warning(\"No tools available for execution\")\n",
        "+                return state\n",
        "             \n",
        "-            # Extract tool calls from the latest AI message\n",
        "-            latest_ai_message = tool_messages[-1]\n",
        "-            tool_calls = latest_ai_message.tool_calls\n",
        "+            # Create ToolNode with available tools\n",
        "+            tool_node = ToolNode(tools_list)\n",
        "             \n",
        "-            if not tool_calls:\n",
        "-                logger.warning(\"No tool calls in latest AI message\")\n",
        "-                return {\"tool_results\": []}\n",
        "+            # Execute tools using ToolNode\n",
        "+            result = await tool_node.ainvoke(state)\n",
        "             \n",
        "-            logger.info(f\"Executing {len(tool_calls)} tool calls\", tool_names=[call[\"name\"] for call in tool_calls])\n",
        "+            return result\n",
        "             \n",
        "-            # Get available tools from the registry\n",
        "-            from composer.tools.registry import ToolRegistry\n",
        "-            from runner.pipeline_factory import pipeline_factory\n",
        "+        except Exception as e:\n",
        "+            logger.error(f\"Tool executor wrapper failed: {e}\")\n",
        "+            # Return current state on error\n",
        "+            return state\n",
        "+    \n",
        "+    def _build_graph(self) -> None:\n",
        "+        \"\"\"Build the complete agent subgraph using proper dependency injection.\"\"\"\n",
        "+        try:\n",
        "+            # Build graph with StateGraph pattern like main builder\n",
        "+            builder = StateGraph(ToolsState)\n",
        "             \n",
        "-            registry = ToolRegistry(pipeline_factory)\n",
        "-            executable_tools = registry.get_all_executable_tools()\n",
        "+            # Add chat agent node - will be created at runtime with proper context\n",
        "+            builder.add_node(\"chat_agent\", self._chat_agent_wrapper)\n",
        "             \n",
        "-            if not executable_tools:\n",
        "-                logger.error(\"No executable tools available\")\n",
        "-                return {\"tool_results\": []}\n",
        "+            # Add tool executor node - using wrapper for ToolNode\n",
        "+            builder.add_node(\"tool_executor\", self._tool_executor_wrapper)\n",
        "             \n",
        "-            # Execute each tool call\n",
        "-            tool_results = []\n",
        "-            new_messages = list(state[\"messages\"])  # Copy existing messages\n",
        "+            # Add conditional routing between chat agent and tool executor\n",
        "+            builder.add_conditional_edges(\n",
        "+                \"chat_agent\",\n",
        "+                self._should_continue,\n",
        "+                {\n",
        "+                    \"continue\": \"tool_executor\", \n",
        "+                    \"end\": END\n",
        "+                }\n",
        "+            )\n",
        "             \n",
        "-            for call in tool_calls:\n",
        "-                tool_name = call.get(\"name\")\n",
        "-                args = call.get(\"args\") or call.get(\"arguments\") or {}\n",
        "-                tool_call_id = call.get(\"id\", f\"call_{tool_name}\")\n",
        "-                \n",
        "-                if tool_name not in executable_tools:\n",
        "-                    logger.error(f\"Tool '{tool_name}' not found in available tools\")\n",
        "-                    tool_results.append({\n",
        "-                        \"tool_call_id\": tool_call_id,\n",
        "-                        \"name\": tool_name,\n",
        "-                        \"content\": f\"Error: Tool '{tool_name}' not available\",\n",
        "-                        \"status\": \"failed\"\n",
        "-                    })\n",
        "-                    continue\n",
        "-                \n",
        "-                tool = executable_tools[tool_name]\n",
        "-                \n",
        "-                try:\n",
        "-                    # Check tool function signature to determine execution method\n",
        "-                    import inspect\n",
        "-                    \n",
        "-                    # Get the actual function to inspect its signature\n",
        "-                    # Check both 'func' and 'coroutine' attributes for LangGraph tools\n",
        "-                    tool_func = getattr(tool, 'func', None) or getattr(tool, 'coroutine', None)\n",
        "-                    if tool_func and (inspect.isfunction(tool_func) or inspect.iscoroutinefunction(tool_func)):\n",
        "-                        sig = inspect.signature(tool_func)\n",
        "-                        param_names = list(sig.parameters.keys())\n",
        "-                        \n",
        "-                        # Check if this tool expects injected parameters\n",
        "-                        if 'tool_call_id' in param_names and 'state' in param_names:\n",
        "-                            # This is a LangGraph tool with injection - call directly with injected params\n",
        "-                            minimal_state = self._create_minimal_workflow_state(state)\n",
        "-                            \n",
        "-                            # Call the function directly with injected parameters\n",
        "-                            if inspect.iscoroutinefunction(tool_func):\n",
        "-                                # Async function - call directly\n",
        "-                                result = await tool_func(\n",
        "-                                    tool_call_id=tool_call_id,\n",
        "-                                    state=minimal_state,\n",
        "-                                    **args\n",
        "-                                )\n",
        "-                            else:\n",
        "-                                # Sync function - use thread\n",
        "-                                result = await asyncio.create_task(\n",
        "-                                    asyncio.to_thread(\n",
        "-                                        tool_func,\n",
        "-                                        tool_call_id=tool_call_id,\n",
        "-                                        state=minimal_state,\n",
        "-                                        **args\n",
        "-                                    )\n",
        "-                                )\n",
        "-                            \n",
        "-                            # Handle Command returns (like web_search)\n",
        "-                            if hasattr(result, 'update') and result.update:\n",
        "-                                # Apply command updates to our minimal state\n",
        "-                                for key, value in result.update.items():\n",
        "-                                    if hasattr(minimal_state, key):\n",
        "-                                        # Special handling for messages - convert LangChain messages to LangChainMessage schema\n",
        "-                                        if key == 'messages' and isinstance(value, list):\n",
        "-                                            from composer.utils.langchain_compat import _coerce_to_langchain_message_dict\n",
        "-                                            converted_messages = []\n",
        "-                                            for msg in value:\n",
        "-                                                converted_messages.append(_coerce_to_langchain_message_dict(msg))\n",
        "-                                            setattr(minimal_state, key, converted_messages)\n",
        "-                                        else:\n",
        "-                                            setattr(minimal_state, key, value)\n",
        "-                                        \n",
        "-                                # Extract the actual result content\n",
        "-                                result_content = \"Tool completed and results added to state\"\n",
        "-                            else:\n",
        "-                                result_content = str(result)\n",
        "-                        else:\n",
        "-                            # Regular LangChain tool - use the standard execution pattern\n",
        "-                            from langchain_core.runnables import RunnableConfig\n",
        "-                            \n",
        "-                            tool_config = RunnableConfig()\n",
        "-                            \n",
        "-                            if hasattr(tool, \"_arun\"):\n",
        "-                                result_content = await tool._arun(config=tool_config, **args)\n",
        "-                            else:\n",
        "-                                # Fallback to sync _run\n",
        "-                                run_fn = getattr(tool, \"_run\", None) or getattr(tool, \"run\", None)\n",
        "-                                if run_fn is None:\n",
        "-                                    raise RuntimeError(f\"Tool '{tool_name}' has no runnable method\")\n",
        "-                                result_content = run_fn(**args)\n",
        "-                    else:\n",
        "-                        # Fallback: try different execution methods\n",
        "-                        if hasattr(tool, \"_arun\"):\n",
        "-                            # Try with injection parameters first\n",
        "-                            try:\n",
        "-                                minimal_state = self._create_minimal_workflow_state(state)\n",
        "-                                result_content = await tool._arun(\n",
        "-                                    tool_call_id=tool_call_id,\n",
        "-                                    state=minimal_state,\n",
        "-                                    **args\n",
        "-                                )\n",
        "-                            except TypeError:\n",
        "-                                # If that fails, try without injection\n",
        "-                                from langchain_core.runnables import RunnableConfig\n",
        "-                                tool_config = RunnableConfig()\n",
        "-                                result_content = await tool._arun(config=tool_config, **args)\n",
        "-                        else:\n",
        "-                            run_fn = getattr(tool, \"_run\", None) or getattr(tool, \"run\", None)\n",
        "-                            if run_fn is None:\n",
        "-                                raise RuntimeError(f\"Tool '{tool_name}' has no runnable method\")\n",
        "-                            result_content = run_fn(**args)\n",
        "-                    \n",
        "-                    # Create tool message\n",
        "-                    tool_message = ToolMessage(\n",
        "-                        content=str(result_content),\n",
        "-                        tool_call_id=tool_call_id,\n",
        "-                        name=tool_name\n",
        "-                    )\n",
        "-                    new_messages.append(tool_message)\n",
        "-                    \n",
        "-                    tool_results.append({\n",
        "-                        \"tool_call_id\": tool_call_id,\n",
        "-                        \"name\": tool_name,\n",
        "-                        \"content\": str(result_content),\n",
        "-                        \"status\": \"success\"\n",
        "-                    })\n",
        "-                    \n",
        "-                    logger.info(f\"Successfully executed tool '{tool_name}'\")\n",
        "-                    \n",
        "-                except Exception as e:\n",
        "-                    logger.error(f\"Tool '{tool_name}' execution failed: {e}\", exc_info=True)\n",
        "-                    \n",
        "-                    error_message = ToolMessage(\n",
        "-                        content=f\"Error executing {tool_name}: {str(e)}\",\n",
        "-                        tool_call_id=tool_call_id,\n",
        "-                        name=tool_name\n",
        "+            # Tool executor always goes back to chat agent for potential follow-up\n",
        "+            builder.add_edge(\"tool_executor\", \"chat_agent\")\n",
        "+            \n",
        "+            # Start with chat agent\n",
        "+            builder.add_edge(START, \"chat_agent\")\n",
        "+            \n",
        "+            # Compile the graph\n",
        "+            self.graph = builder.compile()\n",
        "+            logger.info(\"Agent subgraph built with proper dependency injection\")\n",
        "+            \n",
        "+        except Exception as e:\n",
        "+            logger.error(f\"Failed to build agent subgraph: {e}\")\n",
        "+            raise\n",
        "+    \n",
        "+    async def _chat_agent_wrapper(self, state: ToolsState) -> Dict[str, Any]:\n",
        "+        \"\"\"Wrapper that creates ChatAgent at runtime and executes it.\"\"\"\n",
        "+        try:\n",
        "+            # Extract user context from state\n",
        "+            user_id = state.get(\"user_id\", \"system\")\n",
        "+            conversation_id = state.get(\"conversation_id\", 0)\n",
        "+            \n",
        "+            # Create ChatAgent with runtime context\n",
        "+            chat_agent = self._create_chat_agent(user_id, conversation_id)\n",
        "+            \n",
        "+            # Convert BaseMessage list to LangChainMessage format for ChatAgent\n",
        "+            from models import LangChainMessage\n",
        "+            \n",
        "+            messages = state[\"messages\"]\n",
        "+            langchain_messages = []\n",
        "+            \n",
        "+            for msg in messages:\n",
        "+                if isinstance(msg, (HumanMessage, AIMessage, ToolMessage)):\n",
        "+                    # Convert to LangChainMessage format\n",
        "+                    langchain_msg = LangChainMessage(\n",
        "+                        content=msg.content,\n",
        "+                        type=msg.type,\n",
        "+                        additional_kwargs=getattr(msg, 'additional_kwargs', {}),\n",
        "+                        response_metadata=getattr(msg, 'response_metadata', {})\n",
        "                     )\n",
        "-                    new_messages.append(error_message)\n",
        "-                    \n",
        "-                    tool_results.append({\n",
        "-                        \"tool_call_id\": tool_call_id,\n",
        "-                        \"name\": tool_name,\n",
        "-                        \"content\": f\"Error: {str(e)}\",\n",
        "-                        \"status\": \"failed\"\n",
        "-                    })\n",
        "+                    langchain_messages.append(langchain_msg)\n",
        "+                else:\n",
        "+                    # Already in correct format or compatible\n",
        "+                    langchain_messages.append(msg)\n",
        "             \n",
        "-            logger.info(f\"Successfully executed {len(tool_results)} tools\")\n",
        "-            return {\"tool_results\": tool_results, \"messages\": new_messages}\n",
        "+            # Get tools from registry for the agent\n",
        "+            executable_tools = self.tool_registry.get_all_executable_tools()\n",
        "+            tools_list = list(executable_tools.values()) if executable_tools else None\n",
        "+            \n",
        "+            # Execute chat completion with tools\n",
        "+            response_msg = await chat_agent.chat_completion_with_conversion(\n",
        "+                messages=langchain_messages,\n",
        "+                tools=tools_list\n",
        "+            )\n",
        "+            \n",
        "+            # Return new message in state update format\n",
        "+            return {\"messages\": [response_msg]}\n",
        "             \n",
        "         except Exception as e:\n",
        "-            logger.error(f\"Tool execution failed: {e}\", exc_info=True)\n",
        "-            return {\n",
        "-                \"tool_results\": [{\n",
        "-                    \"error\": str(e),\n",
        "-                    \"status\": \"failed\"\n",
        "-                }]\n",
        "-            }\n",
        "+            logger.error(f\"Chat agent wrapper failed: {e}\")\n",
        "+            # Return error message\n",
        "+            error_msg = AIMessage(content=f\"Agent error: {str(e)}\")\n",
        "+            return {\"messages\": [error_msg]}\n",
        "     \n",
        "-    def _create_minimal_workflow_state(self, tools_state: ToolsState) -> 'WorkflowState':\n",
        "-        \"\"\"\n",
        "-        Create a minimal WorkflowState for tool injection from ToolsState.\n",
        "-        \n",
        "-        This allows tools that expect InjectedState to work with our subgraph.\n",
        "-        \"\"\"\n",
        "-        # Import WorkflowState here to avoid circular imports\n",
        "-        from composer.graph.state import WorkflowState\n",
        "-        from models import LangChainMessage, UserConfig\n",
        "-        from models.default_configs import (\n",
        "-            DEFAULT_SUMMARIZATION_CONFIG,\n",
        "-            DEFAULT_MEMORY_CONFIG, \n",
        "-            DEFAULT_WEB_SEARCH_CONFIG,\n",
        "-            DEFAULT_PREFERENCES_CONFIG,\n",
        "-            DEFAULT_MODEL_PROFILE_CONFIG,\n",
        "-            DEFAULT_REFINEMENT_CONFIG,\n",
        "-            DEFAULT_IMAGE_GENERATION_CONFIG,\n",
        "-            DEFAULT_CIRCUIT_BREAKER_CONFIG,\n",
        "-            DEFAULT_GPU_CONFIG,\n",
        "-            DEFAULT_WORKFLOW_CONFIG,\n",
        "-            DEFAULT_TOOL_CONFIG,\n",
        "-            DEFAULT_CONTEXT_WINDOW_CONFIG\n",
        "-        )\n",
        "-        \n",
        "-        # Create a minimal state object with required fields\n",
        "-        minimal_state = WorkflowState()\n",
        "-        \n",
        "-        # Convert LangChain core messages to LangChainMessage format\n",
        "-        converted_messages = []\n",
        "-        for msg in tools_state[\"messages\"]:\n",
        "-            if isinstance(msg, (HumanMessage, AIMessage, ToolMessage)):\n",
        "-                # Convert to LangChainMessage format\n",
        "-                lang_chain_msg = LangChainMessage(\n",
        "-                    content=msg.content,\n",
        "-                    type=msg.type,\n",
        "-                    additional_kwargs=getattr(msg, 'additional_kwargs', {}),\n",
        "-                    response_metadata=getattr(msg, 'response_metadata', {})\n",
        "-                )\n",
        "-                converted_messages.append(lang_chain_msg)\n",
        "-            else:\n",
        "-                # Already in correct format\n",
        "-                converted_messages.append(msg)\n",
        "-        \n",
        "-        # Set essential fields from tools_state\n",
        "-        minimal_state.messages = converted_messages\n",
        "-        minimal_state.user_id = tools_state[\"user_id\"]\n",
        "-        minimal_state.conversation_id = tools_state[\"conversation_id\"]\n",
        "-        \n",
        "-        # Create a minimal but valid UserConfig using defaults and subsets\n",
        "-        web_search_config = tools_state.get(\"web_search_config\", {})\n",
        "-        memory_config = tools_state.get(\"memory_config\", {})\n",
        "-        \n",
        "-        # Merge with defaults\n",
        "-        merged_web_search = {**DEFAULT_WEB_SEARCH_CONFIG.model_dump(), **web_search_config}\n",
        "-        merged_memory = {**DEFAULT_MEMORY_CONFIG.model_dump(), **memory_config}\n",
        "-        \n",
        "-        minimal_user_config = UserConfig(\n",
        "-            user_id=tools_state[\"user_id\"],\n",
        "-            summarization=DEFAULT_SUMMARIZATION_CONFIG,\n",
        "-            memory=type(DEFAULT_MEMORY_CONFIG)(**merged_memory),\n",
        "-            web_search=type(DEFAULT_WEB_SEARCH_CONFIG)(**merged_web_search),\n",
        "-            preferences=DEFAULT_PREFERENCES_CONFIG,\n",
        "-            model_profiles=DEFAULT_MODEL_PROFILE_CONFIG,\n",
        "-            refinement=DEFAULT_REFINEMENT_CONFIG,\n",
        "-            image_generation=DEFAULT_IMAGE_GENERATION_CONFIG,\n",
        "-            circuit_breaker=DEFAULT_CIRCUIT_BREAKER_CONFIG,\n",
        "-            gpu_config=DEFAULT_GPU_CONFIG,\n",
        "-            workflow=DEFAULT_WORKFLOW_CONFIG,\n",
        "-            tool=DEFAULT_TOOL_CONFIG,\n",
        "-            context_window=DEFAULT_CONTEXT_WINDOW_CONFIG\n",
        "-        )\n",
        "-        \n",
        "-        minimal_state.user_config = minimal_user_config\n",
        "+    def _should_continue(self, state: ToolsState) -> Literal[\"continue\", \"end\"]:\n",
        "+        \"\"\"Determine if agent should continue to tools or end.\"\"\"\n",
        "+        messages = state[\"messages\"]\n",
        "+        if not messages:\n",
        "+            return \"end\"\n",
        "+            \n",
        "+        last_message = messages[-1]\n",
        "         \n",
        "-        # Set other required fields to defaults\n",
        "-        minimal_state.current_date = \"\"\n",
        "-        minimal_state.things_to_remember = []\n",
        "-        minimal_state.web_search_results = []\n",
        "-        minimal_state.tool_calls = []\n",
        "+        # Check if last message has tool calls\n",
        "+        if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
        "+            return \"continue\"\n",
        "         \n",
        "-        return minimal_state\n",
        "+        return \"end\"\n",
        "+    \n",
        "+\n",
        "+    \n",
        "+\n",
        "+    \n",
        "+\n",
        "     \n",
        "     def transform_to_tools_state(self, main_state: WorkflowState) -> ToolsState:\n",
        "-        \"\"\"\n",
        "-        Transform main WorkflowState to minimal ToolsState.\n",
        "-        \n",
        "-        Extracts only the essential data needed for tool execution to minimize\n",
        "-        context window usage.\n",
        "-        \"\"\"\n",
        "-        # Get only recent messages (last 10 to keep context minimal)\n",
        "+        \"\"\"Transform main WorkflowState to minimal ToolsState for agent subgraph.\"\"\"\n",
        "+        # Get recent messages for agent context and convert to LangChain core messages\n",
        "         recent_messages = getattr(main_state, \"messages\", [])[-10:]\n",
        "+        langchain_messages = []\n",
        "         \n",
        "-        # Extract user config subsets\n",
        "-        user_config = getattr(main_state, \"user_config\", None)\n",
        "-        web_search_config = {}\n",
        "-        memory_config = {}\n",
        "+        for msg in recent_messages:\n",
        "+            if hasattr(msg, 'type') and hasattr(msg, 'content'):\n",
        "+                # Convert custom LangChainMessage to proper LangChain core message\n",
        "+                if msg.type == \"human\":\n",
        "+                    langchain_messages.append(HumanMessage(content=msg.content))\n",
        "+                elif msg.type == \"ai\":\n",
        "+                    # Check if this AI message has tool calls\n",
        "+                    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
        "+                        langchain_messages.append(AIMessage(\n",
        "+                            content=msg.content,\n",
        "+                            tool_calls=msg.tool_calls\n",
        "+                        ))\n",
        "+                    else:\n",
        "+                        langchain_messages.append(AIMessage(content=msg.content))\n",
        "+                elif msg.type == \"tool\":\n",
        "+                    langchain_messages.append(ToolMessage(\n",
        "+                        content=msg.content,\n",
        "+                        tool_call_id=getattr(msg, 'id', None) or \"unknown\"\n",
        "+                    ))\n",
        "+                else:\n",
        "+                    # Default to human message for unknown types\n",
        "+                    langchain_messages.append(HumanMessage(content=str(msg.content)))\n",
        "+            else:\n",
        "+                # Already a proper LangChain message, use as-is\n",
        "+                langchain_messages.append(msg)\n",
        "         \n",
        "-        if user_config:\n",
        "-            web_search_config = getattr(user_config, \"web_search\", {})\n",
        "-            memory_config = getattr(user_config, \"memory\", {})\n",
        "+        # Serialize configs to dict format for minimal state\n",
        "+        user_config_dict = None\n",
        "+        if hasattr(main_state, \"user_config\") and main_state.user_config:\n",
        "+            try:\n",
        "+                user_config_dict = main_state.user_config.model_dump()\n",
        "+            except:\n",
        "+                user_config_dict = None\n",
        "         \n",
        "-        return ToolsState(\n",
        "-            messages=recent_messages,\n",
        "-            user_id=getattr(main_state, \"user_id\", \"\"),\n",
        "-            conversation_id=getattr(main_state, \"conversation_id\", 0),\n",
        "-            web_search_config=web_search_config,\n",
        "-            memory_config=memory_config,\n",
        "-            tool_results=[]\n",
        "-        )\n",
        "+        return {\n",
        "+            \"messages\": langchain_messages,\n",
        "+            \"user_id\": getattr(main_state, \"user_id\", \"\"),\n",
        "+            \"conversation_id\": getattr(main_state, \"conversation_id\", 0),\n",
        "+            \"user_config\": user_config_dict,\n",
        "+            \"system_config\": None,  # Not available in WorkflowState\n",
        "+            \"current_date\": getattr(main_state, \"current_date\", \"\"),\n",
        "+            \"tool_call_count\": 0\n",
        "+        }\n",
        "     \n",
        "-    def transform_to_main_state(self, tools_state: ToolsState, main_state: WorkflowState) -> Dict[str, Any]:\n",
        "-        \"\"\"\n",
        "-        Transform ToolsState results back to main WorkflowState updates.\n",
        "-        \n",
        "-        Returns a state update dictionary that can be applied to the main workflow.\n",
        "-        \"\"\"\n",
        "+    def transform_to_main_state(self, agent_result: Dict[str, Any], main_state: WorkflowState) -> Dict[str, Any]:\n",
        "+        \"\"\"Transform agent subgraph results back to main WorkflowState updates.\"\"\"\n",
        "         from models import LangChainMessage\n",
        "         \n",
        "         updates = {}\n",
        "         \n",
        "-        # Add tool messages to main state\n",
        "-        if tools_state.get(\"messages\"):\n",
        "-            # Only add new tool messages\n",
        "+        # Add new messages from agent execution\n",
        "+        if agent_result.get(\"messages\"):\n",
        "             main_messages = getattr(main_state, \"messages\", [])\n",
        "-            tools_messages = tools_state[\"messages\"]\n",
        "+            agent_messages = agent_result[\"messages\"]\n",
        "             \n",
        "-            # Find new messages (tool responses) and convert to LangChainMessage format\n",
        "+            # Find messages that weren't in the original main state\n",
        "+            original_count = len(main_messages)\n",
        "             new_messages = []\n",
        "-            for msg in tools_messages:\n",
        "-                if isinstance(msg, ToolMessage) and msg not in main_messages:\n",
        "-                    # Convert ToolMessage to LangChainMessage format\n",
        "-                    lang_chain_msg = LangChainMessage(\n",
        "-                        content=msg.content,\n",
        "-                        type=\"tool\",\n",
        "-                        name=getattr(msg, 'name', None),\n",
        "-                        id=getattr(msg, 'tool_call_id', None),\n",
        "-                        additional_kwargs=getattr(msg, 'additional_kwargs', {}),\n",
        "-                        response_metadata=getattr(msg, 'response_metadata', {})\n",
        "-                    )\n",
        "-                    new_messages.append(lang_chain_msg)\n",
        "-                elif not isinstance(msg, ToolMessage) and msg not in main_messages:\n",
        "-                    # For non-tool messages, add as-is if they're already LangChainMessage\n",
        "-                    new_messages.append(msg)\n",
        "+            \n",
        "+            for i, msg in enumerate(agent_messages):\n",
        "+                if i >= original_count:  # This is a new message from agent\n",
        "+                    if isinstance(msg, (AIMessage, ToolMessage)):\n",
        "+                        # Convert to LangChainMessage format for main state\n",
        "+                        lang_chain_msg = LangChainMessage(\n",
        "+                            content=msg.content,\n",
        "+                            type=msg.type,\n",
        "+                            name=getattr(msg, 'name', None),\n",
        "+                            id=getattr(msg, 'id', None) or getattr(msg, 'tool_call_id', None),\n",
        "+                            additional_kwargs=getattr(msg, 'additional_kwargs', {}),\n",
        "+                            response_metadata=getattr(msg, 'response_metadata', {})\n",
        "+                        )\n",
        "+                        new_messages.append(lang_chain_msg)\n",
        "             \n",
        "             if new_messages:\n",
        "                 updates[\"messages\"] = main_messages + new_messages\n",
        "         \n",
        "-        # Add tool results to things_to_remember if they have useful content\n",
        "-        tool_results = tools_state.get(\"tool_results\", [])\n",
        "-        if tool_results:\n",
        "-            things_to_remember = getattr(main_state, \"things_to_remember\", [])\n",
        "-            for result in tool_results:\n",
        "-                if result.get(\"status\") == \"success\" and result.get(\"content\"):\n",
        "-                    things_to_remember.append({\n",
        "-                        \"type\": \"tool_result\",\n",
        "-                        \"tool_name\": result.get(\"name\", \"unknown\"),\n",
        "-                        \"content\": result[\"content\"],\n",
        "-                        \"timestamp\": getattr(main_state, \"current_date\", \"\")\n",
        "-                    })\n",
        "-            updates[\"things_to_remember\"] = things_to_remember\n",
        "-        \n",
        "         return updates\n",
        "     \n",
        "     async def execute(self, main_state: WorkflowState) -> Command:\n",
        "-        \"\"\"\n",
        "-        Execute the tools subgraph and return a Command with state updates.\n",
        "-        \n",
        "-        This is the main entry point for using the subgraph from the main workflow.\n",
        "-        \"\"\"\n",
        "+        \"\"\"Execute the agent subgraph and return Command with state updates.\"\"\"\n",
        "         try:\n",
        "-            # Transform to minimal tools state\n",
        "+            if not self.graph:\n",
        "+                logger.error(\"Agent subgraph not initialized\")\n",
        "+                return Command(update={})\n",
        "+            \n",
        "+            # Transform to agent state\n",
        "             tools_state = self.transform_to_tools_state(main_state)\n",
        "             \n",
        "-            # Execute the subgraph\n",
        "+            # Execute the agent subgraph\n",
        "             result = await self.graph.ainvoke(tools_state)\n",
        "             \n",
        "             # Transform results back to main state updates\n",
        "             updates = self.transform_to_main_state(result, main_state)\n",
        "             \n",
        "-            logger.info(f\"Tools subgraph completed with {len(updates)} state updates\")\n",
        "+            logger.info(f\"Agent subgraph completed with {len(updates)} state updates\")\n",
        "             return Command(update=updates)\n",
        "             \n",
        "         except Exception as e:\n",
        "-            logger.error(f\"Tools subgraph execution failed: {e}\", exc_info=True)\n",
        "-            # Return empty update on failure\n",
        "+            logger.error(f\"Agent subgraph execution failed: {e}\", exc_info=True)\n",
        "             return Command(update={})\n",
        " \n",
        " \n",
        "-# Global instance for use in main workflow\n",
        "-tools_agent_subgraph = ToolsAgentSubgraph()\n",
        "\\ No newline at end of file\n",
        "+class _LazyToolsAgentSubgraph:\n",
        "+    \"\"\"Lazy initializer for tools agent subgraph with dependency injection.\"\"\"\n",
        "+    \n",
        "+    def __init__(self):\n",
        "+        self._subgraph = None\n",
        "+    \n",
        "+    def _ensure_initialized(self):\n",
        "+        \"\"\"Initialize the subgraph if not already done.\"\"\"\n",
        "+        if self._subgraph is None:\n",
        "+            # Import here to avoid circular imports\n",
        "+            from runner.pipeline_factory import pipeline_factory\n",
        "+            from composer.tools.registry import ToolRegistry\n",
        "+            \n",
        "+            # Create registry - this should be improved to use proper DI in the future\n",
        "+            tool_registry = ToolRegistry(pipeline_factory)\n",
        "+            \n",
        "+            self._subgraph = ToolsAgentSubgraph(pipeline_factory, tool_registry)\n",
        "+        return self._subgraph\n",
        "+    \n",
        "+    async def execute(self, main_state: WorkflowState):\n",
        "+        \"\"\"Execute the subgraph (lazy initialization).\"\"\"\n",
        "+        subgraph = self._ensure_initialized()\n",
        "+        return await subgraph.execute(main_state)\n",
        "+\n",
        "+\n",
        "+# Global instance for backward compatibility\n",
        "+tools_agent_subgraph = _LazyToolsAgentSubgraph()\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "inference/composer/requirements.txt",
      "status": "modified",
      "additions": 6,
      "deletions": 6,
      "patch": "@@ -1,10 +1,10 @@\n # LangChain/LangGraph V1.0 alpha\n-langchain>=1.0.0a13\n-langchain-core>=1.0.0a8\n-langgraph>=1.0.0a4\n-langchain-openai>=1.0.0a1\n-langchain-anthropic>=1.0.0a1\n-langchain-classic>=1.0.0a1\n+langchain>=1.0.0\n+langchain-core>=1.0.0\n+langgraph>=1.0.0\n+langchain-openai>=1.0.0\n+langchain-anthropic>=1.0.0\n+langchain-classic>=1.0.0\n langchain-community>=0.3.31\n \n # Database and caching",
      "patch_lines": [
        "@@ -1,10 +1,10 @@\n",
        " # LangChain/LangGraph V1.0 alpha\n",
        "-langchain>=1.0.0a13\n",
        "-langchain-core>=1.0.0a8\n",
        "-langgraph>=1.0.0a4\n",
        "-langchain-openai>=1.0.0a1\n",
        "-langchain-anthropic>=1.0.0a1\n",
        "-langchain-classic>=1.0.0a1\n",
        "+langchain>=1.0.0\n",
        "+langchain-core>=1.0.0\n",
        "+langgraph>=1.0.0\n",
        "+langchain-openai>=1.0.0\n",
        "+langchain-anthropic>=1.0.0\n",
        "+langchain-classic>=1.0.0\n",
        " langchain-community>=0.3.31\n",
        " \n",
        " # Database and caching\n"
      ]
    },
    {
      "path": "inference/composer/tools/static/web_search_tool.py",
      "status": "modified",
      "additions": 10,
      "deletions": 7,
      "patch": "@@ -34,9 +34,9 @@\n import os\n from typing import Annotated\n \n-from langchain_core.tools import tool, InjectedToolCallId\n+from langchain_core.tools import tool\n+from langchain.tools import ToolRuntime\n from langchain_core.messages import ToolMessage\n-from langgraph.prebuilt import InjectedState\n from langgraph.types import Command\n from composer.graph.state import WorkflowState\n from utils.logging import llmmllogger\n@@ -149,12 +149,11 @@ async def search(self, query: str, max_results: int) -> SearchResult:\n             )\n \n \n-# Single web search tool using Command pattern with strong typing\n+# Single web search tool using ToolRuntime pattern for clean state access\n @tool\n async def web_search(\n     query: str,\n-    tool_call_id: Annotated[str, InjectedToolCallId],\n-    state: Annotated[WorkflowState, InjectedState],\n+    tool_runtime: ToolRuntime,\n ) -> Command:\n     \"\"\"\n     Search the web for information and automatically add results to workflow state.\n@@ -176,10 +175,14 @@ async def web_search(\n     logger = llmmllogger.logger.bind(component=\"WebSearch\")\n \n     try:\n-        # Get user_config directly from injected state (much more efficient!)\n+        # Access state and tool_call_id through runtime\n+        state = tool_runtime.state\n+        tool_call_id = tool_runtime.tool_call_id\n+        \n+        # Get user_config directly from runtime state (much more efficient!)\n         if state.user_config and hasattr(state.user_config, \"web_search\"):\n             web_config = state.user_config.web_search\n-            logger.debug(\"Using web search config from injected state\")\n+            logger.debug(\"Using web search config from runtime state\")\n         else:\n             web_config = DEFAULT_WEB_SEARCH_CONFIG\n             logger.debug(\"Using default web search config - no user_config in state\")",
      "patch_lines": [
        "@@ -34,9 +34,9 @@\n",
        " import os\n",
        " from typing import Annotated\n",
        " \n",
        "-from langchain_core.tools import tool, InjectedToolCallId\n",
        "+from langchain_core.tools import tool\n",
        "+from langchain.tools import ToolRuntime\n",
        " from langchain_core.messages import ToolMessage\n",
        "-from langgraph.prebuilt import InjectedState\n",
        " from langgraph.types import Command\n",
        " from composer.graph.state import WorkflowState\n",
        " from utils.logging import llmmllogger\n",
        "@@ -149,12 +149,11 @@ async def search(self, query: str, max_results: int) -> SearchResult:\n",
        "             )\n",
        " \n",
        " \n",
        "-# Single web search tool using Command pattern with strong typing\n",
        "+# Single web search tool using ToolRuntime pattern for clean state access\n",
        " @tool\n",
        " async def web_search(\n",
        "     query: str,\n",
        "-    tool_call_id: Annotated[str, InjectedToolCallId],\n",
        "-    state: Annotated[WorkflowState, InjectedState],\n",
        "+    tool_runtime: ToolRuntime,\n",
        " ) -> Command:\n",
        "     \"\"\"\n",
        "     Search the web for information and automatically add results to workflow state.\n",
        "@@ -176,10 +175,14 @@ async def web_search(\n",
        "     logger = llmmllogger.logger.bind(component=\"WebSearch\")\n",
        " \n",
        "     try:\n",
        "-        # Get user_config directly from injected state (much more efficient!)\n",
        "+        # Access state and tool_call_id through runtime\n",
        "+        state = tool_runtime.state\n",
        "+        tool_call_id = tool_runtime.tool_call_id\n",
        "+        \n",
        "+        # Get user_config directly from runtime state (much more efficient!)\n",
        "         if state.user_config and hasattr(state.user_config, \"web_search\"):\n",
        "             web_config = state.user_config.web_search\n",
        "-            logger.debug(\"Using web search config from injected state\")\n",
        "+            logger.debug(\"Using web search config from runtime state\")\n",
        "         else:\n",
        "             web_config = DEFAULT_WEB_SEARCH_CONFIG\n",
        "             logger.debug(\"Using default web search config - no user_config in state\")\n"
      ]
    },
    {
      "path": "inference/composer/tools/utils/schema_filter.py",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "patch": "@@ -32,7 +32,7 @@ def create_filtered_args_schema(tool_func) -> Type[BaseModel]:\n         param_type = param.annotation\n         \n         # Skip parameters that are clearly injection parameters by name\n-        if param_name in ['tool_call_id', 'state']:\n+        if param_name in ['tool_call_id', 'state', 'tool_runtime']:\n             print(f\"    \ud83d\udeab Skipping injection parameter: {param_name}\")\n             continue\n             ",
      "patch_lines": [
        "@@ -32,7 +32,7 @@ def create_filtered_args_schema(tool_func) -> Type[BaseModel]:\n",
        "         param_type = param.annotation\n",
        "         \n",
        "         # Skip parameters that are clearly injection parameters by name\n",
        "-        if param_name in ['tool_call_id', 'state']:\n",
        "+        if param_name in ['tool_call_id', 'state', 'tool_runtime']:\n",
        "             print(f\"    \ud83d\udeab Skipping injection parameter: {param_name}\")\n",
        "             continue\n",
        "             \n"
      ]
    },
    {
      "path": "inference/debug/out/.manifest",
      "status": "modified",
      "additions": 3,
      "deletions": 0,
      "patch": "@@ -21,6 +21,7 @@\n ./composer_llm_output_qwen3_30b_a3b_q4_k_m_20251021_163804.txt\n ./composer_llm_output_qwen3_30b_a3b_q4_k_m_20251021_172832.txt\n ./composer_llm_output_qwen3_30b_a3b_q4_k_m_20251021_174620.txt\n+./composer_llm_output_qwen3_30b_a3b_q4_k_m_20251021_175249.txt\n ./composer_test_20251021_024209.json\n ./composer_test_20251021_024651.json\n ./composer_test_20251021_024904.json\n@@ -41,6 +42,7 @@\n ./composer_test_20251021_164040.json\n ./composer_test_20251021_173022.json\n ./composer_test_20251021_174720.json\n+./composer_test_20251021_175506.json\n ./workflow_graph_20251021_024131.md\n ./workflow_graph_20251021_024614.md\n ./workflow_graph_20251021_024827.md\n@@ -64,3 +66,4 @@\n ./workflow_graph_20251021_163810.md\n ./workflow_graph_20251021_172838.md\n ./workflow_graph_20251021_174626.md\n+./workflow_graph_20251021_175256.md",
      "patch_lines": [
        "@@ -21,6 +21,7 @@\n",
        " ./composer_llm_output_qwen3_30b_a3b_q4_k_m_20251021_163804.txt\n",
        " ./composer_llm_output_qwen3_30b_a3b_q4_k_m_20251021_172832.txt\n",
        " ./composer_llm_output_qwen3_30b_a3b_q4_k_m_20251021_174620.txt\n",
        "+./composer_llm_output_qwen3_30b_a3b_q4_k_m_20251021_175249.txt\n",
        " ./composer_test_20251021_024209.json\n",
        " ./composer_test_20251021_024651.json\n",
        " ./composer_test_20251021_024904.json\n",
        "@@ -41,6 +42,7 @@\n",
        " ./composer_test_20251021_164040.json\n",
        " ./composer_test_20251021_173022.json\n",
        " ./composer_test_20251021_174720.json\n",
        "+./composer_test_20251021_175506.json\n",
        " ./workflow_graph_20251021_024131.md\n",
        " ./workflow_graph_20251021_024614.md\n",
        " ./workflow_graph_20251021_024827.md\n",
        "@@ -64,3 +66,4 @@\n",
        " ./workflow_graph_20251021_163810.md\n",
        " ./workflow_graph_20251021_172838.md\n",
        " ./workflow_graph_20251021_174626.md\n",
        "+./workflow_graph_20251021_175256.md\n"
      ]
    },
    {
      "path": "inference/debug/test_tools_agent_subgraph.py",
      "status": "added",
      "additions": 69,
      "deletions": 0,
      "patch": "@@ -0,0 +1,69 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Test script for the redesigned tools agent subgraph with proper LangGraph agent pattern.\n+\n+This script tests the complete agent workflow with chat_agent + tool_node cycling.\n+\"\"\"\n+\n+import asyncio\n+from models import LangChainMessage\n+from composer.graph.subgraphs.tools_agent import tools_agent_subgraph\n+from composer.graph.state import WorkflowState\n+from utils.logging import llmmllogger\n+\n+logger = llmmllogger.bind(component=\"ToolsAgentTest\")\n+\n+\n+async def test_tools_agent_subgraph():\n+    \"\"\"Test the tools agent subgraph with a simple query that should trigger web search.\"\"\"\n+    try:\n+        logger.info(\"Starting tools agent subgraph test\")\n+        \n+        # Create a minimal WorkflowState for testing\n+        test_state = WorkflowState()\n+        test_state.user_id = \"test_user\"\n+        test_state.conversation_id = 123\n+        test_state.current_date = \"2024-10-21\"\n+        test_state.messages = [\n+            LangChainMessage(\n+                content=\"What is the current weather in San Francisco? Please search for recent information.\",\n+                type=\"human\"\n+            )\n+        ]\n+        \n+        # Set up minimal user config for web search\n+        from models.default_configs import create_default_user_config\n+        test_state.user_config = create_default_user_config(\"test_user\")\n+        \n+        logger.info(\"Created test state with web search query\")\n+        \n+        # Execute the subgraph\n+        result_command = await tools_agent_subgraph.execute(test_state)\n+        \n+        logger.info(f\"Subgraph completed, result type: {type(result_command)}\")\n+        \n+        if hasattr(result_command, 'update') and result_command.update:\n+            logger.info(f\"State updates: {list(result_command.update.keys())}\")\n+            \n+            # Check for new messages\n+            if 'messages' in result_command.update:\n+                new_messages = result_command.update['messages']\n+                logger.info(f\"Added {len(new_messages)} new messages\")\n+                \n+                for i, msg in enumerate(new_messages):\n+                    logger.info(f\"Message {i+1}: type={type(msg).__name__}, content_length={len(str(msg.content))}\")\n+                    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n+                        logger.info(f\"  Tool calls: {[call['name'] for call in msg.tool_calls]}\")\n+        else:\n+            logger.warning(\"No state updates in result\")\n+        \n+        logger.info(\"\u2705 Tools agent subgraph test completed successfully\")\n+        return True\n+        \n+    except Exception as e:\n+        logger.error(f\"\u274c Tools agent subgraph test failed: {e}\", exc_info=True)\n+        return False\n+\n+\n+if __name__ == \"__main__\":\n+    asyncio.run(test_tools_agent_subgraph())\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -0,0 +1,69 @@\n",
        "+#!/usr/bin/env python3\n",
        "+\"\"\"\n",
        "+Test script for the redesigned tools agent subgraph with proper LangGraph agent pattern.\n",
        "+\n",
        "+This script tests the complete agent workflow with chat_agent + tool_node cycling.\n",
        "+\"\"\"\n",
        "+\n",
        "+import asyncio\n",
        "+from models import LangChainMessage\n",
        "+from composer.graph.subgraphs.tools_agent import tools_agent_subgraph\n",
        "+from composer.graph.state import WorkflowState\n",
        "+from utils.logging import llmmllogger\n",
        "+\n",
        "+logger = llmmllogger.bind(component=\"ToolsAgentTest\")\n",
        "+\n",
        "+\n",
        "+async def test_tools_agent_subgraph():\n",
        "+    \"\"\"Test the tools agent subgraph with a simple query that should trigger web search.\"\"\"\n",
        "+    try:\n",
        "+        logger.info(\"Starting tools agent subgraph test\")\n",
        "+        \n",
        "+        # Create a minimal WorkflowState for testing\n",
        "+        test_state = WorkflowState()\n",
        "+        test_state.user_id = \"test_user\"\n",
        "+        test_state.conversation_id = 123\n",
        "+        test_state.current_date = \"2024-10-21\"\n",
        "+        test_state.messages = [\n",
        "+            LangChainMessage(\n",
        "+                content=\"What is the current weather in San Francisco? Please search for recent information.\",\n",
        "+                type=\"human\"\n",
        "+            )\n",
        "+        ]\n",
        "+        \n",
        "+        # Set up minimal user config for web search\n",
        "+        from models.default_configs import create_default_user_config\n",
        "+        test_state.user_config = create_default_user_config(\"test_user\")\n",
        "+        \n",
        "+        logger.info(\"Created test state with web search query\")\n",
        "+        \n",
        "+        # Execute the subgraph\n",
        "+        result_command = await tools_agent_subgraph.execute(test_state)\n",
        "+        \n",
        "+        logger.info(f\"Subgraph completed, result type: {type(result_command)}\")\n",
        "+        \n",
        "+        if hasattr(result_command, 'update') and result_command.update:\n",
        "+            logger.info(f\"State updates: {list(result_command.update.keys())}\")\n",
        "+            \n",
        "+            # Check for new messages\n",
        "+            if 'messages' in result_command.update:\n",
        "+                new_messages = result_command.update['messages']\n",
        "+                logger.info(f\"Added {len(new_messages)} new messages\")\n",
        "+                \n",
        "+                for i, msg in enumerate(new_messages):\n",
        "+                    logger.info(f\"Message {i+1}: type={type(msg).__name__}, content_length={len(str(msg.content))}\")\n",
        "+                    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
        "+                        logger.info(f\"  Tool calls: {[call['name'] for call in msg.tool_calls]}\")\n",
        "+        else:\n",
        "+            logger.warning(\"No state updates in result\")\n",
        "+        \n",
        "+        logger.info(\"\u2705 Tools agent subgraph test completed successfully\")\n",
        "+        return True\n",
        "+        \n",
        "+    except Exception as e:\n",
        "+        logger.error(f\"\u274c Tools agent subgraph test failed: {e}\", exc_info=True)\n",
        "+        return False\n",
        "+\n",
        "+\n",
        "+if __name__ == \"__main__\":\n",
        "+    asyncio.run(test_tools_agent_subgraph())\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "langchain_migration_plan.md",
      "status": "added",
      "additions": 147,
      "deletions": 0,
      "patch": "@@ -0,0 +1,147 @@\n+# LangChain/LangGraph Migration Plan\n+\n+## Current Issues\n+\n+Our current implementation is fighting against LangGraph best practices:\n+\n+1. **Using deprecated `InjectedState` instead of `ToolRuntime`**\n+2. **Complex schema filtering that shouldn't be necessary**\n+3. **Manual Command content extraction that defeats the purpose**\n+4. **Creating fake `WorkflowState` instead of injecting proper subgraph state**\n+5. **Not leveraging built-in middleware for agent loops**\n+\n+## Migration Path\n+\n+### Phase 1: Tool Runtime Migration (Immediate)\n+\n+**Current Pattern:**\n+```python\n+async def web_search(\n+    query: str,\n+    tool_call_id: Annotated[str, InjectedToolCallId],\n+    state: Annotated[WorkflowState, InjectedState],\n+) -> Command:\n+```\n+\n+**New Pattern:**\n+```python\n+from langchain_core.tools import ToolRuntime\n+\n+async def web_search(\n+    query: str,\n+    runtime: ToolRuntime[ToolsState],\n+) -> Command:\n+    # Access state via runtime.state\n+    # Access tool_call_id via runtime.tool_call_id\n+    # ToolRuntime parameters are NOT visible to the model\n+```\n+\n+**Benefits:**\n+- No schema filtering needed\n+- Clean parameter injection\n+- Proper LangGraph integration\n+- Command objects work as intended\n+\n+### Phase 2: Subgraph State Injection (Immediate)\n+\n+**Current:** Inject fake `WorkflowState` into tools\n+**New:** Inject proper `ToolsState` from subgraph\n+\n+```python\n+# In tools_agent.py subgraph\n+class ToolsState(TypedDict):\n+    messages: List[BaseMessage]\n+    user_id: str\n+    conversation_id: int\n+    web_search_config: Dict[str, Any]\n+    # ... other tool-specific config\n+\n+# Tools access this clean state instead of massive WorkflowState\n+```\n+\n+### Phase 3: Built-in Middleware (Near-term)\n+\n+**Current:** Manual agent cycling with conditional edges\n+**New:** LangGraph built-in middleware for agent loops\n+\n+```python\n+from langgraph.middleware import AgentLoopMiddleware\n+\n+# Use built-in middleware for agent->tool cycling\n+# Much cleaner than manual conditional routing\n+```\n+\n+### Phase 4: Memory Store Migration (Medium-term)\n+\n+**Current:** Custom memory storage in database\n+**New:** LangGraph built-in memory stores\n+\n+- **Persistent Memory**: https://docs.langchain.com/oss/python/langgraph/persistence\n+- **Long-term Memory**: https://docs.langchain.com/oss/python/langchain/long-term-memory\n+- **Time Travel**: https://docs.langchain.com/oss/python/langgraph/use-time-travel\n+\n+### Phase 5: MCP Server Integration (Future)\n+\n+**Goal:** Better tool distribution via Model Context Protocol\n+- **MCP Docs**: https://docs.langchain.com/oss/python/langchain/mcp\n+- Distribute tools across services\n+- Better scalability and separation of concerns\n+\n+## Implementation Steps\n+\n+### Step 1: Update Web Search Tool\n+```python\n+from langchain_core.tools import ToolRuntime\n+\n+@tool\n+async def web_search(\n+    query: str,\n+    runtime: ToolRuntime[ToolsState],\n+) -> Command:\n+    \"\"\"Search the web for information.\"\"\"\n+    state = runtime.state\n+    tool_call_id = runtime.tool_call_id\n+    \n+    # Use state.web_search_config directly\n+    # Return Command with updates to ToolsState\n+    return Command(update={\n+        \"web_search_results\": results,\n+        \"messages\": [ToolMessage(content, tool_call_id=tool_call_id)]\n+    })\n+```\n+\n+### Step 2: Remove Schema Filtering\n+- Delete `schema_filter.py` complexity\n+- Remove wrapper functions\n+- Let ToolRuntime handle parameter injection cleanly\n+\n+### Step 3: Fix Command Handling\n+- Stop extracting content from Commands\n+- Let LangGraph handle ToolMessage creation\n+- Use Commands for proper state updates\n+\n+### Step 4: Use Built-in Agent Patterns\n+- Replace manual conditional routing\n+- Use LangGraph's agent middleware\n+- Follow DeepAgents patterns for subgraphs\n+\n+## Benefits of Migration\n+\n+1. **Simpler Code**: Remove complex workarounds\n+2. **Better Performance**: Built-in optimizations\n+3. **Future-proof**: Following LangGraph evolution\n+4. **Better Debugging**: Standard patterns and tools\n+5. **Less Maintenance**: Fewer custom solutions\n+\n+## Testing Strategy\n+\n+1. **Tool by Tool**: Migrate one tool at a time\n+2. **Backward Compatibility**: Keep old patterns until migration complete\n+3. **E2E Testing**: Verify each step works\n+4. **Performance Testing**: Ensure no regressions\n+\n+## Documentation\n+\n+- Update tool documentation to reflect ToolRuntime usage\n+- Create migration guide for future tool development\n+- Document state injection patterns for subgraphs\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -0,0 +1,147 @@\n",
        "+# LangChain/LangGraph Migration Plan\n",
        "+\n",
        "+## Current Issues\n",
        "+\n",
        "+Our current implementation is fighting against LangGraph best practices:\n",
        "+\n",
        "+1. **Using deprecated `InjectedState` instead of `ToolRuntime`**\n",
        "+2. **Complex schema filtering that shouldn't be necessary**\n",
        "+3. **Manual Command content extraction that defeats the purpose**\n",
        "+4. **Creating fake `WorkflowState` instead of injecting proper subgraph state**\n",
        "+5. **Not leveraging built-in middleware for agent loops**\n",
        "+\n",
        "+## Migration Path\n",
        "+\n",
        "+### Phase 1: Tool Runtime Migration (Immediate)\n",
        "+\n",
        "+**Current Pattern:**\n",
        "+```python\n",
        "+async def web_search(\n",
        "+    query: str,\n",
        "+    tool_call_id: Annotated[str, InjectedToolCallId],\n",
        "+    state: Annotated[WorkflowState, InjectedState],\n",
        "+) -> Command:\n",
        "+```\n",
        "+\n",
        "+**New Pattern:**\n",
        "+```python\n",
        "+from langchain_core.tools import ToolRuntime\n",
        "+\n",
        "+async def web_search(\n",
        "+    query: str,\n",
        "+    runtime: ToolRuntime[ToolsState],\n",
        "+) -> Command:\n",
        "+    # Access state via runtime.state\n",
        "+    # Access tool_call_id via runtime.tool_call_id\n",
        "+    # ToolRuntime parameters are NOT visible to the model\n",
        "+```\n",
        "+\n",
        "+**Benefits:**\n",
        "+- No schema filtering needed\n",
        "+- Clean parameter injection\n",
        "+- Proper LangGraph integration\n",
        "+- Command objects work as intended\n",
        "+\n",
        "+### Phase 2: Subgraph State Injection (Immediate)\n",
        "+\n",
        "+**Current:** Inject fake `WorkflowState` into tools\n",
        "+**New:** Inject proper `ToolsState` from subgraph\n",
        "+\n",
        "+```python\n",
        "+# In tools_agent.py subgraph\n",
        "+class ToolsState(TypedDict):\n",
        "+    messages: List[BaseMessage]\n",
        "+    user_id: str\n",
        "+    conversation_id: int\n",
        "+    web_search_config: Dict[str, Any]\n",
        "+    # ... other tool-specific config\n",
        "+\n",
        "+# Tools access this clean state instead of massive WorkflowState\n",
        "+```\n",
        "+\n",
        "+### Phase 3: Built-in Middleware (Near-term)\n",
        "+\n",
        "+**Current:** Manual agent cycling with conditional edges\n",
        "+**New:** LangGraph built-in middleware for agent loops\n",
        "+\n",
        "+```python\n",
        "+from langgraph.middleware import AgentLoopMiddleware\n",
        "+\n",
        "+# Use built-in middleware for agent->tool cycling\n",
        "+# Much cleaner than manual conditional routing\n",
        "+```\n",
        "+\n",
        "+### Phase 4: Memory Store Migration (Medium-term)\n",
        "+\n",
        "+**Current:** Custom memory storage in database\n",
        "+**New:** LangGraph built-in memory stores\n",
        "+\n",
        "+- **Persistent Memory**: https://docs.langchain.com/oss/python/langgraph/persistence\n",
        "+- **Long-term Memory**: https://docs.langchain.com/oss/python/langchain/long-term-memory\n",
        "+- **Time Travel**: https://docs.langchain.com/oss/python/langgraph/use-time-travel\n",
        "+\n",
        "+### Phase 5: MCP Server Integration (Future)\n",
        "+\n",
        "+**Goal:** Better tool distribution via Model Context Protocol\n",
        "+- **MCP Docs**: https://docs.langchain.com/oss/python/langchain/mcp\n",
        "+- Distribute tools across services\n",
        "+- Better scalability and separation of concerns\n",
        "+\n",
        "+## Implementation Steps\n",
        "+\n",
        "+### Step 1: Update Web Search Tool\n",
        "+```python\n",
        "+from langchain_core.tools import ToolRuntime\n",
        "+\n",
        "+@tool\n",
        "+async def web_search(\n",
        "+    query: str,\n",
        "+    runtime: ToolRuntime[ToolsState],\n",
        "+) -> Command:\n",
        "+    \"\"\"Search the web for information.\"\"\"\n",
        "+    state = runtime.state\n",
        "+    tool_call_id = runtime.tool_call_id\n",
        "+    \n",
        "+    # Use state.web_search_config directly\n",
        "+    # Return Command with updates to ToolsState\n",
        "+    return Command(update={\n",
        "+        \"web_search_results\": results,\n",
        "+        \"messages\": [ToolMessage(content, tool_call_id=tool_call_id)]\n",
        "+    })\n",
        "+```\n",
        "+\n",
        "+### Step 2: Remove Schema Filtering\n",
        "+- Delete `schema_filter.py` complexity\n",
        "+- Remove wrapper functions\n",
        "+- Let ToolRuntime handle parameter injection cleanly\n",
        "+\n",
        "+### Step 3: Fix Command Handling\n",
        "+- Stop extracting content from Commands\n",
        "+- Let LangGraph handle ToolMessage creation\n",
        "+- Use Commands for proper state updates\n",
        "+\n",
        "+### Step 4: Use Built-in Agent Patterns\n",
        "+- Replace manual conditional routing\n",
        "+- Use LangGraph's agent middleware\n",
        "+- Follow DeepAgents patterns for subgraphs\n",
        "+\n",
        "+## Benefits of Migration\n",
        "+\n",
        "+1. **Simpler Code**: Remove complex workarounds\n",
        "+2. **Better Performance**: Built-in optimizations\n",
        "+3. **Future-proof**: Following LangGraph evolution\n",
        "+4. **Better Debugging**: Standard patterns and tools\n",
        "+5. **Less Maintenance**: Fewer custom solutions\n",
        "+\n",
        "+## Testing Strategy\n",
        "+\n",
        "+1. **Tool by Tool**: Migrate one tool at a time\n",
        "+2. **Backward Compatibility**: Keep old patterns until migration complete\n",
        "+3. **E2E Testing**: Verify each step works\n",
        "+4. **Performance Testing**: Ensure no regressions\n",
        "+\n",
        "+## Documentation\n",
        "+\n",
        "+- Update tool documentation to reflect ToolRuntime usage\n",
        "+- Create migration guide for future tool development\n",
        "+- Document state injection patterns for subgraphs\n",
        "\\ No newline at end of file\n"
      ]
    }
  ]
}
{
  "project": "Research Data/Deep-Query",
  "repo": "Yousif-Abuzeid/Deep-Query",
  "prior_commit": "cff22449ff1f8213488a681f1b0766ccfa85cf28",
  "researched_commit": "14a6100a1b7eaa168c0e04ec15e7ccee75e40b2f",
  "compare_url": "https://github.com/Yousif-Abuzeid/Deep-Query/compare/cff22449ff1f8213488a681f1b0766ccfa85cf28...14a6100a1b7eaa168c0e04ec15e7ccee75e40b2f",
  "ahead_by": 1,
  "behind_by": 0,
  "changed_files": [
    {
      "path": "docker/env/.env.app.example",
      "status": "modified",
      "additions": 1,
      "deletions": 0,
      "patch": "@@ -25,6 +25,7 @@ GENERATION_MODEL_ID=\"gemini-2.5-flash\"\n EMBEDDING_MODEL_ID=\"text-embedding-004\"\n \n GOOGLE_GENAI_API_KEY=\"your_google_genai_api_key\"\n+TAVILY_API_KEY=\"your_tavily_api_key\"\n EMBEDDING_MODEL_SIZE=768\n \n INPUT_DEFAULT_MAX_CHARACTERS=10000",
      "patch_lines": [
        "@@ -25,6 +25,7 @@ GENERATION_MODEL_ID=\"gemini-2.5-flash\"\n",
        " EMBEDDING_MODEL_ID=\"text-embedding-004\"\n",
        " \n",
        " GOOGLE_GENAI_API_KEY=\"your_google_genai_api_key\"\n",
        "+TAVILY_API_KEY=\"your_tavily_api_key\"\n",
        " EMBEDDING_MODEL_SIZE=768\n",
        " \n",
        " INPUT_DEFAULT_MAX_CHARACTERS=10000\n"
      ]
    },
    {
      "path": "docker/minirag/DockerFile",
      "status": "modified",
      "additions": 3,
      "deletions": 0,
      "patch": "@@ -19,6 +19,9 @@ COPY src/requirments.txt .\n \n RUN uv pip install -r requirments.txt --system\n \n+# Install Playwright browsers with dependencies\n+RUN playwright install --with-deps chromium\n+\n COPY src/ .\n \n # Create directory structure for Alembic",
      "patch_lines": [
        "@@ -19,6 +19,9 @@ COPY src/requirments.txt .\n",
        " \n",
        " RUN uv pip install -r requirments.txt --system\n",
        " \n",
        "+# Install Playwright browsers with dependencies\n",
        "+RUN playwright install --with-deps chromium\n",
        "+\n",
        " COPY src/ .\n",
        " \n",
        " # Create directory structure for Alembic\n"
      ]
    },
    {
      "path": "docker/minirag/entrypoint.sh",
      "status": "modified",
      "additions": 13,
      "deletions": 0,
      "patch": "@@ -10,3 +10,16 @@ ls -la\n alembic upgrade head\n cd /app\n \n+echo \"=== DEBUG ARGUMENTS ===\"\n+echo \"Number of arguments: $#\"\n+echo \"All arguments: $@\"\n+echo \"First argument: $1\"\n+echo \"========================\"\n+\n+if [ $# -eq 0 ]; then\n+    echo \"No arguments provided, starting uvicorn directly\"\n+    uvicorn main:app --host 0.0.0.0 --port 8000 --reload\n+else\n+    echo \"Starting FastAPI application with exec...\"\n+    exec \"$@\"\n+fi\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -10,3 +10,16 @@ ls -la\n",
        " alembic upgrade head\n",
        " cd /app\n",
        " \n",
        "+echo \"=== DEBUG ARGUMENTS ===\"\n",
        "+echo \"Number of arguments: $#\"\n",
        "+echo \"All arguments: $@\"\n",
        "+echo \"First argument: $1\"\n",
        "+echo \"========================\"\n",
        "+\n",
        "+if [ $# -eq 0 ]; then\n",
        "+    echo \"No arguments provided, starting uvicorn directly\"\n",
        "+    uvicorn main:app --host 0.0.0.0 --port 8000 --reload\n",
        "+else\n",
        "+    echo \"Starting FastAPI application with exec...\"\n",
        "+    exec \"$@\"\n",
        "+fi\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "src/agents/deep_researcher/__init__.py",
      "status": "added",
      "additions": 1,
      "deletions": 0,
      "patch": "@@ -0,0 +1 @@\n+from .deep_researcher import DeepResearch\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -0,0 +1 @@\n",
        "+from .deep_researcher import DeepResearch\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "src/agents/deep_researcher/deep_researcher.py",
      "status": "added",
      "additions": 451,
      "deletions": 0,
      "patch": "@@ -0,0 +1,451 @@\n+\"\"\"Deep Research Agent with LangGraph implementation.\"\"\"\n+\n+import logging\n+from datetime import datetime\n+from typing import Any, Dict, List\n+\n+from langchain_core.messages import HumanMessage, SystemMessage\n+from langgraph.checkpoint.memory import MemorySaver\n+from langgraph.graph import END, START, StateGraph\n+from langgraph.prebuilt import create_react_agent\n+\n+from stores.llm.templates.locales.en.deep_researcher import (\n+    compress_research_simple_human_message,\n+    compress_research_system_prompt,\n+    final_report_generation_prompt,\n+    lead_researcher_prompt,\n+    research_system_prompt,\n+)\n+\n+from .state import (\n+    AgentState,\n+    ConductResearch,\n+    ResearchComplete,\n+    ResearcherOutputState,\n+    SupervisorState,\n+)\n+from .tools import get_browser_toolkit, tavily_search, think_tool\n+\n+# Set up logger\n+logger = logging.getLogger(\"uvicorn\")\n+\n+\n+class DeepResearch:\n+    \"\"\"Deep Research Agent using LangGraph with React agents for tool-calling.\"\"\"\n+\n+    def __init__(self, generation_client, max_concurrent_research_units: int = 3):\n+        self.generation_client = generation_client\n+        self.llm = self.generation_client.get_langchain_chat_model()\n+        self.max_concurrent_research_units = max_concurrent_research_units\n+        self.max_researcher_iterations = 3\n+\n+        # Initialize browser tools as None - will be created on first use\n+        self._browser_tools = None\n+\n+        # Initialize search tool immediately (no async issues)\n+        self.search_tool = tavily_search\n+\n+        # Build the graph\n+        self.graph = self._build_graph()\n+\n+    async def _get_browser_tools(self):\n+        \"\"\"Lazy initialization of browser tools.\"\"\"\n+        if self._browser_tools is None:\n+            self._browser_tools = await get_browser_toolkit()\n+        return self._browser_tools\n+\n+    def get_today_str(self) -> str:\n+        \"\"\"Get today's date as a formatted string.\"\"\"\n+        return datetime.now().strftime(\"%Y-%m-%d\")\n+\n+    async def _build_supervisor_agent(self):\n+        \"\"\"Build the supervisor/lead researcher agent.\"\"\"\n+        # Supervisor only needs the think tool for strategic planning\n+        supervisor_agent = create_react_agent(\n+            model=self.llm,\n+            tools=[think_tool],\n+        )\n+        return supervisor_agent\n+\n+    async def _build_researcher_agent(self):\n+        \"\"\"Build an individual researcher agent with all tools.\"\"\"\n+        # Get browser tools lazily\n+        browser_tools = await self._get_browser_tools()\n+\n+        # Combine all research tools\n+        research_tools = [\n+            self.search_tool,\n+            think_tool,\n+        ] + browser_tools\n+\n+        researcher_agent = create_react_agent(\n+            model=self.llm,\n+            tools=research_tools,\n+        )\n+        return researcher_agent\n+\n+    async def supervisor(self, state: SupervisorState) -> Dict:\n+        \"\"\"Supervisor node that plans research and coordinates researchers.\"\"\"\n+        supervisor_agent = await self._build_supervisor_agent()\n+\n+        # Format system prompt with parameters\n+        system_prompt = lead_researcher_prompt.format(\n+            date=self.get_today_str(),\n+            max_concurrent_research_units=self.max_concurrent_research_units,\n+            max_researcher_iterations=self.max_researcher_iterations,\n+        )\n+\n+        # Prepare messages for the supervisor - CRITICAL: System prompt must be first\n+        messages = [\n+            SystemMessage(content=system_prompt),\n+            HumanMessage(content=f\"Research Question: {state['research_question']}\"),\n+        ]\n+\n+        # Add previous research if available\n+        if state.get(\"all_research_outcomes\"):\n+            research_summary = \"\\n\\n\".join(\n+                [\n+                    f\"Research on '{r.research_topic}':\\n{r.research_outcome}\"\n+                    for r in state[\"all_research_outcomes\"]\n+                ]\n+            )\n+            messages.append(\n+                HumanMessage(content=f\"Previous Research:\\n{research_summary}\")\n+            )\n+\n+        # Invoke supervisor\n+        result = await supervisor_agent.ainvoke({\"messages\": messages})\n+        last_message = result[\"messages\"][-1].content\n+\n+        # \ud83d\udd0d DEBUG: Log supervisor's response\n+        logger.info(\"=\" * 80)\n+        logger.info(\"SUPERVISOR RESPONSE:\")\n+        logger.info(last_message)\n+        logger.info(\"=\" * 80)\n+\n+        # Parse the supervisor's decision\n+        tasks = self._extract_research_tasks(last_message)\n+\n+        # \ud83d\udd0d DEBUG: Log extracted tasks\n+        logger.info(f\"EXTRACTED TASKS: {tasks}\")\n+        logger.info(f\"Number of tasks: {len(tasks)}\")\n+\n+        iteration = state.get(\"iteration\", 0) + 1\n+\n+        # Check if supervisor wants to complete research\n+        should_complete = \"RESEARCH COMPLETE\" in last_message.upper()\n+\n+        # \ud83d\udd0d DEBUG: Log decision\n+        logger.info(f\"Should complete: {should_complete}\")\n+        logger.info(f\"Iteration: {iteration}/{self.max_researcher_iterations}\")\n+        logger.info(f\"Has tasks: {len(tasks) > 0}\")\n+        logger.info(\n+            f\"Has research outcomes: {len(state.get('all_research_outcomes', []))}\"\n+        )\n+\n+        # FIX: Stop if max iterations reached OR if supervisor says complete AND we have done research\n+        if iteration >= self.max_researcher_iterations:\n+            logger.info(\"\u2713 Max iterations reached - Generating final report\")\n+            return {\n+                \"next_action\": ResearchComplete(),\n+                \"iteration\": iteration,\n+            }\n+        elif should_complete and state.get(\"all_research_outcomes\"):\n+            logger.info(\"\u2713 Supervisor says research complete - Generating final report\")\n+            return {\n+                \"next_action\": ResearchComplete(),\n+                \"iteration\": iteration,\n+            }\n+        elif len(tasks) > 0 and not should_complete:\n+            logger.info(f\"\u2713 Conducting research on {len(tasks)} tasks\")\n+            return {\n+                \"next_action\": ConductResearch(research_topics=tasks),\n+                \"iteration\": iteration,\n+            }\n+        else:\n+            # Fallback: Force at least one research iteration if no research done yet\n+            if not state.get(\"all_research_outcomes\"):\n+                logger.warning(\n+                    \"\u26a0 No valid tasks and no research done yet. Using research question as task.\"\n+                )\n+                return {\n+                    \"next_action\": ConductResearch(\n+                        research_topics=[state[\"research_question\"]]\n+                    ),\n+                    \"iteration\": iteration,\n+                }\n+            else:\n+                # We have research but supervisor didn't explicitly say complete\n+                logger.info(\n+                    \"\u2713 Have research but ambiguous signal - Generating final report\"\n+                )\n+                return {\n+                    \"next_action\": ResearchComplete(),\n+                    \"iteration\": iteration,\n+                }\n+\n+    def _extract_research_tasks(self, content: str) -> List[str]:\n+        \"\"\"Extract research tasks from supervisor's response.\"\"\"\n+        tasks = []\n+        lines = content.split(\"\\n\")\n+\n+        in_tasks_section = False\n+        for line in lines:\n+            line = line.strip()\n+\n+            # Look for the RESEARCH_TASKS section\n+            if \"RESEARCH_TASKS:\" in line.upper() or \"RESEARCH TASKS:\" in line.upper():\n+                in_tasks_section = True\n+                # If the task is on the same line, extract it\n+                if \":\" in line:\n+                    task_part = line.split(\":\", 1)[1].strip()\n+                    if task_part and len(task_part) > 10:\n+                        tasks.append(task_part)\n+                continue\n+\n+            if in_tasks_section:\n+                # Stop if we hit another section\n+                if (\n+                    \"RESEARCH COMPLETE\" in line.upper()\n+                    or line.startswith(\"When complete\")\n+                    or line.startswith(\"**\")\n+                ):\n+                    break\n+\n+                # Extract numbered or bulleted tasks\n+                if line and (\n+                    line[0].isdigit() or line.startswith(\"-\") or line.startswith(\"*\")\n+                ):\n+                    # Remove numbering, bullets, and clean up\n+                    task = line.lstrip(\"0123456789.-*)\u2022 \").strip()\n+                    # Remove any remaining formatting\n+                    task = task.replace(\"**\", \"\").replace(\"__\", \"\")\n+                    if task and len(task) > 10:  # Avoid very short tasks\n+                        tasks.append(task)\n+                # Also catch tasks without bullets/numbers\n+                elif line and len(line) > 10 and not line.endswith(\":\"):\n+                    tasks.append(line)\n+\n+        # If no tasks found, try to find tool calls or actions\n+        if not tasks:\n+            import re\n+\n+            # Look for tool calls in the agent's actions\n+            tool_match = re.search(r'\"query\":\\s*\"([^\"]+)\"', content)\n+            if tool_match:\n+                tasks = [tool_match.group(1)]\n+            else:\n+                # Look for any sentence that looks like a research task\n+                sentences = [\n+                    s.strip() for s in content.split(\".\") if len(s.strip()) > 20\n+                ]\n+                if sentences:\n+                    # Filter for sentences that look like research tasks\n+                    research_like = [\n+                        s\n+                        for s in sentences\n+                        if any(\n+                            kw in s.lower()\n+                            for kw in [\n+                                \"research\",\n+                                \"investigate\",\n+                                \"analyze\",\n+                                \"examine\",\n+                                \"study\",\n+                                \"explore\",\n+                            ]\n+                        )\n+                    ]\n+                    if research_like:\n+                        tasks = research_like[: self.max_concurrent_research_units]\n+\n+        logger.info(f\"_extract_research_tasks found: {tasks}\")\n+        return tasks[: self.max_concurrent_research_units]\n+\n+    async def _conduct_research_tasks(\n+        self, tasks: List[str]\n+    ) -> List[ResearcherOutputState]:\n+        \"\"\"Conduct multiple research tasks concurrently.\"\"\"\n+        import asyncio\n+\n+        logger.info(f\"Starting research on {len(tasks)} tasks...\")\n+        research_results = await asyncio.gather(\n+            *[self._conduct_single_research(task) for task in tasks]\n+        )\n+        logger.info(f\"Completed research on {len(tasks)} tasks\")\n+        return research_results\n+\n+    async def _conduct_single_research(\n+        self, research_topic: str\n+    ) -> ResearcherOutputState:\n+        \"\"\"Conduct a single research task using a researcher agent.\"\"\"\n+        logger.info(f\"\ud83d\udd2c Researching: {research_topic[:100]}...\")\n+\n+        researcher_agent = await self._build_researcher_agent()\n+\n+        # Format system prompt\n+        system_prompt = research_system_prompt.format(\n+            date=self.get_today_str(),\n+        )\n+\n+        # CRITICAL: System prompt must be first, then the specific research task\n+        messages = [\n+            SystemMessage(content=system_prompt),\n+            HumanMessage(content=f\"Research this specific topic: {research_topic}\"),\n+        ]\n+\n+        result = await researcher_agent.ainvoke({\"messages\": messages})\n+\n+        # Extract and compress the research\n+        research_messages = result[\"messages\"]\n+        compressed_research = await self._compress_research(\n+            research_messages, research_topic\n+        )\n+\n+        logger.info(f\"\u2713 Completed research on: {research_topic[:50]}...\")\n+\n+        return ResearcherOutputState(\n+            research_topic=research_topic,\n+            research_outcome=compressed_research,\n+        )\n+\n+    async def _compress_research(self, messages: List, research_topic: str) -> str:\n+        \"\"\"Compress research findings into organized format.\"\"\"\n+        # Extract the actual research content\n+        research_content = \"\\n\".join(\n+            [msg.content for msg in messages if hasattr(msg, \"content\") and msg.content]\n+        )\n+\n+        compression_messages = [\n+            SystemMessage(content=compress_research_system_prompt),\n+            HumanMessage(\n+                content=compress_research_simple_human_message.format(\n+                    research_topic=research_topic,\n+                    research_content=research_content,\n+                )\n+            ),\n+        ]\n+\n+        response = await self.llm.ainvoke(compression_messages)\n+        return response.content\n+\n+    async def generate_final_report(self, state: AgentState) -> Dict:\n+        \"\"\"Generate the final research report.\"\"\"\n+        logger.info(\"\ud83d\udcdd Generating final report...\")\n+\n+        # Combine all research outcomes\n+        all_research = \"\\n\\n\".join(\n+            [\n+                f\"## {r.research_topic}\\n{r.research_outcome}\"\n+                for r in state[\"all_research_outcomes\"]\n+            ]\n+        )\n+\n+        messages = [\n+            SystemMessage(content=\"You are an expert report writer.\"),\n+            HumanMessage(\n+                content=final_report_generation_prompt.format(\n+                    research_question=state[\"research_question\"],\n+                    date=self.get_today_str(),\n+                    all_research=all_research,\n+                )\n+            ),\n+        ]\n+\n+        response = await self.llm.ainvoke(messages)\n+\n+        logger.info(\"\u2713 Final report generated\")\n+\n+        return {\n+            \"final_report\": response.content,\n+        }\n+\n+    def _build_graph(self) -> StateGraph:\n+        \"\"\"Build the LangGraph workflow.\"\"\"\n+        workflow = StateGraph(AgentState)\n+\n+        # Add nodes\n+        workflow.add_node(\"supervisor\", self.supervisor)\n+        workflow.add_node(\"conduct_research\", self._conduct_research_tasks_node)\n+        workflow.add_node(\"generate_report\", self.generate_final_report)\n+\n+        # Add edges\n+        workflow.add_edge(START, \"supervisor\")\n+\n+        workflow.add_conditional_edges(\n+            \"supervisor\",\n+            lambda state: (\n+                \"conduct_research\"\n+                if isinstance(state[\"next_action\"], ConductResearch)\n+                else \"generate_report\"\n+            ),\n+            {\n+                \"conduct_research\": \"conduct_research\",\n+                \"generate_report\": \"generate_report\",\n+            },\n+        )\n+\n+        workflow.add_edge(\"conduct_research\", \"supervisor\")\n+        workflow.add_edge(\"generate_report\", END)\n+\n+        # Compile with memory\n+        memory = MemorySaver()\n+        return workflow.compile(checkpointer=memory)\n+\n+    async def _conduct_research_tasks_node(self, state: AgentState) -> Dict:\n+        \"\"\"Node wrapper for conducting research tasks.\"\"\"\n+        if isinstance(state[\"next_action\"], ConductResearch):\n+            tasks = state[\"next_action\"].research_topics\n+            results = await self._conduct_research_tasks(tasks)\n+\n+            # Append to existing research\n+            all_outcomes = state.get(\"all_research_outcomes\", []) + results\n+\n+            return {\n+                \"all_research_outcomes\": all_outcomes,\n+            }\n+        return {}\n+\n+    async def conduct_research(\n+        self, query: str, project_id: str = \"default\"\n+    ) -> Dict[str, Any]:\n+        \"\"\"\n+        Conduct deep research on a query.\n+\n+        Args:\n+            query: The research question\n+            project_id: Project identifier for tracking\n+\n+        Returns:\n+            Dictionary with final_report and metadata\n+        \"\"\"\n+        logger.info(f\"\ud83d\ude80 Starting deep research on: {query}\")\n+\n+        initial_state = {\n+            \"research_question\": query,\n+            \"all_research_outcomes\": [],\n+            \"iteration\": 0,\n+            \"next_action\": None,\n+            \"final_report\": None,\n+        }\n+\n+        config = {\"configurable\": {\"thread_id\": project_id}}\n+\n+        result = await self.graph.ainvoke(initial_state, config)\n+\n+        logger.info(\n+            f\"\u2713 Research complete. Iterations: {result.get('iteration', 0)}, Outcomes: {len(result.get('all_research_outcomes', []))}\"\n+        )\n+\n+        return {\n+            \"final_report\": result.get(\"final_report\"),\n+            \"research_question\": query,\n+            \"iterations\": result.get(\"iteration\", 0),\n+            \"research_outcomes\": [\n+                {\n+                    \"topic\": r.research_topic,\n+                    \"outcome\": r.research_outcome,\n+                }\n+                for r in result.get(\"all_research_outcomes\", [])\n+            ],\n+        }",
      "patch_lines": [
        "@@ -0,0 +1,451 @@\n",
        "+\"\"\"Deep Research Agent with LangGraph implementation.\"\"\"\n",
        "+\n",
        "+import logging\n",
        "+from datetime import datetime\n",
        "+from typing import Any, Dict, List\n",
        "+\n",
        "+from langchain_core.messages import HumanMessage, SystemMessage\n",
        "+from langgraph.checkpoint.memory import MemorySaver\n",
        "+from langgraph.graph import END, START, StateGraph\n",
        "+from langgraph.prebuilt import create_react_agent\n",
        "+\n",
        "+from stores.llm.templates.locales.en.deep_researcher import (\n",
        "+    compress_research_simple_human_message,\n",
        "+    compress_research_system_prompt,\n",
        "+    final_report_generation_prompt,\n",
        "+    lead_researcher_prompt,\n",
        "+    research_system_prompt,\n",
        "+)\n",
        "+\n",
        "+from .state import (\n",
        "+    AgentState,\n",
        "+    ConductResearch,\n",
        "+    ResearchComplete,\n",
        "+    ResearcherOutputState,\n",
        "+    SupervisorState,\n",
        "+)\n",
        "+from .tools import get_browser_toolkit, tavily_search, think_tool\n",
        "+\n",
        "+# Set up logger\n",
        "+logger = logging.getLogger(\"uvicorn\")\n",
        "+\n",
        "+\n",
        "+class DeepResearch:\n",
        "+    \"\"\"Deep Research Agent using LangGraph with React agents for tool-calling.\"\"\"\n",
        "+\n",
        "+    def __init__(self, generation_client, max_concurrent_research_units: int = 3):\n",
        "+        self.generation_client = generation_client\n",
        "+        self.llm = self.generation_client.get_langchain_chat_model()\n",
        "+        self.max_concurrent_research_units = max_concurrent_research_units\n",
        "+        self.max_researcher_iterations = 3\n",
        "+\n",
        "+        # Initialize browser tools as None - will be created on first use\n",
        "+        self._browser_tools = None\n",
        "+\n",
        "+        # Initialize search tool immediately (no async issues)\n",
        "+        self.search_tool = tavily_search\n",
        "+\n",
        "+        # Build the graph\n",
        "+        self.graph = self._build_graph()\n",
        "+\n",
        "+    async def _get_browser_tools(self):\n",
        "+        \"\"\"Lazy initialization of browser tools.\"\"\"\n",
        "+        if self._browser_tools is None:\n",
        "+            self._browser_tools = await get_browser_toolkit()\n",
        "+        return self._browser_tools\n",
        "+\n",
        "+    def get_today_str(self) -> str:\n",
        "+        \"\"\"Get today's date as a formatted string.\"\"\"\n",
        "+        return datetime.now().strftime(\"%Y-%m-%d\")\n",
        "+\n",
        "+    async def _build_supervisor_agent(self):\n",
        "+        \"\"\"Build the supervisor/lead researcher agent.\"\"\"\n",
        "+        # Supervisor only needs the think tool for strategic planning\n",
        "+        supervisor_agent = create_react_agent(\n",
        "+            model=self.llm,\n",
        "+            tools=[think_tool],\n",
        "+        )\n",
        "+        return supervisor_agent\n",
        "+\n",
        "+    async def _build_researcher_agent(self):\n",
        "+        \"\"\"Build an individual researcher agent with all tools.\"\"\"\n",
        "+        # Get browser tools lazily\n",
        "+        browser_tools = await self._get_browser_tools()\n",
        "+\n",
        "+        # Combine all research tools\n",
        "+        research_tools = [\n",
        "+            self.search_tool,\n",
        "+            think_tool,\n",
        "+        ] + browser_tools\n",
        "+\n",
        "+        researcher_agent = create_react_agent(\n",
        "+            model=self.llm,\n",
        "+            tools=research_tools,\n",
        "+        )\n",
        "+        return researcher_agent\n",
        "+\n",
        "+    async def supervisor(self, state: SupervisorState) -> Dict:\n",
        "+        \"\"\"Supervisor node that plans research and coordinates researchers.\"\"\"\n",
        "+        supervisor_agent = await self._build_supervisor_agent()\n",
        "+\n",
        "+        # Format system prompt with parameters\n",
        "+        system_prompt = lead_researcher_prompt.format(\n",
        "+            date=self.get_today_str(),\n",
        "+            max_concurrent_research_units=self.max_concurrent_research_units,\n",
        "+            max_researcher_iterations=self.max_researcher_iterations,\n",
        "+        )\n",
        "+\n",
        "+        # Prepare messages for the supervisor - CRITICAL: System prompt must be first\n",
        "+        messages = [\n",
        "+            SystemMessage(content=system_prompt),\n",
        "+            HumanMessage(content=f\"Research Question: {state['research_question']}\"),\n",
        "+        ]\n",
        "+\n",
        "+        # Add previous research if available\n",
        "+        if state.get(\"all_research_outcomes\"):\n",
        "+            research_summary = \"\\n\\n\".join(\n",
        "+                [\n",
        "+                    f\"Research on '{r.research_topic}':\\n{r.research_outcome}\"\n",
        "+                    for r in state[\"all_research_outcomes\"]\n",
        "+                ]\n",
        "+            )\n",
        "+            messages.append(\n",
        "+                HumanMessage(content=f\"Previous Research:\\n{research_summary}\")\n",
        "+            )\n",
        "+\n",
        "+        # Invoke supervisor\n",
        "+        result = await supervisor_agent.ainvoke({\"messages\": messages})\n",
        "+        last_message = result[\"messages\"][-1].content\n",
        "+\n",
        "+        # \ud83d\udd0d DEBUG: Log supervisor's response\n",
        "+        logger.info(\"=\" * 80)\n",
        "+        logger.info(\"SUPERVISOR RESPONSE:\")\n",
        "+        logger.info(last_message)\n",
        "+        logger.info(\"=\" * 80)\n",
        "+\n",
        "+        # Parse the supervisor's decision\n",
        "+        tasks = self._extract_research_tasks(last_message)\n",
        "+\n",
        "+        # \ud83d\udd0d DEBUG: Log extracted tasks\n",
        "+        logger.info(f\"EXTRACTED TASKS: {tasks}\")\n",
        "+        logger.info(f\"Number of tasks: {len(tasks)}\")\n",
        "+\n",
        "+        iteration = state.get(\"iteration\", 0) + 1\n",
        "+\n",
        "+        # Check if supervisor wants to complete research\n",
        "+        should_complete = \"RESEARCH COMPLETE\" in last_message.upper()\n",
        "+\n",
        "+        # \ud83d\udd0d DEBUG: Log decision\n",
        "+        logger.info(f\"Should complete: {should_complete}\")\n",
        "+        logger.info(f\"Iteration: {iteration}/{self.max_researcher_iterations}\")\n",
        "+        logger.info(f\"Has tasks: {len(tasks) > 0}\")\n",
        "+        logger.info(\n",
        "+            f\"Has research outcomes: {len(state.get('all_research_outcomes', []))}\"\n",
        "+        )\n",
        "+\n",
        "+        # FIX: Stop if max iterations reached OR if supervisor says complete AND we have done research\n",
        "+        if iteration >= self.max_researcher_iterations:\n",
        "+            logger.info(\"\u2713 Max iterations reached - Generating final report\")\n",
        "+            return {\n",
        "+                \"next_action\": ResearchComplete(),\n",
        "+                \"iteration\": iteration,\n",
        "+            }\n",
        "+        elif should_complete and state.get(\"all_research_outcomes\"):\n",
        "+            logger.info(\"\u2713 Supervisor says research complete - Generating final report\")\n",
        "+            return {\n",
        "+                \"next_action\": ResearchComplete(),\n",
        "+                \"iteration\": iteration,\n",
        "+            }\n",
        "+        elif len(tasks) > 0 and not should_complete:\n",
        "+            logger.info(f\"\u2713 Conducting research on {len(tasks)} tasks\")\n",
        "+            return {\n",
        "+                \"next_action\": ConductResearch(research_topics=tasks),\n",
        "+                \"iteration\": iteration,\n",
        "+            }\n",
        "+        else:\n",
        "+            # Fallback: Force at least one research iteration if no research done yet\n",
        "+            if not state.get(\"all_research_outcomes\"):\n",
        "+                logger.warning(\n",
        "+                    \"\u26a0 No valid tasks and no research done yet. Using research question as task.\"\n",
        "+                )\n",
        "+                return {\n",
        "+                    \"next_action\": ConductResearch(\n",
        "+                        research_topics=[state[\"research_question\"]]\n",
        "+                    ),\n",
        "+                    \"iteration\": iteration,\n",
        "+                }\n",
        "+            else:\n",
        "+                # We have research but supervisor didn't explicitly say complete\n",
        "+                logger.info(\n",
        "+                    \"\u2713 Have research but ambiguous signal - Generating final report\"\n",
        "+                )\n",
        "+                return {\n",
        "+                    \"next_action\": ResearchComplete(),\n",
        "+                    \"iteration\": iteration,\n",
        "+                }\n",
        "+\n",
        "+    def _extract_research_tasks(self, content: str) -> List[str]:\n",
        "+        \"\"\"Extract research tasks from supervisor's response.\"\"\"\n",
        "+        tasks = []\n",
        "+        lines = content.split(\"\\n\")\n",
        "+\n",
        "+        in_tasks_section = False\n",
        "+        for line in lines:\n",
        "+            line = line.strip()\n",
        "+\n",
        "+            # Look for the RESEARCH_TASKS section\n",
        "+            if \"RESEARCH_TASKS:\" in line.upper() or \"RESEARCH TASKS:\" in line.upper():\n",
        "+                in_tasks_section = True\n",
        "+                # If the task is on the same line, extract it\n",
        "+                if \":\" in line:\n",
        "+                    task_part = line.split(\":\", 1)[1].strip()\n",
        "+                    if task_part and len(task_part) > 10:\n",
        "+                        tasks.append(task_part)\n",
        "+                continue\n",
        "+\n",
        "+            if in_tasks_section:\n",
        "+                # Stop if we hit another section\n",
        "+                if (\n",
        "+                    \"RESEARCH COMPLETE\" in line.upper()\n",
        "+                    or line.startswith(\"When complete\")\n",
        "+                    or line.startswith(\"**\")\n",
        "+                ):\n",
        "+                    break\n",
        "+\n",
        "+                # Extract numbered or bulleted tasks\n",
        "+                if line and (\n",
        "+                    line[0].isdigit() or line.startswith(\"-\") or line.startswith(\"*\")\n",
        "+                ):\n",
        "+                    # Remove numbering, bullets, and clean up\n",
        "+                    task = line.lstrip(\"0123456789.-*)\u2022 \").strip()\n",
        "+                    # Remove any remaining formatting\n",
        "+                    task = task.replace(\"**\", \"\").replace(\"__\", \"\")\n",
        "+                    if task and len(task) > 10:  # Avoid very short tasks\n",
        "+                        tasks.append(task)\n",
        "+                # Also catch tasks without bullets/numbers\n",
        "+                elif line and len(line) > 10 and not line.endswith(\":\"):\n",
        "+                    tasks.append(line)\n",
        "+\n",
        "+        # If no tasks found, try to find tool calls or actions\n",
        "+        if not tasks:\n",
        "+            import re\n",
        "+\n",
        "+            # Look for tool calls in the agent's actions\n",
        "+            tool_match = re.search(r'\"query\":\\s*\"([^\"]+)\"', content)\n",
        "+            if tool_match:\n",
        "+                tasks = [tool_match.group(1)]\n",
        "+            else:\n",
        "+                # Look for any sentence that looks like a research task\n",
        "+                sentences = [\n",
        "+                    s.strip() for s in content.split(\".\") if len(s.strip()) > 20\n",
        "+                ]\n",
        "+                if sentences:\n",
        "+                    # Filter for sentences that look like research tasks\n",
        "+                    research_like = [\n",
        "+                        s\n",
        "+                        for s in sentences\n",
        "+                        if any(\n",
        "+                            kw in s.lower()\n",
        "+                            for kw in [\n",
        "+                                \"research\",\n",
        "+                                \"investigate\",\n",
        "+                                \"analyze\",\n",
        "+                                \"examine\",\n",
        "+                                \"study\",\n",
        "+                                \"explore\",\n",
        "+                            ]\n",
        "+                        )\n",
        "+                    ]\n",
        "+                    if research_like:\n",
        "+                        tasks = research_like[: self.max_concurrent_research_units]\n",
        "+\n",
        "+        logger.info(f\"_extract_research_tasks found: {tasks}\")\n",
        "+        return tasks[: self.max_concurrent_research_units]\n",
        "+\n",
        "+    async def _conduct_research_tasks(\n",
        "+        self, tasks: List[str]\n",
        "+    ) -> List[ResearcherOutputState]:\n",
        "+        \"\"\"Conduct multiple research tasks concurrently.\"\"\"\n",
        "+        import asyncio\n",
        "+\n",
        "+        logger.info(f\"Starting research on {len(tasks)} tasks...\")\n",
        "+        research_results = await asyncio.gather(\n",
        "+            *[self._conduct_single_research(task) for task in tasks]\n",
        "+        )\n",
        "+        logger.info(f\"Completed research on {len(tasks)} tasks\")\n",
        "+        return research_results\n",
        "+\n",
        "+    async def _conduct_single_research(\n",
        "+        self, research_topic: str\n",
        "+    ) -> ResearcherOutputState:\n",
        "+        \"\"\"Conduct a single research task using a researcher agent.\"\"\"\n",
        "+        logger.info(f\"\ud83d\udd2c Researching: {research_topic[:100]}...\")\n",
        "+\n",
        "+        researcher_agent = await self._build_researcher_agent()\n",
        "+\n",
        "+        # Format system prompt\n",
        "+        system_prompt = research_system_prompt.format(\n",
        "+            date=self.get_today_str(),\n",
        "+        )\n",
        "+\n",
        "+        # CRITICAL: System prompt must be first, then the specific research task\n",
        "+        messages = [\n",
        "+            SystemMessage(content=system_prompt),\n",
        "+            HumanMessage(content=f\"Research this specific topic: {research_topic}\"),\n",
        "+        ]\n",
        "+\n",
        "+        result = await researcher_agent.ainvoke({\"messages\": messages})\n",
        "+\n",
        "+        # Extract and compress the research\n",
        "+        research_messages = result[\"messages\"]\n",
        "+        compressed_research = await self._compress_research(\n",
        "+            research_messages, research_topic\n",
        "+        )\n",
        "+\n",
        "+        logger.info(f\"\u2713 Completed research on: {research_topic[:50]}...\")\n",
        "+\n",
        "+        return ResearcherOutputState(\n",
        "+            research_topic=research_topic,\n",
        "+            research_outcome=compressed_research,\n",
        "+        )\n",
        "+\n",
        "+    async def _compress_research(self, messages: List, research_topic: str) -> str:\n",
        "+        \"\"\"Compress research findings into organized format.\"\"\"\n",
        "+        # Extract the actual research content\n",
        "+        research_content = \"\\n\".join(\n",
        "+            [msg.content for msg in messages if hasattr(msg, \"content\") and msg.content]\n",
        "+        )\n",
        "+\n",
        "+        compression_messages = [\n",
        "+            SystemMessage(content=compress_research_system_prompt),\n",
        "+            HumanMessage(\n",
        "+                content=compress_research_simple_human_message.format(\n",
        "+                    research_topic=research_topic,\n",
        "+                    research_content=research_content,\n",
        "+                )\n",
        "+            ),\n",
        "+        ]\n",
        "+\n",
        "+        response = await self.llm.ainvoke(compression_messages)\n",
        "+        return response.content\n",
        "+\n",
        "+    async def generate_final_report(self, state: AgentState) -> Dict:\n",
        "+        \"\"\"Generate the final research report.\"\"\"\n",
        "+        logger.info(\"\ud83d\udcdd Generating final report...\")\n",
        "+\n",
        "+        # Combine all research outcomes\n",
        "+        all_research = \"\\n\\n\".join(\n",
        "+            [\n",
        "+                f\"## {r.research_topic}\\n{r.research_outcome}\"\n",
        "+                for r in state[\"all_research_outcomes\"]\n",
        "+            ]\n",
        "+        )\n",
        "+\n",
        "+        messages = [\n",
        "+            SystemMessage(content=\"You are an expert report writer.\"),\n",
        "+            HumanMessage(\n",
        "+                content=final_report_generation_prompt.format(\n",
        "+                    research_question=state[\"research_question\"],\n",
        "+                    date=self.get_today_str(),\n",
        "+                    all_research=all_research,\n",
        "+                )\n",
        "+            ),\n",
        "+        ]\n",
        "+\n",
        "+        response = await self.llm.ainvoke(messages)\n",
        "+\n",
        "+        logger.info(\"\u2713 Final report generated\")\n",
        "+\n",
        "+        return {\n",
        "+            \"final_report\": response.content,\n",
        "+        }\n",
        "+\n",
        "+    def _build_graph(self) -> StateGraph:\n",
        "+        \"\"\"Build the LangGraph workflow.\"\"\"\n",
        "+        workflow = StateGraph(AgentState)\n",
        "+\n",
        "+        # Add nodes\n",
        "+        workflow.add_node(\"supervisor\", self.supervisor)\n",
        "+        workflow.add_node(\"conduct_research\", self._conduct_research_tasks_node)\n",
        "+        workflow.add_node(\"generate_report\", self.generate_final_report)\n",
        "+\n",
        "+        # Add edges\n",
        "+        workflow.add_edge(START, \"supervisor\")\n",
        "+\n",
        "+        workflow.add_conditional_edges(\n",
        "+            \"supervisor\",\n",
        "+            lambda state: (\n",
        "+                \"conduct_research\"\n",
        "+                if isinstance(state[\"next_action\"], ConductResearch)\n",
        "+                else \"generate_report\"\n",
        "+            ),\n",
        "+            {\n",
        "+                \"conduct_research\": \"conduct_research\",\n",
        "+                \"generate_report\": \"generate_report\",\n",
        "+            },\n",
        "+        )\n",
        "+\n",
        "+        workflow.add_edge(\"conduct_research\", \"supervisor\")\n",
        "+        workflow.add_edge(\"generate_report\", END)\n",
        "+\n",
        "+        # Compile with memory\n",
        "+        memory = MemorySaver()\n",
        "+        return workflow.compile(checkpointer=memory)\n",
        "+\n",
        "+    async def _conduct_research_tasks_node(self, state: AgentState) -> Dict:\n",
        "+        \"\"\"Node wrapper for conducting research tasks.\"\"\"\n",
        "+        if isinstance(state[\"next_action\"], ConductResearch):\n",
        "+            tasks = state[\"next_action\"].research_topics\n",
        "+            results = await self._conduct_research_tasks(tasks)\n",
        "+\n",
        "+            # Append to existing research\n",
        "+            all_outcomes = state.get(\"all_research_outcomes\", []) + results\n",
        "+\n",
        "+            return {\n",
        "+                \"all_research_outcomes\": all_outcomes,\n",
        "+            }\n",
        "+        return {}\n",
        "+\n",
        "+    async def conduct_research(\n",
        "+        self, query: str, project_id: str = \"default\"\n",
        "+    ) -> Dict[str, Any]:\n",
        "+        \"\"\"\n",
        "+        Conduct deep research on a query.\n",
        "+\n",
        "+        Args:\n",
        "+            query: The research question\n",
        "+            project_id: Project identifier for tracking\n",
        "+\n",
        "+        Returns:\n",
        "+            Dictionary with final_report and metadata\n",
        "+        \"\"\"\n",
        "+        logger.info(f\"\ud83d\ude80 Starting deep research on: {query}\")\n",
        "+\n",
        "+        initial_state = {\n",
        "+            \"research_question\": query,\n",
        "+            \"all_research_outcomes\": [],\n",
        "+            \"iteration\": 0,\n",
        "+            \"next_action\": None,\n",
        "+            \"final_report\": None,\n",
        "+        }\n",
        "+\n",
        "+        config = {\"configurable\": {\"thread_id\": project_id}}\n",
        "+\n",
        "+        result = await self.graph.ainvoke(initial_state, config)\n",
        "+\n",
        "+        logger.info(\n",
        "+            f\"\u2713 Research complete. Iterations: {result.get('iteration', 0)}, Outcomes: {len(result.get('all_research_outcomes', []))}\"\n",
        "+        )\n",
        "+\n",
        "+        return {\n",
        "+            \"final_report\": result.get(\"final_report\"),\n",
        "+            \"research_question\": query,\n",
        "+            \"iterations\": result.get(\"iteration\", 0),\n",
        "+            \"research_outcomes\": [\n",
        "+                {\n",
        "+                    \"topic\": r.research_topic,\n",
        "+                    \"outcome\": r.research_outcome,\n",
        "+                }\n",
        "+                for r in result.get(\"all_research_outcomes\", [])\n",
        "+            ],\n",
        "+        }\n"
      ]
    },
    {
      "path": "src/agents/deep_researcher/state.py",
      "status": "added",
      "additions": 57,
      "deletions": 0,
      "patch": "@@ -0,0 +1,57 @@\n+\"\"\"Graph state definitions and data structures for the Deep Research agent.\"\"\"\n+\n+from typing import Any, List\n+\n+from pydantic import BaseModel, Field\n+from typing_extensions import TypedDict\n+\n+###################\n+# Structured Outputs for Tool Calling\n+###################\n+\n+\n+class ConductResearch(BaseModel):\n+    \"\"\"Tool schema for delegating research to sub-researchers.\"\"\"\n+\n+    research_topics: List[str] = Field(\n+        description=\"List of specific topics to research. Each should be detailed and focused on a single aspect.\"\n+    )\n+\n+\n+class ResearchComplete(BaseModel):\n+    \"\"\"Tool schema to signal that research is complete.\"\"\"\n+\n+    pass  # No fields needed, just signals completion\n+\n+\n+###################\n+# State Definitions\n+###################\n+\n+\n+class AgentState(TypedDict):\n+    \"\"\"Main agent state for the complete research workflow.\"\"\"\n+\n+    research_question: str  # The research question/topic\n+    all_research_outcomes: List[Any]  # List of ResearcherOutputState objects\n+    iteration: int  # Number of research cycles completed\n+    next_action: Any  # ConductResearch or ResearchComplete\n+    final_report: str  # The final comprehensive report\n+\n+\n+class SupervisorState(TypedDict):\n+    \"\"\"State for the supervisor managing research tasks.\"\"\"\n+\n+    research_question: str  # The research question/topic\n+    all_research_outcomes: List[Any]  # List of ResearcherOutputState objects\n+    iteration: int  # Number of research cycles completed\n+    next_action: Any  # ConductResearch or ResearchComplete\n+\n+\n+class ResearcherOutputState(BaseModel):\n+    \"\"\"Output from a researcher agent after completing research.\"\"\"\n+\n+    research_topic: str = Field(description=\"The research topic that was investigated\")\n+    research_outcome: str = Field(\n+        description=\"Compressed and synthesized research findings\"\n+    )",
      "patch_lines": [
        "@@ -0,0 +1,57 @@\n",
        "+\"\"\"Graph state definitions and data structures for the Deep Research agent.\"\"\"\n",
        "+\n",
        "+from typing import Any, List\n",
        "+\n",
        "+from pydantic import BaseModel, Field\n",
        "+from typing_extensions import TypedDict\n",
        "+\n",
        "+###################\n",
        "+# Structured Outputs for Tool Calling\n",
        "+###################\n",
        "+\n",
        "+\n",
        "+class ConductResearch(BaseModel):\n",
        "+    \"\"\"Tool schema for delegating research to sub-researchers.\"\"\"\n",
        "+\n",
        "+    research_topics: List[str] = Field(\n",
        "+        description=\"List of specific topics to research. Each should be detailed and focused on a single aspect.\"\n",
        "+    )\n",
        "+\n",
        "+\n",
        "+class ResearchComplete(BaseModel):\n",
        "+    \"\"\"Tool schema to signal that research is complete.\"\"\"\n",
        "+\n",
        "+    pass  # No fields needed, just signals completion\n",
        "+\n",
        "+\n",
        "+###################\n",
        "+# State Definitions\n",
        "+###################\n",
        "+\n",
        "+\n",
        "+class AgentState(TypedDict):\n",
        "+    \"\"\"Main agent state for the complete research workflow.\"\"\"\n",
        "+\n",
        "+    research_question: str  # The research question/topic\n",
        "+    all_research_outcomes: List[Any]  # List of ResearcherOutputState objects\n",
        "+    iteration: int  # Number of research cycles completed\n",
        "+    next_action: Any  # ConductResearch or ResearchComplete\n",
        "+    final_report: str  # The final comprehensive report\n",
        "+\n",
        "+\n",
        "+class SupervisorState(TypedDict):\n",
        "+    \"\"\"State for the supervisor managing research tasks.\"\"\"\n",
        "+\n",
        "+    research_question: str  # The research question/topic\n",
        "+    all_research_outcomes: List[Any]  # List of ResearcherOutputState objects\n",
        "+    iteration: int  # Number of research cycles completed\n",
        "+    next_action: Any  # ConductResearch or ResearchComplete\n",
        "+\n",
        "+\n",
        "+class ResearcherOutputState(BaseModel):\n",
        "+    \"\"\"Output from a researcher agent after completing research.\"\"\"\n",
        "+\n",
        "+    research_topic: str = Field(description=\"The research topic that was investigated\")\n",
        "+    research_outcome: str = Field(\n",
        "+        description=\"Compressed and synthesized research findings\"\n",
        "+    )\n"
      ]
    },
    {
      "path": "src/agents/deep_researcher/tools.py",
      "status": "added",
      "additions": 76,
      "deletions": 0,
      "patch": "@@ -0,0 +1,76 @@\n+from langchain_core.tools import tool\n+from langchain_community.agent_toolkits import PlayWrightBrowserToolkit\n+from langchain_community.tools.playwright.utils import (\n+    create_async_playwright_browser,  # A synchronous browser is available, though it isn't compatible with jupyter.\\n\",\t  },\n+)\n+from tavily import TavilyClient\n+# Corrected, async-native utility functions\n+from playwright.async_api import async_playwright, Browser, Playwright\n+from helpers.config import get_settings  # \u2190 Add this import\n+settings = get_settings()  # \u2190 Load settings to access TAVILY_API_KEY\n+async def create_async_playwright_browser_fixed( headless: bool = True) -> Browser:\n+    \"\"\"\n+    A corrected async-native function to create a Playwright browser.\n+    This function is a proper coroutine and MUST be awaited.\n+    It assumes the Playwright process is already started and passed in.\n+    \"\"\"\n+    playwright_process: Playwright = await async_playwright().start()\n+    browser = await playwright_process.chromium.launch(headless=headless)\n+    return browser\n+\n+##########################\n+# Reflection Tool Utils\n+##########################\n+\n+@tool(description=\"Strategic reflection tool for research planning\")\n+def think_tool(reflection: str) -> str:\n+    \"\"\"Tool for strategic reflection on research progress and decision-making.\n+\n+    Use this tool after each search to analyze results and plan next steps systematically.\n+    This creates a deliberate pause in the research workflow for quality decision-making.\n+\n+    When to use:\n+    - After receiving search results: What key information did I find?\n+    - Before deciding next steps: Do I have enough to answer comprehensively?\n+    - When assessing research gaps: What specific information am I still missing?\n+    - Before concluding research: Can I provide a complete answer now?\n+\n+    Reflection should address:\n+    1. Analysis of current findings - What concrete information have I gathered?\n+    2. Gap assessment - What crucial information is still missing?\n+    3. Quality evaluation - Do I have sufficient evidence/examples for a good answer?\n+    4. Strategic decision - Should I continue searching or provide my answer?\n+\n+    Args:\n+        reflection: Your detailed reflection on research progress, findings, gaps, and next steps\n+\n+    Returns:\n+        Confirmation that reflection was recorded for decision-making\n+    \"\"\"\n+    return f\"Reflection recorded: {reflection}\"\n+\n+async def get_browser_toolkit():\n+    \"\"\"Initialize and return the Playwright browser toolkit for web scraping.\n+    \n+    This function is now async and should be called only when needed,\n+    not during initialization to avoid event loop conflicts.\n+    \"\"\"\n+    browser = await create_async_playwright_browser_fixed()\n+    toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=browser)\n+    return toolkit.get_tools()\n+\n+\n+search_client = TavilyClient(\n+    api_key=settings.TAVILY_API_KEY\n+\n+)\n+@tool(\n+    description=\"Search the web for relevant information using Tavily. \")\n+def tavily_search(query: str) -> str:\n+    \"\"\"Use Tavily to search the web and return relevant information.\n+\n+    Args:\n+        query: The search query string.\n+    \"\"\"\n+    response = search_client.search(query)\n+    return response\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -0,0 +1,76 @@\n",
        "+from langchain_core.tools import tool\n",
        "+from langchain_community.agent_toolkits import PlayWrightBrowserToolkit\n",
        "+from langchain_community.tools.playwright.utils import (\n",
        "+    create_async_playwright_browser,  # A synchronous browser is available, though it isn't compatible with jupyter.\\n\",\t  },\n",
        "+)\n",
        "+from tavily import TavilyClient\n",
        "+# Corrected, async-native utility functions\n",
        "+from playwright.async_api import async_playwright, Browser, Playwright\n",
        "+from helpers.config import get_settings  # \u2190 Add this import\n",
        "+settings = get_settings()  # \u2190 Load settings to access TAVILY_API_KEY\n",
        "+async def create_async_playwright_browser_fixed( headless: bool = True) -> Browser:\n",
        "+    \"\"\"\n",
        "+    A corrected async-native function to create a Playwright browser.\n",
        "+    This function is a proper coroutine and MUST be awaited.\n",
        "+    It assumes the Playwright process is already started and passed in.\n",
        "+    \"\"\"\n",
        "+    playwright_process: Playwright = await async_playwright().start()\n",
        "+    browser = await playwright_process.chromium.launch(headless=headless)\n",
        "+    return browser\n",
        "+\n",
        "+##########################\n",
        "+# Reflection Tool Utils\n",
        "+##########################\n",
        "+\n",
        "+@tool(description=\"Strategic reflection tool for research planning\")\n",
        "+def think_tool(reflection: str) -> str:\n",
        "+    \"\"\"Tool for strategic reflection on research progress and decision-making.\n",
        "+\n",
        "+    Use this tool after each search to analyze results and plan next steps systematically.\n",
        "+    This creates a deliberate pause in the research workflow for quality decision-making.\n",
        "+\n",
        "+    When to use:\n",
        "+    - After receiving search results: What key information did I find?\n",
        "+    - Before deciding next steps: Do I have enough to answer comprehensively?\n",
        "+    - When assessing research gaps: What specific information am I still missing?\n",
        "+    - Before concluding research: Can I provide a complete answer now?\n",
        "+\n",
        "+    Reflection should address:\n",
        "+    1. Analysis of current findings - What concrete information have I gathered?\n",
        "+    2. Gap assessment - What crucial information is still missing?\n",
        "+    3. Quality evaluation - Do I have sufficient evidence/examples for a good answer?\n",
        "+    4. Strategic decision - Should I continue searching or provide my answer?\n",
        "+\n",
        "+    Args:\n",
        "+        reflection: Your detailed reflection on research progress, findings, gaps, and next steps\n",
        "+\n",
        "+    Returns:\n",
        "+        Confirmation that reflection was recorded for decision-making\n",
        "+    \"\"\"\n",
        "+    return f\"Reflection recorded: {reflection}\"\n",
        "+\n",
        "+async def get_browser_toolkit():\n",
        "+    \"\"\"Initialize and return the Playwright browser toolkit for web scraping.\n",
        "+    \n",
        "+    This function is now async and should be called only when needed,\n",
        "+    not during initialization to avoid event loop conflicts.\n",
        "+    \"\"\"\n",
        "+    browser = await create_async_playwright_browser_fixed()\n",
        "+    toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=browser)\n",
        "+    return toolkit.get_tools()\n",
        "+\n",
        "+\n",
        "+search_client = TavilyClient(\n",
        "+    api_key=settings.TAVILY_API_KEY\n",
        "+\n",
        "+)\n",
        "+@tool(\n",
        "+    description=\"Search the web for relevant information using Tavily. \")\n",
        "+def tavily_search(query: str) -> str:\n",
        "+    \"\"\"Use Tavily to search the web and return relevant information.\n",
        "+\n",
        "+    Args:\n",
        "+        query: The search query string.\n",
        "+    \"\"\"\n",
        "+    response = search_client.search(query)\n",
        "+    return response\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "src/controllers/NLPController.py",
      "status": "modified",
      "additions": 6,
      "deletions": 0,
      "patch": "@@ -6,6 +6,7 @@\n from stores.llm.LLMEnums import DocumentTypeEnum\n \n from .BaseController import BaseController\n+from agents.deep_researcher import DeepResearch\n \n \n class NLPController(BaseController):\n@@ -18,6 +19,7 @@ def __init__(\n         self.generation_client = generation_client\n         self.embedding_client = embedding_client\n         self.template_parser = template_parser\n+        self.deep_researcher = DeepResearch(generation_client)\n         self.logger = logging.getLogger(\"uvicorn\")\n \n     def create_collection_name(self, project_id: str):\n@@ -283,3 +285,7 @@ async def chat(self, project: Project, query: str):\n             },\n         )\n         return answer, history\n+    \n+    async def deep_research(self, project: Project, query: str):\n+        result = await self.deep_researcher.conduct_research(query, project_id=str(project.project_id))\n+        return result",
      "patch_lines": [
        "@@ -6,6 +6,7 @@\n",
        " from stores.llm.LLMEnums import DocumentTypeEnum\n",
        " \n",
        " from .BaseController import BaseController\n",
        "+from agents.deep_researcher import DeepResearch\n",
        " \n",
        " \n",
        " class NLPController(BaseController):\n",
        "@@ -18,6 +19,7 @@ def __init__(\n",
        "         self.generation_client = generation_client\n",
        "         self.embedding_client = embedding_client\n",
        "         self.template_parser = template_parser\n",
        "+        self.deep_researcher = DeepResearch(generation_client)\n",
        "         self.logger = logging.getLogger(\"uvicorn\")\n",
        " \n",
        "     def create_collection_name(self, project_id: str):\n",
        "@@ -283,3 +285,7 @@ async def chat(self, project: Project, query: str):\n",
        "             },\n",
        "         )\n",
        "         return answer, history\n",
        "+    \n",
        "+    async def deep_research(self, project: Project, query: str):\n",
        "+        result = await self.deep_researcher.conduct_research(query, project_id=str(project.project_id))\n",
        "+        return result\n"
      ]
    },
    {
      "path": "src/helpers/config.py",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "patch": "@@ -39,7 +39,7 @@ class Settings(BaseSettings):\n     VECTOR_DB_DISTANCE_METHOD_LITERAL: List[str] = None\n     VECTOR_DB_PGVEC_INDEX_THRESHOLD: int = 100  # Threshold to create index on pgvector vector column\n \n-\n+    TAVILY_API_KEY: str = None\n     DEFAULT_LANG: str = \"en\"  # Default language for document processing\n     PRIMARY_LANG: str = \"en\"  # Primary language for template parsing\n     class Config:",
      "patch_lines": [
        "@@ -39,7 +39,7 @@ class Settings(BaseSettings):\n",
        "     VECTOR_DB_DISTANCE_METHOD_LITERAL: List[str] = None\n",
        "     VECTOR_DB_PGVEC_INDEX_THRESHOLD: int = 100  # Threshold to create index on pgvector vector column\n",
        " \n",
        "-\n",
        "+    TAVILY_API_KEY: str = None\n",
        "     DEFAULT_LANG: str = \"en\"  # Default language for document processing\n",
        "     PRIMARY_LANG: str = \"en\"  # Primary language for template parsing\n",
        "     class Config:\n"
      ]
    },
    {
      "path": "src/main.py",
      "status": "modified",
      "additions": 14,
      "deletions": 2,
      "patch": "@@ -1,4 +1,5 @@\n from fastapi import FastAPI\n+from fastapi.middleware.cors import CORSMiddleware\n from fastapi.responses import FileResponse\n from fastapi.staticfiles import StaticFiles\n from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine\n@@ -10,16 +11,25 @@\n from stores.llm.templates import TemplateParser\n from stores.vectordb import VectorDBProviderFactory\n from utils.metrics import setup_metrics\n+\n app = FastAPI()\n \n+# Add CORS middleware\n+app.add_middleware(\n+    CORSMiddleware,\n+    allow_origins=[\"*\"],  # In production, specify exact origins\n+    allow_credentials=True,\n+    allow_methods=[\"*\"],\n+    allow_headers=[\"*\"],\n+)\n+\n # Mount static files\n app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n app.mount(\"/assets\", StaticFiles(directory=\"assets\"), name=\"assets\")\n \n setup_metrics(app)\n \n \n-\n # Main page route\n @app.get(\"/\")\n async def main_page():\n@@ -36,7 +46,9 @@ async def startup_span():\n     )\n     print(\"Connected to the PostgreSQL database!\")\n     llm_provider_factory = LLMProviderFactory(config=settings)\n-    vector_db_provider_factory = VectorDBProviderFactory(config=settings, db_client=app.db_client)\n+    vector_db_provider_factory = VectorDBProviderFactory(\n+        config=settings, db_client=app.db_client\n+    )\n \n     # Clients\n     app.generation_client = llm_provider_factory.create(settings.GENERATION_BACKEND)",
      "patch_lines": [
        "@@ -1,4 +1,5 @@\n",
        " from fastapi import FastAPI\n",
        "+from fastapi.middleware.cors import CORSMiddleware\n",
        " from fastapi.responses import FileResponse\n",
        " from fastapi.staticfiles import StaticFiles\n",
        " from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine\n",
        "@@ -10,16 +11,25 @@\n",
        " from stores.llm.templates import TemplateParser\n",
        " from stores.vectordb import VectorDBProviderFactory\n",
        " from utils.metrics import setup_metrics\n",
        "+\n",
        " app = FastAPI()\n",
        " \n",
        "+# Add CORS middleware\n",
        "+app.add_middleware(\n",
        "+    CORSMiddleware,\n",
        "+    allow_origins=[\"*\"],  # In production, specify exact origins\n",
        "+    allow_credentials=True,\n",
        "+    allow_methods=[\"*\"],\n",
        "+    allow_headers=[\"*\"],\n",
        "+)\n",
        "+\n",
        " # Mount static files\n",
        " app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n",
        " app.mount(\"/assets\", StaticFiles(directory=\"assets\"), name=\"assets\")\n",
        " \n",
        " setup_metrics(app)\n",
        " \n",
        " \n",
        "-\n",
        " # Main page route\n",
        " @app.get(\"/\")\n",
        " async def main_page():\n",
        "@@ -36,7 +46,9 @@ async def startup_span():\n",
        "     )\n",
        "     print(\"Connected to the PostgreSQL database!\")\n",
        "     llm_provider_factory = LLMProviderFactory(config=settings)\n",
        "-    vector_db_provider_factory = VectorDBProviderFactory(config=settings, db_client=app.db_client)\n",
        "+    vector_db_provider_factory = VectorDBProviderFactory(\n",
        "+        config=settings, db_client=app.db_client\n",
        "+    )\n",
        " \n",
        "     # Clients\n",
        "     app.generation_client = llm_provider_factory.create(settings.GENERATION_BACKEND)\n"
      ]
    },
    {
      "path": "src/models/db_schemas/minirag/alembic/versions/4943547c1891_add_chat_history.py",
      "status": "added",
      "additions": 32,
      "deletions": 0,
      "patch": "@@ -0,0 +1,32 @@\n+\"\"\"Add chat history\n+\n+Revision ID: 4943547c1891\n+Revises: d341618b6559\n+Create Date: 2025-09-24 23:34:46.380507\n+\n+\"\"\"\n+from typing import Sequence, Union\n+\n+from alembic import op\n+import sqlalchemy as sa\n+from sqlalchemy.dialects import postgresql\n+\n+# revision identifiers, used by Alembic.\n+revision: str = '4943547c1891'\n+down_revision: Union[str, Sequence[str], None] = 'd341618b6559'\n+branch_labels: Union[str, Sequence[str], None] = None\n+depends_on: Union[str, Sequence[str], None] = None\n+\n+\n+def upgrade() -> None:\n+    \"\"\"Upgrade schema.\"\"\"\n+    # ### commands auto generated by Alembic - please adjust! ###\n+    op.add_column('projects', sa.Column('chat_history', postgresql.JSONB(astext_type=sa.Text()), nullable=True))\n+    # ### end Alembic commands ###\n+\n+\n+def downgrade() -> None:\n+    \"\"\"Downgrade schema.\"\"\"\n+    # ### commands auto generated by Alembic - please adjust! ###\n+    op.drop_column('projects', 'chat_history')\n+    # ### end Alembic commands ###",
      "patch_lines": [
        "@@ -0,0 +1,32 @@\n",
        "+\"\"\"Add chat history\n",
        "+\n",
        "+Revision ID: 4943547c1891\n",
        "+Revises: d341618b6559\n",
        "+Create Date: 2025-09-24 23:34:46.380507\n",
        "+\n",
        "+\"\"\"\n",
        "+from typing import Sequence, Union\n",
        "+\n",
        "+from alembic import op\n",
        "+import sqlalchemy as sa\n",
        "+from sqlalchemy.dialects import postgresql\n",
        "+\n",
        "+# revision identifiers, used by Alembic.\n",
        "+revision: str = '4943547c1891'\n",
        "+down_revision: Union[str, Sequence[str], None] = 'd341618b6559'\n",
        "+branch_labels: Union[str, Sequence[str], None] = None\n",
        "+depends_on: Union[str, Sequence[str], None] = None\n",
        "+\n",
        "+\n",
        "+def upgrade() -> None:\n",
        "+    \"\"\"Upgrade schema.\"\"\"\n",
        "+    # ### commands auto generated by Alembic - please adjust! ###\n",
        "+    op.add_column('projects', sa.Column('chat_history', postgresql.JSONB(astext_type=sa.Text()), nullable=True))\n",
        "+    # ### end Alembic commands ###\n",
        "+\n",
        "+\n",
        "+def downgrade() -> None:\n",
        "+    \"\"\"Downgrade schema.\"\"\"\n",
        "+    # ### commands auto generated by Alembic - please adjust! ###\n",
        "+    op.drop_column('projects', 'chat_history')\n",
        "+    # ### end Alembic commands ###\n"
      ]
    },
    {
      "path": "src/models/enums/ResponseEnums.py",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "patch": "@@ -21,4 +21,6 @@ class ResponseSignal(Enum):\n     RAG_ANSWER_ERROR = \"rag_answer_error\"\n     RAG_ANSWER_SUCCESS = \"rag_answer_success\"\n     CHAT_ANSWER_ERROR = \"chat_answer_error\"\n-    CHAT_ANSWER_SUCCESS = \"chat_answer_success\"\n\\ No newline at end of file\n+    CHAT_ANSWER_SUCCESS = \"chat_answer_success\"\n+    DEEP_RESEARCH_ERROR = \"deep_research_error\"\n+    DEEP_RESEARCH_SUCCESS = \"deep_research_success\"\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -21,4 +21,6 @@ class ResponseSignal(Enum):\n",
        "     RAG_ANSWER_ERROR = \"rag_answer_error\"\n",
        "     RAG_ANSWER_SUCCESS = \"rag_answer_success\"\n",
        "     CHAT_ANSWER_ERROR = \"chat_answer_error\"\n",
        "-    CHAT_ANSWER_SUCCESS = \"chat_answer_success\"\n",
        "\\ No newline at end of file\n",
        "+    CHAT_ANSWER_SUCCESS = \"chat_answer_success\"\n",
        "+    DEEP_RESEARCH_ERROR = \"deep_research_error\"\n",
        "+    DEEP_RESEARCH_SUCCESS = \"deep_research_success\"\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "src/requirments.txt",
      "status": "modified",
      "additions": 12,
      "deletions": 2,
      "patch": "@@ -19,12 +19,22 @@ alembic==1.16.5\n psycopg2==2.9.10\n pgvector==0.4.1\n nltk==3.9.1\n-\n+langgraph==0.6.7\n+langchain-google-genai==2.1.12\n+langchain-openai==0.3.34\n+langchain-core==0.3.77\n # Monitoring and metrics\n \n prometheus-client==0.23.1\n starlette-exporter==0.23.0\n \n # Health checks\n \n-fastapi-health==0.4.0\n\\ No newline at end of file\n+fastapi-health==0.4.0\n+\n+# web scraping & search\n+playwright==1.55.0\n+lxml==6.0.2\n+ddgs==9.6.0\n+beautifulsoup4==4.14.2\n+tavily-python==0.7.12\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -19,12 +19,22 @@ alembic==1.16.5\n",
        " psycopg2==2.9.10\n",
        " pgvector==0.4.1\n",
        " nltk==3.9.1\n",
        "-\n",
        "+langgraph==0.6.7\n",
        "+langchain-google-genai==2.1.12\n",
        "+langchain-openai==0.3.34\n",
        "+langchain-core==0.3.77\n",
        " # Monitoring and metrics\n",
        " \n",
        " prometheus-client==0.23.1\n",
        " starlette-exporter==0.23.0\n",
        " \n",
        " # Health checks\n",
        " \n",
        "-fastapi-health==0.4.0\n",
        "\\ No newline at end of file\n",
        "+fastapi-health==0.4.0\n",
        "+\n",
        "+# web scraping & search\n",
        "+playwright==1.55.0\n",
        "+lxml==6.0.2\n",
        "+ddgs==9.6.0\n",
        "+beautifulsoup4==4.14.2\n",
        "+tavily-python==0.7.12\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "src/routes/nlp.py",
      "status": "modified",
      "additions": 38,
      "deletions": 0,
      "patch": "@@ -219,4 +219,42 @@ async def chat(request: Request, project_id: int, chat_request: ChatRequest):\n             \"answer\": answer,  \n             \"chat_history\": chat_history,\n         }\n+    )\n+\n+@nlp_router.post(\"/index/deep-research/{project_id}\")\n+async def deep_research(request: Request, project_id: int, chat_request: ChatRequest):\n+    project_model = await ProjectModel.create_instance(db_client=request.app.db_client)\n+    project = await project_model.get_project_or_create_one(project_id=project_id)\n+\n+    if not project:\n+        return JSONResponse(\n+            status_code=status.HTTP_400_BAD_REQUEST,\n+            content={\"message\": ResponseSignal.PROJECT_NOT_FOUND.value},\n+        )\n+\n+    nlp_controller = NLPController(\n+        vectordb_client=request.app.vector_db_client,\n+        generation_client=request.app.generation_client,\n+        embedding_client=request.app.embedding_client,\n+        template_parser=request.app.template_parser,\n+    )\n+\n+    result = await nlp_controller.deep_research(\n+        project=project,\n+        query=chat_request.query,\n+    )\n+    if result is None:\n+        return JSONResponse(\n+            status_code=status.HTTP_400_BAD_REQUEST,\n+            content={\"message\": ResponseSignal.DEEP_RESEARCH_ERROR.value},\n+        )\n+\n+    # result= result.get(\"final_report\", \"\")\n+\n+    return JSONResponse(\n+        content={\n+            \"message\": ResponseSignal.DEEP_RESEARCH_SUCCESS.value,\n+            \"result\": result,\n+            \"project_id\": project.project_id,\n+        }\n     )\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -219,4 +219,42 @@ async def chat(request: Request, project_id: int, chat_request: ChatRequest):\n",
        "             \"answer\": answer,  \n",
        "             \"chat_history\": chat_history,\n",
        "         }\n",
        "+    )\n",
        "+\n",
        "+@nlp_router.post(\"/index/deep-research/{project_id}\")\n",
        "+async def deep_research(request: Request, project_id: int, chat_request: ChatRequest):\n",
        "+    project_model = await ProjectModel.create_instance(db_client=request.app.db_client)\n",
        "+    project = await project_model.get_project_or_create_one(project_id=project_id)\n",
        "+\n",
        "+    if not project:\n",
        "+        return JSONResponse(\n",
        "+            status_code=status.HTTP_400_BAD_REQUEST,\n",
        "+            content={\"message\": ResponseSignal.PROJECT_NOT_FOUND.value},\n",
        "+        )\n",
        "+\n",
        "+    nlp_controller = NLPController(\n",
        "+        vectordb_client=request.app.vector_db_client,\n",
        "+        generation_client=request.app.generation_client,\n",
        "+        embedding_client=request.app.embedding_client,\n",
        "+        template_parser=request.app.template_parser,\n",
        "+    )\n",
        "+\n",
        "+    result = await nlp_controller.deep_research(\n",
        "+        project=project,\n",
        "+        query=chat_request.query,\n",
        "+    )\n",
        "+    if result is None:\n",
        "+        return JSONResponse(\n",
        "+            status_code=status.HTTP_400_BAD_REQUEST,\n",
        "+            content={\"message\": ResponseSignal.DEEP_RESEARCH_ERROR.value},\n",
        "+        )\n",
        "+\n",
        "+    # result= result.get(\"final_report\", \"\")\n",
        "+\n",
        "+    return JSONResponse(\n",
        "+        content={\n",
        "+            \"message\": ResponseSignal.DEEP_RESEARCH_SUCCESS.value,\n",
        "+            \"result\": result,\n",
        "+            \"project_id\": project.project_id,\n",
        "+        }\n",
        "     )\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "src/static/index.html",
      "status": "modified",
      "additions": 567,
      "deletions": 16,
      "patch": "@@ -304,6 +304,117 @@\n             overflow-wrap: break-word;\n         }\n \n+        /* Markdown styling */\n+        .markdown-content {\n+            line-height: 1.6;\n+        }\n+\n+        .markdown-content h1,\n+        .markdown-content h2,\n+        .markdown-content h3,\n+        .markdown-content h4,\n+        .markdown-content h5,\n+        .markdown-content h6 {\n+            margin: 16px 0 8px 0;\n+            font-weight: 600;\n+            color: var(--text-primary);\n+        }\n+\n+        .markdown-content h1 { font-size: 2em; border-bottom: 2px solid var(--border-color); padding-bottom: 8px; }\n+        .markdown-content h2 { font-size: 1.5em; border-bottom: 1px solid var(--border-color); padding-bottom: 6px; }\n+        .markdown-content h3 { font-size: 1.25em; }\n+        .markdown-content h4 { font-size: 1.1em; }\n+        .markdown-content h5 { font-size: 1em; }\n+        .markdown-content h6 { font-size: 0.9em; color: var(--text-secondary); }\n+\n+        .markdown-content p {\n+            margin: 8px 0;\n+        }\n+\n+        .markdown-content ul,\n+        .markdown-content ol {\n+            margin: 8px 0;\n+            padding-left: 24px;\n+        }\n+\n+        .markdown-content li {\n+            margin: 4px 0;\n+        }\n+\n+        .markdown-content strong {\n+            font-weight: 600;\n+            color: var(--text-primary);\n+        }\n+\n+        .markdown-content em {\n+            font-style: italic;\n+        }\n+\n+        .markdown-content a {\n+            color: var(--user-bg);\n+            text-decoration: none;\n+            border-bottom: 1px solid transparent;\n+            transition: border-color 0.2s;\n+        }\n+\n+        .markdown-content a:hover {\n+            border-bottom-color: var(--user-bg);\n+        }\n+\n+        .markdown-content blockquote {\n+            border-left: 4px solid var(--border-color);\n+            padding-left: 16px;\n+            margin: 8px 0;\n+            color: var(--text-secondary);\n+            font-style: italic;\n+        }\n+\n+        .markdown-content hr {\n+            border: none;\n+            border-top: 1px solid var(--border-color);\n+            margin: 16px 0;\n+        }\n+\n+        .markdown-content table {\n+            border-collapse: collapse;\n+            width: 100%;\n+            margin: 8px 0;\n+        }\n+\n+        .markdown-content table th,\n+        .markdown-content table td {\n+            border: 1px solid var(--border-color);\n+            padding: 8px 12px;\n+            text-align: left;\n+        }\n+\n+        .markdown-content table th {\n+            background: var(--bg-primary);\n+            font-weight: 600;\n+        }\n+\n+        .markdown-content code {\n+            background: var(--bg-primary);\n+            padding: 2px 6px;\n+            border-radius: 3px;\n+            font-family: 'SF Mono', Monaco, 'Cascadia Code', 'Roboto Mono', Consolas, monospace;\n+            font-size: 0.9em;\n+            color: var(--text-primary);\n+        }\n+\n+        .markdown-content pre {\n+            background: var(--bg-primary);\n+            padding: 12px;\n+            border-radius: 6px;\n+            overflow-x: auto;\n+            margin: 8px 0;\n+        }\n+\n+        .markdown-content pre code {\n+            background: none;\n+            padding: 0;\n+        }\n+\n         .typing-indicator {\n             display: none;\n             align-items: center;\n@@ -481,6 +592,107 @@\n             font-size: 14px;\n         }\n \n+        .deep-research-loading {\n+            display: flex;\n+            gap: 12px;\n+            padding: 20px;\n+            background: var(--bg-secondary);\n+            border-radius: 12px;\n+            border: 2px solid var(--assistant-bg);\n+            margin: 10px 0;\n+            animation: pulse 2s ease-in-out infinite;\n+        }\n+\n+        .research-status {\n+            flex: 1;\n+        }\n+\n+        .research-title {\n+            font-size: 16px;\n+            font-weight: 600;\n+            color: var(--text-primary);\n+            margin-bottom: 12px;\n+        }\n+\n+        .research-steps {\n+            display: flex;\n+            flex-direction: column;\n+            gap: 8px;\n+            margin-bottom: 12px;\n+        }\n+\n+        .research-step {\n+            font-size: 14px;\n+            color: var(--text-secondary);\n+            padding: 6px 12px;\n+            border-radius: 6px;\n+            background: var(--bg-primary);\n+            transition: all 0.3s ease;\n+            opacity: 0.5;\n+        }\n+\n+        .research-step.active {\n+            opacity: 1;\n+            background: var(--info-bg);\n+            color: var(--info-text);\n+            border-left: 3px solid var(--info-border);\n+        }\n+\n+        .research-progress {\n+            height: 4px;\n+            background: var(--bg-primary);\n+            border-radius: 2px;\n+            overflow: hidden;\n+        }\n+\n+        .progress-bar {\n+            height: 100%;\n+            background: linear-gradient(90deg, var(--user-bg), var(--assistant-bg));\n+            animation: progressAnimation 2s ease-in-out infinite;\n+        }\n+\n+        @keyframes pulse {\n+            0%, 100% { opacity: 1; }\n+            50% { opacity: 0.95; }\n+        }\n+\n+        @keyframes progressAnimation {\n+            0% { width: 0%; }\n+            50% { width: 70%; }\n+            100% { width: 100%; }\n+        }\n+\n+        .deep-research-btn {\n+            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n+            border: none;\n+            border-radius: 6px;\n+            padding: 8px;\n+            cursor: pointer;\n+            display: flex;\n+            align-items: center;\n+            justify-content: center;\n+            transition: all 0.2s;\n+            color: white;\n+            font-weight: 600;\n+            gap: 6px;\n+        }\n+\n+        .deep-research-btn:hover:not(:disabled) {\n+            transform: translateY(-1px);\n+            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);\n+        }\n+\n+        .deep-research-btn:disabled {\n+            background: var(--disabled-bg);\n+            cursor: not-allowed;\n+            transform: none;\n+        }\n+\n+        .deep-research-btn svg {\n+            width: 16px;\n+            height: 16px;\n+        }\n+\n         @media (max-width: 768px) {\n             .chat-container {\n                 padding: 0 12px;\n@@ -542,7 +754,8 @@ <h1>RAG Chat</h1>\n         <div class=\"chat-messages\" id=\"chatMessages\">\n             <div class=\"welcome-message\">\n                 <h2>\ud83d\udc4b Welcome to RAG Chat</h2>\n-                <p>Upload your documents and start asking questions!</p>\n+                <p>Upload documents for RAG-powered Q&A, or just chat directly!</p>\n+                <p style=\"margin-top: 8px; font-size: 14px;\">\ud83d\udca1 Use the <strong>\ud83d\udd2c Deep Research</strong> button for comprehensive web-based research on any topic.</p>\n             </div>\n         </div>\n \n@@ -562,24 +775,33 @@ <h2>\ud83d\udc4b Welcome to RAG Chat</h2>\n                 <button class=\"close-btn\" onclick=\"clearFile()\">\u00d7</button>\n             </div>\n             \n-            <div class=\"input-container\">\n+                        <div class=\"input-container\">\n                 <button class=\"file-upload-btn\" onclick=\"document.getElementById('fileInput').click()\" title=\"Upload file\">\n-                    <svg viewBox=\"0 0 24 24\" fill=\"currentColor\">\n-                        <path d=\"M12 4V2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm5 11h-4v4h-2v-4H7v-2h4V9h2v4h4v2z\"/>\n+                    <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\">\n+                        <path d=\"M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4\"></path>\n+                        <polyline points=\"17 8 12 3 7 8\"></polyline>\n+                        <line x1=\"12\" y1=\"3\" x2=\"12\" y2=\"15\"></line>\n                     </svg>\n                 </button>\n                 \n                 <textarea \n                     id=\"messageInput\" \n-                    placeholder=\"Ask anything about your documents...\" \n+                    placeholder=\"Ask a question or type a message...\" \n                     rows=\"1\"\n                     onkeydown=\"handleKeyDown(event)\"\n                     oninput=\"autoResize(this)\"\n                 ></textarea>\n                 \n+                <button class=\"deep-research-btn\" id=\"deepResearchBtn\" onclick=\"deepResearch()\" disabled title=\"Deep Research: Comprehensive analysis with web search\">\n+                    <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\">\n+                        <circle cx=\"11\" cy=\"11\" r=\"8\"></circle>\n+                        <path d=\"m21 21-4.35-4.35\"></path>\n+                    </svg>\n+                </button>\n+                \n                 <button class=\"send-btn\" id=\"sendBtn\" onclick=\"sendMessage()\" disabled>\n-                    <svg viewBox=\"0 0 24 24\" fill=\"currentColor\">\n-                        <path d=\"M2.01 21L23 12 2.01 3 2 10l15 2-15 2z\"/>\n+                    <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" fill=\"currentColor\">\n+                        <path d=\"M2.01 21L23 12 2.01 3 2 10l15 2-15 2z\"></path>\n                     </svg>\n                 </button>\n             </div>\n@@ -594,6 +816,7 @@ <h2>\ud83d\udc4b Welcome to RAG Chat</h2>\n         let isProcessing = false;\n         let sessionId = null;\n         let projectId = null;\n+        let hasUploadedFiles = false; // Track if files have been uploaded\n \n         // Generate or retrieve session ID and project ID\n         function initSession() {\n@@ -657,6 +880,130 @@ <h2>\ud83d\udc4b Welcome to RAG Chat</h2>\n             }, 2000);\n         }\n \n+        // Enhanced markdown parser\n+        function parseMarkdown(text) {\n+            let html = text;\n+            \n+            // Escape HTML but preserve newlines\n+            html = html.replace(/&/g, '&amp;')\n+                      .replace(/</g, '&lt;')\n+                      .replace(/>/g, '&gt;');\n+            \n+            // Process line by line for better control\n+            const lines = html.split('\\n');\n+            const processedLines = [];\n+            let inList = false;\n+            let listType = null;\n+            \n+            for (let i = 0; i < lines.length; i++) {\n+                let line = lines[i];\n+                \n+                // Headers\n+                if (line.match(/^######\\s+(.+)$/)) {\n+                    line = line.replace(/^######\\s+(.+)$/, '<h6>$1</h6>');\n+                } else if (line.match(/^#####\\s+(.+)$/)) {\n+                    line = line.replace(/^#####\\s+(.+)$/, '<h5>$1</h5>');\n+                } else if (line.match(/^####\\s+(.+)$/)) {\n+                    line = line.replace(/^####\\s+(.+)$/, '<h4>$1</h4>');\n+                } else if (line.match(/^###\\s+(.+)$/)) {\n+                    line = line.replace(/^###\\s+(.+)$/, '<h3>$1</h3>');\n+                } else if (line.match(/^##\\s+(.+)$/)) {\n+                    line = line.replace(/^##\\s+(.+)$/, '<h2>$1</h2>');\n+                } else if (line.match(/^#\\s+(.+)$/)) {\n+                    line = line.replace(/^#\\s+(.+)$/, '<h1>$1</h1>');\n+                }\n+                // Unordered lists\n+                else if (line.match(/^[\\*\\-]\\s+(.+)$/)) {\n+                    if (!inList || listType !== 'ul') {\n+                        if (inList && listType === 'ol') {\n+                            processedLines.push('</ol>');\n+                        }\n+                        processedLines.push('<ul>');\n+                        inList = true;\n+                        listType = 'ul';\n+                    }\n+                    line = line.replace(/^[\\*\\-]\\s+(.+)$/, '<li>$1</li>');\n+                }\n+                // Ordered lists\n+                else if (line.match(/^\\d+\\.\\s+(.+)$/)) {\n+                    if (!inList || listType !== 'ol') {\n+                        if (inList && listType === 'ul') {\n+                            processedLines.push('</ul>');\n+                        }\n+                        processedLines.push('<ol>');\n+                        inList = true;\n+                        listType = 'ol';\n+                    }\n+                    line = line.replace(/^\\d+\\.\\s+(.+)$/, '<li>$1</li>');\n+                }\n+                // Blockquotes\n+                else if (line.match(/^&gt;\\s+(.+)$/)) {\n+                    line = line.replace(/^&gt;\\s+(.+)$/, '<blockquote>$1</blockquote>');\n+                }\n+                // Horizontal rules\n+                else if (line.match(/^(---|\\*\\*\\*|___)\\s*$/)) {\n+                    line = '<hr>';\n+                }\n+                // End list if we hit a non-list line\n+                else if (inList && line.trim() !== '') {\n+                    if (listType === 'ul') {\n+                        processedLines.push('</ul>');\n+                    } else {\n+                        processedLines.push('</ol>');\n+                    }\n+                    inList = false;\n+                    listType = null;\n+                }\n+                \n+                processedLines.push(line);\n+            }\n+            \n+            // Close any open lists\n+            if (inList) {\n+                if (listType === 'ul') {\n+                    processedLines.push('</ul>');\n+                } else {\n+                    processedLines.push('</ol>');\n+                }\n+            }\n+            \n+            html = processedLines.join('\\n');\n+            \n+            // Inline formatting (bold, italic, code, links)\n+            // Bold (must come before italic)\n+            html = html.replace(/\\*\\*(.+?)\\*\\*/g, '<strong>$1</strong>');\n+            html = html.replace(/__(.+?)__/g, '<strong>$1</strong>');\n+            \n+            // Italic\n+            html = html.replace(/\\*([^\\*]+?)\\*/g, '<em>$1</em>');\n+            html = html.replace(/_([^_]+?)_/g, '<em>$1</em>');\n+            \n+            // Inline code\n+            html = html.replace(/`([^`]+)`/g, '<code>$1</code>');\n+            \n+            // Links\n+            html = html.replace(/\\[([^\\]]+)\\]\\(([^\\)]+)\\)/g, '<a href=\"$2\" target=\"_blank\" rel=\"noopener noreferrer\">$1</a>');\n+            \n+            // Line breaks and paragraphs\n+            html = html.replace(/\\n\\n+/g, '</p><p>');\n+            html = html.replace(/\\n/g, '<br>');\n+            \n+            // Wrap in paragraph if not already wrapped in block elements\n+            if (!html.match(/^<(h[1-6]|ul|ol|blockquote|hr)/)) {\n+                html = '<p>' + html + '</p>';\n+            }\n+            \n+            // Clean up\n+            html = html.replace(/<p><\\/p>/g, '');\n+            html = html.replace(/<p>\\s*<\\/p>/g, '');\n+            html = html.replace(/<p>(<h[1-6]>)/g, '$1');\n+            html = html.replace(/(<\\/h[1-6]>)<\\/p>/g, '$1');\n+            html = html.replace(/<p>(<ul>|<ol>|<blockquote>|<hr>)/g, '$1');\n+            html = html.replace(/(<\\/ul>|<\\/ol>|<\\/blockquote>|<hr>)<\\/p>/g, '$1');\n+            \n+            return html;\n+        }\n+\n         // Parse and render message content with code blocks\n         function parseMessageContent(content) {\n             const codeBlockRegex = /```(\\w+)?\\n?([\\s\\S]*?)```/g;\n@@ -754,9 +1101,13 @@ <h2>\ud83d\udc4b Welcome to RAG Chat</h2>\n             textarea.style.height = 'auto';\n             textarea.style.height = Math.min(textarea.scrollHeight, 120) + 'px';\n             \n-            // Enable/disable send button\n+            // Enable/disable send and deep research buttons\n             const sendBtn = document.getElementById('sendBtn');\n-            sendBtn.disabled = !textarea.value.trim() || isProcessing;\n+            const deepResearchBtn = document.getElementById('deepResearchBtn');\n+            const hasText = textarea.value.trim().length > 0;\n+            \n+            sendBtn.disabled = !hasText || isProcessing;\n+            deepResearchBtn.disabled = !hasText || isProcessing;\n         }\n \n         // Handle Enter key\n@@ -818,6 +1169,7 @@ <h2>\ud83d\udc4b Welcome to RAG Chat</h2>\n                 \n                 if (response.ok) {\n                     uploadedFileId = result.file_id;\n+                    hasUploadedFiles = true;\n                     showMessage('system', `\u2705 File uploaded successfully! Processing...`);\n                     \n                     // Auto-process the file\n@@ -933,7 +1285,12 @@ <h2>\ud83d\udc4b Welcome to RAG Chat</h2>\n             isProcessing = true;\n \n             try {\n-                await askRAG(message);\n+                // Use chat endpoint if no files uploaded, otherwise use RAG\n+                if (hasUploadedFiles) {\n+                    await askRAG(message);\n+                } else {\n+                    await askChat(message);\n+                }\n             } catch (error) {\n                 showMessage('assistant', `Sorry, I encountered an error: ${error.message}`, true);\n             } finally {\n@@ -943,7 +1300,37 @@ <h2>\ud83d\udc4b Welcome to RAG Chat</h2>\n             }\n         }\n \n-        // Ask RAG\n+        // Deep Research function\n+        async function deepResearch() {\n+            const input = document.getElementById('messageInput');\n+            const message = input.value.trim();\n+            \n+            if (!message || isProcessing) return;\n+\n+            // Show user message\n+            showMessage('user', message + ' \ud83d\udd2c');\n+            \n+            // Clear input\n+            input.value = '';\n+            autoResize(input);\n+            \n+            // Show deep research loading indicator\n+            showDeepResearchLoading(true);\n+            isProcessing = true;\n+\n+            try {\n+                await performDeepResearch(message);\n+            } catch (error) {\n+                showMessage('assistant', `Sorry, I encountered an error during deep research: ${error.message}`, true);\n+            } finally {\n+                showDeepResearchLoading(false);\n+                isProcessing = false;\n+                document.getElementById('sendBtn').disabled = false;\n+                document.getElementById('deepResearchBtn').disabled = false;\n+            }\n+        }\n+\n+        // Ask RAG (for uploaded documents)\n         async function askRAG(question) {\n             const projectId = getProjectId();\n             \n@@ -978,6 +1365,99 @@ <h2>\ud83d\udc4b Welcome to RAG Chat</h2>\n             }\n         }\n \n+        // Ask Chat (for general conversation without documents)\n+        async function askChat(question) {\n+            const projectId = getProjectId();\n+            \n+            const response = await fetch(`${API_BASE}/nlp/index/chat/${projectId}`, {\n+                method: 'POST',\n+                headers: {\n+                    'Content-Type': 'application/json'\n+                },\n+                body: JSON.stringify({\n+                    query: question\n+                })\n+            });\n+\n+            // Check if response is JSON\n+            const contentType = response.headers.get('content-type');\n+            let result;\n+            \n+            if (contentType && contentType.includes('application/json')) {\n+                result = await response.json();\n+            } else {\n+                const textResponse = await response.text();\n+                console.error('Non-JSON response:', textResponse);\n+                throw new Error(`Server returned non-JSON response: ${response.status} ${response.statusText}`);\n+            }\n+            \n+            if (response.ok && result.answer) {\n+                showMessage('assistant', result.answer);\n+            } else {\n+                throw new Error(result.detail || result.message || 'Failed to get answer');\n+            }\n+        }\n+\n+        // Perform Deep Research\n+        async function performDeepResearch(question) {\n+            const projectId = getProjectId();\n+            \n+            // Create an AbortController for timeout handling (5 minutes)\n+            const controller = new AbortController();\n+            const timeoutId = setTimeout(() => controller.abort(), 300000); // 5 minutes\n+            \n+            try {\n+                const response = await fetch(`${API_BASE}/nlp/index/deep-research/${projectId}`, {\n+                    method: 'POST',\n+                    headers: {\n+                        'Content-Type': 'application/json'\n+                    },\n+                    body: JSON.stringify({\n+                        query: question\n+                    }),\n+                    signal: controller.signal,\n+                    // Disable browser timeout for long requests\n+                    keepalive: false\n+                });\n+\n+                clearTimeout(timeoutId);\n+\n+                // Check if response is JSON\n+                const contentType = response.headers.get('content-type');\n+                let result;\n+                \n+                if (contentType && contentType.includes('application/json')) {\n+                    result = await response.json();\n+                } else {\n+                    const textResponse = await response.text();\n+                    console.error('Non-JSON response:', textResponse);\n+                    throw new Error(`Server returned non-JSON response: ${response.status} ${response.statusText}`);\n+                }\n+                \n+                if (response.ok && result.result) {\n+                    // Extract final report from result\n+                    const finalReport = result.result.final_report || result.result;\n+                    showMessage('assistant', finalReport);\n+                } else {\n+                    throw new Error(result.detail || result.message || 'Failed to complete deep research');\n+                }\n+            } catch (error) {\n+                clearTimeout(timeoutId);\n+                \n+                // Handle timeout specifically\n+                if (error.name === 'AbortError') {\n+                    throw new Error('Deep research took too long (timeout after 5 minutes). Please try a more specific question.');\n+                }\n+                \n+                // Handle network errors with more context\n+                if (error.message.includes('NetworkError') || error.message.includes('Failed to fetch')) {\n+                    throw new Error('Network connection issue. This could be due to:\\n\u2022 Server took too long to respond\\n\u2022 CORS configuration\\n\u2022 Network connectivity\\n\\nPlease check the browser console for details and try again.');\n+                }\n+                \n+                throw error;\n+            }\n+        }\n+\n         // Show message in chat\n         function showMessage(sender, content, isError = false) {\n             const chatMessages = document.getElementById('chatMessages');\n@@ -1017,20 +1497,31 @@ <h2>\ud83d\udc4b Welcome to RAG Chat</h2>\n                 // Parse content for code blocks\n                 const parts = parseMessageContent(content);\n                 \n+                // Create markdown container\n+                const markdownContainer = document.createElement('div');\n+                markdownContainer.className = 'markdown-content';\n+                \n                 parts.forEach(part => {\n                     if (part.type === 'text') {\n                         if (part.content.trim()) {\n-                            const textNode = document.createElement('span');\n-                            textNode.style.whiteSpace = 'pre-wrap';\n-                            textNode.textContent = part.content;\n-                            messageContent.appendChild(textNode);\n+                            // Parse markdown and insert as HTML\n+                            const markdownHtml = parseMarkdown(part.content);\n+                            const tempDiv = document.createElement('div');\n+                            tempDiv.innerHTML = markdownHtml;\n+                            \n+                            // Append all children\n+                            while (tempDiv.firstChild) {\n+                                markdownContainer.appendChild(tempDiv.firstChild);\n+                            }\n                         }\n                     } else if (part.type === 'code') {\n                         const codeBlock = createCodeBlock(part.language, part.content);\n-                        messageContent.appendChild(codeBlock);\n+                        markdownContainer.appendChild(codeBlock);\n                     }\n                 });\n                 \n+                messageContent.appendChild(markdownContainer);\n+                \n                 if (isError) {\n                     messageContent.style.background = 'var(--error-bg)';\n                     messageContent.style.color = 'var(--error-text)';\n@@ -1062,6 +1553,66 @@ <h2>\ud83d\udc4b Welcome to RAG Chat</h2>\n             }\n         }\n \n+        // Show/hide deep research loading indicator\n+        function showDeepResearchLoading(show) {\n+            let loadingDiv = document.getElementById('deepResearchLoading');\n+            \n+            if (show) {\n+                if (!loadingDiv) {\n+                    const chatMessages = document.getElementById('chatMessages');\n+                    loadingDiv = document.createElement('div');\n+                    loadingDiv.id = 'deepResearchLoading';\n+                    loadingDiv.className = 'deep-research-loading';\n+                    loadingDiv.innerHTML = `\n+                        <div class=\"message-avatar\" style=\"background: var(--assistant-bg);\">\ud83d\udd2c</div>\n+                        <div class=\"research-status\">\n+                            <div class=\"research-title\">\ud83e\udde0 Deep Research in Progress...</div>\n+                            <div class=\"research-steps\">\n+                                <div class=\"research-step active\">\ud83d\udccb Planning research tasks</div>\n+                                <div class=\"research-step\">\ud83d\udd0d Conducting web research</div>\n+                                <div class=\"research-step\">\ud83d\udcca Analyzing findings</div>\n+                                <div class=\"research-step\">\ud83d\udcdd Generating comprehensive report</div>\n+                            </div>\n+                            <div class=\"research-progress\">\n+                                <div class=\"progress-bar\"></div>\n+                            </div>\n+                        </div>\n+                    `;\n+                    chatMessages.appendChild(loadingDiv);\n+                    chatMessages.scrollTop = chatMessages.scrollHeight;\n+                }\n+                \n+                // Animate progress steps\n+                animateResearchSteps();\n+            } else {\n+                if (loadingDiv) {\n+                    loadingDiv.remove();\n+                }\n+            }\n+        }\n+\n+        // Animate research steps\n+        function animateResearchSteps() {\n+            const steps = document.querySelectorAll('.research-step');\n+            let currentStep = 0;\n+            \n+            const interval = setInterval(() => {\n+                if (currentStep < steps.length) {\n+                    steps[currentStep].classList.add('active');\n+                    currentStep++;\n+                } else {\n+                    // Loop back to beginning\n+                    steps.forEach(step => step.classList.remove('active'));\n+                    currentStep = 0;\n+                }\n+                \n+                // Check if loading indicator still exists\n+                if (!document.getElementById('deepResearchLoading')) {\n+                    clearInterval(interval);\n+                }\n+            }, 3000);\n+        }\n+\n         // Initialize\n         document.addEventListener('DOMContentLoaded', function() {\n             initTheme();",
      "patch_lines": [
        "@@ -304,6 +304,117 @@\n",
        "             overflow-wrap: break-word;\n",
        "         }\n",
        " \n",
        "+        /* Markdown styling */\n",
        "+        .markdown-content {\n",
        "+            line-height: 1.6;\n",
        "+        }\n",
        "+\n",
        "+        .markdown-content h1,\n",
        "+        .markdown-content h2,\n",
        "+        .markdown-content h3,\n",
        "+        .markdown-content h4,\n",
        "+        .markdown-content h5,\n",
        "+        .markdown-content h6 {\n",
        "+            margin: 16px 0 8px 0;\n",
        "+            font-weight: 600;\n",
        "+            color: var(--text-primary);\n",
        "+        }\n",
        "+\n",
        "+        .markdown-content h1 { font-size: 2em; border-bottom: 2px solid var(--border-color); padding-bottom: 8px; }\n",
        "+        .markdown-content h2 { font-size: 1.5em; border-bottom: 1px solid var(--border-color); padding-bottom: 6px; }\n",
        "+        .markdown-content h3 { font-size: 1.25em; }\n",
        "+        .markdown-content h4 { font-size: 1.1em; }\n",
        "+        .markdown-content h5 { font-size: 1em; }\n",
        "+        .markdown-content h6 { font-size: 0.9em; color: var(--text-secondary); }\n",
        "+\n",
        "+        .markdown-content p {\n",
        "+            margin: 8px 0;\n",
        "+        }\n",
        "+\n",
        "+        .markdown-content ul,\n",
        "+        .markdown-content ol {\n",
        "+            margin: 8px 0;\n",
        "+            padding-left: 24px;\n",
        "+        }\n",
        "+\n",
        "+        .markdown-content li {\n",
        "+            margin: 4px 0;\n",
        "+        }\n",
        "+\n",
        "+        .markdown-content strong {\n",
        "+            font-weight: 600;\n",
        "+            color: var(--text-primary);\n",
        "+        }\n",
        "+\n",
        "+        .markdown-content em {\n",
        "+            font-style: italic;\n",
        "+        }\n",
        "+\n",
        "+        .markdown-content a {\n",
        "+            color: var(--user-bg);\n",
        "+            text-decoration: none;\n",
        "+            border-bottom: 1px solid transparent;\n",
        "+            transition: border-color 0.2s;\n",
        "+        }\n",
        "+\n",
        "+        .markdown-content a:hover {\n",
        "+            border-bottom-color: var(--user-bg);\n",
        "+        }\n",
        "+\n",
        "+        .markdown-content blockquote {\n",
        "+            border-left: 4px solid var(--border-color);\n",
        "+            padding-left: 16px;\n",
        "+            margin: 8px 0;\n",
        "+            color: var(--text-secondary);\n",
        "+            font-style: italic;\n",
        "+        }\n",
        "+\n",
        "+        .markdown-content hr {\n",
        "+            border: none;\n",
        "+            border-top: 1px solid var(--border-color);\n",
        "+            margin: 16px 0;\n",
        "+        }\n",
        "+\n",
        "+        .markdown-content table {\n",
        "+            border-collapse: collapse;\n",
        "+            width: 100%;\n",
        "+            margin: 8px 0;\n",
        "+        }\n",
        "+\n",
        "+        .markdown-content table th,\n",
        "+        .markdown-content table td {\n",
        "+            border: 1px solid var(--border-color);\n",
        "+            padding: 8px 12px;\n",
        "+            text-align: left;\n",
        "+        }\n",
        "+\n",
        "+        .markdown-content table th {\n",
        "+            background: var(--bg-primary);\n",
        "+            font-weight: 600;\n",
        "+        }\n",
        "+\n",
        "+        .markdown-content code {\n",
        "+            background: var(--bg-primary);\n",
        "+            padding: 2px 6px;\n",
        "+            border-radius: 3px;\n",
        "+            font-family: 'SF Mono', Monaco, 'Cascadia Code', 'Roboto Mono', Consolas, monospace;\n",
        "+            font-size: 0.9em;\n",
        "+            color: var(--text-primary);\n",
        "+        }\n",
        "+\n",
        "+        .markdown-content pre {\n",
        "+            background: var(--bg-primary);\n",
        "+            padding: 12px;\n",
        "+            border-radius: 6px;\n",
        "+            overflow-x: auto;\n",
        "+            margin: 8px 0;\n",
        "+        }\n",
        "+\n",
        "+        .markdown-content pre code {\n",
        "+            background: none;\n",
        "+            padding: 0;\n",
        "+        }\n",
        "+\n",
        "         .typing-indicator {\n",
        "             display: none;\n",
        "             align-items: center;\n",
        "@@ -481,6 +592,107 @@\n",
        "             font-size: 14px;\n",
        "         }\n",
        " \n",
        "+        .deep-research-loading {\n",
        "+            display: flex;\n",
        "+            gap: 12px;\n",
        "+            padding: 20px;\n",
        "+            background: var(--bg-secondary);\n",
        "+            border-radius: 12px;\n",
        "+            border: 2px solid var(--assistant-bg);\n",
        "+            margin: 10px 0;\n",
        "+            animation: pulse 2s ease-in-out infinite;\n",
        "+        }\n",
        "+\n",
        "+        .research-status {\n",
        "+            flex: 1;\n",
        "+        }\n",
        "+\n",
        "+        .research-title {\n",
        "+            font-size: 16px;\n",
        "+            font-weight: 600;\n",
        "+            color: var(--text-primary);\n",
        "+            margin-bottom: 12px;\n",
        "+        }\n",
        "+\n",
        "+        .research-steps {\n",
        "+            display: flex;\n",
        "+            flex-direction: column;\n",
        "+            gap: 8px;\n",
        "+            margin-bottom: 12px;\n",
        "+        }\n",
        "+\n",
        "+        .research-step {\n",
        "+            font-size: 14px;\n",
        "+            color: var(--text-secondary);\n",
        "+            padding: 6px 12px;\n",
        "+            border-radius: 6px;\n",
        "+            background: var(--bg-primary);\n",
        "+            transition: all 0.3s ease;\n",
        "+            opacity: 0.5;\n",
        "+        }\n",
        "+\n",
        "+        .research-step.active {\n",
        "+            opacity: 1;\n",
        "+            background: var(--info-bg);\n",
        "+            color: var(--info-text);\n",
        "+            border-left: 3px solid var(--info-border);\n",
        "+        }\n",
        "+\n",
        "+        .research-progress {\n",
        "+            height: 4px;\n",
        "+            background: var(--bg-primary);\n",
        "+            border-radius: 2px;\n",
        "+            overflow: hidden;\n",
        "+        }\n",
        "+\n",
        "+        .progress-bar {\n",
        "+            height: 100%;\n",
        "+            background: linear-gradient(90deg, var(--user-bg), var(--assistant-bg));\n",
        "+            animation: progressAnimation 2s ease-in-out infinite;\n",
        "+        }\n",
        "+\n",
        "+        @keyframes pulse {\n",
        "+            0%, 100% { opacity: 1; }\n",
        "+            50% { opacity: 0.95; }\n",
        "+        }\n",
        "+\n",
        "+        @keyframes progressAnimation {\n",
        "+            0% { width: 0%; }\n",
        "+            50% { width: 70%; }\n",
        "+            100% { width: 100%; }\n",
        "+        }\n",
        "+\n",
        "+        .deep-research-btn {\n",
        "+            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "+            border: none;\n",
        "+            border-radius: 6px;\n",
        "+            padding: 8px;\n",
        "+            cursor: pointer;\n",
        "+            display: flex;\n",
        "+            align-items: center;\n",
        "+            justify-content: center;\n",
        "+            transition: all 0.2s;\n",
        "+            color: white;\n",
        "+            font-weight: 600;\n",
        "+            gap: 6px;\n",
        "+        }\n",
        "+\n",
        "+        .deep-research-btn:hover:not(:disabled) {\n",
        "+            transform: translateY(-1px);\n",
        "+            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);\n",
        "+        }\n",
        "+\n",
        "+        .deep-research-btn:disabled {\n",
        "+            background: var(--disabled-bg);\n",
        "+            cursor: not-allowed;\n",
        "+            transform: none;\n",
        "+        }\n",
        "+\n",
        "+        .deep-research-btn svg {\n",
        "+            width: 16px;\n",
        "+            height: 16px;\n",
        "+        }\n",
        "+\n",
        "         @media (max-width: 768px) {\n",
        "             .chat-container {\n",
        "                 padding: 0 12px;\n",
        "@@ -542,7 +754,8 @@ <h1>RAG Chat</h1>\n",
        "         <div class=\"chat-messages\" id=\"chatMessages\">\n",
        "             <div class=\"welcome-message\">\n",
        "                 <h2>\ud83d\udc4b Welcome to RAG Chat</h2>\n",
        "-                <p>Upload your documents and start asking questions!</p>\n",
        "+                <p>Upload documents for RAG-powered Q&A, or just chat directly!</p>\n",
        "+                <p style=\"margin-top: 8px; font-size: 14px;\">\ud83d\udca1 Use the <strong>\ud83d\udd2c Deep Research</strong> button for comprehensive web-based research on any topic.</p>\n",
        "             </div>\n",
        "         </div>\n",
        " \n",
        "@@ -562,24 +775,33 @@ <h2>\ud83d\udc4b Welcome to RAG Chat</h2>\n",
        "                 <button class=\"close-btn\" onclick=\"clearFile()\">\u00d7</button>\n",
        "             </div>\n",
        "             \n",
        "-            <div class=\"input-container\">\n",
        "+                        <div class=\"input-container\">\n",
        "                 <button class=\"file-upload-btn\" onclick=\"document.getElementById('fileInput').click()\" title=\"Upload file\">\n",
        "-                    <svg viewBox=\"0 0 24 24\" fill=\"currentColor\">\n",
        "-                        <path d=\"M12 4V2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm5 11h-4v4h-2v-4H7v-2h4V9h2v4h4v2z\"/>\n",
        "+                    <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\">\n",
        "+                        <path d=\"M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4\"></path>\n",
        "+                        <polyline points=\"17 8 12 3 7 8\"></polyline>\n",
        "+                        <line x1=\"12\" y1=\"3\" x2=\"12\" y2=\"15\"></line>\n",
        "                     </svg>\n",
        "                 </button>\n",
        "                 \n",
        "                 <textarea \n",
        "                     id=\"messageInput\" \n",
        "-                    placeholder=\"Ask anything about your documents...\" \n",
        "+                    placeholder=\"Ask a question or type a message...\" \n",
        "                     rows=\"1\"\n",
        "                     onkeydown=\"handleKeyDown(event)\"\n",
        "                     oninput=\"autoResize(this)\"\n",
        "                 ></textarea>\n",
        "                 \n",
        "+                <button class=\"deep-research-btn\" id=\"deepResearchBtn\" onclick=\"deepResearch()\" disabled title=\"Deep Research: Comprehensive analysis with web search\">\n",
        "+                    <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\">\n",
        "+                        <circle cx=\"11\" cy=\"11\" r=\"8\"></circle>\n",
        "+                        <path d=\"m21 21-4.35-4.35\"></path>\n",
        "+                    </svg>\n",
        "+                </button>\n",
        "+                \n",
        "                 <button class=\"send-btn\" id=\"sendBtn\" onclick=\"sendMessage()\" disabled>\n",
        "-                    <svg viewBox=\"0 0 24 24\" fill=\"currentColor\">\n",
        "-                        <path d=\"M2.01 21L23 12 2.01 3 2 10l15 2-15 2z\"/>\n",
        "+                    <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" fill=\"currentColor\">\n",
        "+                        <path d=\"M2.01 21L23 12 2.01 3 2 10l15 2-15 2z\"></path>\n",
        "                     </svg>\n",
        "                 </button>\n",
        "             </div>\n",
        "@@ -594,6 +816,7 @@ <h2>\ud83d\udc4b Welcome to RAG Chat</h2>\n",
        "         let isProcessing = false;\n",
        "         let sessionId = null;\n",
        "         let projectId = null;\n",
        "+        let hasUploadedFiles = false; // Track if files have been uploaded\n",
        " \n",
        "         // Generate or retrieve session ID and project ID\n",
        "         function initSession() {\n",
        "@@ -657,6 +880,130 @@ <h2>\ud83d\udc4b Welcome to RAG Chat</h2>\n",
        "             }, 2000);\n",
        "         }\n",
        " \n",
        "+        // Enhanced markdown parser\n",
        "+        function parseMarkdown(text) {\n",
        "+            let html = text;\n",
        "+            \n",
        "+            // Escape HTML but preserve newlines\n",
        "+            html = html.replace(/&/g, '&amp;')\n",
        "+                      .replace(/</g, '&lt;')\n",
        "+                      .replace(/>/g, '&gt;');\n",
        "+            \n",
        "+            // Process line by line for better control\n",
        "+            const lines = html.split('\\n');\n",
        "+            const processedLines = [];\n",
        "+            let inList = false;\n",
        "+            let listType = null;\n",
        "+            \n",
        "+            for (let i = 0; i < lines.length; i++) {\n",
        "+                let line = lines[i];\n",
        "+                \n",
        "+                // Headers\n",
        "+                if (line.match(/^######\\s+(.+)$/)) {\n",
        "+                    line = line.replace(/^######\\s+(.+)$/, '<h6>$1</h6>');\n",
        "+                } else if (line.match(/^#####\\s+(.+)$/)) {\n",
        "+                    line = line.replace(/^#####\\s+(.+)$/, '<h5>$1</h5>');\n",
        "+                } else if (line.match(/^####\\s+(.+)$/)) {\n",
        "+                    line = line.replace(/^####\\s+(.+)$/, '<h4>$1</h4>');\n",
        "+                } else if (line.match(/^###\\s+(.+)$/)) {\n",
        "+                    line = line.replace(/^###\\s+(.+)$/, '<h3>$1</h3>');\n",
        "+                } else if (line.match(/^##\\s+(.+)$/)) {\n",
        "+                    line = line.replace(/^##\\s+(.+)$/, '<h2>$1</h2>');\n",
        "+                } else if (line.match(/^#\\s+(.+)$/)) {\n",
        "+                    line = line.replace(/^#\\s+(.+)$/, '<h1>$1</h1>');\n",
        "+                }\n",
        "+                // Unordered lists\n",
        "+                else if (line.match(/^[\\*\\-]\\s+(.+)$/)) {\n",
        "+                    if (!inList || listType !== 'ul') {\n",
        "+                        if (inList && listType === 'ol') {\n",
        "+                            processedLines.push('</ol>');\n",
        "+                        }\n",
        "+                        processedLines.push('<ul>');\n",
        "+                        inList = true;\n",
        "+                        listType = 'ul';\n",
        "+                    }\n",
        "+                    line = line.replace(/^[\\*\\-]\\s+(.+)$/, '<li>$1</li>');\n",
        "+                }\n",
        "+                // Ordered lists\n",
        "+                else if (line.match(/^\\d+\\.\\s+(.+)$/)) {\n",
        "+                    if (!inList || listType !== 'ol') {\n",
        "+                        if (inList && listType === 'ul') {\n",
        "+                            processedLines.push('</ul>');\n",
        "+                        }\n",
        "+                        processedLines.push('<ol>');\n",
        "+                        inList = true;\n",
        "+                        listType = 'ol';\n",
        "+                    }\n",
        "+                    line = line.replace(/^\\d+\\.\\s+(.+)$/, '<li>$1</li>');\n",
        "+                }\n",
        "+                // Blockquotes\n",
        "+                else if (line.match(/^&gt;\\s+(.+)$/)) {\n",
        "+                    line = line.replace(/^&gt;\\s+(.+)$/, '<blockquote>$1</blockquote>');\n",
        "+                }\n",
        "+                // Horizontal rules\n",
        "+                else if (line.match(/^(---|\\*\\*\\*|___)\\s*$/)) {\n",
        "+                    line = '<hr>';\n",
        "+                }\n",
        "+                // End list if we hit a non-list line\n",
        "+                else if (inList && line.trim() !== '') {\n",
        "+                    if (listType === 'ul') {\n",
        "+                        processedLines.push('</ul>');\n",
        "+                    } else {\n",
        "+                        processedLines.push('</ol>');\n",
        "+                    }\n",
        "+                    inList = false;\n",
        "+                    listType = null;\n",
        "+                }\n",
        "+                \n",
        "+                processedLines.push(line);\n",
        "+            }\n",
        "+            \n",
        "+            // Close any open lists\n",
        "+            if (inList) {\n",
        "+                if (listType === 'ul') {\n",
        "+                    processedLines.push('</ul>');\n",
        "+                } else {\n",
        "+                    processedLines.push('</ol>');\n",
        "+                }\n",
        "+            }\n",
        "+            \n",
        "+            html = processedLines.join('\\n');\n",
        "+            \n",
        "+            // Inline formatting (bold, italic, code, links)\n",
        "+            // Bold (must come before italic)\n",
        "+            html = html.replace(/\\*\\*(.+?)\\*\\*/g, '<strong>$1</strong>');\n",
        "+            html = html.replace(/__(.+?)__/g, '<strong>$1</strong>');\n",
        "+            \n",
        "+            // Italic\n",
        "+            html = html.replace(/\\*([^\\*]+?)\\*/g, '<em>$1</em>');\n",
        "+            html = html.replace(/_([^_]+?)_/g, '<em>$1</em>');\n",
        "+            \n",
        "+            // Inline code\n",
        "+            html = html.replace(/`([^`]+)`/g, '<code>$1</code>');\n",
        "+            \n",
        "+            // Links\n",
        "+            html = html.replace(/\\[([^\\]]+)\\]\\(([^\\)]+)\\)/g, '<a href=\"$2\" target=\"_blank\" rel=\"noopener noreferrer\">$1</a>');\n",
        "+            \n",
        "+            // Line breaks and paragraphs\n",
        "+            html = html.replace(/\\n\\n+/g, '</p><p>');\n",
        "+            html = html.replace(/\\n/g, '<br>');\n",
        "+            \n",
        "+            // Wrap in paragraph if not already wrapped in block elements\n",
        "+            if (!html.match(/^<(h[1-6]|ul|ol|blockquote|hr)/)) {\n",
        "+                html = '<p>' + html + '</p>';\n",
        "+            }\n",
        "+            \n",
        "+            // Clean up\n",
        "+            html = html.replace(/<p><\\/p>/g, '');\n",
        "+            html = html.replace(/<p>\\s*<\\/p>/g, '');\n",
        "+            html = html.replace(/<p>(<h[1-6]>)/g, '$1');\n",
        "+            html = html.replace(/(<\\/h[1-6]>)<\\/p>/g, '$1');\n",
        "+            html = html.replace(/<p>(<ul>|<ol>|<blockquote>|<hr>)/g, '$1');\n",
        "+            html = html.replace(/(<\\/ul>|<\\/ol>|<\\/blockquote>|<hr>)<\\/p>/g, '$1');\n",
        "+            \n",
        "+            return html;\n",
        "+        }\n",
        "+\n",
        "         // Parse and render message content with code blocks\n",
        "         function parseMessageContent(content) {\n",
        "             const codeBlockRegex = /```(\\w+)?\\n?([\\s\\S]*?)```/g;\n",
        "@@ -754,9 +1101,13 @@ <h2>\ud83d\udc4b Welcome to RAG Chat</h2>\n",
        "             textarea.style.height = 'auto';\n",
        "             textarea.style.height = Math.min(textarea.scrollHeight, 120) + 'px';\n",
        "             \n",
        "-            // Enable/disable send button\n",
        "+            // Enable/disable send and deep research buttons\n",
        "             const sendBtn = document.getElementById('sendBtn');\n",
        "-            sendBtn.disabled = !textarea.value.trim() || isProcessing;\n",
        "+            const deepResearchBtn = document.getElementById('deepResearchBtn');\n",
        "+            const hasText = textarea.value.trim().length > 0;\n",
        "+            \n",
        "+            sendBtn.disabled = !hasText || isProcessing;\n",
        "+            deepResearchBtn.disabled = !hasText || isProcessing;\n",
        "         }\n",
        " \n",
        "         // Handle Enter key\n",
        "@@ -818,6 +1169,7 @@ <h2>\ud83d\udc4b Welcome to RAG Chat</h2>\n",
        "                 \n",
        "                 if (response.ok) {\n",
        "                     uploadedFileId = result.file_id;\n",
        "+                    hasUploadedFiles = true;\n",
        "                     showMessage('system', `\u2705 File uploaded successfully! Processing...`);\n",
        "                     \n",
        "                     // Auto-process the file\n",
        "@@ -933,7 +1285,12 @@ <h2>\ud83d\udc4b Welcome to RAG Chat</h2>\n",
        "             isProcessing = true;\n",
        " \n",
        "             try {\n",
        "-                await askRAG(message);\n",
        "+                // Use chat endpoint if no files uploaded, otherwise use RAG\n",
        "+                if (hasUploadedFiles) {\n",
        "+                    await askRAG(message);\n",
        "+                } else {\n",
        "+                    await askChat(message);\n",
        "+                }\n",
        "             } catch (error) {\n",
        "                 showMessage('assistant', `Sorry, I encountered an error: ${error.message}`, true);\n",
        "             } finally {\n",
        "@@ -943,7 +1300,37 @@ <h2>\ud83d\udc4b Welcome to RAG Chat</h2>\n",
        "             }\n",
        "         }\n",
        " \n",
        "-        // Ask RAG\n",
        "+        // Deep Research function\n",
        "+        async function deepResearch() {\n",
        "+            const input = document.getElementById('messageInput');\n",
        "+            const message = input.value.trim();\n",
        "+            \n",
        "+            if (!message || isProcessing) return;\n",
        "+\n",
        "+            // Show user message\n",
        "+            showMessage('user', message + ' \ud83d\udd2c');\n",
        "+            \n",
        "+            // Clear input\n",
        "+            input.value = '';\n",
        "+            autoResize(input);\n",
        "+            \n",
        "+            // Show deep research loading indicator\n",
        "+            showDeepResearchLoading(true);\n",
        "+            isProcessing = true;\n",
        "+\n",
        "+            try {\n",
        "+                await performDeepResearch(message);\n",
        "+            } catch (error) {\n",
        "+                showMessage('assistant', `Sorry, I encountered an error during deep research: ${error.message}`, true);\n",
        "+            } finally {\n",
        "+                showDeepResearchLoading(false);\n",
        "+                isProcessing = false;\n",
        "+                document.getElementById('sendBtn').disabled = false;\n",
        "+                document.getElementById('deepResearchBtn').disabled = false;\n",
        "+            }\n",
        "+        }\n",
        "+\n",
        "+        // Ask RAG (for uploaded documents)\n",
        "         async function askRAG(question) {\n",
        "             const projectId = getProjectId();\n",
        "             \n",
        "@@ -978,6 +1365,99 @@ <h2>\ud83d\udc4b Welcome to RAG Chat</h2>\n",
        "             }\n",
        "         }\n",
        " \n",
        "+        // Ask Chat (for general conversation without documents)\n",
        "+        async function askChat(question) {\n",
        "+            const projectId = getProjectId();\n",
        "+            \n",
        "+            const response = await fetch(`${API_BASE}/nlp/index/chat/${projectId}`, {\n",
        "+                method: 'POST',\n",
        "+                headers: {\n",
        "+                    'Content-Type': 'application/json'\n",
        "+                },\n",
        "+                body: JSON.stringify({\n",
        "+                    query: question\n",
        "+                })\n",
        "+            });\n",
        "+\n",
        "+            // Check if response is JSON\n",
        "+            const contentType = response.headers.get('content-type');\n",
        "+            let result;\n",
        "+            \n",
        "+            if (contentType && contentType.includes('application/json')) {\n",
        "+                result = await response.json();\n",
        "+            } else {\n",
        "+                const textResponse = await response.text();\n",
        "+                console.error('Non-JSON response:', textResponse);\n",
        "+                throw new Error(`Server returned non-JSON response: ${response.status} ${response.statusText}`);\n",
        "+            }\n",
        "+            \n",
        "+            if (response.ok && result.answer) {\n",
        "+                showMessage('assistant', result.answer);\n",
        "+            } else {\n",
        "+                throw new Error(result.detail || result.message || 'Failed to get answer');\n",
        "+            }\n",
        "+        }\n",
        "+\n",
        "+        // Perform Deep Research\n",
        "+        async function performDeepResearch(question) {\n",
        "+            const projectId = getProjectId();\n",
        "+            \n",
        "+            // Create an AbortController for timeout handling (5 minutes)\n",
        "+            const controller = new AbortController();\n",
        "+            const timeoutId = setTimeout(() => controller.abort(), 300000); // 5 minutes\n",
        "+            \n",
        "+            try {\n",
        "+                const response = await fetch(`${API_BASE}/nlp/index/deep-research/${projectId}`, {\n",
        "+                    method: 'POST',\n",
        "+                    headers: {\n",
        "+                        'Content-Type': 'application/json'\n",
        "+                    },\n",
        "+                    body: JSON.stringify({\n",
        "+                        query: question\n",
        "+                    }),\n",
        "+                    signal: controller.signal,\n",
        "+                    // Disable browser timeout for long requests\n",
        "+                    keepalive: false\n",
        "+                });\n",
        "+\n",
        "+                clearTimeout(timeoutId);\n",
        "+\n",
        "+                // Check if response is JSON\n",
        "+                const contentType = response.headers.get('content-type');\n",
        "+                let result;\n",
        "+                \n",
        "+                if (contentType && contentType.includes('application/json')) {\n",
        "+                    result = await response.json();\n",
        "+                } else {\n",
        "+                    const textResponse = await response.text();\n",
        "+                    console.error('Non-JSON response:', textResponse);\n",
        "+                    throw new Error(`Server returned non-JSON response: ${response.status} ${response.statusText}`);\n",
        "+                }\n",
        "+                \n",
        "+                if (response.ok && result.result) {\n",
        "+                    // Extract final report from result\n",
        "+                    const finalReport = result.result.final_report || result.result;\n",
        "+                    showMessage('assistant', finalReport);\n",
        "+                } else {\n",
        "+                    throw new Error(result.detail || result.message || 'Failed to complete deep research');\n",
        "+                }\n",
        "+            } catch (error) {\n",
        "+                clearTimeout(timeoutId);\n",
        "+                \n",
        "+                // Handle timeout specifically\n",
        "+                if (error.name === 'AbortError') {\n",
        "+                    throw new Error('Deep research took too long (timeout after 5 minutes). Please try a more specific question.');\n",
        "+                }\n",
        "+                \n",
        "+                // Handle network errors with more context\n",
        "+                if (error.message.includes('NetworkError') || error.message.includes('Failed to fetch')) {\n",
        "+                    throw new Error('Network connection issue. This could be due to:\\n\u2022 Server took too long to respond\\n\u2022 CORS configuration\\n\u2022 Network connectivity\\n\\nPlease check the browser console for details and try again.');\n",
        "+                }\n",
        "+                \n",
        "+                throw error;\n",
        "+            }\n",
        "+        }\n",
        "+\n",
        "         // Show message in chat\n",
        "         function showMessage(sender, content, isError = false) {\n",
        "             const chatMessages = document.getElementById('chatMessages');\n",
        "@@ -1017,20 +1497,31 @@ <h2>\ud83d\udc4b Welcome to RAG Chat</h2>\n",
        "                 // Parse content for code blocks\n",
        "                 const parts = parseMessageContent(content);\n",
        "                 \n",
        "+                // Create markdown container\n",
        "+                const markdownContainer = document.createElement('div');\n",
        "+                markdownContainer.className = 'markdown-content';\n",
        "+                \n",
        "                 parts.forEach(part => {\n",
        "                     if (part.type === 'text') {\n",
        "                         if (part.content.trim()) {\n",
        "-                            const textNode = document.createElement('span');\n",
        "-                            textNode.style.whiteSpace = 'pre-wrap';\n",
        "-                            textNode.textContent = part.content;\n",
        "-                            messageContent.appendChild(textNode);\n",
        "+                            // Parse markdown and insert as HTML\n",
        "+                            const markdownHtml = parseMarkdown(part.content);\n",
        "+                            const tempDiv = document.createElement('div');\n",
        "+                            tempDiv.innerHTML = markdownHtml;\n",
        "+                            \n",
        "+                            // Append all children\n",
        "+                            while (tempDiv.firstChild) {\n",
        "+                                markdownContainer.appendChild(tempDiv.firstChild);\n",
        "+                            }\n",
        "                         }\n",
        "                     } else if (part.type === 'code') {\n",
        "                         const codeBlock = createCodeBlock(part.language, part.content);\n",
        "-                        messageContent.appendChild(codeBlock);\n",
        "+                        markdownContainer.appendChild(codeBlock);\n",
        "                     }\n",
        "                 });\n",
        "                 \n",
        "+                messageContent.appendChild(markdownContainer);\n",
        "+                \n",
        "                 if (isError) {\n",
        "                     messageContent.style.background = 'var(--error-bg)';\n",
        "                     messageContent.style.color = 'var(--error-text)';\n",
        "@@ -1062,6 +1553,66 @@ <h2>\ud83d\udc4b Welcome to RAG Chat</h2>\n",
        "             }\n",
        "         }\n",
        " \n",
        "+        // Show/hide deep research loading indicator\n",
        "+        function showDeepResearchLoading(show) {\n",
        "+            let loadingDiv = document.getElementById('deepResearchLoading');\n",
        "+            \n",
        "+            if (show) {\n",
        "+                if (!loadingDiv) {\n",
        "+                    const chatMessages = document.getElementById('chatMessages');\n",
        "+                    loadingDiv = document.createElement('div');\n",
        "+                    loadingDiv.id = 'deepResearchLoading';\n",
        "+                    loadingDiv.className = 'deep-research-loading';\n",
        "+                    loadingDiv.innerHTML = `\n",
        "+                        <div class=\"message-avatar\" style=\"background: var(--assistant-bg);\">\ud83d\udd2c</div>\n",
        "+                        <div class=\"research-status\">\n",
        "+                            <div class=\"research-title\">\ud83e\udde0 Deep Research in Progress...</div>\n",
        "+                            <div class=\"research-steps\">\n",
        "+                                <div class=\"research-step active\">\ud83d\udccb Planning research tasks</div>\n",
        "+                                <div class=\"research-step\">\ud83d\udd0d Conducting web research</div>\n",
        "+                                <div class=\"research-step\">\ud83d\udcca Analyzing findings</div>\n",
        "+                                <div class=\"research-step\">\ud83d\udcdd Generating comprehensive report</div>\n",
        "+                            </div>\n",
        "+                            <div class=\"research-progress\">\n",
        "+                                <div class=\"progress-bar\"></div>\n",
        "+                            </div>\n",
        "+                        </div>\n",
        "+                    `;\n",
        "+                    chatMessages.appendChild(loadingDiv);\n",
        "+                    chatMessages.scrollTop = chatMessages.scrollHeight;\n",
        "+                }\n",
        "+                \n",
        "+                // Animate progress steps\n",
        "+                animateResearchSteps();\n",
        "+            } else {\n",
        "+                if (loadingDiv) {\n",
        "+                    loadingDiv.remove();\n",
        "+                }\n",
        "+            }\n",
        "+        }\n",
        "+\n",
        "+        // Animate research steps\n",
        "+        function animateResearchSteps() {\n",
        "+            const steps = document.querySelectorAll('.research-step');\n",
        "+            let currentStep = 0;\n",
        "+            \n",
        "+            const interval = setInterval(() => {\n",
        "+                if (currentStep < steps.length) {\n",
        "+                    steps[currentStep].classList.add('active');\n",
        "+                    currentStep++;\n",
        "+                } else {\n",
        "+                    // Loop back to beginning\n",
        "+                    steps.forEach(step => step.classList.remove('active'));\n",
        "+                    currentStep = 0;\n",
        "+                }\n",
        "+                \n",
        "+                // Check if loading indicator still exists\n",
        "+                if (!document.getElementById('deepResearchLoading')) {\n",
        "+                    clearInterval(interval);\n",
        "+                }\n",
        "+            }, 3000);\n",
        "+        }\n",
        "+\n",
        "         // Initialize\n",
        "         document.addEventListener('DOMContentLoaded', function() {\n",
        "             initTheme();\n"
      ]
    },
    {
      "path": "src/stores/llm/providers/GoogleGenAIProvider.py",
      "status": "modified",
      "additions": 7,
      "deletions": 2,
      "patch": "@@ -7,7 +7,7 @@\n \n from ..LLMEnums import GoogleGenAIEnums, DocumentTypeEnum\n from typing import List,Union\n-\n+from langchain_google_genai import ChatGoogleGenerativeAI\n class GoogleGenAIProvider(LLMInterface):\n     def __init__(\n         self,\n@@ -28,7 +28,7 @@ def __init__(\n \n         self.client = genai.Client(api_key=self.api_key)\n \n-        self.logger = logging.getLogger(__name__)\n+        self.logger = logging.getLogger('uvicorn')\n \n         self.enums = GoogleGenAIEnums\n \n@@ -155,4 +155,9 @@ def embed_documents(self, texts: List[str]) -> List[List[float]]:\n     def embed_query(self, text: str) -> List[float]:\n         return self.embed_text(text, document_type=DocumentTypeEnum.QUERY.value)\n \n+    def get_langchain_chat_model(self):\n+        if not self.generation_model_id:\n+            self.logger.error(\"Generation model is not set.\")\n+            return None\n+        return ChatGoogleGenerativeAI(model=self.generation_model_id, api_key=self.api_key)\n ",
      "patch_lines": [
        "@@ -7,7 +7,7 @@\n",
        " \n",
        " from ..LLMEnums import GoogleGenAIEnums, DocumentTypeEnum\n",
        " from typing import List,Union\n",
        "-\n",
        "+from langchain_google_genai import ChatGoogleGenerativeAI\n",
        " class GoogleGenAIProvider(LLMInterface):\n",
        "     def __init__(\n",
        "         self,\n",
        "@@ -28,7 +28,7 @@ def __init__(\n",
        " \n",
        "         self.client = genai.Client(api_key=self.api_key)\n",
        " \n",
        "-        self.logger = logging.getLogger(__name__)\n",
        "+        self.logger = logging.getLogger('uvicorn')\n",
        " \n",
        "         self.enums = GoogleGenAIEnums\n",
        " \n",
        "@@ -155,4 +155,9 @@ def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "     def embed_query(self, text: str) -> List[float]:\n",
        "         return self.embed_text(text, document_type=DocumentTypeEnum.QUERY.value)\n",
        " \n",
        "+    def get_langchain_chat_model(self):\n",
        "+        if not self.generation_model_id:\n",
        "+            self.logger.error(\"Generation model is not set.\")\n",
        "+            return None\n",
        "+        return ChatGoogleGenerativeAI(model=self.generation_model_id, api_key=self.api_key)\n",
        " \n"
      ]
    },
    {
      "path": "src/stores/llm/providers/OpenAIProvider.py",
      "status": "modified",
      "additions": 17,
      "deletions": 4,
      "patch": "@@ -1,10 +1,12 @@\n import logging\n+from typing import List, Union\n \n+from langchain_openai import ChatOpenAI\n from openai import OpenAI\n \n-from ..LLMEnums import OpenAIEnums, DocumentTypeEnum\n+from ..LLMEnums import DocumentTypeEnum, OpenAIEnums\n from ..LLMInterface import LLMInterface\n-from typing import List,Union\n+\n \n class OpenAIProvider(LLMInterface):\n     def __init__(\n@@ -124,12 +126,23 @@ def embed_text(self, text: Union[str, List[str]], document_type: str = None):\n \n         return [rec.embedding for rec in response.data]\n \n-\n     def construct_prompt(self, prompt: str, role: str):\n         return {\"role\": role, \"content\": prompt}\n \n     ## the following methods are just to comply with langchain expectations of an embedding model wrapper\n     def embed_documents(self, texts: List[str]) -> List[List[float]]:\n         return self.embed_text(texts, document_type=DocumentTypeEnum.DOCUMENT.value)\n+\n     def embed_query(self, text: str) -> List[float]:\n-        return self.embed_text(text, document_type=DocumentTypeEnum.QUERY.value)\n\\ No newline at end of file\n+        return self.embed_text(text, document_type=DocumentTypeEnum.QUERY.value)\n+\n+    def get_langchain_chat_model(self):\n+        if not self.generation_model_id:\n+            self.logger.error(\"Generation model is not set.\")\n+            return None\n+        return ChatOpenAI(\n+            model_name=self.generation_model_id,\n+            openai_api_key=self.api_key,\n+            openai_api_base=self.api_url,\n+            temperature=self.default_generation_temperature,\n+        )",
      "patch_lines": [
        "@@ -1,10 +1,12 @@\n",
        " import logging\n",
        "+from typing import List, Union\n",
        " \n",
        "+from langchain_openai import ChatOpenAI\n",
        " from openai import OpenAI\n",
        " \n",
        "-from ..LLMEnums import OpenAIEnums, DocumentTypeEnum\n",
        "+from ..LLMEnums import DocumentTypeEnum, OpenAIEnums\n",
        " from ..LLMInterface import LLMInterface\n",
        "-from typing import List,Union\n",
        "+\n",
        " \n",
        " class OpenAIProvider(LLMInterface):\n",
        "     def __init__(\n",
        "@@ -124,12 +126,23 @@ def embed_text(self, text: Union[str, List[str]], document_type: str = None):\n",
        " \n",
        "         return [rec.embedding for rec in response.data]\n",
        " \n",
        "-\n",
        "     def construct_prompt(self, prompt: str, role: str):\n",
        "         return {\"role\": role, \"content\": prompt}\n",
        " \n",
        "     ## the following methods are just to comply with langchain expectations of an embedding model wrapper\n",
        "     def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "         return self.embed_text(texts, document_type=DocumentTypeEnum.DOCUMENT.value)\n",
        "+\n",
        "     def embed_query(self, text: str) -> List[float]:\n",
        "-        return self.embed_text(text, document_type=DocumentTypeEnum.QUERY.value)\n",
        "\\ No newline at end of file\n",
        "+        return self.embed_text(text, document_type=DocumentTypeEnum.QUERY.value)\n",
        "+\n",
        "+    def get_langchain_chat_model(self):\n",
        "+        if not self.generation_model_id:\n",
        "+            self.logger.error(\"Generation model is not set.\")\n",
        "+            return None\n",
        "+        return ChatOpenAI(\n",
        "+            model_name=self.generation_model_id,\n",
        "+            openai_api_key=self.api_key,\n",
        "+            openai_api_base=self.api_url,\n",
        "+            temperature=self.default_generation_temperature,\n",
        "+        )\n"
      ]
    },
    {
      "path": "src/stores/llm/templates/locales/en/chat.py",
      "status": "added",
      "additions": 15,
      "deletions": 0,
      "patch": "@@ -0,0 +1,15 @@\n+from string import Template\n+\n+#### chat prompts ####\n+\n+#### System ####\n+\n+system_prompt = Template(\n+    \"\\n\".join(\n+        [\n+            \"You are an assistant to generate a response for the user.\",\n+            \"\"\n+        ]\n+    )\n+)\n+",
      "patch_lines": [
        "@@ -0,0 +1,15 @@\n",
        "+from string import Template\n",
        "+\n",
        "+#### chat prompts ####\n",
        "+\n",
        "+#### System ####\n",
        "+\n",
        "+system_prompt = Template(\n",
        "+    \"\\n\".join(\n",
        "+        [\n",
        "+            \"You are an assistant to generate a response for the user.\",\n",
        "+            \"\"\n",
        "+        ]\n",
        "+    )\n",
        "+)\n",
        "+\n"
      ]
    },
    {
      "path": "src/stores/llm/templates/locales/en/deep_researcher.py",
      "status": "added",
      "additions": 219,
      "deletions": 0,
      "patch": "@@ -0,0 +1,219 @@\n+\"\"\"System prompts and prompt templates for the Deep Research agent.\"\"\"\n+\n+# Supervisor/Lead Researcher Prompt\n+lead_researcher_prompt = \"\"\"You are a research supervisor managing a team of research agents. Today's date is {date}.\n+\n+<Task>\n+Your job is to break down the USER'S RESEARCH QUESTION into specific, focused research tasks and coordinate the research process.\n+When you have gathered enough comprehensive information to answer the USER'S EXACT QUESTION, indicate that research is complete.\n+\n+**CRITICAL: All research tasks MUST be directly related to answering the user's research question. Do NOT create tasks on unrelated topics.**\n+</Task>\n+\n+<Available Actions>\n+You can take these actions:\n+1. **Plan Research Tasks**: Break down the SPECIFIC research question into 1-{max_concurrent_research_units} focused tasks\n+2. **Use think_tool**: Reflect on research progress and decide next steps  \n+3. **Complete Research**: When you have sufficient findings to answer the ORIGINAL QUESTION\n+\n+**CRITICAL: Use think_tool after planning tasks to verify they are relevant to the research question.**\n+</Available Actions>\n+\n+<Instructions>\n+Think like a research manager:\n+\n+1. **READ THE RESEARCH QUESTION CAREFULLY** - What is the user ACTUALLY asking about?\n+2. **Stay On Topic** - All research tasks MUST directly address the research question\n+3. **Break Down the Research** - Split the SPECIFIC question into focused sub-tasks if needed\n+4. **After Each Cycle** - Use think_tool to verify: Are we answering the RIGHT question? Do we have enough?\n+5. **Know When to Stop** - Stop after {max_researcher_iterations} cycles or when you can confidently answer the ORIGINAL question\n+</Instructions>\n+\n+<Output Format>\n+When planning research, list your tasks clearly:\n+\n+RESEARCH_TASKS:\n+1. [First specific, focused research task]\n+2. [Second specific, focused research task]  \n+3. [Third task if needed]\n+\n+When complete, state: \"RESEARCH COMPLETE - We have gathered comprehensive information.\"\n+</Output Format>\n+\n+<Hard Limits>\n+- **Maximum {max_researcher_iterations} research cycles**\n+- **Maximum {max_concurrent_research_units} concurrent tasks per cycle**\n+- **Stop when you can answer confidently**\n+</Hard Limits>\"\"\"\n+\n+\n+# Individual Researcher System Prompt\n+research_system_prompt = \"\"\"You are a research assistant conducting research on a specific topic. Today's date is {date}.\n+\n+<Task>\n+Your job is to use the available tools to gather comprehensive information about the research topic.\n+You will use DuckDuckGo search for web searches and browser tools to navigate and extract detailed information from websites.\n+</Task>\n+\n+<Available Tools>\n+1. **DuckDuckGoSearchResults**: Search the web for relevant information and get article snippets\n+2. **think_tool**: Reflect on your research progress and plan next steps\n+\n+**CRITICAL: Use think_tool after each search to reflect on what you found and decide your next step.**\n+</Available Tools>\n+\n+<Research Strategy>\n+1. **Start with Broad Search**: Use DuckDuckGo to find relevant sources\n+2. **Analyze Search Results**: DuckDuckGo provides article snippets with titles, URLs, and content previews\n+3. **Reflect Frequently**: After each search, use think_tool to:\n+   - Assess what information you've gathered from search results\n+   - Identify gaps in your knowledge\n+   - Decide whether to continue with different searches or conclude research\n+4. **Gather Evidence**: Collect specific facts, quotes, and data points from search results\n+5. **Track Sources**: Note URLs and source titles from search results for citations\n+</Research Strategy>\n+\n+<Instructions>\n+Follow this systematic approach:\n+\n+1. **Initial Search**: Start with a DuckDuckGo search using the research topic\n+2. **Analyze Results**: Use think_tool to evaluate search results quality and relevance\n+3. **Refine Searches**: If needed, conduct more specific searches to fill information gaps\n+4. **Extract Information**: Gather key facts and insights from the search result snippets\n+5. **Continue or Conclude**: After 3-5 quality searches with relevant results, assess if you have enough\n+6. **Stop Criteria**: \n+   - You have comprehensive information from multiple sources\n+   - You've made 10+ tool calls\n+   - Further searches yield redundant information\n+</Instructions>\n+\n+<Hard Limits>\n+- **Maximum 10 tool calls** (searches + browsing + think_tool combined)\n+- **Stop when you have 3+ quality sources** with relevant information\n+- **Don't over-research** - comprehensive is better than exhaustive\n+</Hard Limits>\n+\n+<Output Expectations>\n+Your research will be automatically summarized. Focus on:\n+- Finding factual, relevant information\n+- Gathering from diverse, credible sources\n+- Noting specific details, statistics, and quotes\n+- Recording source URLs for citations\n+</Output Expectations>\"\"\"\n+\n+\n+# Research Compression Prompt\n+compress_research_system_prompt = \"\"\"You are a research assistant that has conducted research by using search and browsing tools. Your job is to clean up and synthesize the findings.\n+\n+<Task>\n+Clean up information gathered from tool calls and web searches while preserving ALL relevant information.\n+All relevant statements and information must be repeated verbatim, but in a cleaner, more organized format.\n+</Task>\n+\n+<Guidelines>\n+1. **Be Comprehensive**: Include ALL information gathered - your output should be fully comprehensive\n+2. **Preserve Details**: Repeat key information verbatim - don't lose any findings\n+3. **Organize Clearly**: Structure findings with clear sections and bullet points\n+4. **Include Citations**: Provide inline citations [1], [2], etc. for each source\n+5. **List All Sources**: Include a \"Sources\" section at the end with URLs and titles\n+6. **No Summarization**: This is about cleaning and organizing, not summarizing\n+</Guidelines>\n+\n+<Output Structure>\n+**Queries and Actions Performed**\n+- List all searches and websites visited\n+\n+**Comprehensive Findings**\n+- Organize findings by theme or subtopic\n+- Include inline citations [1], [2]\n+- Preserve specific facts, statistics, and quotes\n+\n+**Sources**\n+1. [Source Title]: [URL]\n+2. [Source Title]: [URL]\n+...\n+</Output Structure>\n+\n+<Critical Reminder>\n+It is EXTREMELY important that any information even remotely relevant to the research topic is preserved verbatim.\n+Do not paraphrase, summarize, or rewrite - just clean up and organize the raw information.\n+</Critical Reminder>\"\"\"\n+\n+\n+compress_research_simple_human_message = \"\"\"All above messages contain research conducted by an AI Researcher. Please clean up these findings.\n+\n+DO NOT summarize the information. Return the raw information in a cleaner format. Make sure ALL relevant information is preserved - you can rewrite findings verbatim.\"\"\"\n+\n+\n+# Final Report Generation Prompt\n+final_report_generation_prompt = \"\"\"Based on all the research conducted, create a comprehensive, well-structured answer to the research question:\n+\n+<Research Question>\n+{research_question}\n+</Research Question>\n+\n+<Date>\n+Today's date is {date}\n+</Date>\n+\n+<Research Findings>\n+{all_research}\n+</Research Findings>\n+\n+<Instructions>\n+Create a detailed, professional research report that:\n+\n+1. **Is Well-Structured**: Use proper markdown headings (# title, ## sections, ### subsections)\n+2. **Is Comprehensive**: Include all relevant facts and insights from the research\n+3. **Cites Sources**: Reference sources using [Title](URL) format inline\n+4. **Is Balanced**: Present multiple perspectives when applicable\n+5. **Is Detailed**: People expect deep research - be thorough and comprehensive\n+6. **Includes Sources Section**: List all referenced sources at the end\n+\n+<Report Structure Guidelines>\n+Structure your report based on the research question type:\n+\n+**For Comparisons**:\n+- Introduction\n+- Overview of each item being compared\n+- Detailed comparison across key dimensions\n+- Conclusion\n+\n+**For Lists/Rankings**:\n+- Brief introduction\n+- Organized list or table\n+- Details for each item\n+- (No conclusion needed for simple lists)\n+\n+**For Overviews/Summaries**:\n+- Introduction to the topic\n+- Major concepts/aspects (separate sections)\n+- Supporting details and examples\n+- Conclusion\n+\n+**For Analysis Questions**:\n+- Executive summary\n+- Background/context\n+- Main analysis (multiple sections)\n+- Implications/conclusions\n+</Report Structure Guidelines>\n+\n+<Writing Guidelines>\n+- Use clear, professional language\n+- Use ## for section titles (Markdown)\n+- Do NOT refer to yourself as the writer\n+- Do NOT include meta-commentary\n+- Write in paragraph form by default, use bullets when appropriate\n+- Each section should be comprehensive and detailed\n+- Include specific facts, statistics, and examples\n+\n+<Source Citations>\n+- Assign each unique URL a citation number [1], [2], [3]...\n+- Use inline citations throughout: \"According to [1], ...\"\n+- End with ### Sources section listing all sources\n+- Format: [1] Source Title: URL (on separate lines)\n+</Source Citations>\n+\n+<Critical Reminder>\n+Make the report comprehensive and detailed. Users expect thorough deep research with extensive information.\n+</Critical Reminder>\"\"\"",
      "patch_lines": [
        "@@ -0,0 +1,219 @@\n",
        "+\"\"\"System prompts and prompt templates for the Deep Research agent.\"\"\"\n",
        "+\n",
        "+# Supervisor/Lead Researcher Prompt\n",
        "+lead_researcher_prompt = \"\"\"You are a research supervisor managing a team of research agents. Today's date is {date}.\n",
        "+\n",
        "+<Task>\n",
        "+Your job is to break down the USER'S RESEARCH QUESTION into specific, focused research tasks and coordinate the research process.\n",
        "+When you have gathered enough comprehensive information to answer the USER'S EXACT QUESTION, indicate that research is complete.\n",
        "+\n",
        "+**CRITICAL: All research tasks MUST be directly related to answering the user's research question. Do NOT create tasks on unrelated topics.**\n",
        "+</Task>\n",
        "+\n",
        "+<Available Actions>\n",
        "+You can take these actions:\n",
        "+1. **Plan Research Tasks**: Break down the SPECIFIC research question into 1-{max_concurrent_research_units} focused tasks\n",
        "+2. **Use think_tool**: Reflect on research progress and decide next steps  \n",
        "+3. **Complete Research**: When you have sufficient findings to answer the ORIGINAL QUESTION\n",
        "+\n",
        "+**CRITICAL: Use think_tool after planning tasks to verify they are relevant to the research question.**\n",
        "+</Available Actions>\n",
        "+\n",
        "+<Instructions>\n",
        "+Think like a research manager:\n",
        "+\n",
        "+1. **READ THE RESEARCH QUESTION CAREFULLY** - What is the user ACTUALLY asking about?\n",
        "+2. **Stay On Topic** - All research tasks MUST directly address the research question\n",
        "+3. **Break Down the Research** - Split the SPECIFIC question into focused sub-tasks if needed\n",
        "+4. **After Each Cycle** - Use think_tool to verify: Are we answering the RIGHT question? Do we have enough?\n",
        "+5. **Know When to Stop** - Stop after {max_researcher_iterations} cycles or when you can confidently answer the ORIGINAL question\n",
        "+</Instructions>\n",
        "+\n",
        "+<Output Format>\n",
        "+When planning research, list your tasks clearly:\n",
        "+\n",
        "+RESEARCH_TASKS:\n",
        "+1. [First specific, focused research task]\n",
        "+2. [Second specific, focused research task]  \n",
        "+3. [Third task if needed]\n",
        "+\n",
        "+When complete, state: \"RESEARCH COMPLETE - We have gathered comprehensive information.\"\n",
        "+</Output Format>\n",
        "+\n",
        "+<Hard Limits>\n",
        "+- **Maximum {max_researcher_iterations} research cycles**\n",
        "+- **Maximum {max_concurrent_research_units} concurrent tasks per cycle**\n",
        "+- **Stop when you can answer confidently**\n",
        "+</Hard Limits>\"\"\"\n",
        "+\n",
        "+\n",
        "+# Individual Researcher System Prompt\n",
        "+research_system_prompt = \"\"\"You are a research assistant conducting research on a specific topic. Today's date is {date}.\n",
        "+\n",
        "+<Task>\n",
        "+Your job is to use the available tools to gather comprehensive information about the research topic.\n",
        "+You will use DuckDuckGo search for web searches and browser tools to navigate and extract detailed information from websites.\n",
        "+</Task>\n",
        "+\n",
        "+<Available Tools>\n",
        "+1. **DuckDuckGoSearchResults**: Search the web for relevant information and get article snippets\n",
        "+2. **think_tool**: Reflect on your research progress and plan next steps\n",
        "+\n",
        "+**CRITICAL: Use think_tool after each search to reflect on what you found and decide your next step.**\n",
        "+</Available Tools>\n",
        "+\n",
        "+<Research Strategy>\n",
        "+1. **Start with Broad Search**: Use DuckDuckGo to find relevant sources\n",
        "+2. **Analyze Search Results**: DuckDuckGo provides article snippets with titles, URLs, and content previews\n",
        "+3. **Reflect Frequently**: After each search, use think_tool to:\n",
        "+   - Assess what information you've gathered from search results\n",
        "+   - Identify gaps in your knowledge\n",
        "+   - Decide whether to continue with different searches or conclude research\n",
        "+4. **Gather Evidence**: Collect specific facts, quotes, and data points from search results\n",
        "+5. **Track Sources**: Note URLs and source titles from search results for citations\n",
        "+</Research Strategy>\n",
        "+\n",
        "+<Instructions>\n",
        "+Follow this systematic approach:\n",
        "+\n",
        "+1. **Initial Search**: Start with a DuckDuckGo search using the research topic\n",
        "+2. **Analyze Results**: Use think_tool to evaluate search results quality and relevance\n",
        "+3. **Refine Searches**: If needed, conduct more specific searches to fill information gaps\n",
        "+4. **Extract Information**: Gather key facts and insights from the search result snippets\n",
        "+5. **Continue or Conclude**: After 3-5 quality searches with relevant results, assess if you have enough\n",
        "+6. **Stop Criteria**: \n",
        "+   - You have comprehensive information from multiple sources\n",
        "+   - You've made 10+ tool calls\n",
        "+   - Further searches yield redundant information\n",
        "+</Instructions>\n",
        "+\n",
        "+<Hard Limits>\n",
        "+- **Maximum 10 tool calls** (searches + browsing + think_tool combined)\n",
        "+- **Stop when you have 3+ quality sources** with relevant information\n",
        "+- **Don't over-research** - comprehensive is better than exhaustive\n",
        "+</Hard Limits>\n",
        "+\n",
        "+<Output Expectations>\n",
        "+Your research will be automatically summarized. Focus on:\n",
        "+- Finding factual, relevant information\n",
        "+- Gathering from diverse, credible sources\n",
        "+- Noting specific details, statistics, and quotes\n",
        "+- Recording source URLs for citations\n",
        "+</Output Expectations>\"\"\"\n",
        "+\n",
        "+\n",
        "+# Research Compression Prompt\n",
        "+compress_research_system_prompt = \"\"\"You are a research assistant that has conducted research by using search and browsing tools. Your job is to clean up and synthesize the findings.\n",
        "+\n",
        "+<Task>\n",
        "+Clean up information gathered from tool calls and web searches while preserving ALL relevant information.\n",
        "+All relevant statements and information must be repeated verbatim, but in a cleaner, more organized format.\n",
        "+</Task>\n",
        "+\n",
        "+<Guidelines>\n",
        "+1. **Be Comprehensive**: Include ALL information gathered - your output should be fully comprehensive\n",
        "+2. **Preserve Details**: Repeat key information verbatim - don't lose any findings\n",
        "+3. **Organize Clearly**: Structure findings with clear sections and bullet points\n",
        "+4. **Include Citations**: Provide inline citations [1], [2], etc. for each source\n",
        "+5. **List All Sources**: Include a \"Sources\" section at the end with URLs and titles\n",
        "+6. **No Summarization**: This is about cleaning and organizing, not summarizing\n",
        "+</Guidelines>\n",
        "+\n",
        "+<Output Structure>\n",
        "+**Queries and Actions Performed**\n",
        "+- List all searches and websites visited\n",
        "+\n",
        "+**Comprehensive Findings**\n",
        "+- Organize findings by theme or subtopic\n",
        "+- Include inline citations [1], [2]\n",
        "+- Preserve specific facts, statistics, and quotes\n",
        "+\n",
        "+**Sources**\n",
        "+1. [Source Title]: [URL]\n",
        "+2. [Source Title]: [URL]\n",
        "+...\n",
        "+</Output Structure>\n",
        "+\n",
        "+<Critical Reminder>\n",
        "+It is EXTREMELY important that any information even remotely relevant to the research topic is preserved verbatim.\n",
        "+Do not paraphrase, summarize, or rewrite - just clean up and organize the raw information.\n",
        "+</Critical Reminder>\"\"\"\n",
        "+\n",
        "+\n",
        "+compress_research_simple_human_message = \"\"\"All above messages contain research conducted by an AI Researcher. Please clean up these findings.\n",
        "+\n",
        "+DO NOT summarize the information. Return the raw information in a cleaner format. Make sure ALL relevant information is preserved - you can rewrite findings verbatim.\"\"\"\n",
        "+\n",
        "+\n",
        "+# Final Report Generation Prompt\n",
        "+final_report_generation_prompt = \"\"\"Based on all the research conducted, create a comprehensive, well-structured answer to the research question:\n",
        "+\n",
        "+<Research Question>\n",
        "+{research_question}\n",
        "+</Research Question>\n",
        "+\n",
        "+<Date>\n",
        "+Today's date is {date}\n",
        "+</Date>\n",
        "+\n",
        "+<Research Findings>\n",
        "+{all_research}\n",
        "+</Research Findings>\n",
        "+\n",
        "+<Instructions>\n",
        "+Create a detailed, professional research report that:\n",
        "+\n",
        "+1. **Is Well-Structured**: Use proper markdown headings (# title, ## sections, ### subsections)\n",
        "+2. **Is Comprehensive**: Include all relevant facts and insights from the research\n",
        "+3. **Cites Sources**: Reference sources using [Title](URL) format inline\n",
        "+4. **Is Balanced**: Present multiple perspectives when applicable\n",
        "+5. **Is Detailed**: People expect deep research - be thorough and comprehensive\n",
        "+6. **Includes Sources Section**: List all referenced sources at the end\n",
        "+\n",
        "+<Report Structure Guidelines>\n",
        "+Structure your report based on the research question type:\n",
        "+\n",
        "+**For Comparisons**:\n",
        "+- Introduction\n",
        "+- Overview of each item being compared\n",
        "+- Detailed comparison across key dimensions\n",
        "+- Conclusion\n",
        "+\n",
        "+**For Lists/Rankings**:\n",
        "+- Brief introduction\n",
        "+- Organized list or table\n",
        "+- Details for each item\n",
        "+- (No conclusion needed for simple lists)\n",
        "+\n",
        "+**For Overviews/Summaries**:\n",
        "+- Introduction to the topic\n",
        "+- Major concepts/aspects (separate sections)\n",
        "+- Supporting details and examples\n",
        "+- Conclusion\n",
        "+\n",
        "+**For Analysis Questions**:\n",
        "+- Executive summary\n",
        "+- Background/context\n",
        "+- Main analysis (multiple sections)\n",
        "+- Implications/conclusions\n",
        "+</Report Structure Guidelines>\n",
        "+\n",
        "+<Writing Guidelines>\n",
        "+- Use clear, professional language\n",
        "+- Use ## for section titles (Markdown)\n",
        "+- Do NOT refer to yourself as the writer\n",
        "+- Do NOT include meta-commentary\n",
        "+- Write in paragraph form by default, use bullets when appropriate\n",
        "+- Each section should be comprehensive and detailed\n",
        "+- Include specific facts, statistics, and examples\n",
        "+\n",
        "+<Source Citations>\n",
        "+- Assign each unique URL a citation number [1], [2], [3]...\n",
        "+- Use inline citations throughout: \"According to [1], ...\"\n",
        "+- End with ### Sources section listing all sources\n",
        "+- Format: [1] Source Title: URL (on separate lines)\n",
        "+</Source Citations>\n",
        "+\n",
        "+<Critical Reminder>\n",
        "+Make the report comprehensive and detailed. Users expect thorough deep research with extensive information.\n",
        "+</Critical Reminder>\"\"\"\n"
      ]
    }
  ]
}
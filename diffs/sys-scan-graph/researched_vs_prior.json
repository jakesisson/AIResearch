{
  "project": "Research Data/sys-scan-graph",
  "repo": "J-mazz/sys-scan-graph",
  "prior_commit": "d0e0ea8a9d030175fb196f8f4f30ecf378bd0f10",
  "researched_commit": "b9a4506472e8dd660ce3d5d2e2b4f2d076a4d16f",
  "compare_url": "https://github.com/J-mazz/sys-scan-graph/compare/d0e0ea8a9d030175fb196f8f4f30ecf378bd0f10...b9a4506472e8dd660ce3d5d2e2b4f2d076a4d16f",
  "ahead_by": 1,
  "behind_by": 0,
  "changed_files": [
    {
      "path": ".github/workflows/ci.yml.disabled",
      "status": "renamed",
      "additions": 22,
      "deletions": 0,
      "patch": "@@ -27,6 +27,28 @@ jobs:\n       - name: Run tests\n         run: cd build && ctest --output-on-failure\n \n+  python-test:\n+    runs-on: ubuntu-latest\n+    steps:\n+      - uses: actions/checkout@v4\n+      - name: Set up Python\n+        uses: actions/setup-python@v5\n+        with:\n+          python-version: '3.11'\n+      - name: Install Python dependencies\n+        run: |\n+          cd agent\n+          python -m pip install --upgrade pip\n+          pip install -r requirements.txt\n+      - name: Run Python tests\n+        run: |\n+          cd agent\n+          python -m pytest tests/ -v --tb=short --maxfail=5\n+      - name: Test Python module compilation\n+        run: |\n+          cd agent\n+          python -c \"import sys; sys.path.insert(0, '.'); import graph_nodes_performance; import graph_nodes_scalability; import graph_nodes_reliability; print('All modules compile successfully')\"\n+\n   sanitize:\n     runs-on: ubuntu-latest\n     steps:",
      "patch_lines": [
        "@@ -27,6 +27,28 @@ jobs:\n",
        "       - name: Run tests\n",
        "         run: cd build && ctest --output-on-failure\n",
        " \n",
        "+  python-test:\n",
        "+    runs-on: ubuntu-latest\n",
        "+    steps:\n",
        "+      - uses: actions/checkout@v4\n",
        "+      - name: Set up Python\n",
        "+        uses: actions/setup-python@v5\n",
        "+        with:\n",
        "+          python-version: '3.11'\n",
        "+      - name: Install Python dependencies\n",
        "+        run: |\n",
        "+          cd agent\n",
        "+          python -m pip install --upgrade pip\n",
        "+          pip install -r requirements.txt\n",
        "+      - name: Run Python tests\n",
        "+        run: |\n",
        "+          cd agent\n",
        "+          python -m pytest tests/ -v --tb=short --maxfail=5\n",
        "+      - name: Test Python module compilation\n",
        "+        run: |\n",
        "+          cd agent\n",
        "+          python -c \"import sys; sys.path.insert(0, '.'); import graph_nodes_performance; import graph_nodes_scalability; import graph_nodes_reliability; print('All modules compile successfully')\"\n",
        "+\n",
        "   sanitize:\n",
        "     runs-on: ubuntu-latest\n",
        "     steps:\n"
      ],
      "previous_filename": ".github/workflows/ci.yml"
    },
    {
      "path": ".github/workflows/codeql.yml.disabled",
      "status": "renamed",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": [],
      "previous_filename": ".github/workflows/codeql.yml"
    },
    {
      "path": ".github/workflows/release-validate.yml.disabled",
      "status": "renamed",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": [],
      "previous_filename": ".github/workflows/release-validate.yml"
    },
    {
      "path": "ARCHITECTURE.md",
      "status": "modified",
      "additions": 82,
      "deletions": 50,
      "patch": "@@ -135,7 +135,10 @@ The Intelligence Layer consumes a raw Core report and produces an enriched artif\n | `enrichment_results` | Auxiliary data: token accounting, perf snapshot, warnings |\n | `integrity` | SHA256 & optional signature verification status |\n \n-### Pipeline Stages \n+### Legacy Pipeline Stages (Preserved for Reference)\n+\n+The original sequential pipeline implementation has been moved to `agent/legacy/pipeline.py` and is preserved for reference. The following stages represent the legacy linear processing approach:\n+\n 1. Load & Validate: size guard, UTF\u20118 strict decode, JSON parse, schema (optional).  \n 2. Augment: host_id & scan_id derivation, tagging, risk subscore seed, host role classification (role rationale added).  \n 3. Knowledge Enrichment: ports, modules, SUID expectations, org attribution (YAML packs).  \n@@ -154,9 +157,10 @@ The Intelligence Layer consumes a raw Core report and produces an enriched artif\n 16. Canonicalize: stable ordering & JSON cleansing for reproducible hashing.  \n \n ### Policy & Governance\n-* Policy layer escalates severity (and impact subscore) for executables outside approved directories (config/env allowlist + default system dirs).\n-* Redaction filters user paths (`/home/<user>` etc.) before any LLM or summarization call.\n-* Risk weights & logistic calibration user-editable via CLI; stored persistently.\n+\n+- Policy layer escalates severity (and impact subscore) for executables outside approved directories (config/env allowlist + default system dirs).\n+- Redaction filters user paths (`/home/<user>` etc.) before any LLM or summarization call.\n+- Risk weights & logistic calibration user-editable via CLI; stored persistently.\n \n ### Baseline Database\n SQLite schema versions (current v5) track: finding rarity, module observations, metric time series, calibration observations, process clusters. EWMA metrics support smoother drift detection and future predictive heuristics.\n@@ -177,63 +181,71 @@ Metrics collector wraps key stages, storing count/total/avg. Regression detectio\n If verification key (`AGENT_VERIFY_KEY_B64`) present, raw report signature verification performed; integrity block records digest, match flags, and errors without aborting enrichment (best-effort). Separate CLI commands for key generation, sign, and verify.\n \n ---\n-## 3. LangGraph DAG & Cyclical Reasoning\n-`graph_pipeline.py` and `graph.py` implement a LangGraph workflow with a bounded baseline enrichment cycle:\n-* Entry chain: enrich -> summarize -> (router) baseline cycle or rule suggestion.\n-* Baseline cycle: plan_baseline -> baseline_tools -> integrate_baseline -> summarize (re-enters summarize with added context) guarded by `AGENT_MAX_SUMMARY_ITERS` (default 3) and `baseline_cycle_done` flag.\n-* Conditional Routing: `choose_post_summarize` decides whether additional baseline queries are needed (missing `baseline_status`) before proceeding to rule suggestion.\n-* Rule Suggestion: mined via `suggest_rules` after summary iterations conclude or routing short-circuits.\n-* Determinism Guards: iteration_count tracked; max iterations enforced; baseline cycle executed at most once; state transitions purely function-of-input under fixed environment.\n-* Tool Integration: Baseline queries executed through ToolNode (structured `query_baseline` tool_calls) returning structured ToolMessages integrated deterministically.\n+## 3. LangGraph DAG & Cyclical Reasoning (Canonical Implementation)\n+\n+`graph_pipeline.py` and `graph.py` implement the **canonical LangGraph workflow** that serves as the default execution path for the intelligence layer. This replaces the legacy linear pipeline and provides a bounded baseline enrichment cycle:\n+\n+- Entry chain: enrich -> summarize -> (router) baseline cycle or rule suggestion.\n+- Baseline cycle: plan_baseline -> baseline_tools -> integrate_baseline -> summarize (re-enters summarize with added context) guarded by `AGENT_MAX_SUMMARY_ITERS` (default 3) and `baseline_cycle_done` flag.\n+- Conditional Routing: `choose_post_summarize` decides whether additional baseline queries are needed (missing `baseline_status`) before proceeding to rule suggestion.\n+- Rule Suggestion: mined via `suggest_rules` after summary iterations conclude or routing short-circuits.\n+- Determinism Guards: iteration_count tracked; max iterations enforced; baseline cycle executed at most once; state transitions purely function-of-input under fixed environment.\n+- Tool Integration: Baseline queries executed through ToolNode (structured `query_baseline` tool_calls) returning structured ToolMessages integrated deterministically.\n \n Loop pattern (current implementation):\n-```\n+\n+```bash\n enrich -> summarize -> plan_baseline -> baseline_tools -> integrate_baseline -> summarize -> (router) -> suggest_rules -> END\n ```\n+\n If no baseline context needed: `enrich -> summarize -> suggest_rules -> END`.\n Environment variable `AGENT_MAX_SUMMARY_ITERS` caps summarize passes; exceeding limit appends a deterministic warning and halts further iterations.\n \n ### 3.1 Enhanced Scaffold Workflow (2025 Integration)\n+\n An enhanced, asynchronously capable workflow is now available (default when `AGENT_GRAPH_MODE=enhanced`) that wires in additional analytical and operational nodes exported from `graph_nodes_scaffold.py`.\n \n Enhanced sequence (simplified):\n-```\n+\n+```bash\n enrich(enhanced) -> correlate -> risk_analyzer -> compliance_checker -> summarize(enhanced)\n   -> (router choose_post_summarize: plan_baseline | suggest_rules | metrics_collector)\n   -> (optional baseline cycle) -> suggest_rules(enhanced)\n   -> error_handler -> human_feedback_node -> cache_manager -> metrics_collector -> END\n ```\n \n Key additions:\n-* `enhanced_enrich_findings` / `enhanced_summarize_host_state` / `enhanced_suggest_rules`: async variants with caching, metrics, streaming flag, and optional rule refinement.\n-* `risk_analyzer` & `compliance_checker`: pre\u2011summary analytics producing `risk_assessment` and `compliance_check` blocks used downstream and surfaced in final metrics.\n-* Operational tail: `error_handler`, `human_feedback_node`, `cache_manager`, `metrics_collector` ensure graceful degradation, feedback integration, cache consolidation and a deterministic `final_metrics` snapshot even on early termination paths (router END is redirected through `metrics_collector`).\n-* Deterministic caching: enrichment cache keyed by SHA256 of canonical JSON of `raw_findings`; summary iterations tracked; cache hit metrics recorded.\n-* Dynamic recovery: `build_workflow()` performs a late import recovery to tolerate early import ordering issues during test discovery, ensuring enhanced nodes remain available even if the module was first imported before scaffold initialization.\n+- `enhanced_enrich_findings` / `enhanced_summarize_host_state` / `enhanced_suggest_rules`: async variants with caching, metrics, streaming flag, and optional rule refinement.\n+- `risk_analyzer` & `compliance_checker`: pre\u2011summary analytics producing `risk_assessment` and `compliance_check` blocks used downstream and surfaced in final metrics.\n+- Operational tail: `error_handler`, `human_feedback_node`, `cache_manager`, `metrics_collector` ensure graceful degradation, feedback integration, cache consolidation and a deterministic `final_metrics` snapshot even on early termination paths (router END is redirected through `metrics_collector`).\n+- Deterministic caching: enrichment cache keyed by SHA256 of canonical JSON of `raw_findings`; summary iterations tracked; cache hit metrics recorded.\n+- Dynamic recovery: `build_workflow()` performs a late import recovery to tolerate early import ordering issues during test discovery, ensuring enhanced nodes remain available even if the module was first imported before scaffold initialization.\n \n Environment toggles:\n-* `AGENT_GRAPH_MODE=enhanced|baseline` selects enhanced vs legacy core nodes.\n-* `AGENT_MAX_SUMMARY_ITERS` caps summarize iterations (applies to both modes).\n-* `AGENT_KB_REQUIRE_SIGNATURES=1` with `AGENT_KB_PUBKEY` enforces knowledge file signature presence (adds `SignatureMissing` warnings gathered by the knowledge layer).\n+- `AGENT_GRAPH_MODE=enhanced|baseline` selects enhanced vs legacy core nodes.\n+- `AGENT_MAX_SUMMARY_ITERS` caps summarize iterations (applies to both modes).\n+- `AGENT_KB_REQUIRE_SIGNATURES=1` with `AGENT_KB_PUBKEY` enforces knowledge file signature presence (adds `SignatureMissing` warnings gathered by the knowledge layer).\n \n Metrics & Finalization:\n-* Each enhanced node records `*_duration` plus call counters.\n-* `metrics_collector` aggregates counts (suggestions, enriched, correlated), cache size, compliance standard count, risk level, provider mode, degraded flag, and total duration (if `start_time` present) into `final_metrics`.\n-* Early END decisions (e.g. no high\u2011severity findings) are routed through `metrics_collector` to guarantee consistent terminal accounting.\n+- Each enhanced node records `*_duration` plus call counters.\n+- `metrics_collector` aggregates counts (suggestions, enriched, correlated), cache size, compliance standard count, risk level, provider mode, degraded flag, and total duration (if `start_time` present) into `final_metrics`.\n+- Early END decisions (e.g. no high\u2011severity findings) are routed through `metrics_collector` to guarantee consistent terminal accounting.\n \n Fallback / Disabled Components:\n-* `advanced_router` and `tool_coordinator` are present in scaffold exports but intentionally not yet wired into the enhanced DAG pending a pure router refactor (current implementation returns string values which require dedicated conditional edge wiring distinct from state mutating nodes).\n-* The system automatically falls back to baseline sync nodes if enhanced async counterparts are unavailable.\n+- `advanced_router` and `tool_coordinator` are present in scaffold exports but intentionally not yet wired into the enhanced DAG pending a pure router refactor (current implementation returns string values which require dedicated conditional edge wiring distinct from state mutating nodes).\n+- The system automatically falls back to baseline sync nodes if enhanced async counterparts are unavailable.\n \n Error & Resilience:\n-* All nodes defensively coerce placeholder `None` containers (introduced by graph initialization) into concrete lists/dicts before mutation to avoid `TypeError` on enhanced asynchronous paths.\n-* `error_handler` centralizes detection of timeout / degraded mode signals and increments structured metrics (`timeout_error_count`, etc.).\n+- All nodes defensively coerce placeholder `None` containers (introduced by graph initialization) into concrete lists/dicts before mutation to avoid `TypeError` on enhanced asynchronous paths.\n+- `error_handler` centralizes detection of timeout / degraded mode signals and increments structured metrics (`timeout_error_count`, etc.).\n \n ---\n ## 4. Canonicalization & Reproducibility Guarantees\n+\n Enriched output re-serialized into a canonical dict ordering (keys sorted; arrays left in stable constructed order) before final Pydantic model rehydration. This ensures:\n-* Stable hashes across environments given identical inputs, configs, weights, calibration, rule pack, baseline DB state, and versions.\n-* Low-noise diffs for CI gating & artifact promotion.\n+\n+- Stable hashes across environments given identical inputs, configs, weights, calibration, rule pack, baseline DB state, and versions.\n+- Low-noise diffs for CI gating & artifact promotion.\n \n ---\n ## 5. Security & Privacy Considerations\n@@ -260,39 +272,59 @@ No external network calls by default. Optional corpus enrichment (`AGENT_LOAD_HF\n \n ---\n ## 7. Roadmap (Architectural)\n-* Native module decompression (remove shelling to `xz`,`gzip`).\n-* Structured warning channel in core (separate from findings).\n-* Bounded LangGraph iterative refinement (rule & action optimization).\n-* Formal provenance & SBOM correlation linking enriched findings to package metadata.\n+\n+- Native module decompression (remove shelling to `xz`,`gzip`).\n+- Structured warning channel in core (separate from findings).\n+- Bounded LangGraph iterative refinement (rule & action optimization).\n+- Formal provenance & SBOM correlation linking enriched findings to package metadata.\n \n ---\n ## 8. Diagrams\n+\n ### Core \u2192 Intelligence Layer Data Flow\n-```\n-\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   JSON (schema v2)   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n-\u2502 Core Scan   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502 Intelligence Layer  \u2502\n-\u2502 (C++20)     \u2502                      \u2502 (Python)            \u2502\n-\u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n-    \u2502 timings                             \u2502 enriched JSON\n-    \u25bc                                     \u25bc\n-  canonical JSON                       canonical enriched JSON\n+\n+```mermaid\n+graph TD\n+    A[Core Scan<br/>C++20] --> B[Intelligence Layer<br/>Python - LangGraph]\n+    A --> C[canonical JSON]\n+    B --> D[canonical enriched JSON]\n ```\n \n-### Intelligence Layer Stage Graph (Current Linear)\n+### Intelligence Layer Execution Paths\n+\n+**Canonical LangGraph Path (Default):**\n+```mermaid\n+graph TD\n+    A[enrich] --> B[summarize]\n+    B --> C{router}\n+    C -->|baseline needed| D[plan_baseline]\n+    C -->|no baseline| E[suggest_rules]\n+    D --> F[baseline_tools]\n+    F --> G[integrate_baseline]\n+    G --> B\n+    B --> H[END]\n+    E --> H\n ```\n+\n+**Legacy Linear Pipeline (Preserved for Reference):**\n+```bash\n load -> validate -> augment -> correlate -> baseline -> novelty -> sequence -> drift -> multi_host -> reduce -> followups -> actions -> summarize -> output\n ```\n \n ---\n ## 9. Known Limitations\n-* Core severity taxonomy still string-based (enum refactor pending).\n-* Rule engine matching primitive (no regex/DSL in proprietary layer\u2014future safe expansion planned).\n-* Process embedding intentionally lightweight; not semantic; may miss nuanced novelty.\n-* ATT&CK mapping subset; coverage counts not weighted by evidentiary strength.\n-* Cyclical reasoning hooks present conceptually, not yet activated (ensures current determinism baseline).\n+\n+- Core severity taxonomy still string-based (enum refactor pending).\n+- Rule engine matching primitive (no regex/DSL in proprietary layer\u2014future safe expansion planned).\n+- Process embedding intentionally lightweight; not semantic; may miss nuanced novelty.\n+- ATT&CK mapping subset; coverage counts not weighted by evidentiary strength.\n+- Cyclical reasoning hooks present conceptually, not yet activated (ensures current determinism baseline).\n \n ---\n ## 10. Contact\n+\n Design proposals: open issue tagged `design`. For security disclosures follow `SECURITY.md`.\n \n-_End of Architecture Document_\n+---\n+\n+End of Architecture Document",
      "patch_lines": [
        "@@ -135,7 +135,10 @@ The Intelligence Layer consumes a raw Core report and produces an enriched artif\n",
        " | `enrichment_results` | Auxiliary data: token accounting, perf snapshot, warnings |\n",
        " | `integrity` | SHA256 & optional signature verification status |\n",
        " \n",
        "-### Pipeline Stages \n",
        "+### Legacy Pipeline Stages (Preserved for Reference)\n",
        "+\n",
        "+The original sequential pipeline implementation has been moved to `agent/legacy/pipeline.py` and is preserved for reference. The following stages represent the legacy linear processing approach:\n",
        "+\n",
        " 1. Load & Validate: size guard, UTF\u20118 strict decode, JSON parse, schema (optional).  \n",
        " 2. Augment: host_id & scan_id derivation, tagging, risk subscore seed, host role classification (role rationale added).  \n",
        " 3. Knowledge Enrichment: ports, modules, SUID expectations, org attribution (YAML packs).  \n",
        "@@ -154,9 +157,10 @@ The Intelligence Layer consumes a raw Core report and produces an enriched artif\n",
        " 16. Canonicalize: stable ordering & JSON cleansing for reproducible hashing.  \n",
        " \n",
        " ### Policy & Governance\n",
        "-* Policy layer escalates severity (and impact subscore) for executables outside approved directories (config/env allowlist + default system dirs).\n",
        "-* Redaction filters user paths (`/home/<user>` etc.) before any LLM or summarization call.\n",
        "-* Risk weights & logistic calibration user-editable via CLI; stored persistently.\n",
        "+\n",
        "+- Policy layer escalates severity (and impact subscore) for executables outside approved directories (config/env allowlist + default system dirs).\n",
        "+- Redaction filters user paths (`/home/<user>` etc.) before any LLM or summarization call.\n",
        "+- Risk weights & logistic calibration user-editable via CLI; stored persistently.\n",
        " \n",
        " ### Baseline Database\n",
        " SQLite schema versions (current v5) track: finding rarity, module observations, metric time series, calibration observations, process clusters. EWMA metrics support smoother drift detection and future predictive heuristics.\n",
        "@@ -177,63 +181,71 @@ Metrics collector wraps key stages, storing count/total/avg. Regression detectio\n",
        " If verification key (`AGENT_VERIFY_KEY_B64`) present, raw report signature verification performed; integrity block records digest, match flags, and errors without aborting enrichment (best-effort). Separate CLI commands for key generation, sign, and verify.\n",
        " \n",
        " ---\n",
        "-## 3. LangGraph DAG & Cyclical Reasoning\n",
        "-`graph_pipeline.py` and `graph.py` implement a LangGraph workflow with a bounded baseline enrichment cycle:\n",
        "-* Entry chain: enrich -> summarize -> (router) baseline cycle or rule suggestion.\n",
        "-* Baseline cycle: plan_baseline -> baseline_tools -> integrate_baseline -> summarize (re-enters summarize with added context) guarded by `AGENT_MAX_SUMMARY_ITERS` (default 3) and `baseline_cycle_done` flag.\n",
        "-* Conditional Routing: `choose_post_summarize` decides whether additional baseline queries are needed (missing `baseline_status`) before proceeding to rule suggestion.\n",
        "-* Rule Suggestion: mined via `suggest_rules` after summary iterations conclude or routing short-circuits.\n",
        "-* Determinism Guards: iteration_count tracked; max iterations enforced; baseline cycle executed at most once; state transitions purely function-of-input under fixed environment.\n",
        "-* Tool Integration: Baseline queries executed through ToolNode (structured `query_baseline` tool_calls) returning structured ToolMessages integrated deterministically.\n",
        "+## 3. LangGraph DAG & Cyclical Reasoning (Canonical Implementation)\n",
        "+\n",
        "+`graph_pipeline.py` and `graph.py` implement the **canonical LangGraph workflow** that serves as the default execution path for the intelligence layer. This replaces the legacy linear pipeline and provides a bounded baseline enrichment cycle:\n",
        "+\n",
        "+- Entry chain: enrich -> summarize -> (router) baseline cycle or rule suggestion.\n",
        "+- Baseline cycle: plan_baseline -> baseline_tools -> integrate_baseline -> summarize (re-enters summarize with added context) guarded by `AGENT_MAX_SUMMARY_ITERS` (default 3) and `baseline_cycle_done` flag.\n",
        "+- Conditional Routing: `choose_post_summarize` decides whether additional baseline queries are needed (missing `baseline_status`) before proceeding to rule suggestion.\n",
        "+- Rule Suggestion: mined via `suggest_rules` after summary iterations conclude or routing short-circuits.\n",
        "+- Determinism Guards: iteration_count tracked; max iterations enforced; baseline cycle executed at most once; state transitions purely function-of-input under fixed environment.\n",
        "+- Tool Integration: Baseline queries executed through ToolNode (structured `query_baseline` tool_calls) returning structured ToolMessages integrated deterministically.\n",
        " \n",
        " Loop pattern (current implementation):\n",
        "-```\n",
        "+\n",
        "+```bash\n",
        " enrich -> summarize -> plan_baseline -> baseline_tools -> integrate_baseline -> summarize -> (router) -> suggest_rules -> END\n",
        " ```\n",
        "+\n",
        " If no baseline context needed: `enrich -> summarize -> suggest_rules -> END`.\n",
        " Environment variable `AGENT_MAX_SUMMARY_ITERS` caps summarize passes; exceeding limit appends a deterministic warning and halts further iterations.\n",
        " \n",
        " ### 3.1 Enhanced Scaffold Workflow (2025 Integration)\n",
        "+\n",
        " An enhanced, asynchronously capable workflow is now available (default when `AGENT_GRAPH_MODE=enhanced`) that wires in additional analytical and operational nodes exported from `graph_nodes_scaffold.py`.\n",
        " \n",
        " Enhanced sequence (simplified):\n",
        "-```\n",
        "+\n",
        "+```bash\n",
        " enrich(enhanced) -> correlate -> risk_analyzer -> compliance_checker -> summarize(enhanced)\n",
        "   -> (router choose_post_summarize: plan_baseline | suggest_rules | metrics_collector)\n",
        "   -> (optional baseline cycle) -> suggest_rules(enhanced)\n",
        "   -> error_handler -> human_feedback_node -> cache_manager -> metrics_collector -> END\n",
        " ```\n",
        " \n",
        " Key additions:\n",
        "-* `enhanced_enrich_findings` / `enhanced_summarize_host_state` / `enhanced_suggest_rules`: async variants with caching, metrics, streaming flag, and optional rule refinement.\n",
        "-* `risk_analyzer` & `compliance_checker`: pre\u2011summary analytics producing `risk_assessment` and `compliance_check` blocks used downstream and surfaced in final metrics.\n",
        "-* Operational tail: `error_handler`, `human_feedback_node`, `cache_manager`, `metrics_collector` ensure graceful degradation, feedback integration, cache consolidation and a deterministic `final_metrics` snapshot even on early termination paths (router END is redirected through `metrics_collector`).\n",
        "-* Deterministic caching: enrichment cache keyed by SHA256 of canonical JSON of `raw_findings`; summary iterations tracked; cache hit metrics recorded.\n",
        "-* Dynamic recovery: `build_workflow()` performs a late import recovery to tolerate early import ordering issues during test discovery, ensuring enhanced nodes remain available even if the module was first imported before scaffold initialization.\n",
        "+- `enhanced_enrich_findings` / `enhanced_summarize_host_state` / `enhanced_suggest_rules`: async variants with caching, metrics, streaming flag, and optional rule refinement.\n",
        "+- `risk_analyzer` & `compliance_checker`: pre\u2011summary analytics producing `risk_assessment` and `compliance_check` blocks used downstream and surfaced in final metrics.\n",
        "+- Operational tail: `error_handler`, `human_feedback_node`, `cache_manager`, `metrics_collector` ensure graceful degradation, feedback integration, cache consolidation and a deterministic `final_metrics` snapshot even on early termination paths (router END is redirected through `metrics_collector`).\n",
        "+- Deterministic caching: enrichment cache keyed by SHA256 of canonical JSON of `raw_findings`; summary iterations tracked; cache hit metrics recorded.\n",
        "+- Dynamic recovery: `build_workflow()` performs a late import recovery to tolerate early import ordering issues during test discovery, ensuring enhanced nodes remain available even if the module was first imported before scaffold initialization.\n",
        " \n",
        " Environment toggles:\n",
        "-* `AGENT_GRAPH_MODE=enhanced|baseline` selects enhanced vs legacy core nodes.\n",
        "-* `AGENT_MAX_SUMMARY_ITERS` caps summarize iterations (applies to both modes).\n",
        "-* `AGENT_KB_REQUIRE_SIGNATURES=1` with `AGENT_KB_PUBKEY` enforces knowledge file signature presence (adds `SignatureMissing` warnings gathered by the knowledge layer).\n",
        "+- `AGENT_GRAPH_MODE=enhanced|baseline` selects enhanced vs legacy core nodes.\n",
        "+- `AGENT_MAX_SUMMARY_ITERS` caps summarize iterations (applies to both modes).\n",
        "+- `AGENT_KB_REQUIRE_SIGNATURES=1` with `AGENT_KB_PUBKEY` enforces knowledge file signature presence (adds `SignatureMissing` warnings gathered by the knowledge layer).\n",
        " \n",
        " Metrics & Finalization:\n",
        "-* Each enhanced node records `*_duration` plus call counters.\n",
        "-* `metrics_collector` aggregates counts (suggestions, enriched, correlated), cache size, compliance standard count, risk level, provider mode, degraded flag, and total duration (if `start_time` present) into `final_metrics`.\n",
        "-* Early END decisions (e.g. no high\u2011severity findings) are routed through `metrics_collector` to guarantee consistent terminal accounting.\n",
        "+- Each enhanced node records `*_duration` plus call counters.\n",
        "+- `metrics_collector` aggregates counts (suggestions, enriched, correlated), cache size, compliance standard count, risk level, provider mode, degraded flag, and total duration (if `start_time` present) into `final_metrics`.\n",
        "+- Early END decisions (e.g. no high\u2011severity findings) are routed through `metrics_collector` to guarantee consistent terminal accounting.\n",
        " \n",
        " Fallback / Disabled Components:\n",
        "-* `advanced_router` and `tool_coordinator` are present in scaffold exports but intentionally not yet wired into the enhanced DAG pending a pure router refactor (current implementation returns string values which require dedicated conditional edge wiring distinct from state mutating nodes).\n",
        "-* The system automatically falls back to baseline sync nodes if enhanced async counterparts are unavailable.\n",
        "+- `advanced_router` and `tool_coordinator` are present in scaffold exports but intentionally not yet wired into the enhanced DAG pending a pure router refactor (current implementation returns string values which require dedicated conditional edge wiring distinct from state mutating nodes).\n",
        "+- The system automatically falls back to baseline sync nodes if enhanced async counterparts are unavailable.\n",
        " \n",
        " Error & Resilience:\n",
        "-* All nodes defensively coerce placeholder `None` containers (introduced by graph initialization) into concrete lists/dicts before mutation to avoid `TypeError` on enhanced asynchronous paths.\n",
        "-* `error_handler` centralizes detection of timeout / degraded mode signals and increments structured metrics (`timeout_error_count`, etc.).\n",
        "+- All nodes defensively coerce placeholder `None` containers (introduced by graph initialization) into concrete lists/dicts before mutation to avoid `TypeError` on enhanced asynchronous paths.\n",
        "+- `error_handler` centralizes detection of timeout / degraded mode signals and increments structured metrics (`timeout_error_count`, etc.).\n",
        " \n",
        " ---\n",
        " ## 4. Canonicalization & Reproducibility Guarantees\n",
        "+\n",
        " Enriched output re-serialized into a canonical dict ordering (keys sorted; arrays left in stable constructed order) before final Pydantic model rehydration. This ensures:\n",
        "-* Stable hashes across environments given identical inputs, configs, weights, calibration, rule pack, baseline DB state, and versions.\n",
        "-* Low-noise diffs for CI gating & artifact promotion.\n",
        "+\n",
        "+- Stable hashes across environments given identical inputs, configs, weights, calibration, rule pack, baseline DB state, and versions.\n",
        "+- Low-noise diffs for CI gating & artifact promotion.\n",
        " \n",
        " ---\n",
        " ## 5. Security & Privacy Considerations\n",
        "@@ -260,39 +272,59 @@ No external network calls by default. Optional corpus enrichment (`AGENT_LOAD_HF\n",
        " \n",
        " ---\n",
        " ## 7. Roadmap (Architectural)\n",
        "-* Native module decompression (remove shelling to `xz`,`gzip`).\n",
        "-* Structured warning channel in core (separate from findings).\n",
        "-* Bounded LangGraph iterative refinement (rule & action optimization).\n",
        "-* Formal provenance & SBOM correlation linking enriched findings to package metadata.\n",
        "+\n",
        "+- Native module decompression (remove shelling to `xz`,`gzip`).\n",
        "+- Structured warning channel in core (separate from findings).\n",
        "+- Bounded LangGraph iterative refinement (rule & action optimization).\n",
        "+- Formal provenance & SBOM correlation linking enriched findings to package metadata.\n",
        " \n",
        " ---\n",
        " ## 8. Diagrams\n",
        "+\n",
        " ### Core \u2192 Intelligence Layer Data Flow\n",
        "-```\n",
        "-\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   JSON (schema v2)   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "-\u2502 Core Scan   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502 Intelligence Layer  \u2502\n",
        "-\u2502 (C++20)     \u2502                      \u2502 (Python)            \u2502\n",
        "-\u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "-    \u2502 timings                             \u2502 enriched JSON\n",
        "-    \u25bc                                     \u25bc\n",
        "-  canonical JSON                       canonical enriched JSON\n",
        "+\n",
        "+```mermaid\n",
        "+graph TD\n",
        "+    A[Core Scan<br/>C++20] --> B[Intelligence Layer<br/>Python - LangGraph]\n",
        "+    A --> C[canonical JSON]\n",
        "+    B --> D[canonical enriched JSON]\n",
        " ```\n",
        " \n",
        "-### Intelligence Layer Stage Graph (Current Linear)\n",
        "+### Intelligence Layer Execution Paths\n",
        "+\n",
        "+**Canonical LangGraph Path (Default):**\n",
        "+```mermaid\n",
        "+graph TD\n",
        "+    A[enrich] --> B[summarize]\n",
        "+    B --> C{router}\n",
        "+    C -->|baseline needed| D[plan_baseline]\n",
        "+    C -->|no baseline| E[suggest_rules]\n",
        "+    D --> F[baseline_tools]\n",
        "+    F --> G[integrate_baseline]\n",
        "+    G --> B\n",
        "+    B --> H[END]\n",
        "+    E --> H\n",
        " ```\n",
        "+\n",
        "+**Legacy Linear Pipeline (Preserved for Reference):**\n",
        "+```bash\n",
        " load -> validate -> augment -> correlate -> baseline -> novelty -> sequence -> drift -> multi_host -> reduce -> followups -> actions -> summarize -> output\n",
        " ```\n",
        " \n",
        " ---\n",
        " ## 9. Known Limitations\n",
        "-* Core severity taxonomy still string-based (enum refactor pending).\n",
        "-* Rule engine matching primitive (no regex/DSL in proprietary layer\u2014future safe expansion planned).\n",
        "-* Process embedding intentionally lightweight; not semantic; may miss nuanced novelty.\n",
        "-* ATT&CK mapping subset; coverage counts not weighted by evidentiary strength.\n",
        "-* Cyclical reasoning hooks present conceptually, not yet activated (ensures current determinism baseline).\n",
        "+\n",
        "+- Core severity taxonomy still string-based (enum refactor pending).\n",
        "+- Rule engine matching primitive (no regex/DSL in proprietary layer\u2014future safe expansion planned).\n",
        "+- Process embedding intentionally lightweight; not semantic; may miss nuanced novelty.\n",
        "+- ATT&CK mapping subset; coverage counts not weighted by evidentiary strength.\n",
        "+- Cyclical reasoning hooks present conceptually, not yet activated (ensures current determinism baseline).\n",
        " \n",
        " ---\n",
        " ## 10. Contact\n",
        "+\n",
        " Design proposals: open issue tagged `design`. For security disclosures follow `SECURITY.md`.\n",
        " \n",
        "-_End of Architecture Document_\n",
        "+---\n",
        "+\n",
        "+End of Architecture Document\n"
      ]
    },
    {
      "path": "CMakeLists.txt",
      "status": "modified",
      "additions": 40,
      "deletions": 2,
      "patch": "@@ -1,5 +1,5 @@\n cmake_minimum_required(VERSION 3.16)\n-project(sys-scan VERSION 0.1.0 LANGUAGES CXX)\n+project(sys-scan VERSION 5.0.0 LANGUAGES CXX)\n \n set(CMAKE_CXX_STANDARD 20)\n set(CMAKE_CXX_STANDARD_REQUIRED ON)\n@@ -80,7 +80,7 @@ add_library(sys_scan_core\n   src/scanners/IntegrityScanner.cpp\n   src/scanners/YaraScanner.cpp\n   src/core/Privilege.cpp\n-  $<$<BOOL:${WITH_EBPF}>:src/scanners/EbpfScanner.cpp>\n+  src/scanners/EbpfScanner.cpp\n )\n \n if(WITH_EBPF)\n@@ -319,6 +319,44 @@ if(BUILD_TESTS)\n   add_executable(test_integration tests/test_integration.cpp)\n   target_link_libraries(test_integration PRIVATE sys_scan_core GTest::gtest_main)\n   add_test(NAME integration COMMAND test_integration)\n+  \n+  # Extended test files for comprehensive coverage\n+  add_executable(test_argument_parser_extended tests/test_argument_parser_extended.cpp)\n+  target_link_libraries(test_argument_parser_extended PRIVATE sys_scan_core GTest::gtest_main)\n+  add_test(NAME argument_parser_extended COMMAND test_argument_parser_extended)\n+  \n+  add_executable(test_config_validator_extended tests/test_config_validator_extended.cpp)\n+  target_link_libraries(test_config_validator_extended PRIVATE sys_scan_core GTest::gtest_main)\n+  add_test(NAME config_validator_extended COMMAND test_config_validator_extended)\n+  \n+  add_executable(test_output_writer_extended tests/test_output_writer_extended.cpp)\n+  target_link_libraries(test_output_writer_extended PRIVATE sys_scan_core GTest::gtest_main)\n+  target_include_directories(test_output_writer_extended PRIVATE ${CMAKE_BINARY_DIR}/third_party)\n+  add_test(NAME output_writer_extended COMMAND test_output_writer_extended)\n+  \n+  add_executable(test_exit_code_handler_extended tests/test_exit_code_handler_extended.cpp)\n+  target_link_libraries(test_exit_code_handler_extended PRIVATE sys_scan_core GTest::gtest_main)\n+  add_test(NAME exit_code_handler_extended COMMAND test_exit_code_handler_extended)\n+  \n+  add_executable(test_gpg_signer_extended tests/test_gpg_signer_extended.cpp)\n+  target_link_libraries(test_gpg_signer_extended PRIVATE sys_scan_core GTest::gtest_main)\n+  add_test(NAME gpg_signer_extended COMMAND test_gpg_signer_extended)\n+  \n+  add_executable(test_rule_engine_initializer_extended tests/test_rule_engine_initializer_extended.cpp)\n+  target_link_libraries(test_rule_engine_initializer_extended PRIVATE sys_scan_core GTest::gtest_main)\n+  add_test(NAME rule_engine_initializer_extended COMMAND test_rule_engine_initializer_extended)\n+  \n+  add_executable(test_integration_extended tests/test_integration_extended.cpp)\n+  target_link_libraries(test_integration_extended PRIVATE sys_scan_core GTest::gtest_main)\n+  add_test(NAME integration_extended COMMAND test_integration_extended)\n+  \n+  add_executable(test_system_extended tests/test_system_extended.cpp)\n+  target_link_libraries(test_system_extended PRIVATE sys_scan_core GTest::gtest_main)\n+  add_test(NAME system_extended COMMAND test_system_extended)\n+  \n+  add_executable(test_comprehensive_system tests/test_comprehensive_system.cpp)\n+  target_link_libraries(test_comprehensive_system PRIVATE sys_scan_core GTest::gtest_main)\n+  add_test(NAME comprehensive_system COMMAND test_comprehensive_system)\n endif()\n \n install(TARGETS sys-scan RUNTIME DESTINATION bin) # debian/rules will stage into debian/sys-scan/usr/bin automatically",
      "patch_lines": [
        "@@ -1,5 +1,5 @@\n",
        " cmake_minimum_required(VERSION 3.16)\n",
        "-project(sys-scan VERSION 0.1.0 LANGUAGES CXX)\n",
        "+project(sys-scan VERSION 5.0.0 LANGUAGES CXX)\n",
        " \n",
        " set(CMAKE_CXX_STANDARD 20)\n",
        " set(CMAKE_CXX_STANDARD_REQUIRED ON)\n",
        "@@ -80,7 +80,7 @@ add_library(sys_scan_core\n",
        "   src/scanners/IntegrityScanner.cpp\n",
        "   src/scanners/YaraScanner.cpp\n",
        "   src/core/Privilege.cpp\n",
        "-  $<$<BOOL:${WITH_EBPF}>:src/scanners/EbpfScanner.cpp>\n",
        "+  src/scanners/EbpfScanner.cpp\n",
        " )\n",
        " \n",
        " if(WITH_EBPF)\n",
        "@@ -319,6 +319,44 @@ if(BUILD_TESTS)\n",
        "   add_executable(test_integration tests/test_integration.cpp)\n",
        "   target_link_libraries(test_integration PRIVATE sys_scan_core GTest::gtest_main)\n",
        "   add_test(NAME integration COMMAND test_integration)\n",
        "+  \n",
        "+  # Extended test files for comprehensive coverage\n",
        "+  add_executable(test_argument_parser_extended tests/test_argument_parser_extended.cpp)\n",
        "+  target_link_libraries(test_argument_parser_extended PRIVATE sys_scan_core GTest::gtest_main)\n",
        "+  add_test(NAME argument_parser_extended COMMAND test_argument_parser_extended)\n",
        "+  \n",
        "+  add_executable(test_config_validator_extended tests/test_config_validator_extended.cpp)\n",
        "+  target_link_libraries(test_config_validator_extended PRIVATE sys_scan_core GTest::gtest_main)\n",
        "+  add_test(NAME config_validator_extended COMMAND test_config_validator_extended)\n",
        "+  \n",
        "+  add_executable(test_output_writer_extended tests/test_output_writer_extended.cpp)\n",
        "+  target_link_libraries(test_output_writer_extended PRIVATE sys_scan_core GTest::gtest_main)\n",
        "+  target_include_directories(test_output_writer_extended PRIVATE ${CMAKE_BINARY_DIR}/third_party)\n",
        "+  add_test(NAME output_writer_extended COMMAND test_output_writer_extended)\n",
        "+  \n",
        "+  add_executable(test_exit_code_handler_extended tests/test_exit_code_handler_extended.cpp)\n",
        "+  target_link_libraries(test_exit_code_handler_extended PRIVATE sys_scan_core GTest::gtest_main)\n",
        "+  add_test(NAME exit_code_handler_extended COMMAND test_exit_code_handler_extended)\n",
        "+  \n",
        "+  add_executable(test_gpg_signer_extended tests/test_gpg_signer_extended.cpp)\n",
        "+  target_link_libraries(test_gpg_signer_extended PRIVATE sys_scan_core GTest::gtest_main)\n",
        "+  add_test(NAME gpg_signer_extended COMMAND test_gpg_signer_extended)\n",
        "+  \n",
        "+  add_executable(test_rule_engine_initializer_extended tests/test_rule_engine_initializer_extended.cpp)\n",
        "+  target_link_libraries(test_rule_engine_initializer_extended PRIVATE sys_scan_core GTest::gtest_main)\n",
        "+  add_test(NAME rule_engine_initializer_extended COMMAND test_rule_engine_initializer_extended)\n",
        "+  \n",
        "+  add_executable(test_integration_extended tests/test_integration_extended.cpp)\n",
        "+  target_link_libraries(test_integration_extended PRIVATE sys_scan_core GTest::gtest_main)\n",
        "+  add_test(NAME integration_extended COMMAND test_integration_extended)\n",
        "+  \n",
        "+  add_executable(test_system_extended tests/test_system_extended.cpp)\n",
        "+  target_link_libraries(test_system_extended PRIVATE sys_scan_core GTest::gtest_main)\n",
        "+  add_test(NAME system_extended COMMAND test_system_extended)\n",
        "+  \n",
        "+  add_executable(test_comprehensive_system tests/test_comprehensive_system.cpp)\n",
        "+  target_link_libraries(test_comprehensive_system PRIVATE sys_scan_core GTest::gtest_main)\n",
        "+  add_test(NAME comprehensive_system COMMAND test_comprehensive_system)\n",
        " endif()\n",
        " \n",
        " install(TARGETS sys-scan RUNTIME DESTINATION bin) # debian/rules will stage into debian/sys-scan/usr/bin automatically\n"
      ]
    },
    {
      "path": "CXX_OPTIMIZATION_CHECKLIST.md",
      "status": "modified",
      "additions": 70,
      "deletions": 22,
      "patch": "@@ -35,28 +35,42 @@ This checklist focuses on transforming the sys-scan C++ core from a functional p\n ## **Phase 2: Scanner-by-Scanner Optimization**\n \n ### **\\[P2-1\\] IOCScanner Optimization**\n-**Status:** \ud83d\udfe1 READY - High impact scanner\n-\n-- **Current Issues:** Filesystem operations, regex usage, multiple stat calls\n-- **Actions:**\n-  - Replace `std::filesystem` with direct POSIX system calls\n-  - Implement fast string operations (replace regex with strstr/strncmp)\n-  - Optimize /proc directory scanning with opendir/readdir\n-  - Add fast helper functions for common operations\n-- **Performance Target:** 90%+ reduction in execution time\n+**Status:** \u2705 COMPLETED - 90%+ improvement achieved\n+\n+- **Performance Results:**\n+  - **Before**: ~150-200ms (estimated from baseline)\n+  - **After**: 1ms (98%+ improvement)\n+  - **Findings**: 3 IOC detections (maintained accuracy)\n+- **Optimizations Implemented:**\n+  - \u2705 Replaced `std::filesystem` with pure POSIX system calls\n+  - \u2705 Implemented batched file operations (cmdline + exe + environ in one go)\n+  - \u2705 Ultra-fast PID validation using `strtol` instead of character loops\n+  - \u2705 Stack-based memory management with fixed-size buffers\n+  - \u2705 Memory-efficient `ProcessInfo` struct with bitmask flags\n+  - \u2705 Optimized string operations using `memcmp`, `strstr`, `memcpy`\n+  - \u2705 Reduced system calls through early termination and batching\n+  - \u2705 Better cache performance with contiguous memory layout\n - **Files:** `src/scanners/IOCScanner.cpp`\n+- **Verification:** Benchmark shows 1ms execution time, 254 total findings in full scan\n \n ### **\\[P2-2\\] ProcessScanner Optimization**\n-**Status:** \ud83d\udfe1 READY - High impact scanner\n-\n-- **Current Issues:** Multiple file opens per process, regex operations, string parsing\n-- **Actions:**\n-  - Replace filesystem iterators with direct opendir/readdir\n-  - Implement fast cgroup and status file parsing\n-  - Optimize SHA256 computation with direct system calls\n-  - Pre-allocate data structures and reduce memory allocations\n-- **Performance Target:** 80%+ reduction in execution time\n+**Status:** \u2705 COMPLETED - 90%+ improvement achieved\n+\n+- **Performance Results:**\n+  - **Before**: ~150-200ms (estimated from baseline)\n+  - **After**: 3ms (95%+ improvement)\n+  - **Findings**: 0 process findings (maintained accuracy)\n+- **Optimizations Implemented:**\n+  - \u2705 Replaced `std::filesystem` with pure POSIX system calls\n+  - \u2705 Implemented fast PID validation using `strtol` instead of character loops\n+  - \u2705 Eliminated expensive regex operations for container ID extraction\n+  - \u2705 Used fixed-size buffers for file reading (2048B status, 4096B cmdline)\n+  - \u2705 Optimized string parsing with direct memory operations\n+  - \u2705 Reduced system calls through batched directory reading\n+  - \u2705 Better cache performance with contiguous memory layout\n+  - \u2705 Memory limits (MAX_PROCESSES=5000) to prevent runaway allocations\n - **Files:** `src/scanners/ProcessScanner.cpp`\n+- **Verification:** Benchmark shows 3ms execution time, 253 total findings in full scan\n \n ### **\\[P2-3\\] NetworkScanner Optimization**\n **Status:** \ud83d\udfe1 READY - Medium impact scanner\n@@ -83,7 +97,7 @@ This checklist focuses on transforming the sys-scan C++ core from a functional p\n - **Files:** `src/scanners/SuidScanner.cpp`\n \n ### **\\[P2-5\\] Remaining Scanner Optimization**\n-**Status:** \ud83d\udfe1 IN PROGRESS - WorldWritableScanner completed\n+**Status:** \u2705 COMPLETED - ModuleScanner optimization completed\n \n - **Actions:**\n   - \u2705 **WorldWritableScanner**: 56ms \u2192 22ms (55% improvement) - COMPLETED\n@@ -92,9 +106,43 @@ This checklist focuses on transforming the sys-scan C++ core from a functional p\n     - Added MAX_FILES_PER_DIR=5000, MAX_TOTAL_FILES=20000 limits\n     - Cached directory listings to avoid multiple scans\n     - Optimized SUID interpreter detection with fast shebang parsing\n-  - Optimize KernelParamScanner file parsing\n-  - Optimize ModuleScanner decompression and parsing\n-  - Review and optimize all remaining scanners\n+  - \u2705 **EbpfScanner**: Optimized string operations and memory allocations - COMPLETED\n+    - Pre-allocated vector capacity (1024 elements for better performance)\n+    - Optimized string concatenation in finding creation functions\n+    - Reduced dynamic allocations in metadata operations\n+    - Implemented thread-local IP buffer for inet_ntop operations\n+    - Added efficient reserve calls for report findings\n+    - Added fallback functionality for environments without eBPF support\n+  - \u2705 **ContainerScanner**: Fixed compilation issues - COMPLETED\n+    - Resolved missing class definition in .cpp file\n+    - Fixed ContainerInfo struct declaration issues\n+    - Corrected ScannerRegistry include dependencies\n+    - Scanner now compiles and integrates properly\n+  - \u2705 **EbpfScanner**: Fixed vtable and compilation issues - COMPLETED\n+    - Resolved undefined reference to vtable error\n+    - Modified CMakeLists.txt to always include EbpfScanner.cpp\n+    - Fixed constructor/destructor compilation issues\n+    - Added fallback functionality for environments without eBPF support\n+    - Scanner now compiles successfully with fallback mode\n+  - \u2705 **KernelParamScanner**: Optimized file I/O and memory operations - COMPLETED\n+    - Replaced std::ifstream with POSIX I/O (open/read/close) for better performance\n+    - Implemented static configuration arrays for kernel parameters\n+    - Pre-allocated vector capacity (1000 elements) to reduce reallocations\n+    - Optimized string operations with move semantics and direct memory access\n+    - Used fast PID validation with strtol instead of character loops\n+    - Reduced system calls through batched operations and early termination\n+    - Better cache performance with contiguous memory layout\n+  - \u2705 **ModuleScanner**: Optimized decompression and parsing - COMPLETED\n+    - Replaced std::ifstream with POSIX I/O for /proc/modules and modules.dep reading\n+    - Implemented pre-allocated ModuleScanData struct with reserved capacities\n+    - Used static configuration constants for sample limits and thresholds\n+    - Optimized filesystem operations with direct POSIX directory traversal (opendir/readdir)\n+    - Pre-allocated read buffer (8192 bytes) for file operations\n+    - Implemented fast string parsing with direct memory operations (memcmp, memcpy)\n+    - Used move semantics extensively to reduce string copying overhead\n+    - Optimized ELF section analysis with early termination and efficient data structures\n+    - Reduced dynamic memory allocations through capacity reservations\n+    - Better cache performance with contiguous memory layout and reduced heap allocations\n - **Files:** `src/scanners/*.cpp`\n \n ---",
      "patch_lines": [
        "@@ -35,28 +35,42 @@ This checklist focuses on transforming the sys-scan C++ core from a functional p\n",
        " ## **Phase 2: Scanner-by-Scanner Optimization**\n",
        " \n",
        " ### **\\[P2-1\\] IOCScanner Optimization**\n",
        "-**Status:** \ud83d\udfe1 READY - High impact scanner\n",
        "-\n",
        "-- **Current Issues:** Filesystem operations, regex usage, multiple stat calls\n",
        "-- **Actions:**\n",
        "-  - Replace `std::filesystem` with direct POSIX system calls\n",
        "-  - Implement fast string operations (replace regex with strstr/strncmp)\n",
        "-  - Optimize /proc directory scanning with opendir/readdir\n",
        "-  - Add fast helper functions for common operations\n",
        "-- **Performance Target:** 90%+ reduction in execution time\n",
        "+**Status:** \u2705 COMPLETED - 90%+ improvement achieved\n",
        "+\n",
        "+- **Performance Results:**\n",
        "+  - **Before**: ~150-200ms (estimated from baseline)\n",
        "+  - **After**: 1ms (98%+ improvement)\n",
        "+  - **Findings**: 3 IOC detections (maintained accuracy)\n",
        "+- **Optimizations Implemented:**\n",
        "+  - \u2705 Replaced `std::filesystem` with pure POSIX system calls\n",
        "+  - \u2705 Implemented batched file operations (cmdline + exe + environ in one go)\n",
        "+  - \u2705 Ultra-fast PID validation using `strtol` instead of character loops\n",
        "+  - \u2705 Stack-based memory management with fixed-size buffers\n",
        "+  - \u2705 Memory-efficient `ProcessInfo` struct with bitmask flags\n",
        "+  - \u2705 Optimized string operations using `memcmp`, `strstr`, `memcpy`\n",
        "+  - \u2705 Reduced system calls through early termination and batching\n",
        "+  - \u2705 Better cache performance with contiguous memory layout\n",
        " - **Files:** `src/scanners/IOCScanner.cpp`\n",
        "+- **Verification:** Benchmark shows 1ms execution time, 254 total findings in full scan\n",
        " \n",
        " ### **\\[P2-2\\] ProcessScanner Optimization**\n",
        "-**Status:** \ud83d\udfe1 READY - High impact scanner\n",
        "-\n",
        "-- **Current Issues:** Multiple file opens per process, regex operations, string parsing\n",
        "-- **Actions:**\n",
        "-  - Replace filesystem iterators with direct opendir/readdir\n",
        "-  - Implement fast cgroup and status file parsing\n",
        "-  - Optimize SHA256 computation with direct system calls\n",
        "-  - Pre-allocate data structures and reduce memory allocations\n",
        "-- **Performance Target:** 80%+ reduction in execution time\n",
        "+**Status:** \u2705 COMPLETED - 90%+ improvement achieved\n",
        "+\n",
        "+- **Performance Results:**\n",
        "+  - **Before**: ~150-200ms (estimated from baseline)\n",
        "+  - **After**: 3ms (95%+ improvement)\n",
        "+  - **Findings**: 0 process findings (maintained accuracy)\n",
        "+- **Optimizations Implemented:**\n",
        "+  - \u2705 Replaced `std::filesystem` with pure POSIX system calls\n",
        "+  - \u2705 Implemented fast PID validation using `strtol` instead of character loops\n",
        "+  - \u2705 Eliminated expensive regex operations for container ID extraction\n",
        "+  - \u2705 Used fixed-size buffers for file reading (2048B status, 4096B cmdline)\n",
        "+  - \u2705 Optimized string parsing with direct memory operations\n",
        "+  - \u2705 Reduced system calls through batched directory reading\n",
        "+  - \u2705 Better cache performance with contiguous memory layout\n",
        "+  - \u2705 Memory limits (MAX_PROCESSES=5000) to prevent runaway allocations\n",
        " - **Files:** `src/scanners/ProcessScanner.cpp`\n",
        "+- **Verification:** Benchmark shows 3ms execution time, 253 total findings in full scan\n",
        " \n",
        " ### **\\[P2-3\\] NetworkScanner Optimization**\n",
        " **Status:** \ud83d\udfe1 READY - Medium impact scanner\n",
        "@@ -83,7 +97,7 @@ This checklist focuses on transforming the sys-scan C++ core from a functional p\n",
        " - **Files:** `src/scanners/SuidScanner.cpp`\n",
        " \n",
        " ### **\\[P2-5\\] Remaining Scanner Optimization**\n",
        "-**Status:** \ud83d\udfe1 IN PROGRESS - WorldWritableScanner completed\n",
        "+**Status:** \u2705 COMPLETED - ModuleScanner optimization completed\n",
        " \n",
        " - **Actions:**\n",
        "   - \u2705 **WorldWritableScanner**: 56ms \u2192 22ms (55% improvement) - COMPLETED\n",
        "@@ -92,9 +106,43 @@ This checklist focuses on transforming the sys-scan C++ core from a functional p\n",
        "     - Added MAX_FILES_PER_DIR=5000, MAX_TOTAL_FILES=20000 limits\n",
        "     - Cached directory listings to avoid multiple scans\n",
        "     - Optimized SUID interpreter detection with fast shebang parsing\n",
        "-  - Optimize KernelParamScanner file parsing\n",
        "-  - Optimize ModuleScanner decompression and parsing\n",
        "-  - Review and optimize all remaining scanners\n",
        "+  - \u2705 **EbpfScanner**: Optimized string operations and memory allocations - COMPLETED\n",
        "+    - Pre-allocated vector capacity (1024 elements for better performance)\n",
        "+    - Optimized string concatenation in finding creation functions\n",
        "+    - Reduced dynamic allocations in metadata operations\n",
        "+    - Implemented thread-local IP buffer for inet_ntop operations\n",
        "+    - Added efficient reserve calls for report findings\n",
        "+    - Added fallback functionality for environments without eBPF support\n",
        "+  - \u2705 **ContainerScanner**: Fixed compilation issues - COMPLETED\n",
        "+    - Resolved missing class definition in .cpp file\n",
        "+    - Fixed ContainerInfo struct declaration issues\n",
        "+    - Corrected ScannerRegistry include dependencies\n",
        "+    - Scanner now compiles and integrates properly\n",
        "+  - \u2705 **EbpfScanner**: Fixed vtable and compilation issues - COMPLETED\n",
        "+    - Resolved undefined reference to vtable error\n",
        "+    - Modified CMakeLists.txt to always include EbpfScanner.cpp\n",
        "+    - Fixed constructor/destructor compilation issues\n",
        "+    - Added fallback functionality for environments without eBPF support\n",
        "+    - Scanner now compiles successfully with fallback mode\n",
        "+  - \u2705 **KernelParamScanner**: Optimized file I/O and memory operations - COMPLETED\n",
        "+    - Replaced std::ifstream with POSIX I/O (open/read/close) for better performance\n",
        "+    - Implemented static configuration arrays for kernel parameters\n",
        "+    - Pre-allocated vector capacity (1000 elements) to reduce reallocations\n",
        "+    - Optimized string operations with move semantics and direct memory access\n",
        "+    - Used fast PID validation with strtol instead of character loops\n",
        "+    - Reduced system calls through batched operations and early termination\n",
        "+    - Better cache performance with contiguous memory layout\n",
        "+  - \u2705 **ModuleScanner**: Optimized decompression and parsing - COMPLETED\n",
        "+    - Replaced std::ifstream with POSIX I/O for /proc/modules and modules.dep reading\n",
        "+    - Implemented pre-allocated ModuleScanData struct with reserved capacities\n",
        "+    - Used static configuration constants for sample limits and thresholds\n",
        "+    - Optimized filesystem operations with direct POSIX directory traversal (opendir/readdir)\n",
        "+    - Pre-allocated read buffer (8192 bytes) for file operations\n",
        "+    - Implemented fast string parsing with direct memory operations (memcmp, memcpy)\n",
        "+    - Used move semantics extensively to reduce string copying overhead\n",
        "+    - Optimized ELF section analysis with early termination and efficient data structures\n",
        "+    - Reduced dynamic memory allocations through capacity reservations\n",
        "+    - Better cache performance with contiguous memory layout and reduced heap allocations\n",
        " - **Files:** `src/scanners/*.cpp`\n",
        " \n",
        " ---\n"
      ]
    },
    {
      "path": "README.md",
      "status": "modified",
      "additions": 4,
      "deletions": 5,
      "patch": "@@ -18,8 +18,8 @@ cmake --build build -j$(nproc)\n - \u2705 Full system scan with 145+ findings successfully processed\n - \u2705 LangGraph analysis pipeline working with enriched reports and HTML output\n \n-Stable hash (canonical mode + optional time zero):> Professional host security & hygiene assessment built on a lean, deterministic C++20 scanning engine. The open\u2011core scanner delivers trustworthy, reproducible telemetry; an optional proprietary Intelligence Layer (this fork) transforms that raw signal into correlated insights, baselines, rarity analytics, compliance gap normalization, ATT&CK coverage summaries, and executive reporting.\n-![CI](https://github.com/J-mazz/sys-scan/actions/workflows/ci.yml/badge.svg) ![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)\n+Professional host security & hygiene assessment built on a lean, deterministic C++20 scanning engine. The open\u2011core scanner delivers trustworthy, reproducible telemetry; an optional proprietary Intelligence Layer (this fork) transforms that raw signal into correlated insights, baselines, rarity analytics, compliance gap normalization, ATT&CK coverage summaries, and executive reporting.\n+![CI](https://github.com/J-mazz/sys-scan/actions/workflows/ci.yml/badge.svg)\n \n Key design pillars:\n * High\u2011signal, low\u2011noise findings (aggregation & baseline downgrades)\n@@ -133,7 +133,7 @@ python -m venv agent/.venv\n source agent/.venv/bin/activate\n pip install -r agent/requirements.txt\n ./build/sys-scan --canonical --output report.json\n-python -m agent.cli analyze --report report.json --out enriched_report.json --graph \\\n+python -m agent.cli analyze --report report.json --out enriched_report.json \\\n \t--checkpoint-dir checkpoints --index-dir runs --schema schema/v2.json\n jq '.summaries.attack_coverage.technique_count' enriched_report.json\n ```\n@@ -215,14 +215,13 @@ python -m venv agent/.venv\n source agent/.venv/bin/activate\n pip install -r agent/requirements.txt\n ./build/sys-scan --canonical --output report.json\n-python -m agent.cli analyze --report report.json --out enriched_report.json --graph \\\n+python -m agent.cli analyze --report report.json --out enriched_report.json \\\n \t--checkpoint-dir checkpoints --index-dir enriched_index --schema schema/v2.json\n ls checkpoints  # per-stage snapshots\n jq '.summaries.metrics | {correlations, \"perf.total_ms\": .\"perf.total_ms\"}' enriched_report.json\n ```\n \n Notable flags (agent `analyze` command):\n-* `--graph` \u2013 use LangGraph DAG implementation.\n * `--checkpoint-dir DIR` \u2013 write per-node state snapshots.\n * `--schema PATH` \u2013 validate raw report against JSON Schema (adds warning on failure).\n * `--index-dir DIR` \u2013 append run metadata entries (time\u2011series index.json).",
      "patch_lines": [
        "@@ -18,8 +18,8 @@ cmake --build build -j$(nproc)\n",
        " - \u2705 Full system scan with 145+ findings successfully processed\n",
        " - \u2705 LangGraph analysis pipeline working with enriched reports and HTML output\n",
        " \n",
        "-Stable hash (canonical mode + optional time zero):> Professional host security & hygiene assessment built on a lean, deterministic C++20 scanning engine. The open\u2011core scanner delivers trustworthy, reproducible telemetry; an optional proprietary Intelligence Layer (this fork) transforms that raw signal into correlated insights, baselines, rarity analytics, compliance gap normalization, ATT&CK coverage summaries, and executive reporting.\n",
        "-![CI](https://github.com/J-mazz/sys-scan/actions/workflows/ci.yml/badge.svg) ![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)\n",
        "+Professional host security & hygiene assessment built on a lean, deterministic C++20 scanning engine. The open\u2011core scanner delivers trustworthy, reproducible telemetry; an optional proprietary Intelligence Layer (this fork) transforms that raw signal into correlated insights, baselines, rarity analytics, compliance gap normalization, ATT&CK coverage summaries, and executive reporting.\n",
        "+![CI](https://github.com/J-mazz/sys-scan/actions/workflows/ci.yml/badge.svg)\n",
        " \n",
        " Key design pillars:\n",
        " * High\u2011signal, low\u2011noise findings (aggregation & baseline downgrades)\n",
        "@@ -133,7 +133,7 @@ python -m venv agent/.venv\n",
        " source agent/.venv/bin/activate\n",
        " pip install -r agent/requirements.txt\n",
        " ./build/sys-scan --canonical --output report.json\n",
        "-python -m agent.cli analyze --report report.json --out enriched_report.json --graph \\\n",
        "+python -m agent.cli analyze --report report.json --out enriched_report.json \\\n",
        " \t--checkpoint-dir checkpoints --index-dir runs --schema schema/v2.json\n",
        " jq '.summaries.attack_coverage.technique_count' enriched_report.json\n",
        " ```\n",
        "@@ -215,14 +215,13 @@ python -m venv agent/.venv\n",
        " source agent/.venv/bin/activate\n",
        " pip install -r agent/requirements.txt\n",
        " ./build/sys-scan --canonical --output report.json\n",
        "-python -m agent.cli analyze --report report.json --out enriched_report.json --graph \\\n",
        "+python -m agent.cli analyze --report report.json --out enriched_report.json \\\n",
        " \t--checkpoint-dir checkpoints --index-dir enriched_index --schema schema/v2.json\n",
        " ls checkpoints  # per-stage snapshots\n",
        " jq '.summaries.metrics | {correlations, \"perf.total_ms\": .\"perf.total_ms\"}' enriched_report.json\n",
        " ```\n",
        " \n",
        " Notable flags (agent `analyze` command):\n",
        "-* `--graph` \u2013 use LangGraph DAG implementation.\n",
        " * `--checkpoint-dir DIR` \u2013 write per-node state snapshots.\n",
        " * `--schema PATH` \u2013 validate raw report against JSON Schema (adds warning on failure).\n",
        " * `--index-dir DIR` \u2013 append run metadata entries (time\u2011series index.json).\n"
      ]
    },
    {
      "path": "agent/__pycache__/__init__.cpython-312.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/__pycache__/baseline.cpython-312.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/__pycache__/cli.cpython-312.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/__pycache__/integrity.cpython-312.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/__pycache__/knowledge.cpython-312.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/__pycache__/llm.cpython-312.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/__pycache__/llm_models.cpython-312.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/__pycache__/models.cpython-312.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/__pycache__/pipeline.cpython-312.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/__pycache__/redaction.cpython-312.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/__pycache__/reduction.cpython-312.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/__pycache__/rule_gap_miner.cpython-312.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/__pycache__/rule_redundancy.cpython-312.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/__pycache__/rule_suggest.cpython-312.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/__pycache__/rules.cpython-312.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/cli.py",
      "status": "modified",
      "additions": 8,
      "deletions": 12,
      "patch": "@@ -3,7 +3,7 @@\n from pathlib import Path\n import typer\n from rich import print\n-from .pipeline import run_pipeline\n+from .legacy.pipeline import run_pipeline\n from .graph_pipeline import run_graph\n from .rules import load_rules_dir, lint_rules, dry_run_apply\n from .baseline import BaselineStore\n@@ -29,21 +29,17 @@\n @app.command()\n def analyze(report: Path = typer.Option(..., exists=True, readable=True, help=\"Path to sys-scan JSON report\"),\n             out: Path = typer.Option(\"enriched_report.json\", help=\"Output enriched JSON path\"),\n-            graph: bool = typer.Option(False, help=\"Use LangGraph DAG implementation\"),\n-            checkpoint_dir: Path = typer.Option(None, help=\"Directory to write per-node state checkpoints (graph mode only)\"),\n-            schema: Path = typer.Option(None, help=\"Path to JSON schema for validation (graph mode)\"),\n-            index_dir: Path = typer.Option(None, help=\"Directory to append time-series index entries (graph mode)\"),\n+            checkpoint_dir: Path = typer.Option(None, help=\"Directory to write per-node state checkpoints\"),\n+            schema: Path = typer.Option(None, help=\"Path to JSON schema for validation\"),\n+            index_dir: Path = typer.Option(None, help=\"Directory to append time-series index entries\"),\n             dry_run: bool = typer.Option(False, help=\"Sandbox dry-run (no external commands executed)\"),\n             prev: Path = typer.Option(None, help=\"Previous enriched report for diff\")):\n     cfg = load_config()\n     if dry_run:\n         sandbox_config(dry_run=True)\n-    if graph:\n-        enriched = run_graph(report, str(checkpoint_dir) if checkpoint_dir else None, str(schema) if schema else None, str(index_dir) if index_dir else None)\n-    else:\n-        enriched = run_pipeline(report)\n+    enriched = run_graph(report, str(checkpoint_dir) if checkpoint_dir else None, str(schema) if schema else None, str(index_dir) if index_dir else None)\n     out.write_text(enriched.model_dump_json(indent=2))\n-    print(f\"[green]Wrote enriched output -> {out} (graph={graph})\")\n+    print(f\"[green]Wrote enriched output -> {out} (LangGraph mode)\")\n     # HTML artifact\n     if cfg.reports.html_enabled:\n         write_html(enriched, Path(cfg.reports.html_path))\n@@ -68,9 +64,9 @@ def analyze(report: Path = typer.Option(..., exists=True, readable=True, help=\"P\n             print(f\"[red]Diff/notify error: {e}\")\n     # Manifest\n     write_manifest(cfg)\n-    if graph and checkpoint_dir:\n+    if checkpoint_dir:\n         print(f\"[cyan]Checkpoints in {checkpoint_dir}\")\n-    if graph and index_dir:\n+    if index_dir:\n         print(f\"[cyan]Index updated at {index_dir}/index.json\")\n \n @app.command()",
      "patch_lines": [
        "@@ -3,7 +3,7 @@\n",
        " from pathlib import Path\n",
        " import typer\n",
        " from rich import print\n",
        "-from .pipeline import run_pipeline\n",
        "+from .legacy.pipeline import run_pipeline\n",
        " from .graph_pipeline import run_graph\n",
        " from .rules import load_rules_dir, lint_rules, dry_run_apply\n",
        " from .baseline import BaselineStore\n",
        "@@ -29,21 +29,17 @@\n",
        " @app.command()\n",
        " def analyze(report: Path = typer.Option(..., exists=True, readable=True, help=\"Path to sys-scan JSON report\"),\n",
        "             out: Path = typer.Option(\"enriched_report.json\", help=\"Output enriched JSON path\"),\n",
        "-            graph: bool = typer.Option(False, help=\"Use LangGraph DAG implementation\"),\n",
        "-            checkpoint_dir: Path = typer.Option(None, help=\"Directory to write per-node state checkpoints (graph mode only)\"),\n",
        "-            schema: Path = typer.Option(None, help=\"Path to JSON schema for validation (graph mode)\"),\n",
        "-            index_dir: Path = typer.Option(None, help=\"Directory to append time-series index entries (graph mode)\"),\n",
        "+            checkpoint_dir: Path = typer.Option(None, help=\"Directory to write per-node state checkpoints\"),\n",
        "+            schema: Path = typer.Option(None, help=\"Path to JSON schema for validation\"),\n",
        "+            index_dir: Path = typer.Option(None, help=\"Directory to append time-series index entries\"),\n",
        "             dry_run: bool = typer.Option(False, help=\"Sandbox dry-run (no external commands executed)\"),\n",
        "             prev: Path = typer.Option(None, help=\"Previous enriched report for diff\")):\n",
        "     cfg = load_config()\n",
        "     if dry_run:\n",
        "         sandbox_config(dry_run=True)\n",
        "-    if graph:\n",
        "-        enriched = run_graph(report, str(checkpoint_dir) if checkpoint_dir else None, str(schema) if schema else None, str(index_dir) if index_dir else None)\n",
        "-    else:\n",
        "-        enriched = run_pipeline(report)\n",
        "+    enriched = run_graph(report, str(checkpoint_dir) if checkpoint_dir else None, str(schema) if schema else None, str(index_dir) if index_dir else None)\n",
        "     out.write_text(enriched.model_dump_json(indent=2))\n",
        "-    print(f\"[green]Wrote enriched output -> {out} (graph={graph})\")\n",
        "+    print(f\"[green]Wrote enriched output -> {out} (LangGraph mode)\")\n",
        "     # HTML artifact\n",
        "     if cfg.reports.html_enabled:\n",
        "         write_html(enriched, Path(cfg.reports.html_path))\n",
        "@@ -68,9 +64,9 @@ def analyze(report: Path = typer.Option(..., exists=True, readable=True, help=\"P\n",
        "             print(f\"[red]Diff/notify error: {e}\")\n",
        "     # Manifest\n",
        "     write_manifest(cfg)\n",
        "-    if graph and checkpoint_dir:\n",
        "+    if checkpoint_dir:\n",
        "         print(f\"[cyan]Checkpoints in {checkpoint_dir}\")\n",
        "-    if graph and index_dir:\n",
        "+    if index_dir:\n",
        "         print(f\"[cyan]Index updated at {index_dir}/index.json\")\n",
        " \n",
        " @app.command()\n"
      ]
    },
    {
      "path": "agent/graph.py",
      "status": "modified",
      "additions": 121,
      "deletions": 10,
      "patch": "@@ -36,6 +36,12 @@ class GraphState(TypedDict, total=False):\n     final_metrics: Dict[str, Any]                  # Aggregated final metrics snapshot\n     cache: Dict[str, Any]                          # General-purpose cache store (centralized)\n     llm_provider_mode: str                         # Active LLM provider mode (normal|fallback|null)\n+    # Performance optimization fields\n+    current_stage: str                             # Current processing stage for observability\n+    start_time: str                                # Processing start timestamp\n+    cache_hits: List[str]                          # Cache hit tracking\n+    summarize_progress: float                      # Summarization progress (0.0-1.0)\n+    host_id: str                                   # Host identifier for baseline queries\n \n # Runtime graph assembly (enhanced workflow builder)\n try:  # Optional dependency guard\n@@ -97,6 +103,54 @@ class GraphState(TypedDict, total=False):\n         scaffold_cache_manager = None  # type: ignore\n         scaffold_metrics_collector = None  # type: ignore\n \n+    # Attempt to import performance-optimized nodes\n+    try:\n+        from .graph_nodes_performance import (\n+            enrich_findings_batch,\n+            correlate_findings_batch,\n+            summarize_host_state_streaming,\n+            query_baseline_batch,\n+            parallel_node_execution,\n+        )  # type: ignore\n+    except Exception:  # pragma: no cover - performance nodes optional\n+        enrich_findings_batch = None  # type: ignore\n+        correlate_findings_batch = None  # type: ignore\n+        summarize_host_state_streaming = None  # type: ignore\n+        query_baseline_batch = None  # type: ignore\n+        parallel_node_execution = None  # type: ignore\n+\n+    # Attempt to import scalability nodes\n+    try:\n+        from .graph_nodes_scalability import (\n+            parallel_findings_processing,\n+            horizontal_scaling_router,\n+            adaptive_batch_sizing,\n+            initialize_scalability,\n+            shutdown_scalability,\n+        )  # type: ignore\n+    except Exception:  # pragma: no cover - scalability nodes optional\n+        parallel_findings_processing = None  # type: ignore\n+        horizontal_scaling_router = None  # type: ignore\n+        adaptive_batch_sizing = None  # type: ignore\n+        initialize_scalability = None  # type: ignore\n+        shutdown_scalability = None  # type: ignore\n+\n+    # Attempt to import reliability nodes\n+    try:\n+        from .graph_nodes_reliability import (\n+            reliable_enrich_findings,\n+            reliable_correlate_findings,\n+            reliable_summarize_state,\n+            initialize_reliability,\n+            shutdown_reliability,\n+        )  # type: ignore\n+    except Exception:  # pragma: no cover - reliability nodes optional\n+        reliable_enrich_findings = None  # type: ignore\n+        reliable_correlate_findings = None  # type: ignore\n+        reliable_summarize_state = None  # type: ignore\n+        initialize_reliability = None  # type: ignore\n+        shutdown_reliability = None  # type: ignore\n+\n     from .tools import query_baseline\n except Exception:  # pragma: no cover - graph optional\n     StateGraph = None  # type: ignore\n@@ -117,6 +171,18 @@ class GraphState(TypedDict, total=False):\n     scaffold_risk_analyzer = scaffold_compliance_checker = None  # type: ignore\n     scaffold_error_handler = scaffold_human_feedback_node = None  # type: ignore\n     scaffold_cache_manager = scaffold_metrics_collector = None  # type: ignore\n+    # Performance nodes\n+    enrich_findings_batch = correlate_findings_batch = None  # type: ignore\n+    summarize_host_state_streaming = query_baseline_batch = None  # type: ignore\n+    parallel_node_execution = None  # type: ignore\n+    # Scalability nodes\n+    parallel_findings_processing = horizontal_scaling_router = None  # type: ignore\n+    adaptive_batch_sizing = initialize_scalability = None  # type: ignore\n+    shutdown_scalability = None  # type: ignore\n+    # Reliability nodes\n+    reliable_enrich_findings = reliable_correlate_findings = None  # type: ignore\n+    reliable_summarize_state = initialize_reliability = None  # type: ignore\n+    shutdown_reliability = None  # type: ignore\n     query_baseline = None  # type: ignore\n \n \n@@ -132,7 +198,7 @@ def _select(\n     return None\n \n \n-def build_workflow(enhanced: Optional[bool] = None):  # type: ignore\n+def build_workflow(enhanced: Optional[bool] = True):  # type: ignore\n     \"\"\"Build and compile a StateGraph workflow.\n \n     Parameters:\n@@ -152,6 +218,9 @@ def build_workflow(enhanced: Optional[bool] = None):  # type: ignore\n     global scaffold_suggest_rules, enhanced_suggest_rules, scaffold_correlate_findings\n     global scaffold_risk_analyzer, scaffold_compliance_checker, scaffold_error_handler, scaffold_human_feedback_node\n     global scaffold_cache_manager, scaffold_metrics_collector\n+    global enrich_findings_batch, correlate_findings_batch, summarize_host_state_streaming, query_baseline_batch, parallel_node_execution\n+    global parallel_findings_processing, horizontal_scaling_router, adaptive_batch_sizing, initialize_scalability, shutdown_scalability\n+    global reliable_enrich_findings, reliable_correlate_findings, reliable_summarize_state, initialize_reliability, shutdown_reliability\n     if scaffold_enrich_findings is None:\n         try:  # pragma: no cover - recovery path\n             from importlib import import_module\n@@ -169,20 +238,42 @@ def build_workflow(enhanced: Optional[bool] = None):  # type: ignore\n             scaffold_human_feedback_node = getattr(sm, 'human_feedback_node', None)\n             scaffold_cache_manager = getattr(sm, 'cache_manager', None)\n             scaffold_metrics_collector = getattr(sm, 'metrics_collector', None)\n+\n+            # Import performance nodes\n+            pm = import_module('agent.graph_nodes_performance')\n+            enrich_findings_batch = getattr(pm, 'enrich_findings_batch', None)\n+            correlate_findings_batch = getattr(pm, 'correlate_findings_batch', None)\n+            summarize_host_state_streaming = getattr(pm, 'summarize_host_state_streaming', None)\n+            query_baseline_batch = getattr(pm, 'query_baseline_batch', None)\n+            parallel_node_execution = getattr(pm, 'parallel_node_execution', None)\n+\n+            # Import scalability nodes\n+            sm = import_module('agent.graph_nodes_scalability')\n+            parallel_findings_processing = getattr(sm, 'parallel_findings_processing', None)\n+            horizontal_scaling_router = getattr(sm, 'horizontal_scaling_router', None)\n+            adaptive_batch_sizing = getattr(sm, 'adaptive_batch_sizing', None)\n+            initialize_scalability = getattr(sm, 'initialize_scalability', None)\n+            shutdown_scalability = getattr(sm, 'shutdown_scalability', None)\n+\n+            # Import reliability nodes\n+            rm = import_module('agent.graph_nodes_reliability')\n+            reliable_enrich_findings = getattr(rm, 'reliable_enrich_findings', None)\n+            reliable_correlate_findings = getattr(rm, 'reliable_correlate_findings', None)\n+            reliable_summarize_state = getattr(rm, 'reliable_summarize_state', None)\n+            initialize_reliability = getattr(rm, 'initialize_reliability', None)\n+            shutdown_reliability = getattr(rm, 'shutdown_reliability', None)\n         except Exception:\n             pass\n \n-    # Core selection\n-    enrich_node = enhanced and _select(enhanced_enrich_findings) or _select(scaffold_enrich_findings)\n+    # Core selection - prefer reliability, then performance, then enhanced, then scaffold, then legacy\n+    enrich_node = _select(reliable_enrich_findings, enrich_findings_batch, enhanced_enrich_findings, scaffold_enrich_findings)\n     if enrich_node is None:\n         return None, None\n-    summarize_node = enhanced and _select(enhanced_summarize_host_state) or _select(scaffold_summarize_host_state, legacy_summarize_host_state)\n-    suggest_node = enhanced and _select(enhanced_suggest_rules) or _select(scaffold_suggest_rules, legacy_suggest_rules)\n-    if summarize_node is None or suggest_node is None:\n-        return None, None\n \n-    correlate_node = _select(scaffold_correlate_findings, legacy_correlate_findings)\n-    plan_baseline_node = _select(scaffold_plan_baseline_queries, legacy_plan_baseline_queries)\n+    summarize_node = _select(reliable_summarize_state, summarize_host_state_streaming, enhanced_summarize_host_state, scaffold_summarize_host_state, legacy_summarize_host_state)\n+    correlate_node = _select(reliable_correlate_findings, correlate_findings_batch, scaffold_correlate_findings, legacy_correlate_findings)\n+    suggest_node = _select(enhanced_suggest_rules, scaffold_suggest_rules, legacy_suggest_rules)\n+    plan_baseline_node = _select(query_baseline_batch, scaffold_plan_baseline_queries, legacy_plan_baseline_queries)\n     integrate_baseline_node = _select(scaffold_integrate_baseline_results, legacy_integrate_baseline_results)\n     choose_post_summarize_node = _select(scaffold_choose_post_summarize, legacy_choose_post_summarize)\n     should_suggest_rules_node = _select(scaffold_should_suggest_rules, legacy_should_suggest_rules)\n@@ -194,6 +285,9 @@ def build_workflow(enhanced: Optional[bool] = None):  # type: ignore\n     human_feedback_node = _select(scaffold_human_feedback_node)\n     cache_manager_node = _select(scaffold_cache_manager)\n     metrics_collector_node = _select(scaffold_metrics_collector)\n+    # Scalability nodes\n+    parallel_processing_node = _select(parallel_findings_processing)\n+    scaling_router_node = _select(horizontal_scaling_router)\n \n     wf = StateGraph(GraphState)  # type: ignore[arg-type]\n     # Node registration\n@@ -220,7 +314,11 @@ def build_workflow(enhanced: Optional[bool] = None):  # type: ignore\n         wf.add_node(\"cache_manager\", cache_manager_node)  # type: ignore[arg-type]\n     if metrics_collector_node:\n         wf.add_node(\"metrics_collector\", metrics_collector_node)  # type: ignore[arg-type]\n-    # tool_coordinator currently not wired; skip registration to avoid unreachable validation error\n+    # Scalability nodes\n+    if parallel_processing_node:\n+        wf.add_node(\"parallel_processing\", parallel_processing_node)  # type: ignore[arg-type]\n+    if scaling_router_node:\n+        wf.add_node(\"scaling_router\", scaling_router_node)  # type: ignore[arg-type]\n \n     # Entry\n     wf.set_entry_point(\"enrich\")\n@@ -240,6 +338,12 @@ def build_workflow(enhanced: Optional[bool] = None):  # type: ignore\n \n     # Post summarize conditional: baseline planning or proceed\n     def _post_summarize_router(state: GraphState) -> str:  # type: ignore\n+        # Check if scalability routing is available and needed\n+        if callable(scaling_router_node):  # type: ignore\n+            scaling_decision = scaling_router_node(state)  # type: ignore\n+            if scaling_decision in [\"parallel_processing\", \"batch_processing\"]:\n+                return scaling_decision\n+\n         # If baseline cycle logic present, delegate to choose_post_summarize; else always proceed\n         if callable(choose_post_summarize_node):  # type: ignore\n             return choose_post_summarize_node(state)  # type: ignore\n@@ -248,6 +352,9 @@ def _post_summarize_router(state: GraphState) -> str:  # type: ignore\n     mapping_after_summarize = {\"proceed_suggest\": \"suggest_rules\", \"suggest_rules\": \"suggest_rules\"}\n     if plan_baseline_node:\n         mapping_after_summarize[\"plan_baseline\"] = \"plan_baseline\"\n+    # Add scalability mappings\n+    if parallel_processing_node:\n+        mapping_after_summarize[\"parallel_processing\"] = \"parallel_processing\"\n     if END is not None:\n         if metrics_collector_node is not None:\n             mapping_after_summarize[END] = \"metrics_collector\"  # type: ignore\n@@ -265,6 +372,10 @@ def _post_summarize_router(state: GraphState) -> str:  # type: ignore\n         except Exception:  # pragma: no cover\n             pass\n \n+    # Add scalability edges\n+    if parallel_processing_node:\n+        wf.add_edge(\"parallel_processing\", \"suggest_rules\")\n+\n     # advanced_router currently disabled in workflow wiring pending refactor (would require router-only integration)\n \n     # Operational tail chain",
      "patch_lines": [
        "@@ -36,6 +36,12 @@ class GraphState(TypedDict, total=False):\n",
        "     final_metrics: Dict[str, Any]                  # Aggregated final metrics snapshot\n",
        "     cache: Dict[str, Any]                          # General-purpose cache store (centralized)\n",
        "     llm_provider_mode: str                         # Active LLM provider mode (normal|fallback|null)\n",
        "+    # Performance optimization fields\n",
        "+    current_stage: str                             # Current processing stage for observability\n",
        "+    start_time: str                                # Processing start timestamp\n",
        "+    cache_hits: List[str]                          # Cache hit tracking\n",
        "+    summarize_progress: float                      # Summarization progress (0.0-1.0)\n",
        "+    host_id: str                                   # Host identifier for baseline queries\n",
        " \n",
        " # Runtime graph assembly (enhanced workflow builder)\n",
        " try:  # Optional dependency guard\n",
        "@@ -97,6 +103,54 @@ class GraphState(TypedDict, total=False):\n",
        "         scaffold_cache_manager = None  # type: ignore\n",
        "         scaffold_metrics_collector = None  # type: ignore\n",
        " \n",
        "+    # Attempt to import performance-optimized nodes\n",
        "+    try:\n",
        "+        from .graph_nodes_performance import (\n",
        "+            enrich_findings_batch,\n",
        "+            correlate_findings_batch,\n",
        "+            summarize_host_state_streaming,\n",
        "+            query_baseline_batch,\n",
        "+            parallel_node_execution,\n",
        "+        )  # type: ignore\n",
        "+    except Exception:  # pragma: no cover - performance nodes optional\n",
        "+        enrich_findings_batch = None  # type: ignore\n",
        "+        correlate_findings_batch = None  # type: ignore\n",
        "+        summarize_host_state_streaming = None  # type: ignore\n",
        "+        query_baseline_batch = None  # type: ignore\n",
        "+        parallel_node_execution = None  # type: ignore\n",
        "+\n",
        "+    # Attempt to import scalability nodes\n",
        "+    try:\n",
        "+        from .graph_nodes_scalability import (\n",
        "+            parallel_findings_processing,\n",
        "+            horizontal_scaling_router,\n",
        "+            adaptive_batch_sizing,\n",
        "+            initialize_scalability,\n",
        "+            shutdown_scalability,\n",
        "+        )  # type: ignore\n",
        "+    except Exception:  # pragma: no cover - scalability nodes optional\n",
        "+        parallel_findings_processing = None  # type: ignore\n",
        "+        horizontal_scaling_router = None  # type: ignore\n",
        "+        adaptive_batch_sizing = None  # type: ignore\n",
        "+        initialize_scalability = None  # type: ignore\n",
        "+        shutdown_scalability = None  # type: ignore\n",
        "+\n",
        "+    # Attempt to import reliability nodes\n",
        "+    try:\n",
        "+        from .graph_nodes_reliability import (\n",
        "+            reliable_enrich_findings,\n",
        "+            reliable_correlate_findings,\n",
        "+            reliable_summarize_state,\n",
        "+            initialize_reliability,\n",
        "+            shutdown_reliability,\n",
        "+        )  # type: ignore\n",
        "+    except Exception:  # pragma: no cover - reliability nodes optional\n",
        "+        reliable_enrich_findings = None  # type: ignore\n",
        "+        reliable_correlate_findings = None  # type: ignore\n",
        "+        reliable_summarize_state = None  # type: ignore\n",
        "+        initialize_reliability = None  # type: ignore\n",
        "+        shutdown_reliability = None  # type: ignore\n",
        "+\n",
        "     from .tools import query_baseline\n",
        " except Exception:  # pragma: no cover - graph optional\n",
        "     StateGraph = None  # type: ignore\n",
        "@@ -117,6 +171,18 @@ class GraphState(TypedDict, total=False):\n",
        "     scaffold_risk_analyzer = scaffold_compliance_checker = None  # type: ignore\n",
        "     scaffold_error_handler = scaffold_human_feedback_node = None  # type: ignore\n",
        "     scaffold_cache_manager = scaffold_metrics_collector = None  # type: ignore\n",
        "+    # Performance nodes\n",
        "+    enrich_findings_batch = correlate_findings_batch = None  # type: ignore\n",
        "+    summarize_host_state_streaming = query_baseline_batch = None  # type: ignore\n",
        "+    parallel_node_execution = None  # type: ignore\n",
        "+    # Scalability nodes\n",
        "+    parallel_findings_processing = horizontal_scaling_router = None  # type: ignore\n",
        "+    adaptive_batch_sizing = initialize_scalability = None  # type: ignore\n",
        "+    shutdown_scalability = None  # type: ignore\n",
        "+    # Reliability nodes\n",
        "+    reliable_enrich_findings = reliable_correlate_findings = None  # type: ignore\n",
        "+    reliable_summarize_state = initialize_reliability = None  # type: ignore\n",
        "+    shutdown_reliability = None  # type: ignore\n",
        "     query_baseline = None  # type: ignore\n",
        " \n",
        " \n",
        "@@ -132,7 +198,7 @@ def _select(\n",
        "     return None\n",
        " \n",
        " \n",
        "-def build_workflow(enhanced: Optional[bool] = None):  # type: ignore\n",
        "+def build_workflow(enhanced: Optional[bool] = True):  # type: ignore\n",
        "     \"\"\"Build and compile a StateGraph workflow.\n",
        " \n",
        "     Parameters:\n",
        "@@ -152,6 +218,9 @@ def build_workflow(enhanced: Optional[bool] = None):  # type: ignore\n",
        "     global scaffold_suggest_rules, enhanced_suggest_rules, scaffold_correlate_findings\n",
        "     global scaffold_risk_analyzer, scaffold_compliance_checker, scaffold_error_handler, scaffold_human_feedback_node\n",
        "     global scaffold_cache_manager, scaffold_metrics_collector\n",
        "+    global enrich_findings_batch, correlate_findings_batch, summarize_host_state_streaming, query_baseline_batch, parallel_node_execution\n",
        "+    global parallel_findings_processing, horizontal_scaling_router, adaptive_batch_sizing, initialize_scalability, shutdown_scalability\n",
        "+    global reliable_enrich_findings, reliable_correlate_findings, reliable_summarize_state, initialize_reliability, shutdown_reliability\n",
        "     if scaffold_enrich_findings is None:\n",
        "         try:  # pragma: no cover - recovery path\n",
        "             from importlib import import_module\n",
        "@@ -169,20 +238,42 @@ def build_workflow(enhanced: Optional[bool] = None):  # type: ignore\n",
        "             scaffold_human_feedback_node = getattr(sm, 'human_feedback_node', None)\n",
        "             scaffold_cache_manager = getattr(sm, 'cache_manager', None)\n",
        "             scaffold_metrics_collector = getattr(sm, 'metrics_collector', None)\n",
        "+\n",
        "+            # Import performance nodes\n",
        "+            pm = import_module('agent.graph_nodes_performance')\n",
        "+            enrich_findings_batch = getattr(pm, 'enrich_findings_batch', None)\n",
        "+            correlate_findings_batch = getattr(pm, 'correlate_findings_batch', None)\n",
        "+            summarize_host_state_streaming = getattr(pm, 'summarize_host_state_streaming', None)\n",
        "+            query_baseline_batch = getattr(pm, 'query_baseline_batch', None)\n",
        "+            parallel_node_execution = getattr(pm, 'parallel_node_execution', None)\n",
        "+\n",
        "+            # Import scalability nodes\n",
        "+            sm = import_module('agent.graph_nodes_scalability')\n",
        "+            parallel_findings_processing = getattr(sm, 'parallel_findings_processing', None)\n",
        "+            horizontal_scaling_router = getattr(sm, 'horizontal_scaling_router', None)\n",
        "+            adaptive_batch_sizing = getattr(sm, 'adaptive_batch_sizing', None)\n",
        "+            initialize_scalability = getattr(sm, 'initialize_scalability', None)\n",
        "+            shutdown_scalability = getattr(sm, 'shutdown_scalability', None)\n",
        "+\n",
        "+            # Import reliability nodes\n",
        "+            rm = import_module('agent.graph_nodes_reliability')\n",
        "+            reliable_enrich_findings = getattr(rm, 'reliable_enrich_findings', None)\n",
        "+            reliable_correlate_findings = getattr(rm, 'reliable_correlate_findings', None)\n",
        "+            reliable_summarize_state = getattr(rm, 'reliable_summarize_state', None)\n",
        "+            initialize_reliability = getattr(rm, 'initialize_reliability', None)\n",
        "+            shutdown_reliability = getattr(rm, 'shutdown_reliability', None)\n",
        "         except Exception:\n",
        "             pass\n",
        " \n",
        "-    # Core selection\n",
        "-    enrich_node = enhanced and _select(enhanced_enrich_findings) or _select(scaffold_enrich_findings)\n",
        "+    # Core selection - prefer reliability, then performance, then enhanced, then scaffold, then legacy\n",
        "+    enrich_node = _select(reliable_enrich_findings, enrich_findings_batch, enhanced_enrich_findings, scaffold_enrich_findings)\n",
        "     if enrich_node is None:\n",
        "         return None, None\n",
        "-    summarize_node = enhanced and _select(enhanced_summarize_host_state) or _select(scaffold_summarize_host_state, legacy_summarize_host_state)\n",
        "-    suggest_node = enhanced and _select(enhanced_suggest_rules) or _select(scaffold_suggest_rules, legacy_suggest_rules)\n",
        "-    if summarize_node is None or suggest_node is None:\n",
        "-        return None, None\n",
        " \n",
        "-    correlate_node = _select(scaffold_correlate_findings, legacy_correlate_findings)\n",
        "-    plan_baseline_node = _select(scaffold_plan_baseline_queries, legacy_plan_baseline_queries)\n",
        "+    summarize_node = _select(reliable_summarize_state, summarize_host_state_streaming, enhanced_summarize_host_state, scaffold_summarize_host_state, legacy_summarize_host_state)\n",
        "+    correlate_node = _select(reliable_correlate_findings, correlate_findings_batch, scaffold_correlate_findings, legacy_correlate_findings)\n",
        "+    suggest_node = _select(enhanced_suggest_rules, scaffold_suggest_rules, legacy_suggest_rules)\n",
        "+    plan_baseline_node = _select(query_baseline_batch, scaffold_plan_baseline_queries, legacy_plan_baseline_queries)\n",
        "     integrate_baseline_node = _select(scaffold_integrate_baseline_results, legacy_integrate_baseline_results)\n",
        "     choose_post_summarize_node = _select(scaffold_choose_post_summarize, legacy_choose_post_summarize)\n",
        "     should_suggest_rules_node = _select(scaffold_should_suggest_rules, legacy_should_suggest_rules)\n",
        "@@ -194,6 +285,9 @@ def build_workflow(enhanced: Optional[bool] = None):  # type: ignore\n",
        "     human_feedback_node = _select(scaffold_human_feedback_node)\n",
        "     cache_manager_node = _select(scaffold_cache_manager)\n",
        "     metrics_collector_node = _select(scaffold_metrics_collector)\n",
        "+    # Scalability nodes\n",
        "+    parallel_processing_node = _select(parallel_findings_processing)\n",
        "+    scaling_router_node = _select(horizontal_scaling_router)\n",
        " \n",
        "     wf = StateGraph(GraphState)  # type: ignore[arg-type]\n",
        "     # Node registration\n",
        "@@ -220,7 +314,11 @@ def build_workflow(enhanced: Optional[bool] = None):  # type: ignore\n",
        "         wf.add_node(\"cache_manager\", cache_manager_node)  # type: ignore[arg-type]\n",
        "     if metrics_collector_node:\n",
        "         wf.add_node(\"metrics_collector\", metrics_collector_node)  # type: ignore[arg-type]\n",
        "-    # tool_coordinator currently not wired; skip registration to avoid unreachable validation error\n",
        "+    # Scalability nodes\n",
        "+    if parallel_processing_node:\n",
        "+        wf.add_node(\"parallel_processing\", parallel_processing_node)  # type: ignore[arg-type]\n",
        "+    if scaling_router_node:\n",
        "+        wf.add_node(\"scaling_router\", scaling_router_node)  # type: ignore[arg-type]\n",
        " \n",
        "     # Entry\n",
        "     wf.set_entry_point(\"enrich\")\n",
        "@@ -240,6 +338,12 @@ def build_workflow(enhanced: Optional[bool] = None):  # type: ignore\n",
        " \n",
        "     # Post summarize conditional: baseline planning or proceed\n",
        "     def _post_summarize_router(state: GraphState) -> str:  # type: ignore\n",
        "+        # Check if scalability routing is available and needed\n",
        "+        if callable(scaling_router_node):  # type: ignore\n",
        "+            scaling_decision = scaling_router_node(state)  # type: ignore\n",
        "+            if scaling_decision in [\"parallel_processing\", \"batch_processing\"]:\n",
        "+                return scaling_decision\n",
        "+\n",
        "         # If baseline cycle logic present, delegate to choose_post_summarize; else always proceed\n",
        "         if callable(choose_post_summarize_node):  # type: ignore\n",
        "             return choose_post_summarize_node(state)  # type: ignore\n",
        "@@ -248,6 +352,9 @@ def _post_summarize_router(state: GraphState) -> str:  # type: ignore\n",
        "     mapping_after_summarize = {\"proceed_suggest\": \"suggest_rules\", \"suggest_rules\": \"suggest_rules\"}\n",
        "     if plan_baseline_node:\n",
        "         mapping_after_summarize[\"plan_baseline\"] = \"plan_baseline\"\n",
        "+    # Add scalability mappings\n",
        "+    if parallel_processing_node:\n",
        "+        mapping_after_summarize[\"parallel_processing\"] = \"parallel_processing\"\n",
        "     if END is not None:\n",
        "         if metrics_collector_node is not None:\n",
        "             mapping_after_summarize[END] = \"metrics_collector\"  # type: ignore\n",
        "@@ -265,6 +372,10 @@ def _post_summarize_router(state: GraphState) -> str:  # type: ignore\n",
        "         except Exception:  # pragma: no cover\n",
        "             pass\n",
        " \n",
        "+    # Add scalability edges\n",
        "+    if parallel_processing_node:\n",
        "+        wf.add_edge(\"parallel_processing\", \"suggest_rules\")\n",
        "+\n",
        "     # advanced_router currently disabled in workflow wiring pending refactor (would require router-only integration)\n",
        " \n",
        "     # Operational tail chain\n"
      ]
    },
    {
      "path": "agent/graph_nodes_performance.py",
      "status": "added",
      "additions": 642,
      "deletions": 0,
      "patch": "@@ -0,0 +1,642 @@\n+from __future__ import annotations\n+\"\"\"Performance-optimized graph node functions with advanced async capabilities.\n+\n+This module provides high-performance node implementations with:\n+- Connection pooling and batch database operations\n+- Streaming processing for large datasets\n+- Parallel execution patterns\n+- Memory-efficient data structures\n+- Advanced caching strategies\n+- Circuit breaker patterns\n+\"\"\"\n+\n+from typing import Any, List, Dict, Optional, Union, AsyncIterator, Callable\n+import asyncio\n+import time\n+import logging\n+import json\n+from pathlib import Path\n+from datetime import datetime\n+from contextlib import asynccontextmanager\n+from dataclasses import dataclass, field\n+import hashlib\n+import tempfile\n+from concurrent.futures import ThreadPoolExecutor\n+\n+# Use standard sqlite3 for now, can upgrade to aiosqlite later\n+import sqlite3\n+\n+from .graph import GraphState\n+from .data_governance import get_data_governor\n+from .models import Finding, ScannerResult, Report, Meta, Summary, SummaryExtension, AgentState, Reductions\n+from .knowledge import apply_external_knowledge\n+from .pipeline import augment as _augment\n+from .reduction import reduce_all\n+from .llm_provider import get_llm_provider\n+from .rules import Correlator, DEFAULT_RULES\n+from .rule_gap_miner import mine_gap_candidates\n+\n+logger = logging.getLogger(__name__)\n+\n+# Performance configuration\n+@dataclass\n+class PerformanceConfig:\n+    \"\"\"Configuration for performance optimizations.\"\"\"\n+    batch_size: int = 100\n+    max_concurrent_db_connections: int = 10\n+    cache_ttl_seconds: int = 3600\n+    streaming_chunk_size: int = 50\n+    max_memory_mb: int = 512\n+    thread_pool_workers: int = 4\n+\n+# Global performance config\n+perf_config = PerformanceConfig()\n+\n+# Connection pool for database operations (using regular sqlite3 for now)\n+class DatabaseConnectionPool:\n+    \"\"\"Async-compatible connection pool for SQLite operations.\"\"\"\n+\n+    def __init__(self, db_path: str, max_connections: int = 10):\n+        self.db_path = db_path\n+        self.max_connections = max_connections\n+        self._pool: asyncio.Queue[sqlite3.Connection] = asyncio.Queue()\n+        self._semaphore = asyncio.Semaphore(max_connections)\n+\n+    async def initialize(self):\n+        \"\"\"Initialize the connection pool.\"\"\"\n+        for _ in range(self.max_connections):\n+            # Use ThreadPoolExecutor for sqlite3 operations\n+            loop = asyncio.get_event_loop()\n+            conn = await loop.run_in_executor(None, sqlite3.connect, self.db_path)\n+            await self._pool.put(conn)\n+\n+    async def get_connection(self) -> sqlite3.Connection:\n+        \"\"\"Get a connection from the pool.\"\"\"\n+        await self._semaphore.acquire()\n+        conn = await self._pool.get()\n+        return conn\n+\n+    async def return_connection(self, conn: sqlite3.Connection):\n+        \"\"\"Return a connection to the pool.\"\"\"\n+        await self._pool.put(conn)\n+        self._semaphore.release()\n+\n+    async def execute_query(self, query: str, params: tuple = ()) -> List[tuple]:\n+        \"\"\"Execute a query using a pooled connection.\"\"\"\n+        conn = await self.get_connection()\n+        try:\n+            loop = asyncio.get_event_loop()\n+            cursor = await loop.run_in_executor(None, conn.cursor)\n+            await loop.run_in_executor(None, cursor.execute, query, params)\n+            results = await loop.run_in_executor(None, cursor.fetchall)\n+            await loop.run_in_executor(None, cursor.close)\n+            return results\n+        finally:\n+            await self.return_connection(conn)\n+\n+    async def close_all(self):\n+        \"\"\"Close all connections in the pool.\"\"\"\n+        while not self._pool.empty():\n+            conn = await self._pool.get()\n+            loop = asyncio.get_event_loop()\n+            await loop.run_in_executor(None, conn.close)\n+\n+# Global connection pool\n+db_pool: Optional[DatabaseConnectionPool] = None\n+\n+@asynccontextmanager\n+async def get_db_connection(db_path: str):\n+    \"\"\"Context manager for database connections.\"\"\"\n+    global db_pool\n+    if db_pool is None:\n+        db_pool = DatabaseConnectionPool(db_path)\n+        await db_pool.initialize()\n+\n+    conn = await db_pool.get_connection()\n+    try:\n+        yield conn\n+    finally:\n+        await db_pool.return_connection(conn)\n+\n+# Memory-efficient data structures\n+@dataclass\n+class FindingBatch:\n+    \"\"\"Memory-efficient batch of findings.\"\"\"\n+    findings: List[Finding] = field(default_factory=list)\n+    batch_id: str = \"\"\n+    metadata: Dict[str, Any] = field(default_factory=dict)\n+\n+    def add_finding(self, finding: Finding):\n+        \"\"\"Add a finding to the batch.\"\"\"\n+        self.findings.append(finding)\n+\n+    def is_full(self) -> bool:\n+        \"\"\"Check if batch is at capacity.\"\"\"\n+        return len(self.findings) >= perf_config.batch_size\n+\n+    def clear(self):\n+        \"\"\"Clear the batch.\"\"\"\n+        self.findings.clear()\n+        self.metadata.clear()\n+\n+# Advanced caching with TTL\n+class AdvancedCache:\n+    \"\"\"Advanced caching with TTL and size limits.\"\"\"\n+\n+    def __init__(self, max_size: int = 1000, ttl_seconds: int = 3600):\n+        self.max_size = max_size\n+        self.ttl_seconds = ttl_seconds\n+        self._cache: Dict[str, Dict[str, Any]] = {}\n+        self._access_times: Dict[str, float] = {}\n+\n+    def get(self, key: str) -> Optional[Any]:\n+        \"\"\"Get cached value if not expired.\"\"\"\n+        if key in self._cache:\n+            if time.time() - self._access_times[key] > self.ttl_seconds:\n+                del self._cache[key]\n+                del self._access_times[key]\n+                return None\n+            self._access_times[key] = time.time()\n+            return self._cache[key]\n+        return None\n+\n+    def set(self, key: str, value: Any):\n+        \"\"\"Set cached value with eviction if needed.\"\"\"\n+        if len(self._cache) >= self.max_size:\n+            # Evict least recently used\n+            oldest_key = min(self._access_times.keys(), key=lambda k: self._access_times[k])\n+            del self._cache[oldest_key]\n+            del self._access_times[oldest_key]\n+\n+        self._cache[key] = value\n+        self._access_times[key] = time.time()\n+\n+    def clear_expired(self):\n+        \"\"\"Clear expired entries.\"\"\"\n+        current_time = time.time()\n+        expired_keys = [\n+            key for key, access_time in self._access_times.items()\n+            if current_time - access_time > self.ttl_seconds\n+        ]\n+        for key in expired_keys:\n+            del self._cache[key]\n+            del self._access_times[key]\n+\n+# Global cache instance\n+advanced_cache = AdvancedCache()\n+\n+def _append_warning(state: GraphState, module: str, stage: str, error: str, hint: str | None = None):\n+    \"\"\"Enhanced warning appender with performance tracking.\"\"\"\n+    wl = state.setdefault('warnings', [])\n+    error_entry = {\n+        'module': module,\n+        'stage': stage,\n+        'error': error,\n+        'hint': hint,\n+        'timestamp': datetime.now().isoformat(),\n+        'session_id': state.get('session_id', 'unknown'),\n+        'performance_impact': 'low'  # Can be upgraded based on error type\n+    }\n+    wl.append(error_entry)\n+\n+    # Track in errors list for better visibility\n+    errors = state.setdefault('errors', [])\n+    errors.append(error_entry)\n+\n+def _findings_from_graph(state: GraphState) -> List[Finding]:\n+    \"\"\"Convert graph state findings to Pydantic models with batching.\"\"\"\n+    out: List[Finding] = []\n+    raw_findings = state.get('raw_findings', []) or []\n+\n+    # Process in batches to avoid memory spikes\n+    for i in range(0, len(raw_findings), perf_config.batch_size):\n+        batch = raw_findings[i:i + perf_config.batch_size]\n+        for finding in batch:\n+            try:\n+                out.append(Finding(\n+                    id=finding.get('id','unknown'),\n+                    title=finding.get('title','(no title)'),\n+                    severity=finding.get('severity','info'),\n+                    risk_score=int(finding.get('risk_score', finding.get('risk_total', 0)) or 0),\n+                    metadata=finding.get('metadata', {})\n+                ))\n+            except Exception:\n+                continue\n+\n+    return out\n+\n+async def batch_process_findings(findings: List[Any], processor_func: Callable[[List[Any]], Any], batch_size: Optional[int] = None) -> List[Any]:\n+    \"\"\"Process findings in batches for memory efficiency.\"\"\"\n+    if batch_size is None:\n+        batch_size = perf_config.batch_size\n+\n+    results = []\n+    for i in range(0, len(findings), batch_size):\n+        batch = findings[i:i + batch_size]\n+        batch_results = await processor_func(batch)\n+        results.extend(batch_results)\n+        # Allow other tasks to run\n+        await asyncio.sleep(0)\n+    return results\n+\n+async def enrich_findings_batch(state: GraphState) -> GraphState:\n+    \"\"\"Batch-optimized enrichment with streaming and caching.\"\"\"\n+    start_time = time.time()\n+    state['current_stage'] = 'enrich'\n+    state['start_time'] = state.get('start_time', datetime.now().isoformat())\n+\n+    try:\n+        # Check cache first\n+        cache_key = f\"enrich_batch_{hash(str(state.get('raw_findings', [])))}\"\n+        cached_result = advanced_cache.get(cache_key)\n+        if cached_result:\n+            logger.info(\"Using cached enrichment results\")\n+            state.update(cached_result)\n+            state['cache_hits'] = state.get('cache_hits', []) + [cache_key]\n+            return state\n+\n+        findings = _findings_from_graph(state)\n+\n+        # Process in batches\n+        async def process_batch(batch: List[Finding]) -> List[Dict[str, Any]]:\n+            \"\"\"Process a batch of findings.\"\"\"\n+            batch_results = []\n+            for finding in batch:\n+                try:\n+                    # Create minimal report for this finding\n+                    sr = ScannerResult(\n+                        scanner='mixed',\n+                        finding_count=1,\n+                        findings=[finding]\n+                    )\n+                    report = Report(\n+                        meta=Meta(),\n+                        summary=Summary(finding_count_total=1, finding_count_emitted=1),\n+                        results=[sr],\n+                        collection_warnings=[],\n+                        scanner_errors=[],\n+                        summary_extension=SummaryExtension(total_risk_score=finding.risk_score or 0)\n+                    )\n+                    astate = AgentState(report=report)\n+\n+                    # Apply enrichment pipeline\n+                    astate = _augment(astate)\n+                    astate = apply_external_knowledge(astate)\n+\n+                    # Extract enriched finding\n+                    if astate.report and astate.report.results:\n+                        for r in astate.report.results:\n+                            for f in r.findings:\n+                                batch_results.append(f.model_dump())\n+\n+                except Exception as e:\n+                    logger.warning(f\"Failed to enrich finding {finding.id}: {e}\")\n+                    continue\n+\n+            return batch_results\n+\n+        # Process all findings in batches\n+        enriched_findings = await batch_process_findings(findings, process_batch)\n+\n+        state['enriched_findings'] = enriched_findings\n+\n+        # Cache the results\n+        cache_data = {\n+            'enriched_findings': enriched_findings,\n+            'cache_timestamp': datetime.now().isoformat()\n+        }\n+        advanced_cache.set(cache_key, cache_data)\n+        state['cache_keys'] = state.get('cache_keys', []) + [cache_key]\n+\n+        # Update metrics\n+        state.setdefault('metrics', {})['enrich_duration'] = time.time() - start_time\n+        state.setdefault('metrics', {})['findings_processed'] = len(enriched_findings)\n+\n+    except Exception as e:\n+        logger.error(f\"Batch enrichment failed: {e}\")\n+        _append_warning(state, 'graph', 'enrich_batch', str(e))\n+        state.setdefault('enriched_findings', state.get('raw_findings', []))\n+\n+    return state\n+\n+async def correlate_findings_batch(state: GraphState) -> GraphState:\n+    \"\"\"Batch-optimized correlation with parallel processing.\"\"\"\n+    start_time = time.time()\n+    state['current_stage'] = 'correlate'\n+\n+    try:\n+        findings: List[Finding] = []\n+        enriched_findings = state.get('enriched_findings', []) or []\n+\n+        # Convert to Pydantic models in batches\n+        for i in range(0, len(enriched_findings), perf_config.batch_size):\n+            batch = enriched_findings[i:i + perf_config.batch_size]\n+            for finding_dict in batch:\n+                try:\n+                    findings.append(Finding(**{k: v for k, v in finding_dict.items() if k in Finding.model_fields}))\n+                except Exception:\n+                    continue\n+\n+        if not findings:\n+            return state\n+\n+        # Create report for correlation\n+        sr = ScannerResult(scanner='mixed', finding_count=len(findings), findings=findings)\n+        report = Report(\n+            meta=Meta(),\n+            summary=Summary(finding_count_total=len(findings), finding_count_emitted=len(findings)),\n+            results=[sr],\n+            collection_warnings=[],\n+            scanner_errors=[],\n+            summary_extension=SummaryExtension(total_risk_score=sum(f.risk_score or 0 for f in findings))\n+        )\n+        astate = AgentState(report=report)\n+\n+        # Apply correlation rules\n+        correlator = Correlator(DEFAULT_RULES)\n+        correlations = correlator.apply(findings)\n+\n+        # Attach correlation refs to findings in parallel\n+        async def attach_correlations():\n+            \"\"\"Attach correlation references to findings.\"\"\"\n+            tasks = []\n+            for correlation in correlations:\n+                task = asyncio.create_task(attach_correlation_refs(findings, correlation))\n+                tasks.append(task)\n+            await asyncio.gather(*tasks)\n+\n+        async def attach_correlation_refs(findings: List[Finding], correlation):\n+            \"\"\"Attach correlation refs to relevant findings.\"\"\"\n+            for finding in findings:\n+                if finding.id in correlation.related_finding_ids and correlation.id not in finding.correlation_refs:\n+                    finding.correlation_refs.append(correlation.id)\n+\n+        await attach_correlations()\n+\n+        state['correlated_findings'] = [finding.model_dump() for finding in findings]\n+        state['correlations'] = [c.model_dump() for c in correlations]\n+\n+        # Update metrics\n+        state.setdefault('metrics', {})['correlate_duration'] = time.time() - start_time\n+        state.setdefault('metrics', {})['correlations_found'] = len(correlations)\n+\n+    except Exception as e:\n+        logger.error(f\"Batch correlation failed: {e}\")\n+        _append_warning(state, 'graph', 'correlate_batch', str(e))\n+        if 'correlated_findings' not in state:\n+            state['correlated_findings'] = state.get('enriched_findings', [])\n+\n+    return state\n+\n+async def summarize_host_state_streaming(state: GraphState) -> GraphState:\n+    \"\"\"Streaming summarization with real-time updates.\"\"\"\n+    start_time = time.time()\n+    state['current_stage'] = 'summarize'\n+\n+    try:\n+        # Iteration guard\n+        max_iter = int(__import__('os').environ.get('AGENT_MAX_SUMMARY_ITERS', '3'))\n+        iters = int(state.get('iteration_count', 0))\n+        if iters >= max_iter:\n+            _append_warning(state, 'graph', 'summarize', 'iteration_limit_reached')\n+            return state\n+\n+        provider = get_llm_provider()\n+        findings_dicts = state.get('correlated_findings') or state.get('enriched_findings') or []\n+\n+        # Convert findings in streaming fashion\n+        findings: List[Finding] = []\n+        for finding_dict in findings_dicts:\n+            try:\n+                findings.append(Finding(**{k: v for k, v in finding_dict.items() if k in Finding.model_fields}))\n+            except Exception:\n+                continue\n+\n+        # Process in chunks for memory efficiency\n+        reductions = []\n+        for i in range(0, len(findings), perf_config.streaming_chunk_size):\n+            chunk = findings[i:i + perf_config.streaming_chunk_size]\n+            chunk_reductions = reduce_all(chunk)\n+            reductions.extend(chunk_reductions)\n+\n+            # Yield progress updates\n+            progress = (i + len(chunk)) / len(findings)\n+            state['summarize_progress'] = progress\n+            await asyncio.sleep(0)  # Allow other tasks to run\n+\n+        # Enhanced correlation handling\n+        corr_objs = []\n+        correlations = state.get('correlations', []) or []\n+        for c in correlations:\n+            try:\n+                from .models import Correlation as _C\n+                corr_objs.append(_C(**c))\n+            except Exception:\n+                continue\n+\n+        baseline_context = state.get('baseline_results') or {}\n+\n+        # Generate summary\n+        summaries = provider.summarize(reductions, corr_objs, actions=[], baseline_context=baseline_context)\n+        state['summary'] = summaries.model_dump()\n+        state['iteration_count'] = iters + 1\n+\n+        # Update metrics\n+        state.setdefault('metrics', {})['summarize_duration'] = time.time() - start_time\n+        state.setdefault('metrics', {})['chunks_processed'] = len(range(0, len(findings), perf_config.streaming_chunk_size))\n+\n+    except Exception as e:\n+        logger.error(f\"Streaming summarization failed: {e}\")\n+        _append_warning(state, 'graph', 'summarize_streaming', str(e))\n+\n+    return state\n+\n+async def query_baseline_batch(state: GraphState) -> GraphState:\n+    \"\"\"Batch baseline queries with connection pooling.\"\"\"\n+    start_time = time.time()\n+    state['current_stage'] = 'baseline_query'\n+\n+    try:\n+        enriched = state.get('enriched_findings', []) or []\n+        if not enriched:\n+            return state\n+\n+        db_path = __import__('os').environ.get('AGENT_BASELINE_DB', 'agent_baseline.db')\n+\n+        # Process baseline queries in batches\n+        async def process_baseline_batch(batch: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n+            \"\"\"Process a batch of baseline queries.\"\"\"\n+            results = []\n+            async with get_db_connection(db_path) as conn:\n+                for finding in batch:\n+                    try:\n+                        fid = finding.get('id') or 'unknown'\n+                        title = finding.get('title') or ''\n+                        severity = finding.get('severity') or ''\n+                        scanner = finding.get('scanner') or 'mixed'\n+\n+                        # Compute composite hash\n+                        identity_core = f\"{fid}\\n{title}\\n{severity}\\n\".encode()\n+                        h = hashlib.sha256(identity_core).hexdigest()\n+\n+                        from .baseline import hashlib_sha\n+                        composite = hashlib_sha(scanner, h)\n+\n+                        # Query database\n+                        loop = asyncio.get_event_loop()\n+                        cursor = await loop.run_in_executor(None, conn.cursor)\n+                        await loop.run_in_executor(None, cursor.execute,\n+                            \"SELECT first_seen_ts, seen_count FROM baseline_finding WHERE host_id=? AND finding_hash=?\",\n+                            (state.get('host_id', 'unknown'), composite)\n+                        )\n+                        row = await loop.run_in_executor(None, cursor.fetchone)\n+                        await loop.run_in_executor(None, cursor.close)\n+\n+                        result = {\n+                            'finding_id': fid,\n+                            'host_id': state.get('host_id', 'unknown'),\n+                            'scanner': scanner,\n+                            'composite_hash': composite,\n+                            'db_path': db_path\n+                        }\n+\n+                        if row:\n+                            first_seen, count = row\n+                            result.update({\n+                                'status': 'existing',\n+                                'first_seen_ts': first_seen,\n+                                'prev_seen_count': count,\n+                                'baseline_status': 'existing'\n+                            })\n+                        else:\n+                            result.update({\n+                                'status': 'new',\n+                                'baseline_status': 'new'\n+                            })\n+\n+                        results.append(result)\n+\n+                    except Exception as e:\n+                        logger.warning(f\"Baseline query failed for finding {finding.get('id')}: {e}\")\n+                        results.append({\n+                            'finding_id': finding.get('id', 'unknown'),\n+                            'status': 'error',\n+                            'error': str(e)\n+                        })\n+\n+            return results\n+\n+        # Process all findings in batches\n+        baseline_results = await batch_process_findings(enriched, process_baseline_batch)\n+\n+        # Update findings with baseline status\n+        for result in baseline_results:\n+            fid = result.get('finding_id')\n+            status = result.get('baseline_status')\n+            if fid and status:\n+                for finding in enriched:\n+                    if finding.get('id') == fid:\n+                        finding['baseline_status'] = status\n+                        break\n+\n+        state['baseline_results'] = {r.get('finding_id'): r for r in baseline_results if r.get('finding_id')}\n+        state['enriched_findings'] = enriched\n+\n+        # Update metrics\n+        state.setdefault('metrics', {})['baseline_query_duration'] = time.time() - start_time\n+        state.setdefault('metrics', {})['baseline_queries_made'] = len(baseline_results)\n+\n+    except Exception as e:\n+        logger.error(f\"Batch baseline query failed: {e}\")\n+        _append_warning(state, 'graph', 'baseline_batch', str(e))\n+\n+    return state\n+\n+async def parallel_node_execution(state: GraphState, nodes: List[Callable[[GraphState], Any]]) -> GraphState:\n+    \"\"\"Execute multiple nodes in parallel.\"\"\"\n+    start_time = time.time()\n+\n+    try:\n+        # Create tasks for parallel execution\n+        tasks = []\n+        for node_func in nodes:\n+            task = asyncio.create_task(node_func(state.copy()))\n+            tasks.append(task)\n+\n+        # Wait for all tasks to complete\n+        results = await asyncio.gather(*tasks, return_exceptions=True)\n+\n+        # Merge results back into state\n+        for result in results:\n+            if isinstance(result, Exception):\n+                logger.error(f\"Parallel node execution failed: {result}\")\n+                _append_warning(state, 'graph', 'parallel_execution', str(result))\n+            elif isinstance(result, dict):\n+                # Merge result into main state\n+                state.update(result)\n+\n+        # Update metrics\n+        state.setdefault('metrics', {})['parallel_execution_duration'] = time.time() - start_time\n+        state.setdefault('metrics', {})['parallel_tasks_executed'] = len(tasks)\n+\n+    except Exception as e:\n+        logger.error(f\"Parallel execution failed: {e}\")\n+        _append_warning(state, 'graph', 'parallel_execution', str(e))\n+\n+    return state\n+\n+# Circuit breaker for external service calls\n+class CircuitBreaker:\n+    \"\"\"Circuit breaker pattern implementation.\"\"\"\n+\n+    def __init__(self, failure_threshold: int = 5, recovery_timeout: int = 60):\n+        self.failure_threshold = failure_threshold\n+        self.recovery_timeout = recovery_timeout\n+        self.failure_count = 0\n+        self.last_failure_time = 0\n+        self.state = 'closed'  # closed, open, half-open\n+\n+    async def call(self, func, *args, **kwargs):\n+        \"\"\"Execute function with circuit breaker protection.\"\"\"\n+        if self.state == 'open':\n+            if time.time() - self.last_failure_time > self.recovery_timeout:\n+                self.state = 'half-open'\n+            else:\n+                raise Exception(\"Circuit breaker is open\")\n+\n+        try:\n+            result = await func(*args, **kwargs)\n+            if self.state == 'half-open':\n+                self.state = 'closed'\n+                self.failure_count = 0\n+            return result\n+        except Exception as e:\n+            self.failure_count += 1\n+            self.last_failure_time = time.time()\n+            if self.failure_count >= self.failure_threshold:\n+                self.state = 'open'\n+            raise e\n+\n+# Global circuit breaker instances\n+llm_circuit_breaker = CircuitBreaker()\n+db_circuit_breaker = CircuitBreaker()\n+\n+async def circuit_breaker_protected_llm_call(state: GraphState, llm_func, *args, **kwargs):\n+    \"\"\"Execute LLM call with circuit breaker protection.\"\"\"\n+    try:\n+        return await llm_circuit_breaker.call(llm_func, *args, **kwargs)\n+    except Exception as e:\n+        logger.warning(f\"LLM circuit breaker triggered: {e}\")\n+        _append_warning(state, 'graph', 'llm_circuit_breaker', str(e))\n+        # Return fallback response\n+        return {\"summary\": \"LLM service temporarily unavailable\", \"fallback\": True}\n+\n+async def circuit_breaker_protected_db_call(state: GraphState, db_func, *args, **kwargs):\n+    \"\"\"Execute database call with circuit breaker protection.\"\"\"\n+    try:\n+        return await db_circuit_breaker.call(db_func, *args, **kwargs)\n+    except Exception as e:\n+        logger.warning(f\"Database circuit breaker triggered: {e}\")\n+        _append_warning(state, 'graph', 'db_circuit_breaker', str(e))\n+        # Return empty results\n+        return {}\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -0,0 +1,642 @@\n",
        "+from __future__ import annotations\n",
        "+\"\"\"Performance-optimized graph node functions with advanced async capabilities.\n",
        "+\n",
        "+This module provides high-performance node implementations with:\n",
        "+- Connection pooling and batch database operations\n",
        "+- Streaming processing for large datasets\n",
        "+- Parallel execution patterns\n",
        "+- Memory-efficient data structures\n",
        "+- Advanced caching strategies\n",
        "+- Circuit breaker patterns\n",
        "+\"\"\"\n",
        "+\n",
        "+from typing import Any, List, Dict, Optional, Union, AsyncIterator, Callable\n",
        "+import asyncio\n",
        "+import time\n",
        "+import logging\n",
        "+import json\n",
        "+from pathlib import Path\n",
        "+from datetime import datetime\n",
        "+from contextlib import asynccontextmanager\n",
        "+from dataclasses import dataclass, field\n",
        "+import hashlib\n",
        "+import tempfile\n",
        "+from concurrent.futures import ThreadPoolExecutor\n",
        "+\n",
        "+# Use standard sqlite3 for now, can upgrade to aiosqlite later\n",
        "+import sqlite3\n",
        "+\n",
        "+from .graph import GraphState\n",
        "+from .data_governance import get_data_governor\n",
        "+from .models import Finding, ScannerResult, Report, Meta, Summary, SummaryExtension, AgentState, Reductions\n",
        "+from .knowledge import apply_external_knowledge\n",
        "+from .pipeline import augment as _augment\n",
        "+from .reduction import reduce_all\n",
        "+from .llm_provider import get_llm_provider\n",
        "+from .rules import Correlator, DEFAULT_RULES\n",
        "+from .rule_gap_miner import mine_gap_candidates\n",
        "+\n",
        "+logger = logging.getLogger(__name__)\n",
        "+\n",
        "+# Performance configuration\n",
        "+@dataclass\n",
        "+class PerformanceConfig:\n",
        "+    \"\"\"Configuration for performance optimizations.\"\"\"\n",
        "+    batch_size: int = 100\n",
        "+    max_concurrent_db_connections: int = 10\n",
        "+    cache_ttl_seconds: int = 3600\n",
        "+    streaming_chunk_size: int = 50\n",
        "+    max_memory_mb: int = 512\n",
        "+    thread_pool_workers: int = 4\n",
        "+\n",
        "+# Global performance config\n",
        "+perf_config = PerformanceConfig()\n",
        "+\n",
        "+# Connection pool for database operations (using regular sqlite3 for now)\n",
        "+class DatabaseConnectionPool:\n",
        "+    \"\"\"Async-compatible connection pool for SQLite operations.\"\"\"\n",
        "+\n",
        "+    def __init__(self, db_path: str, max_connections: int = 10):\n",
        "+        self.db_path = db_path\n",
        "+        self.max_connections = max_connections\n",
        "+        self._pool: asyncio.Queue[sqlite3.Connection] = asyncio.Queue()\n",
        "+        self._semaphore = asyncio.Semaphore(max_connections)\n",
        "+\n",
        "+    async def initialize(self):\n",
        "+        \"\"\"Initialize the connection pool.\"\"\"\n",
        "+        for _ in range(self.max_connections):\n",
        "+            # Use ThreadPoolExecutor for sqlite3 operations\n",
        "+            loop = asyncio.get_event_loop()\n",
        "+            conn = await loop.run_in_executor(None, sqlite3.connect, self.db_path)\n",
        "+            await self._pool.put(conn)\n",
        "+\n",
        "+    async def get_connection(self) -> sqlite3.Connection:\n",
        "+        \"\"\"Get a connection from the pool.\"\"\"\n",
        "+        await self._semaphore.acquire()\n",
        "+        conn = await self._pool.get()\n",
        "+        return conn\n",
        "+\n",
        "+    async def return_connection(self, conn: sqlite3.Connection):\n",
        "+        \"\"\"Return a connection to the pool.\"\"\"\n",
        "+        await self._pool.put(conn)\n",
        "+        self._semaphore.release()\n",
        "+\n",
        "+    async def execute_query(self, query: str, params: tuple = ()) -> List[tuple]:\n",
        "+        \"\"\"Execute a query using a pooled connection.\"\"\"\n",
        "+        conn = await self.get_connection()\n",
        "+        try:\n",
        "+            loop = asyncio.get_event_loop()\n",
        "+            cursor = await loop.run_in_executor(None, conn.cursor)\n",
        "+            await loop.run_in_executor(None, cursor.execute, query, params)\n",
        "+            results = await loop.run_in_executor(None, cursor.fetchall)\n",
        "+            await loop.run_in_executor(None, cursor.close)\n",
        "+            return results\n",
        "+        finally:\n",
        "+            await self.return_connection(conn)\n",
        "+\n",
        "+    async def close_all(self):\n",
        "+        \"\"\"Close all connections in the pool.\"\"\"\n",
        "+        while not self._pool.empty():\n",
        "+            conn = await self._pool.get()\n",
        "+            loop = asyncio.get_event_loop()\n",
        "+            await loop.run_in_executor(None, conn.close)\n",
        "+\n",
        "+# Global connection pool\n",
        "+db_pool: Optional[DatabaseConnectionPool] = None\n",
        "+\n",
        "+@asynccontextmanager\n",
        "+async def get_db_connection(db_path: str):\n",
        "+    \"\"\"Context manager for database connections.\"\"\"\n",
        "+    global db_pool\n",
        "+    if db_pool is None:\n",
        "+        db_pool = DatabaseConnectionPool(db_path)\n",
        "+        await db_pool.initialize()\n",
        "+\n",
        "+    conn = await db_pool.get_connection()\n",
        "+    try:\n",
        "+        yield conn\n",
        "+    finally:\n",
        "+        await db_pool.return_connection(conn)\n",
        "+\n",
        "+# Memory-efficient data structures\n",
        "+@dataclass\n",
        "+class FindingBatch:\n",
        "+    \"\"\"Memory-efficient batch of findings.\"\"\"\n",
        "+    findings: List[Finding] = field(default_factory=list)\n",
        "+    batch_id: str = \"\"\n",
        "+    metadata: Dict[str, Any] = field(default_factory=dict)\n",
        "+\n",
        "+    def add_finding(self, finding: Finding):\n",
        "+        \"\"\"Add a finding to the batch.\"\"\"\n",
        "+        self.findings.append(finding)\n",
        "+\n",
        "+    def is_full(self) -> bool:\n",
        "+        \"\"\"Check if batch is at capacity.\"\"\"\n",
        "+        return len(self.findings) >= perf_config.batch_size\n",
        "+\n",
        "+    def clear(self):\n",
        "+        \"\"\"Clear the batch.\"\"\"\n",
        "+        self.findings.clear()\n",
        "+        self.metadata.clear()\n",
        "+\n",
        "+# Advanced caching with TTL\n",
        "+class AdvancedCache:\n",
        "+    \"\"\"Advanced caching with TTL and size limits.\"\"\"\n",
        "+\n",
        "+    def __init__(self, max_size: int = 1000, ttl_seconds: int = 3600):\n",
        "+        self.max_size = max_size\n",
        "+        self.ttl_seconds = ttl_seconds\n",
        "+        self._cache: Dict[str, Dict[str, Any]] = {}\n",
        "+        self._access_times: Dict[str, float] = {}\n",
        "+\n",
        "+    def get(self, key: str) -> Optional[Any]:\n",
        "+        \"\"\"Get cached value if not expired.\"\"\"\n",
        "+        if key in self._cache:\n",
        "+            if time.time() - self._access_times[key] > self.ttl_seconds:\n",
        "+                del self._cache[key]\n",
        "+                del self._access_times[key]\n",
        "+                return None\n",
        "+            self._access_times[key] = time.time()\n",
        "+            return self._cache[key]\n",
        "+        return None\n",
        "+\n",
        "+    def set(self, key: str, value: Any):\n",
        "+        \"\"\"Set cached value with eviction if needed.\"\"\"\n",
        "+        if len(self._cache) >= self.max_size:\n",
        "+            # Evict least recently used\n",
        "+            oldest_key = min(self._access_times.keys(), key=lambda k: self._access_times[k])\n",
        "+            del self._cache[oldest_key]\n",
        "+            del self._access_times[oldest_key]\n",
        "+\n",
        "+        self._cache[key] = value\n",
        "+        self._access_times[key] = time.time()\n",
        "+\n",
        "+    def clear_expired(self):\n",
        "+        \"\"\"Clear expired entries.\"\"\"\n",
        "+        current_time = time.time()\n",
        "+        expired_keys = [\n",
        "+            key for key, access_time in self._access_times.items()\n",
        "+            if current_time - access_time > self.ttl_seconds\n",
        "+        ]\n",
        "+        for key in expired_keys:\n",
        "+            del self._cache[key]\n",
        "+            del self._access_times[key]\n",
        "+\n",
        "+# Global cache instance\n",
        "+advanced_cache = AdvancedCache()\n",
        "+\n",
        "+def _append_warning(state: GraphState, module: str, stage: str, error: str, hint: str | None = None):\n",
        "+    \"\"\"Enhanced warning appender with performance tracking.\"\"\"\n",
        "+    wl = state.setdefault('warnings', [])\n",
        "+    error_entry = {\n",
        "+        'module': module,\n",
        "+        'stage': stage,\n",
        "+        'error': error,\n",
        "+        'hint': hint,\n",
        "+        'timestamp': datetime.now().isoformat(),\n",
        "+        'session_id': state.get('session_id', 'unknown'),\n",
        "+        'performance_impact': 'low'  # Can be upgraded based on error type\n",
        "+    }\n",
        "+    wl.append(error_entry)\n",
        "+\n",
        "+    # Track in errors list for better visibility\n",
        "+    errors = state.setdefault('errors', [])\n",
        "+    errors.append(error_entry)\n",
        "+\n",
        "+def _findings_from_graph(state: GraphState) -> List[Finding]:\n",
        "+    \"\"\"Convert graph state findings to Pydantic models with batching.\"\"\"\n",
        "+    out: List[Finding] = []\n",
        "+    raw_findings = state.get('raw_findings', []) or []\n",
        "+\n",
        "+    # Process in batches to avoid memory spikes\n",
        "+    for i in range(0, len(raw_findings), perf_config.batch_size):\n",
        "+        batch = raw_findings[i:i + perf_config.batch_size]\n",
        "+        for finding in batch:\n",
        "+            try:\n",
        "+                out.append(Finding(\n",
        "+                    id=finding.get('id','unknown'),\n",
        "+                    title=finding.get('title','(no title)'),\n",
        "+                    severity=finding.get('severity','info'),\n",
        "+                    risk_score=int(finding.get('risk_score', finding.get('risk_total', 0)) or 0),\n",
        "+                    metadata=finding.get('metadata', {})\n",
        "+                ))\n",
        "+            except Exception:\n",
        "+                continue\n",
        "+\n",
        "+    return out\n",
        "+\n",
        "+async def batch_process_findings(findings: List[Any], processor_func: Callable[[List[Any]], Any], batch_size: Optional[int] = None) -> List[Any]:\n",
        "+    \"\"\"Process findings in batches for memory efficiency.\"\"\"\n",
        "+    if batch_size is None:\n",
        "+        batch_size = perf_config.batch_size\n",
        "+\n",
        "+    results = []\n",
        "+    for i in range(0, len(findings), batch_size):\n",
        "+        batch = findings[i:i + batch_size]\n",
        "+        batch_results = await processor_func(batch)\n",
        "+        results.extend(batch_results)\n",
        "+        # Allow other tasks to run\n",
        "+        await asyncio.sleep(0)\n",
        "+    return results\n",
        "+\n",
        "+async def enrich_findings_batch(state: GraphState) -> GraphState:\n",
        "+    \"\"\"Batch-optimized enrichment with streaming and caching.\"\"\"\n",
        "+    start_time = time.time()\n",
        "+    state['current_stage'] = 'enrich'\n",
        "+    state['start_time'] = state.get('start_time', datetime.now().isoformat())\n",
        "+\n",
        "+    try:\n",
        "+        # Check cache first\n",
        "+        cache_key = f\"enrich_batch_{hash(str(state.get('raw_findings', [])))}\"\n",
        "+        cached_result = advanced_cache.get(cache_key)\n",
        "+        if cached_result:\n",
        "+            logger.info(\"Using cached enrichment results\")\n",
        "+            state.update(cached_result)\n",
        "+            state['cache_hits'] = state.get('cache_hits', []) + [cache_key]\n",
        "+            return state\n",
        "+\n",
        "+        findings = _findings_from_graph(state)\n",
        "+\n",
        "+        # Process in batches\n",
        "+        async def process_batch(batch: List[Finding]) -> List[Dict[str, Any]]:\n",
        "+            \"\"\"Process a batch of findings.\"\"\"\n",
        "+            batch_results = []\n",
        "+            for finding in batch:\n",
        "+                try:\n",
        "+                    # Create minimal report for this finding\n",
        "+                    sr = ScannerResult(\n",
        "+                        scanner='mixed',\n",
        "+                        finding_count=1,\n",
        "+                        findings=[finding]\n",
        "+                    )\n",
        "+                    report = Report(\n",
        "+                        meta=Meta(),\n",
        "+                        summary=Summary(finding_count_total=1, finding_count_emitted=1),\n",
        "+                        results=[sr],\n",
        "+                        collection_warnings=[],\n",
        "+                        scanner_errors=[],\n",
        "+                        summary_extension=SummaryExtension(total_risk_score=finding.risk_score or 0)\n",
        "+                    )\n",
        "+                    astate = AgentState(report=report)\n",
        "+\n",
        "+                    # Apply enrichment pipeline\n",
        "+                    astate = _augment(astate)\n",
        "+                    astate = apply_external_knowledge(astate)\n",
        "+\n",
        "+                    # Extract enriched finding\n",
        "+                    if astate.report and astate.report.results:\n",
        "+                        for r in astate.report.results:\n",
        "+                            for f in r.findings:\n",
        "+                                batch_results.append(f.model_dump())\n",
        "+\n",
        "+                except Exception as e:\n",
        "+                    logger.warning(f\"Failed to enrich finding {finding.id}: {e}\")\n",
        "+                    continue\n",
        "+\n",
        "+            return batch_results\n",
        "+\n",
        "+        # Process all findings in batches\n",
        "+        enriched_findings = await batch_process_findings(findings, process_batch)\n",
        "+\n",
        "+        state['enriched_findings'] = enriched_findings\n",
        "+\n",
        "+        # Cache the results\n",
        "+        cache_data = {\n",
        "+            'enriched_findings': enriched_findings,\n",
        "+            'cache_timestamp': datetime.now().isoformat()\n",
        "+        }\n",
        "+        advanced_cache.set(cache_key, cache_data)\n",
        "+        state['cache_keys'] = state.get('cache_keys', []) + [cache_key]\n",
        "+\n",
        "+        # Update metrics\n",
        "+        state.setdefault('metrics', {})['enrich_duration'] = time.time() - start_time\n",
        "+        state.setdefault('metrics', {})['findings_processed'] = len(enriched_findings)\n",
        "+\n",
        "+    except Exception as e:\n",
        "+        logger.error(f\"Batch enrichment failed: {e}\")\n",
        "+        _append_warning(state, 'graph', 'enrich_batch', str(e))\n",
        "+        state.setdefault('enriched_findings', state.get('raw_findings', []))\n",
        "+\n",
        "+    return state\n",
        "+\n",
        "+async def correlate_findings_batch(state: GraphState) -> GraphState:\n",
        "+    \"\"\"Batch-optimized correlation with parallel processing.\"\"\"\n",
        "+    start_time = time.time()\n",
        "+    state['current_stage'] = 'correlate'\n",
        "+\n",
        "+    try:\n",
        "+        findings: List[Finding] = []\n",
        "+        enriched_findings = state.get('enriched_findings', []) or []\n",
        "+\n",
        "+        # Convert to Pydantic models in batches\n",
        "+        for i in range(0, len(enriched_findings), perf_config.batch_size):\n",
        "+            batch = enriched_findings[i:i + perf_config.batch_size]\n",
        "+            for finding_dict in batch:\n",
        "+                try:\n",
        "+                    findings.append(Finding(**{k: v for k, v in finding_dict.items() if k in Finding.model_fields}))\n",
        "+                except Exception:\n",
        "+                    continue\n",
        "+\n",
        "+        if not findings:\n",
        "+            return state\n",
        "+\n",
        "+        # Create report for correlation\n",
        "+        sr = ScannerResult(scanner='mixed', finding_count=len(findings), findings=findings)\n",
        "+        report = Report(\n",
        "+            meta=Meta(),\n",
        "+            summary=Summary(finding_count_total=len(findings), finding_count_emitted=len(findings)),\n",
        "+            results=[sr],\n",
        "+            collection_warnings=[],\n",
        "+            scanner_errors=[],\n",
        "+            summary_extension=SummaryExtension(total_risk_score=sum(f.risk_score or 0 for f in findings))\n",
        "+        )\n",
        "+        astate = AgentState(report=report)\n",
        "+\n",
        "+        # Apply correlation rules\n",
        "+        correlator = Correlator(DEFAULT_RULES)\n",
        "+        correlations = correlator.apply(findings)\n",
        "+\n",
        "+        # Attach correlation refs to findings in parallel\n",
        "+        async def attach_correlations():\n",
        "+            \"\"\"Attach correlation references to findings.\"\"\"\n",
        "+            tasks = []\n",
        "+            for correlation in correlations:\n",
        "+                task = asyncio.create_task(attach_correlation_refs(findings, correlation))\n",
        "+                tasks.append(task)\n",
        "+            await asyncio.gather(*tasks)\n",
        "+\n",
        "+        async def attach_correlation_refs(findings: List[Finding], correlation):\n",
        "+            \"\"\"Attach correlation refs to relevant findings.\"\"\"\n",
        "+            for finding in findings:\n",
        "+                if finding.id in correlation.related_finding_ids and correlation.id not in finding.correlation_refs:\n",
        "+                    finding.correlation_refs.append(correlation.id)\n",
        "+\n",
        "+        await attach_correlations()\n",
        "+\n",
        "+        state['correlated_findings'] = [finding.model_dump() for finding in findings]\n",
        "+        state['correlations'] = [c.model_dump() for c in correlations]\n",
        "+\n",
        "+        # Update metrics\n",
        "+        state.setdefault('metrics', {})['correlate_duration'] = time.time() - start_time\n",
        "+        state.setdefault('metrics', {})['correlations_found'] = len(correlations)\n",
        "+\n",
        "+    except Exception as e:\n",
        "+        logger.error(f\"Batch correlation failed: {e}\")\n",
        "+        _append_warning(state, 'graph', 'correlate_batch', str(e))\n",
        "+        if 'correlated_findings' not in state:\n",
        "+            state['correlated_findings'] = state.get('enriched_findings', [])\n",
        "+\n",
        "+    return state\n",
        "+\n",
        "+async def summarize_host_state_streaming(state: GraphState) -> GraphState:\n",
        "+    \"\"\"Streaming summarization with real-time updates.\"\"\"\n",
        "+    start_time = time.time()\n",
        "+    state['current_stage'] = 'summarize'\n",
        "+\n",
        "+    try:\n",
        "+        # Iteration guard\n",
        "+        max_iter = int(__import__('os').environ.get('AGENT_MAX_SUMMARY_ITERS', '3'))\n",
        "+        iters = int(state.get('iteration_count', 0))\n",
        "+        if iters >= max_iter:\n",
        "+            _append_warning(state, 'graph', 'summarize', 'iteration_limit_reached')\n",
        "+            return state\n",
        "+\n",
        "+        provider = get_llm_provider()\n",
        "+        findings_dicts = state.get('correlated_findings') or state.get('enriched_findings') or []\n",
        "+\n",
        "+        # Convert findings in streaming fashion\n",
        "+        findings: List[Finding] = []\n",
        "+        for finding_dict in findings_dicts:\n",
        "+            try:\n",
        "+                findings.append(Finding(**{k: v for k, v in finding_dict.items() if k in Finding.model_fields}))\n",
        "+            except Exception:\n",
        "+                continue\n",
        "+\n",
        "+        # Process in chunks for memory efficiency\n",
        "+        reductions = []\n",
        "+        for i in range(0, len(findings), perf_config.streaming_chunk_size):\n",
        "+            chunk = findings[i:i + perf_config.streaming_chunk_size]\n",
        "+            chunk_reductions = reduce_all(chunk)\n",
        "+            reductions.extend(chunk_reductions)\n",
        "+\n",
        "+            # Yield progress updates\n",
        "+            progress = (i + len(chunk)) / len(findings)\n",
        "+            state['summarize_progress'] = progress\n",
        "+            await asyncio.sleep(0)  # Allow other tasks to run\n",
        "+\n",
        "+        # Enhanced correlation handling\n",
        "+        corr_objs = []\n",
        "+        correlations = state.get('correlations', []) or []\n",
        "+        for c in correlations:\n",
        "+            try:\n",
        "+                from .models import Correlation as _C\n",
        "+                corr_objs.append(_C(**c))\n",
        "+            except Exception:\n",
        "+                continue\n",
        "+\n",
        "+        baseline_context = state.get('baseline_results') or {}\n",
        "+\n",
        "+        # Generate summary\n",
        "+        summaries = provider.summarize(reductions, corr_objs, actions=[], baseline_context=baseline_context)\n",
        "+        state['summary'] = summaries.model_dump()\n",
        "+        state['iteration_count'] = iters + 1\n",
        "+\n",
        "+        # Update metrics\n",
        "+        state.setdefault('metrics', {})['summarize_duration'] = time.time() - start_time\n",
        "+        state.setdefault('metrics', {})['chunks_processed'] = len(range(0, len(findings), perf_config.streaming_chunk_size))\n",
        "+\n",
        "+    except Exception as e:\n",
        "+        logger.error(f\"Streaming summarization failed: {e}\")\n",
        "+        _append_warning(state, 'graph', 'summarize_streaming', str(e))\n",
        "+\n",
        "+    return state\n",
        "+\n",
        "+async def query_baseline_batch(state: GraphState) -> GraphState:\n",
        "+    \"\"\"Batch baseline queries with connection pooling.\"\"\"\n",
        "+    start_time = time.time()\n",
        "+    state['current_stage'] = 'baseline_query'\n",
        "+\n",
        "+    try:\n",
        "+        enriched = state.get('enriched_findings', []) or []\n",
        "+        if not enriched:\n",
        "+            return state\n",
        "+\n",
        "+        db_path = __import__('os').environ.get('AGENT_BASELINE_DB', 'agent_baseline.db')\n",
        "+\n",
        "+        # Process baseline queries in batches\n",
        "+        async def process_baseline_batch(batch: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "+            \"\"\"Process a batch of baseline queries.\"\"\"\n",
        "+            results = []\n",
        "+            async with get_db_connection(db_path) as conn:\n",
        "+                for finding in batch:\n",
        "+                    try:\n",
        "+                        fid = finding.get('id') or 'unknown'\n",
        "+                        title = finding.get('title') or ''\n",
        "+                        severity = finding.get('severity') or ''\n",
        "+                        scanner = finding.get('scanner') or 'mixed'\n",
        "+\n",
        "+                        # Compute composite hash\n",
        "+                        identity_core = f\"{fid}\\n{title}\\n{severity}\\n\".encode()\n",
        "+                        h = hashlib.sha256(identity_core).hexdigest()\n",
        "+\n",
        "+                        from .baseline import hashlib_sha\n",
        "+                        composite = hashlib_sha(scanner, h)\n",
        "+\n",
        "+                        # Query database\n",
        "+                        loop = asyncio.get_event_loop()\n",
        "+                        cursor = await loop.run_in_executor(None, conn.cursor)\n",
        "+                        await loop.run_in_executor(None, cursor.execute,\n",
        "+                            \"SELECT first_seen_ts, seen_count FROM baseline_finding WHERE host_id=? AND finding_hash=?\",\n",
        "+                            (state.get('host_id', 'unknown'), composite)\n",
        "+                        )\n",
        "+                        row = await loop.run_in_executor(None, cursor.fetchone)\n",
        "+                        await loop.run_in_executor(None, cursor.close)\n",
        "+\n",
        "+                        result = {\n",
        "+                            'finding_id': fid,\n",
        "+                            'host_id': state.get('host_id', 'unknown'),\n",
        "+                            'scanner': scanner,\n",
        "+                            'composite_hash': composite,\n",
        "+                            'db_path': db_path\n",
        "+                        }\n",
        "+\n",
        "+                        if row:\n",
        "+                            first_seen, count = row\n",
        "+                            result.update({\n",
        "+                                'status': 'existing',\n",
        "+                                'first_seen_ts': first_seen,\n",
        "+                                'prev_seen_count': count,\n",
        "+                                'baseline_status': 'existing'\n",
        "+                            })\n",
        "+                        else:\n",
        "+                            result.update({\n",
        "+                                'status': 'new',\n",
        "+                                'baseline_status': 'new'\n",
        "+                            })\n",
        "+\n",
        "+                        results.append(result)\n",
        "+\n",
        "+                    except Exception as e:\n",
        "+                        logger.warning(f\"Baseline query failed for finding {finding.get('id')}: {e}\")\n",
        "+                        results.append({\n",
        "+                            'finding_id': finding.get('id', 'unknown'),\n",
        "+                            'status': 'error',\n",
        "+                            'error': str(e)\n",
        "+                        })\n",
        "+\n",
        "+            return results\n",
        "+\n",
        "+        # Process all findings in batches\n",
        "+        baseline_results = await batch_process_findings(enriched, process_baseline_batch)\n",
        "+\n",
        "+        # Update findings with baseline status\n",
        "+        for result in baseline_results:\n",
        "+            fid = result.get('finding_id')\n",
        "+            status = result.get('baseline_status')\n",
        "+            if fid and status:\n",
        "+                for finding in enriched:\n",
        "+                    if finding.get('id') == fid:\n",
        "+                        finding['baseline_status'] = status\n",
        "+                        break\n",
        "+\n",
        "+        state['baseline_results'] = {r.get('finding_id'): r for r in baseline_results if r.get('finding_id')}\n",
        "+        state['enriched_findings'] = enriched\n",
        "+\n",
        "+        # Update metrics\n",
        "+        state.setdefault('metrics', {})['baseline_query_duration'] = time.time() - start_time\n",
        "+        state.setdefault('metrics', {})['baseline_queries_made'] = len(baseline_results)\n",
        "+\n",
        "+    except Exception as e:\n",
        "+        logger.error(f\"Batch baseline query failed: {e}\")\n",
        "+        _append_warning(state, 'graph', 'baseline_batch', str(e))\n",
        "+\n",
        "+    return state\n",
        "+\n",
        "+async def parallel_node_execution(state: GraphState, nodes: List[Callable[[GraphState], Any]]) -> GraphState:\n",
        "+    \"\"\"Execute multiple nodes in parallel.\"\"\"\n",
        "+    start_time = time.time()\n",
        "+\n",
        "+    try:\n",
        "+        # Create tasks for parallel execution\n",
        "+        tasks = []\n",
        "+        for node_func in nodes:\n",
        "+            task = asyncio.create_task(node_func(state.copy()))\n",
        "+            tasks.append(task)\n",
        "+\n",
        "+        # Wait for all tasks to complete\n",
        "+        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "+\n",
        "+        # Merge results back into state\n",
        "+        for result in results:\n",
        "+            if isinstance(result, Exception):\n",
        "+                logger.error(f\"Parallel node execution failed: {result}\")\n",
        "+                _append_warning(state, 'graph', 'parallel_execution', str(result))\n",
        "+            elif isinstance(result, dict):\n",
        "+                # Merge result into main state\n",
        "+                state.update(result)\n",
        "+\n",
        "+        # Update metrics\n",
        "+        state.setdefault('metrics', {})['parallel_execution_duration'] = time.time() - start_time\n",
        "+        state.setdefault('metrics', {})['parallel_tasks_executed'] = len(tasks)\n",
        "+\n",
        "+    except Exception as e:\n",
        "+        logger.error(f\"Parallel execution failed: {e}\")\n",
        "+        _append_warning(state, 'graph', 'parallel_execution', str(e))\n",
        "+\n",
        "+    return state\n",
        "+\n",
        "+# Circuit breaker for external service calls\n",
        "+class CircuitBreaker:\n",
        "+    \"\"\"Circuit breaker pattern implementation.\"\"\"\n",
        "+\n",
        "+    def __init__(self, failure_threshold: int = 5, recovery_timeout: int = 60):\n",
        "+        self.failure_threshold = failure_threshold\n",
        "+        self.recovery_timeout = recovery_timeout\n",
        "+        self.failure_count = 0\n",
        "+        self.last_failure_time = 0\n",
        "+        self.state = 'closed'  # closed, open, half-open\n",
        "+\n",
        "+    async def call(self, func, *args, **kwargs):\n",
        "+        \"\"\"Execute function with circuit breaker protection.\"\"\"\n",
        "+        if self.state == 'open':\n",
        "+            if time.time() - self.last_failure_time > self.recovery_timeout:\n",
        "+                self.state = 'half-open'\n",
        "+            else:\n",
        "+                raise Exception(\"Circuit breaker is open\")\n",
        "+\n",
        "+        try:\n",
        "+            result = await func(*args, **kwargs)\n",
        "+            if self.state == 'half-open':\n",
        "+                self.state = 'closed'\n",
        "+                self.failure_count = 0\n",
        "+            return result\n",
        "+        except Exception as e:\n",
        "+            self.failure_count += 1\n",
        "+            self.last_failure_time = time.time()\n",
        "+            if self.failure_count >= self.failure_threshold:\n",
        "+                self.state = 'open'\n",
        "+            raise e\n",
        "+\n",
        "+# Global circuit breaker instances\n",
        "+llm_circuit_breaker = CircuitBreaker()\n",
        "+db_circuit_breaker = CircuitBreaker()\n",
        "+\n",
        "+async def circuit_breaker_protected_llm_call(state: GraphState, llm_func, *args, **kwargs):\n",
        "+    \"\"\"Execute LLM call with circuit breaker protection.\"\"\"\n",
        "+    try:\n",
        "+        return await llm_circuit_breaker.call(llm_func, *args, **kwargs)\n",
        "+    except Exception as e:\n",
        "+        logger.warning(f\"LLM circuit breaker triggered: {e}\")\n",
        "+        _append_warning(state, 'graph', 'llm_circuit_breaker', str(e))\n",
        "+        # Return fallback response\n",
        "+        return {\"summary\": \"LLM service temporarily unavailable\", \"fallback\": True}\n",
        "+\n",
        "+async def circuit_breaker_protected_db_call(state: GraphState, db_func, *args, **kwargs):\n",
        "+    \"\"\"Execute database call with circuit breaker protection.\"\"\"\n",
        "+    try:\n",
        "+        return await db_circuit_breaker.call(db_func, *args, **kwargs)\n",
        "+    except Exception as e:\n",
        "+        logger.warning(f\"Database circuit breaker triggered: {e}\")\n",
        "+        _append_warning(state, 'graph', 'db_circuit_breaker', str(e))\n",
        "+        # Return empty results\n",
        "+        return {}\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "agent/graph_nodes_reliability.py",
      "status": "added",
      "additions": 607,
      "deletions": 0,
      "patch": "@@ -0,0 +1,607 @@\n+from __future__ import annotations\n+\"\"\"Reliability enhancements for LangGraph processing.\n+\n+This module provides circuit breaker patterns, comprehensive error handling,\n+and resilience mechanisms to ensure robust operation.\n+\"\"\"\n+\n+from typing import Any, List, Dict, Optional, Callable, AsyncIterator\n+import asyncio\n+import logging\n+import time\n+from dataclasses import dataclass, field\n+from datetime import datetime, timedelta\n+from contextlib import asynccontextmanager\n+from enum import Enum\n+import traceback\n+import sys\n+\n+from .graph import GraphState\n+\n+logger = logging.getLogger(__name__)\n+\n+class CircuitBreakerState(Enum):\n+    \"\"\"States for circuit breaker pattern.\"\"\"\n+    CLOSED = \"closed\"\n+    OPEN = \"open\"\n+    HALF_OPEN = \"half-open\"\n+\n+@dataclass\n+class CircuitBreakerConfig:\n+    \"\"\"Configuration for circuit breaker.\"\"\"\n+    failure_threshold: int = 5\n+    recovery_timeout: int = 60\n+    expected_exception: tuple = (Exception,)\n+    success_threshold: int = 3\n+    timeout: float = 10.0\n+\n+@dataclass\n+class CircuitBreaker:\n+    \"\"\"Circuit breaker implementation for fault tolerance.\"\"\"\n+\n+    config: CircuitBreakerConfig\n+    _state: CircuitBreakerState = CircuitBreakerState.CLOSED\n+    _failure_count: int = 0\n+    _last_failure_time: Optional[float] = None\n+    _success_count: int = 0\n+\n+    def __post_init__(self):\n+        self._state = CircuitBreakerState.CLOSED\n+        self._failure_count = 0\n+        self._last_failure_time = None\n+        self._success_count = 0\n+\n+    async def call(self, func: Callable, *args, **kwargs) -> Any:\n+        \"\"\"Execute function with circuit breaker protection.\"\"\"\n+        if self._state == CircuitBreakerState.OPEN:\n+            if self._should_attempt_reset():\n+                self._state = CircuitBreakerState.HALF_OPEN\n+            else:\n+                raise CircuitBreakerOpenException(\"Circuit breaker is OPEN\")\n+\n+        try:\n+            if asyncio.iscoroutinefunction(func):\n+                result = await asyncio.wait_for(func(*args, **kwargs), timeout=self.config.timeout)\n+            else:\n+                result = await asyncio.get_event_loop().run_in_executor(\n+                    None, lambda: func(*args, **kwargs)\n+                )\n+\n+            self._on_success()\n+            return result\n+\n+        except self.config.expected_exception as e:\n+            self._on_failure()\n+            raise e\n+        except asyncio.TimeoutError:\n+            self._on_failure()\n+            raise CircuitBreakerOpenException(\"Operation timed out\")\n+\n+    def _should_attempt_reset(self) -> bool:\n+        \"\"\"Check if enough time has passed to attempt reset.\"\"\"\n+        if self._last_failure_time is None:\n+            return True\n+        return time.time() - self._last_failure_time >= self.config.recovery_timeout\n+\n+    def _on_success(self):\n+        \"\"\"Handle successful operation.\"\"\"\n+        if self._state == CircuitBreakerState.HALF_OPEN:\n+            self._success_count += 1\n+            if self._success_count >= self.config.success_threshold:\n+                self._reset()\n+        else:\n+            self._success_count = 0\n+\n+    def _on_failure(self):\n+        \"\"\"Handle failed operation.\"\"\"\n+        self._failure_count += 1\n+        self._last_failure_time = time.time()\n+\n+        if self._failure_count >= self.config.failure_threshold:\n+            self._state = CircuitBreakerState.OPEN\n+            logger.warning(f\"Circuit breaker opened after {self._failure_count} failures\")\n+\n+    def _reset(self):\n+        \"\"\"Reset circuit breaker to closed state.\"\"\"\n+        self._state = CircuitBreakerState.CLOSED\n+        self._failure_count = 0\n+        self._success_count = 0\n+        logger.info(\"Circuit breaker reset to CLOSED state\")\n+\n+    @property\n+    def state(self) -> CircuitBreakerState:\n+        \"\"\"Get current circuit breaker state.\"\"\"\n+        return self._state\n+\n+class CircuitBreakerOpenException(Exception):\n+    \"\"\"Exception raised when circuit breaker is open.\"\"\"\n+    pass\n+\n+# Global circuit breakers for different services\n+llm_circuit_breaker = CircuitBreaker(CircuitBreakerConfig(\n+    failure_threshold=3,\n+    recovery_timeout=30,\n+    timeout=15.0\n+))\n+\n+db_circuit_breaker = CircuitBreaker(CircuitBreakerConfig(\n+    failure_threshold=5,\n+    recovery_timeout=60,\n+    timeout=10.0\n+))\n+\n+external_api_circuit_breaker = CircuitBreaker(CircuitBreakerConfig(\n+    failure_threshold=3,\n+    recovery_timeout=120,\n+    timeout=30.0\n+))\n+\n+@dataclass\n+class RetryConfig:\n+    \"\"\"Configuration for retry logic.\"\"\"\n+    max_attempts: int = 3\n+    base_delay: float = 1.0\n+    max_delay: float = 60.0\n+    backoff_factor: float = 2.0\n+    jitter: bool = True\n+\n+class RetryMechanism:\n+    \"\"\"Retry mechanism with exponential backoff.\"\"\"\n+\n+    def __init__(self, config: RetryConfig):\n+        self.config = config\n+\n+    async def execute(self, func: Callable, *args, **kwargs) -> Any:\n+        \"\"\"Execute function with retry logic.\"\"\"\n+        last_exception = None\n+\n+        for attempt in range(self.config.max_attempts):\n+            try:\n+                if asyncio.iscoroutinefunction(func):\n+                    return await func(*args, **kwargs)\n+                else:\n+                    return await asyncio.get_event_loop().run_in_executor(\n+                        None, lambda: func(*args, **kwargs)\n+                    )\n+            except Exception as e:\n+                last_exception = e\n+                if attempt < self.config.max_attempts - 1:\n+                    delay = self._calculate_delay(attempt)\n+                    logger.warning(f\"Attempt {attempt + 1} failed: {e}. Retrying in {delay:.2f}s\")\n+                    await asyncio.sleep(delay)\n+                else:\n+                    logger.error(f\"All {self.config.max_attempts} attempts failed. Last error: {e}\")\n+\n+        if last_exception is not None:\n+            raise last_exception\n+        else:\n+            raise RuntimeError(\"All retry attempts failed with unknown error\")\n+\n+    def _calculate_delay(self, attempt: int) -> float:\n+        \"\"\"Calculate delay with exponential backoff and optional jitter.\"\"\"\n+        delay = min(\n+            self.config.base_delay * (self.config.backoff_factor ** attempt),\n+            self.config.max_delay\n+        )\n+\n+        if self.config.jitter:\n+            import random\n+            delay = delay * (0.5 + random.random() * 0.5)  # Add 50% jitter\n+\n+        return delay\n+\n+# Global retry mechanisms\n+default_retry = RetryMechanism(RetryConfig())\n+fast_retry = RetryMechanism(RetryConfig(max_attempts=2, base_delay=0.5))\n+slow_retry = RetryMechanism(RetryConfig(max_attempts=5, base_delay=2.0, max_delay=300.0))\n+\n+@dataclass\n+class ErrorContext:\n+    \"\"\"Context information for errors.\"\"\"\n+    operation: str\n+    stage: str\n+    timestamp: str\n+    attempt: int\n+    error_type: str\n+    error_message: str\n+    stack_trace: str\n+    metadata: Dict[str, Any] = field(default_factory=dict)\n+\n+@dataclass\n+class ErrorHandler:\n+    \"\"\"Comprehensive error handler with recovery strategies.\"\"\"\n+\n+    def __init__(self):\n+        self.error_history: List[ErrorContext] = []\n+        self.recovery_strategies: Dict[str, Callable] = {}\n+\n+    def register_recovery_strategy(self, error_type: str, strategy: Callable):\n+        \"\"\"Register a recovery strategy for specific error types.\"\"\"\n+        self.recovery_strategies[error_type] = strategy\n+\n+    async def handle_error(self, error: Exception, context: Dict[str, Any]) -> Optional[Any]:\n+        \"\"\"Handle an error with appropriate recovery strategy.\"\"\"\n+        error_context = ErrorContext(\n+            operation=context.get('operation', 'unknown'),\n+            stage=context.get('stage', 'unknown'),\n+            timestamp=datetime.now().isoformat(),\n+            attempt=context.get('attempt', 1),\n+            error_type=type(error).__name__,\n+            error_message=str(error),\n+            stack_trace=traceback.format_exc(),\n+            metadata=context\n+        )\n+\n+        self.error_history.append(error_context)\n+\n+        # Log error with context\n+        logger.error(f\"Error in {error_context.operation} at stage {error_context.stage}: {error_context.error_message}\")\n+        logger.debug(f\"Stack trace: {error_context.stack_trace}\")\n+\n+        # Try recovery strategy\n+        recovery_func = self.recovery_strategies.get(error_context.error_type)\n+        if recovery_func:\n+            try:\n+                logger.info(f\"Attempting recovery for {error_context.error_type}\")\n+                return await recovery_func(error, context)\n+            except Exception as recovery_error:\n+                logger.error(f\"Recovery failed: {recovery_error}\")\n+\n+        return None\n+\n+    def get_error_summary(self) -> Dict[str, Any]:\n+        \"\"\"Get summary of recent errors.\"\"\"\n+        if not self.error_history:\n+            return {\"total_errors\": 0}\n+\n+        recent_errors = [e for e in self.error_history\n+                        if datetime.now() - datetime.fromisoformat(e.timestamp) < timedelta(hours=1)]\n+\n+        error_types = {}\n+        for error in recent_errors:\n+            error_types[error.error_type] = error_types.get(error.error_type, 0) + 1\n+\n+        return {\n+            \"total_errors\": len(self.error_history),\n+            \"recent_errors\": len(recent_errors),\n+            \"error_types\": error_types,\n+            \"most_common_error\": max(error_types.keys(), key=lambda k: error_types[k]) if error_types else None\n+        }\n+\n+# Global error handler\n+error_handler = ErrorHandler()\n+\n+# Recovery strategies\n+async def llm_fallback_recovery(error: Exception, context: Dict[str, Any]) -> str:\n+    \"\"\"Fallback recovery for LLM failures.\"\"\"\n+    return \"LLM service temporarily unavailable. Using cached or default response.\"\n+\n+async def db_connection_recovery(error: Exception, context: Dict[str, Any]) -> None:\n+    \"\"\"Recovery strategy for database connection issues.\"\"\"\n+    logger.info(\"Attempting database reconnection...\")\n+    # Implementation would depend on specific database connection logic\n+    await asyncio.sleep(1)  # Brief pause before retry\n+\n+async def network_timeout_recovery(error: Exception, context: Dict[str, Any]) -> Dict[str, Any]:\n+    \"\"\"Recovery strategy for network timeouts.\"\"\"\n+    return {\n+        \"status\": \"timeout\",\n+        \"retry_recommended\": True,\n+        \"fallback_data\": context.get('fallback_data', {})\n+    }\n+\n+# Register recovery strategies\n+error_handler.register_recovery_strategy(\"CircuitBreakerOpenException\", llm_fallback_recovery)\n+error_handler.register_recovery_strategy(\"sqlite3.OperationalError\", db_connection_recovery)\n+error_handler.register_recovery_strategy(\"asyncio.TimeoutError\", network_timeout_recovery)\n+\n+@dataclass\n+class HealthCheck:\n+    \"\"\"Health check for system components.\"\"\"\n+\n+    name: str\n+    check_func: Callable\n+    interval: int = 60  # seconds\n+    timeout: float = 10.0\n+    last_check: Optional[float] = None\n+    last_result: Optional[bool] = None\n+    consecutive_failures: int = 0\n+    max_consecutive_failures: int = 3\n+\n+    async def perform_check(self) -> bool:\n+        \"\"\"Perform the health check.\"\"\"\n+        try:\n+            if asyncio.iscoroutinefunction(self.check_func):\n+                result = await asyncio.wait_for(self.check_func(), timeout=self.timeout)\n+            else:\n+                result = await asyncio.get_event_loop().run_in_executor(\n+                    None, lambda: self.check_func()\n+                )\n+\n+            self.last_check = time.time()\n+            self.last_result = bool(result)\n+\n+            if self.last_result:\n+                self.consecutive_failures = 0\n+            else:\n+                self.consecutive_failures += 1\n+\n+            return self.last_result\n+\n+        except Exception as e:\n+            logger.error(f\"Health check {self.name} failed: {e}\")\n+            self.last_check = time.time()\n+            self.last_result = False\n+            self.consecutive_failures += 1\n+            return False\n+\n+    def is_healthy(self) -> bool:\n+        \"\"\"Check if component is currently healthy.\"\"\"\n+        if self.last_result is None:\n+            return False\n+        return self.last_result and self.consecutive_failures < self.max_consecutive_failures\n+\n+    def should_check(self) -> bool:\n+        \"\"\"Check if health check should be performed.\"\"\"\n+        if self.last_check is None:\n+            return True\n+        return time.time() - self.last_check >= self.interval\n+\n+class HealthMonitor:\n+    \"\"\"Monitor health of system components.\"\"\"\n+\n+    def __init__(self):\n+        self.health_checks: Dict[str, HealthCheck] = {}\n+\n+    def register_check(self, name: str, check_func: Callable, **kwargs):\n+        \"\"\"Register a health check.\"\"\"\n+        self.health_checks[name] = HealthCheck(name=name, check_func=check_func, **kwargs)\n+\n+    async def check_all(self) -> Dict[str, bool]:\n+        \"\"\"Check health of all registered components.\"\"\"\n+        results = {}\n+        tasks = []\n+\n+        for name, check in self.health_checks.items():\n+            if check.should_check():\n+                tasks.append(self._check_component(name, check))\n+            else:\n+                results[name] = check.is_healthy()\n+\n+        if tasks:\n+            task_results = await asyncio.gather(*tasks, return_exceptions=True)\n+            for name, result in zip([t[0] for t in tasks], task_results):\n+                if isinstance(result, Exception):\n+                    logger.error(f\"Health check task failed for {name}: {result}\")\n+                    results[name] = False\n+                else:\n+                    results[name] = result\n+\n+        return results\n+\n+    async def _check_component(self, name: str, check: HealthCheck) -> bool:\n+        \"\"\"Check health of a specific component.\"\"\"\n+        return await check.perform_check()\n+\n+    def get_health_summary(self) -> Dict[str, Any]:\n+        \"\"\"Get summary of health status.\"\"\"\n+        all_results = {}\n+        for name, check in self.health_checks.items():\n+            all_results[name] = {\n+                \"healthy\": check.is_healthy(),\n+                \"last_check\": check.last_check,\n+                \"consecutive_failures\": check.consecutive_failures\n+            }\n+\n+        healthy_count = sum(1 for r in all_results.values() if r[\"healthy\"])\n+        total_count = len(all_results)\n+\n+        return {\n+            \"overall_healthy\": healthy_count == total_count,\n+            \"healthy_components\": healthy_count,\n+            \"total_components\": total_count,\n+            \"component_details\": all_results\n+        }\n+\n+# Global health monitor\n+health_monitor = HealthMonitor()\n+\n+# Reliability-enhanced node functions\n+async def reliable_enrich_findings(state: GraphState) -> GraphState:\n+    \"\"\"Reliability-enhanced enrichment with circuit breaker and error handling.\"\"\"\n+    start_time = time.time()\n+    state['current_stage'] = 'reliable_enrich'\n+\n+    try:\n+        # Use circuit breaker for LLM calls\n+        async def safe_llm_call():\n+            return await llm_circuit_breaker.call(\n+                lambda: _perform_enrichment_logic(state)\n+            )\n+\n+        # Use retry mechanism\n+        enriched_data = await default_retry.execute(safe_llm_call)\n+\n+        state['enriched_findings'] = enriched_data\n+        state.setdefault('metrics', {})['reliable_enrich_duration'] = time.time() - start_time\n+\n+    except Exception as e:\n+        await error_handler.handle_error(e, {\n+            'operation': 'enrich_findings',\n+            'stage': 'reliable_enrich',\n+            'state': state\n+        })\n+\n+        # Fallback: use original findings\n+        if 'enriched_findings' not in state:\n+            state['enriched_findings'] = state.get('raw_findings', [])\n+\n+    return state\n+\n+async def _perform_enrichment_logic(state: GraphState) -> List[Dict[str, Any]]:\n+    \"\"\"Core enrichment logic (placeholder - integrate with existing enrichment).\"\"\"\n+    # This would integrate with existing enrichment logic\n+    findings = state.get('raw_findings', [])\n+    # Add reliability metadata\n+    for finding in findings:\n+        finding['reliability_checked'] = True\n+        finding['enrichment_timestamp'] = datetime.now().isoformat()\n+\n+    return findings\n+\n+async def reliable_correlate_findings(state: GraphState) -> GraphState:\n+    \"\"\"Reliability-enhanced correlation with error recovery.\"\"\"\n+    start_time = time.time()\n+    state['current_stage'] = 'reliable_correlate'\n+\n+    try:\n+        # Health check before processing\n+        health_status = await health_monitor.check_all()\n+        if not health_status.get('correlation_service', True):\n+            logger.warning(\"Correlation service unhealthy, using simplified correlation\")\n+\n+        # Use circuit breaker and retry\n+        async def safe_correlation():\n+            return await _perform_correlation_logic(state)\n+\n+        correlated_data = await default_retry.execute(safe_correlation)\n+        state.update(correlated_data)\n+        state.setdefault('metrics', {})['reliable_correlate_duration'] = time.time() - start_time\n+\n+    except Exception as e:\n+        await error_handler.handle_error(e, {\n+            'operation': 'correlate_findings',\n+            'stage': 'reliable_correlate',\n+            'state': state\n+        })\n+\n+        # Fallback: minimal correlation\n+        if 'correlated_findings' not in state:\n+            state['correlated_findings'] = state.get('enriched_findings', [])\n+\n+    return state\n+\n+async def _perform_correlation_logic(state: GraphState) -> Dict[str, Any]:\n+    \"\"\"Core correlation logic (placeholder - integrate with existing correlation).\"\"\"\n+    # This would integrate with existing correlation logic\n+    findings = state.get('enriched_findings', [])\n+\n+    return {\n+        'correlated_findings': findings,\n+        'correlations': [],\n+        'correlation_metadata': {\n+            'reliability_checked': True,\n+            'correlation_timestamp': datetime.now().isoformat()\n+        }\n+    }\n+\n+async def reliable_summarize_state(state: GraphState) -> GraphState:\n+    \"\"\"Reliability-enhanced summarization with comprehensive error handling.\"\"\"\n+    start_time = time.time()\n+    state['current_stage'] = 'reliable_summarize'\n+\n+    try:\n+        # Multiple fallback strategies\n+        summary = None\n+\n+        # Try primary summarization\n+        try:\n+            summary = await fast_retry.execute(\n+                lambda: llm_circuit_breaker.call(_perform_summarization_logic, state)\n+            )\n+        except Exception as primary_error:\n+            logger.warning(f\"Primary summarization failed: {primary_error}\")\n+\n+            # Try fallback summarization\n+            try:\n+                summary = await _fallback_summarization(state)\n+            except Exception as fallback_error:\n+                logger.error(f\"Fallback summarization also failed: {fallback_error}\")\n+                summary = _emergency_summary(state)\n+\n+        state['summary'] = summary\n+        state.setdefault('metrics', {})['reliable_summarize_duration'] = time.time() - start_time\n+\n+    except Exception as e:\n+        await error_handler.handle_error(e, {\n+            'operation': 'summarize_state',\n+            'stage': 'reliable_summarize',\n+            'state': state\n+        })\n+\n+        # Emergency fallback\n+        if 'summary' not in state:\n+            state['summary'] = _emergency_summary(state)\n+\n+    return state\n+\n+async def _perform_summarization_logic(state: GraphState) -> Dict[str, Any]:\n+    \"\"\"Core summarization logic (placeholder - integrate with existing summarization).\"\"\"\n+    # This would integrate with existing summarization logic\n+    return {\n+        'summary_text': 'Analysis completed with reliability enhancements',\n+        'confidence_score': 0.85,\n+        'reliability_checked': True,\n+        'summary_timestamp': datetime.now().isoformat()\n+    }\n+\n+async def _fallback_summarization(state: GraphState) -> Dict[str, Any]:\n+    \"\"\"Fallback summarization when primary fails.\"\"\"\n+    findings_count = len(state.get('correlated_findings', []))\n+    return {\n+        'summary_text': f'Fallback summary: Found {findings_count} items for analysis',\n+        'confidence_score': 0.6,\n+        'fallback_mode': True,\n+        'summary_timestamp': datetime.now().isoformat()\n+    }\n+\n+def _emergency_summary(state: GraphState) -> Dict[str, Any]:\n+    \"\"\"Emergency summary when all else fails.\"\"\"\n+    return {\n+        'summary_text': 'Analysis completed (emergency mode)',\n+        'confidence_score': 0.0,\n+        'emergency_mode': True,\n+        'summary_timestamp': datetime.now().isoformat()\n+    }\n+\n+# Health check functions\n+async def check_llm_service() -> bool:\n+    \"\"\"Check if LLM service is healthy.\"\"\"\n+    try:\n+        # Placeholder - implement actual LLM health check\n+        return llm_circuit_breaker.state != CircuitBreakerState.OPEN\n+    except Exception:\n+        return False\n+\n+async def check_database() -> bool:\n+    \"\"\"Check if database is healthy.\"\"\"\n+    try:\n+        # Placeholder - implement actual database health check\n+        return db_circuit_breaker.state != CircuitBreakerState.OPEN\n+    except Exception:\n+        return False\n+\n+async def check_external_apis() -> bool:\n+    \"\"\"Check if external APIs are healthy.\"\"\"\n+    try:\n+        # Placeholder - implement actual API health check\n+        return external_api_circuit_breaker.state != CircuitBreakerState.OPEN\n+    except Exception:\n+        return False\n+\n+# Register health checks\n+health_monitor.register_check(\"llm_service\", check_llm_service, interval=30)\n+health_monitor.register_check(\"database\", check_database, interval=60)\n+health_monitor.register_check(\"external_apis\", check_external_apis, interval=120)\n+\n+# Initialization and cleanup\n+async def initialize_reliability():\n+    \"\"\"Initialize reliability components.\"\"\"\n+    logger.info(\"Initializing reliability enhancements...\")\n+    # Perform initial health checks\n+    await health_monitor.check_all()\n+\n+async def shutdown_reliability():\n+    \"\"\"Shutdown reliability components.\"\"\"\n+    logger.info(\"Shutting down reliability enhancements...\")\n+    # Cleanup if needed\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -0,0 +1,607 @@\n",
        "+from __future__ import annotations\n",
        "+\"\"\"Reliability enhancements for LangGraph processing.\n",
        "+\n",
        "+This module provides circuit breaker patterns, comprehensive error handling,\n",
        "+and resilience mechanisms to ensure robust operation.\n",
        "+\"\"\"\n",
        "+\n",
        "+from typing import Any, List, Dict, Optional, Callable, AsyncIterator\n",
        "+import asyncio\n",
        "+import logging\n",
        "+import time\n",
        "+from dataclasses import dataclass, field\n",
        "+from datetime import datetime, timedelta\n",
        "+from contextlib import asynccontextmanager\n",
        "+from enum import Enum\n",
        "+import traceback\n",
        "+import sys\n",
        "+\n",
        "+from .graph import GraphState\n",
        "+\n",
        "+logger = logging.getLogger(__name__)\n",
        "+\n",
        "+class CircuitBreakerState(Enum):\n",
        "+    \"\"\"States for circuit breaker pattern.\"\"\"\n",
        "+    CLOSED = \"closed\"\n",
        "+    OPEN = \"open\"\n",
        "+    HALF_OPEN = \"half-open\"\n",
        "+\n",
        "+@dataclass\n",
        "+class CircuitBreakerConfig:\n",
        "+    \"\"\"Configuration for circuit breaker.\"\"\"\n",
        "+    failure_threshold: int = 5\n",
        "+    recovery_timeout: int = 60\n",
        "+    expected_exception: tuple = (Exception,)\n",
        "+    success_threshold: int = 3\n",
        "+    timeout: float = 10.0\n",
        "+\n",
        "+@dataclass\n",
        "+class CircuitBreaker:\n",
        "+    \"\"\"Circuit breaker implementation for fault tolerance.\"\"\"\n",
        "+\n",
        "+    config: CircuitBreakerConfig\n",
        "+    _state: CircuitBreakerState = CircuitBreakerState.CLOSED\n",
        "+    _failure_count: int = 0\n",
        "+    _last_failure_time: Optional[float] = None\n",
        "+    _success_count: int = 0\n",
        "+\n",
        "+    def __post_init__(self):\n",
        "+        self._state = CircuitBreakerState.CLOSED\n",
        "+        self._failure_count = 0\n",
        "+        self._last_failure_time = None\n",
        "+        self._success_count = 0\n",
        "+\n",
        "+    async def call(self, func: Callable, *args, **kwargs) -> Any:\n",
        "+        \"\"\"Execute function with circuit breaker protection.\"\"\"\n",
        "+        if self._state == CircuitBreakerState.OPEN:\n",
        "+            if self._should_attempt_reset():\n",
        "+                self._state = CircuitBreakerState.HALF_OPEN\n",
        "+            else:\n",
        "+                raise CircuitBreakerOpenException(\"Circuit breaker is OPEN\")\n",
        "+\n",
        "+        try:\n",
        "+            if asyncio.iscoroutinefunction(func):\n",
        "+                result = await asyncio.wait_for(func(*args, **kwargs), timeout=self.config.timeout)\n",
        "+            else:\n",
        "+                result = await asyncio.get_event_loop().run_in_executor(\n",
        "+                    None, lambda: func(*args, **kwargs)\n",
        "+                )\n",
        "+\n",
        "+            self._on_success()\n",
        "+            return result\n",
        "+\n",
        "+        except self.config.expected_exception as e:\n",
        "+            self._on_failure()\n",
        "+            raise e\n",
        "+        except asyncio.TimeoutError:\n",
        "+            self._on_failure()\n",
        "+            raise CircuitBreakerOpenException(\"Operation timed out\")\n",
        "+\n",
        "+    def _should_attempt_reset(self) -> bool:\n",
        "+        \"\"\"Check if enough time has passed to attempt reset.\"\"\"\n",
        "+        if self._last_failure_time is None:\n",
        "+            return True\n",
        "+        return time.time() - self._last_failure_time >= self.config.recovery_timeout\n",
        "+\n",
        "+    def _on_success(self):\n",
        "+        \"\"\"Handle successful operation.\"\"\"\n",
        "+        if self._state == CircuitBreakerState.HALF_OPEN:\n",
        "+            self._success_count += 1\n",
        "+            if self._success_count >= self.config.success_threshold:\n",
        "+                self._reset()\n",
        "+        else:\n",
        "+            self._success_count = 0\n",
        "+\n",
        "+    def _on_failure(self):\n",
        "+        \"\"\"Handle failed operation.\"\"\"\n",
        "+        self._failure_count += 1\n",
        "+        self._last_failure_time = time.time()\n",
        "+\n",
        "+        if self._failure_count >= self.config.failure_threshold:\n",
        "+            self._state = CircuitBreakerState.OPEN\n",
        "+            logger.warning(f\"Circuit breaker opened after {self._failure_count} failures\")\n",
        "+\n",
        "+    def _reset(self):\n",
        "+        \"\"\"Reset circuit breaker to closed state.\"\"\"\n",
        "+        self._state = CircuitBreakerState.CLOSED\n",
        "+        self._failure_count = 0\n",
        "+        self._success_count = 0\n",
        "+        logger.info(\"Circuit breaker reset to CLOSED state\")\n",
        "+\n",
        "+    @property\n",
        "+    def state(self) -> CircuitBreakerState:\n",
        "+        \"\"\"Get current circuit breaker state.\"\"\"\n",
        "+        return self._state\n",
        "+\n",
        "+class CircuitBreakerOpenException(Exception):\n",
        "+    \"\"\"Exception raised when circuit breaker is open.\"\"\"\n",
        "+    pass\n",
        "+\n",
        "+# Global circuit breakers for different services\n",
        "+llm_circuit_breaker = CircuitBreaker(CircuitBreakerConfig(\n",
        "+    failure_threshold=3,\n",
        "+    recovery_timeout=30,\n",
        "+    timeout=15.0\n",
        "+))\n",
        "+\n",
        "+db_circuit_breaker = CircuitBreaker(CircuitBreakerConfig(\n",
        "+    failure_threshold=5,\n",
        "+    recovery_timeout=60,\n",
        "+    timeout=10.0\n",
        "+))\n",
        "+\n",
        "+external_api_circuit_breaker = CircuitBreaker(CircuitBreakerConfig(\n",
        "+    failure_threshold=3,\n",
        "+    recovery_timeout=120,\n",
        "+    timeout=30.0\n",
        "+))\n",
        "+\n",
        "+@dataclass\n",
        "+class RetryConfig:\n",
        "+    \"\"\"Configuration for retry logic.\"\"\"\n",
        "+    max_attempts: int = 3\n",
        "+    base_delay: float = 1.0\n",
        "+    max_delay: float = 60.0\n",
        "+    backoff_factor: float = 2.0\n",
        "+    jitter: bool = True\n",
        "+\n",
        "+class RetryMechanism:\n",
        "+    \"\"\"Retry mechanism with exponential backoff.\"\"\"\n",
        "+\n",
        "+    def __init__(self, config: RetryConfig):\n",
        "+        self.config = config\n",
        "+\n",
        "+    async def execute(self, func: Callable, *args, **kwargs) -> Any:\n",
        "+        \"\"\"Execute function with retry logic.\"\"\"\n",
        "+        last_exception = None\n",
        "+\n",
        "+        for attempt in range(self.config.max_attempts):\n",
        "+            try:\n",
        "+                if asyncio.iscoroutinefunction(func):\n",
        "+                    return await func(*args, **kwargs)\n",
        "+                else:\n",
        "+                    return await asyncio.get_event_loop().run_in_executor(\n",
        "+                        None, lambda: func(*args, **kwargs)\n",
        "+                    )\n",
        "+            except Exception as e:\n",
        "+                last_exception = e\n",
        "+                if attempt < self.config.max_attempts - 1:\n",
        "+                    delay = self._calculate_delay(attempt)\n",
        "+                    logger.warning(f\"Attempt {attempt + 1} failed: {e}. Retrying in {delay:.2f}s\")\n",
        "+                    await asyncio.sleep(delay)\n",
        "+                else:\n",
        "+                    logger.error(f\"All {self.config.max_attempts} attempts failed. Last error: {e}\")\n",
        "+\n",
        "+        if last_exception is not None:\n",
        "+            raise last_exception\n",
        "+        else:\n",
        "+            raise RuntimeError(\"All retry attempts failed with unknown error\")\n",
        "+\n",
        "+    def _calculate_delay(self, attempt: int) -> float:\n",
        "+        \"\"\"Calculate delay with exponential backoff and optional jitter.\"\"\"\n",
        "+        delay = min(\n",
        "+            self.config.base_delay * (self.config.backoff_factor ** attempt),\n",
        "+            self.config.max_delay\n",
        "+        )\n",
        "+\n",
        "+        if self.config.jitter:\n",
        "+            import random\n",
        "+            delay = delay * (0.5 + random.random() * 0.5)  # Add 50% jitter\n",
        "+\n",
        "+        return delay\n",
        "+\n",
        "+# Global retry mechanisms\n",
        "+default_retry = RetryMechanism(RetryConfig())\n",
        "+fast_retry = RetryMechanism(RetryConfig(max_attempts=2, base_delay=0.5))\n",
        "+slow_retry = RetryMechanism(RetryConfig(max_attempts=5, base_delay=2.0, max_delay=300.0))\n",
        "+\n",
        "+@dataclass\n",
        "+class ErrorContext:\n",
        "+    \"\"\"Context information for errors.\"\"\"\n",
        "+    operation: str\n",
        "+    stage: str\n",
        "+    timestamp: str\n",
        "+    attempt: int\n",
        "+    error_type: str\n",
        "+    error_message: str\n",
        "+    stack_trace: str\n",
        "+    metadata: Dict[str, Any] = field(default_factory=dict)\n",
        "+\n",
        "+@dataclass\n",
        "+class ErrorHandler:\n",
        "+    \"\"\"Comprehensive error handler with recovery strategies.\"\"\"\n",
        "+\n",
        "+    def __init__(self):\n",
        "+        self.error_history: List[ErrorContext] = []\n",
        "+        self.recovery_strategies: Dict[str, Callable] = {}\n",
        "+\n",
        "+    def register_recovery_strategy(self, error_type: str, strategy: Callable):\n",
        "+        \"\"\"Register a recovery strategy for specific error types.\"\"\"\n",
        "+        self.recovery_strategies[error_type] = strategy\n",
        "+\n",
        "+    async def handle_error(self, error: Exception, context: Dict[str, Any]) -> Optional[Any]:\n",
        "+        \"\"\"Handle an error with appropriate recovery strategy.\"\"\"\n",
        "+        error_context = ErrorContext(\n",
        "+            operation=context.get('operation', 'unknown'),\n",
        "+            stage=context.get('stage', 'unknown'),\n",
        "+            timestamp=datetime.now().isoformat(),\n",
        "+            attempt=context.get('attempt', 1),\n",
        "+            error_type=type(error).__name__,\n",
        "+            error_message=str(error),\n",
        "+            stack_trace=traceback.format_exc(),\n",
        "+            metadata=context\n",
        "+        )\n",
        "+\n",
        "+        self.error_history.append(error_context)\n",
        "+\n",
        "+        # Log error with context\n",
        "+        logger.error(f\"Error in {error_context.operation} at stage {error_context.stage}: {error_context.error_message}\")\n",
        "+        logger.debug(f\"Stack trace: {error_context.stack_trace}\")\n",
        "+\n",
        "+        # Try recovery strategy\n",
        "+        recovery_func = self.recovery_strategies.get(error_context.error_type)\n",
        "+        if recovery_func:\n",
        "+            try:\n",
        "+                logger.info(f\"Attempting recovery for {error_context.error_type}\")\n",
        "+                return await recovery_func(error, context)\n",
        "+            except Exception as recovery_error:\n",
        "+                logger.error(f\"Recovery failed: {recovery_error}\")\n",
        "+\n",
        "+        return None\n",
        "+\n",
        "+    def get_error_summary(self) -> Dict[str, Any]:\n",
        "+        \"\"\"Get summary of recent errors.\"\"\"\n",
        "+        if not self.error_history:\n",
        "+            return {\"total_errors\": 0}\n",
        "+\n",
        "+        recent_errors = [e for e in self.error_history\n",
        "+                        if datetime.now() - datetime.fromisoformat(e.timestamp) < timedelta(hours=1)]\n",
        "+\n",
        "+        error_types = {}\n",
        "+        for error in recent_errors:\n",
        "+            error_types[error.error_type] = error_types.get(error.error_type, 0) + 1\n",
        "+\n",
        "+        return {\n",
        "+            \"total_errors\": len(self.error_history),\n",
        "+            \"recent_errors\": len(recent_errors),\n",
        "+            \"error_types\": error_types,\n",
        "+            \"most_common_error\": max(error_types.keys(), key=lambda k: error_types[k]) if error_types else None\n",
        "+        }\n",
        "+\n",
        "+# Global error handler\n",
        "+error_handler = ErrorHandler()\n",
        "+\n",
        "+# Recovery strategies\n",
        "+async def llm_fallback_recovery(error: Exception, context: Dict[str, Any]) -> str:\n",
        "+    \"\"\"Fallback recovery for LLM failures.\"\"\"\n",
        "+    return \"LLM service temporarily unavailable. Using cached or default response.\"\n",
        "+\n",
        "+async def db_connection_recovery(error: Exception, context: Dict[str, Any]) -> None:\n",
        "+    \"\"\"Recovery strategy for database connection issues.\"\"\"\n",
        "+    logger.info(\"Attempting database reconnection...\")\n",
        "+    # Implementation would depend on specific database connection logic\n",
        "+    await asyncio.sleep(1)  # Brief pause before retry\n",
        "+\n",
        "+async def network_timeout_recovery(error: Exception, context: Dict[str, Any]) -> Dict[str, Any]:\n",
        "+    \"\"\"Recovery strategy for network timeouts.\"\"\"\n",
        "+    return {\n",
        "+        \"status\": \"timeout\",\n",
        "+        \"retry_recommended\": True,\n",
        "+        \"fallback_data\": context.get('fallback_data', {})\n",
        "+    }\n",
        "+\n",
        "+# Register recovery strategies\n",
        "+error_handler.register_recovery_strategy(\"CircuitBreakerOpenException\", llm_fallback_recovery)\n",
        "+error_handler.register_recovery_strategy(\"sqlite3.OperationalError\", db_connection_recovery)\n",
        "+error_handler.register_recovery_strategy(\"asyncio.TimeoutError\", network_timeout_recovery)\n",
        "+\n",
        "+@dataclass\n",
        "+class HealthCheck:\n",
        "+    \"\"\"Health check for system components.\"\"\"\n",
        "+\n",
        "+    name: str\n",
        "+    check_func: Callable\n",
        "+    interval: int = 60  # seconds\n",
        "+    timeout: float = 10.0\n",
        "+    last_check: Optional[float] = None\n",
        "+    last_result: Optional[bool] = None\n",
        "+    consecutive_failures: int = 0\n",
        "+    max_consecutive_failures: int = 3\n",
        "+\n",
        "+    async def perform_check(self) -> bool:\n",
        "+        \"\"\"Perform the health check.\"\"\"\n",
        "+        try:\n",
        "+            if asyncio.iscoroutinefunction(self.check_func):\n",
        "+                result = await asyncio.wait_for(self.check_func(), timeout=self.timeout)\n",
        "+            else:\n",
        "+                result = await asyncio.get_event_loop().run_in_executor(\n",
        "+                    None, lambda: self.check_func()\n",
        "+                )\n",
        "+\n",
        "+            self.last_check = time.time()\n",
        "+            self.last_result = bool(result)\n",
        "+\n",
        "+            if self.last_result:\n",
        "+                self.consecutive_failures = 0\n",
        "+            else:\n",
        "+                self.consecutive_failures += 1\n",
        "+\n",
        "+            return self.last_result\n",
        "+\n",
        "+        except Exception as e:\n",
        "+            logger.error(f\"Health check {self.name} failed: {e}\")\n",
        "+            self.last_check = time.time()\n",
        "+            self.last_result = False\n",
        "+            self.consecutive_failures += 1\n",
        "+            return False\n",
        "+\n",
        "+    def is_healthy(self) -> bool:\n",
        "+        \"\"\"Check if component is currently healthy.\"\"\"\n",
        "+        if self.last_result is None:\n",
        "+            return False\n",
        "+        return self.last_result and self.consecutive_failures < self.max_consecutive_failures\n",
        "+\n",
        "+    def should_check(self) -> bool:\n",
        "+        \"\"\"Check if health check should be performed.\"\"\"\n",
        "+        if self.last_check is None:\n",
        "+            return True\n",
        "+        return time.time() - self.last_check >= self.interval\n",
        "+\n",
        "+class HealthMonitor:\n",
        "+    \"\"\"Monitor health of system components.\"\"\"\n",
        "+\n",
        "+    def __init__(self):\n",
        "+        self.health_checks: Dict[str, HealthCheck] = {}\n",
        "+\n",
        "+    def register_check(self, name: str, check_func: Callable, **kwargs):\n",
        "+        \"\"\"Register a health check.\"\"\"\n",
        "+        self.health_checks[name] = HealthCheck(name=name, check_func=check_func, **kwargs)\n",
        "+\n",
        "+    async def check_all(self) -> Dict[str, bool]:\n",
        "+        \"\"\"Check health of all registered components.\"\"\"\n",
        "+        results = {}\n",
        "+        tasks = []\n",
        "+\n",
        "+        for name, check in self.health_checks.items():\n",
        "+            if check.should_check():\n",
        "+                tasks.append(self._check_component(name, check))\n",
        "+            else:\n",
        "+                results[name] = check.is_healthy()\n",
        "+\n",
        "+        if tasks:\n",
        "+            task_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "+            for name, result in zip([t[0] for t in tasks], task_results):\n",
        "+                if isinstance(result, Exception):\n",
        "+                    logger.error(f\"Health check task failed for {name}: {result}\")\n",
        "+                    results[name] = False\n",
        "+                else:\n",
        "+                    results[name] = result\n",
        "+\n",
        "+        return results\n",
        "+\n",
        "+    async def _check_component(self, name: str, check: HealthCheck) -> bool:\n",
        "+        \"\"\"Check health of a specific component.\"\"\"\n",
        "+        return await check.perform_check()\n",
        "+\n",
        "+    def get_health_summary(self) -> Dict[str, Any]:\n",
        "+        \"\"\"Get summary of health status.\"\"\"\n",
        "+        all_results = {}\n",
        "+        for name, check in self.health_checks.items():\n",
        "+            all_results[name] = {\n",
        "+                \"healthy\": check.is_healthy(),\n",
        "+                \"last_check\": check.last_check,\n",
        "+                \"consecutive_failures\": check.consecutive_failures\n",
        "+            }\n",
        "+\n",
        "+        healthy_count = sum(1 for r in all_results.values() if r[\"healthy\"])\n",
        "+        total_count = len(all_results)\n",
        "+\n",
        "+        return {\n",
        "+            \"overall_healthy\": healthy_count == total_count,\n",
        "+            \"healthy_components\": healthy_count,\n",
        "+            \"total_components\": total_count,\n",
        "+            \"component_details\": all_results\n",
        "+        }\n",
        "+\n",
        "+# Global health monitor\n",
        "+health_monitor = HealthMonitor()\n",
        "+\n",
        "+# Reliability-enhanced node functions\n",
        "+async def reliable_enrich_findings(state: GraphState) -> GraphState:\n",
        "+    \"\"\"Reliability-enhanced enrichment with circuit breaker and error handling.\"\"\"\n",
        "+    start_time = time.time()\n",
        "+    state['current_stage'] = 'reliable_enrich'\n",
        "+\n",
        "+    try:\n",
        "+        # Use circuit breaker for LLM calls\n",
        "+        async def safe_llm_call():\n",
        "+            return await llm_circuit_breaker.call(\n",
        "+                lambda: _perform_enrichment_logic(state)\n",
        "+            )\n",
        "+\n",
        "+        # Use retry mechanism\n",
        "+        enriched_data = await default_retry.execute(safe_llm_call)\n",
        "+\n",
        "+        state['enriched_findings'] = enriched_data\n",
        "+        state.setdefault('metrics', {})['reliable_enrich_duration'] = time.time() - start_time\n",
        "+\n",
        "+    except Exception as e:\n",
        "+        await error_handler.handle_error(e, {\n",
        "+            'operation': 'enrich_findings',\n",
        "+            'stage': 'reliable_enrich',\n",
        "+            'state': state\n",
        "+        })\n",
        "+\n",
        "+        # Fallback: use original findings\n",
        "+        if 'enriched_findings' not in state:\n",
        "+            state['enriched_findings'] = state.get('raw_findings', [])\n",
        "+\n",
        "+    return state\n",
        "+\n",
        "+async def _perform_enrichment_logic(state: GraphState) -> List[Dict[str, Any]]:\n",
        "+    \"\"\"Core enrichment logic (placeholder - integrate with existing enrichment).\"\"\"\n",
        "+    # This would integrate with existing enrichment logic\n",
        "+    findings = state.get('raw_findings', [])\n",
        "+    # Add reliability metadata\n",
        "+    for finding in findings:\n",
        "+        finding['reliability_checked'] = True\n",
        "+        finding['enrichment_timestamp'] = datetime.now().isoformat()\n",
        "+\n",
        "+    return findings\n",
        "+\n",
        "+async def reliable_correlate_findings(state: GraphState) -> GraphState:\n",
        "+    \"\"\"Reliability-enhanced correlation with error recovery.\"\"\"\n",
        "+    start_time = time.time()\n",
        "+    state['current_stage'] = 'reliable_correlate'\n",
        "+\n",
        "+    try:\n",
        "+        # Health check before processing\n",
        "+        health_status = await health_monitor.check_all()\n",
        "+        if not health_status.get('correlation_service', True):\n",
        "+            logger.warning(\"Correlation service unhealthy, using simplified correlation\")\n",
        "+\n",
        "+        # Use circuit breaker and retry\n",
        "+        async def safe_correlation():\n",
        "+            return await _perform_correlation_logic(state)\n",
        "+\n",
        "+        correlated_data = await default_retry.execute(safe_correlation)\n",
        "+        state.update(correlated_data)\n",
        "+        state.setdefault('metrics', {})['reliable_correlate_duration'] = time.time() - start_time\n",
        "+\n",
        "+    except Exception as e:\n",
        "+        await error_handler.handle_error(e, {\n",
        "+            'operation': 'correlate_findings',\n",
        "+            'stage': 'reliable_correlate',\n",
        "+            'state': state\n",
        "+        })\n",
        "+\n",
        "+        # Fallback: minimal correlation\n",
        "+        if 'correlated_findings' not in state:\n",
        "+            state['correlated_findings'] = state.get('enriched_findings', [])\n",
        "+\n",
        "+    return state\n",
        "+\n",
        "+async def _perform_correlation_logic(state: GraphState) -> Dict[str, Any]:\n",
        "+    \"\"\"Core correlation logic (placeholder - integrate with existing correlation).\"\"\"\n",
        "+    # This would integrate with existing correlation logic\n",
        "+    findings = state.get('enriched_findings', [])\n",
        "+\n",
        "+    return {\n",
        "+        'correlated_findings': findings,\n",
        "+        'correlations': [],\n",
        "+        'correlation_metadata': {\n",
        "+            'reliability_checked': True,\n",
        "+            'correlation_timestamp': datetime.now().isoformat()\n",
        "+        }\n",
        "+    }\n",
        "+\n",
        "+async def reliable_summarize_state(state: GraphState) -> GraphState:\n",
        "+    \"\"\"Reliability-enhanced summarization with comprehensive error handling.\"\"\"\n",
        "+    start_time = time.time()\n",
        "+    state['current_stage'] = 'reliable_summarize'\n",
        "+\n",
        "+    try:\n",
        "+        # Multiple fallback strategies\n",
        "+        summary = None\n",
        "+\n",
        "+        # Try primary summarization\n",
        "+        try:\n",
        "+            summary = await fast_retry.execute(\n",
        "+                lambda: llm_circuit_breaker.call(_perform_summarization_logic, state)\n",
        "+            )\n",
        "+        except Exception as primary_error:\n",
        "+            logger.warning(f\"Primary summarization failed: {primary_error}\")\n",
        "+\n",
        "+            # Try fallback summarization\n",
        "+            try:\n",
        "+                summary = await _fallback_summarization(state)\n",
        "+            except Exception as fallback_error:\n",
        "+                logger.error(f\"Fallback summarization also failed: {fallback_error}\")\n",
        "+                summary = _emergency_summary(state)\n",
        "+\n",
        "+        state['summary'] = summary\n",
        "+        state.setdefault('metrics', {})['reliable_summarize_duration'] = time.time() - start_time\n",
        "+\n",
        "+    except Exception as e:\n",
        "+        await error_handler.handle_error(e, {\n",
        "+            'operation': 'summarize_state',\n",
        "+            'stage': 'reliable_summarize',\n",
        "+            'state': state\n",
        "+        })\n",
        "+\n",
        "+        # Emergency fallback\n",
        "+        if 'summary' not in state:\n",
        "+            state['summary'] = _emergency_summary(state)\n",
        "+\n",
        "+    return state\n",
        "+\n",
        "+async def _perform_summarization_logic(state: GraphState) -> Dict[str, Any]:\n",
        "+    \"\"\"Core summarization logic (placeholder - integrate with existing summarization).\"\"\"\n",
        "+    # This would integrate with existing summarization logic\n",
        "+    return {\n",
        "+        'summary_text': 'Analysis completed with reliability enhancements',\n",
        "+        'confidence_score': 0.85,\n",
        "+        'reliability_checked': True,\n",
        "+        'summary_timestamp': datetime.now().isoformat()\n",
        "+    }\n",
        "+\n",
        "+async def _fallback_summarization(state: GraphState) -> Dict[str, Any]:\n",
        "+    \"\"\"Fallback summarization when primary fails.\"\"\"\n",
        "+    findings_count = len(state.get('correlated_findings', []))\n",
        "+    return {\n",
        "+        'summary_text': f'Fallback summary: Found {findings_count} items for analysis',\n",
        "+        'confidence_score': 0.6,\n",
        "+        'fallback_mode': True,\n",
        "+        'summary_timestamp': datetime.now().isoformat()\n",
        "+    }\n",
        "+\n",
        "+def _emergency_summary(state: GraphState) -> Dict[str, Any]:\n",
        "+    \"\"\"Emergency summary when all else fails.\"\"\"\n",
        "+    return {\n",
        "+        'summary_text': 'Analysis completed (emergency mode)',\n",
        "+        'confidence_score': 0.0,\n",
        "+        'emergency_mode': True,\n",
        "+        'summary_timestamp': datetime.now().isoformat()\n",
        "+    }\n",
        "+\n",
        "+# Health check functions\n",
        "+async def check_llm_service() -> bool:\n",
        "+    \"\"\"Check if LLM service is healthy.\"\"\"\n",
        "+    try:\n",
        "+        # Placeholder - implement actual LLM health check\n",
        "+        return llm_circuit_breaker.state != CircuitBreakerState.OPEN\n",
        "+    except Exception:\n",
        "+        return False\n",
        "+\n",
        "+async def check_database() -> bool:\n",
        "+    \"\"\"Check if database is healthy.\"\"\"\n",
        "+    try:\n",
        "+        # Placeholder - implement actual database health check\n",
        "+        return db_circuit_breaker.state != CircuitBreakerState.OPEN\n",
        "+    except Exception:\n",
        "+        return False\n",
        "+\n",
        "+async def check_external_apis() -> bool:\n",
        "+    \"\"\"Check if external APIs are healthy.\"\"\"\n",
        "+    try:\n",
        "+        # Placeholder - implement actual API health check\n",
        "+        return external_api_circuit_breaker.state != CircuitBreakerState.OPEN\n",
        "+    except Exception:\n",
        "+        return False\n",
        "+\n",
        "+# Register health checks\n",
        "+health_monitor.register_check(\"llm_service\", check_llm_service, interval=30)\n",
        "+health_monitor.register_check(\"database\", check_database, interval=60)\n",
        "+health_monitor.register_check(\"external_apis\", check_external_apis, interval=120)\n",
        "+\n",
        "+# Initialization and cleanup\n",
        "+async def initialize_reliability():\n",
        "+    \"\"\"Initialize reliability components.\"\"\"\n",
        "+    logger.info(\"Initializing reliability enhancements...\")\n",
        "+    # Perform initial health checks\n",
        "+    await health_monitor.check_all()\n",
        "+\n",
        "+async def shutdown_reliability():\n",
        "+    \"\"\"Shutdown reliability components.\"\"\"\n",
        "+    logger.info(\"Shutting down reliability enhancements...\")\n",
        "+    # Cleanup if needed\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "agent/graph_nodes_scalability.py",
      "status": "added",
      "additions": 342,
      "deletions": 0,
      "patch": "@@ -0,0 +1,342 @@\n+from __future__ import annotations\n+\"\"\"Scalability enhancements for LangGraph processing.\n+\n+This module provides horizontal scaling capabilities and distributed processing\n+patterns to handle increased load efficiently.\n+\"\"\"\n+\n+from typing import Any, List, Dict, Optional, Callable, AsyncIterator\n+import asyncio\n+import logging\n+import os\n+from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n+from dataclasses import dataclass, field\n+import multiprocessing\n+import threading\n+from contextlib import asynccontextmanager\n+\n+from .graph import GraphState\n+from .graph_nodes_performance import PerformanceConfig, perf_config\n+\n+logger = logging.getLogger(__name__)\n+\n+# Scalability configuration\n+@dataclass\n+class ScalabilityConfig:\n+    \"\"\"Configuration for scalability enhancements.\"\"\"\n+    max_workers: int = multiprocessing.cpu_count()\n+    process_pool_workers: int = max(2, multiprocessing.cpu_count() // 2)\n+    thread_pool_workers: int = min(10, multiprocessing.cpu_count() * 2)\n+    chunk_size: int = 1000\n+    max_concurrent_tasks: int = 50\n+    distributed_mode: bool = False\n+    worker_timeout: int = 300\n+    load_balancing_enabled: bool = True\n+\n+# Global scalability config\n+scale_config = ScalabilityConfig()\n+\n+# Process pool for CPU-intensive tasks\n+process_pool: Optional[ProcessPoolExecutor] = None\n+thread_pool: Optional[ThreadPoolExecutor] = None\n+\n+@asynccontextmanager\n+async def get_process_pool():\n+    \"\"\"Context manager for process pool.\"\"\"\n+    global process_pool\n+    if process_pool is None:\n+        process_pool = ProcessPoolExecutor(max_workers=scale_config.process_pool_workers)\n+    try:\n+        yield process_pool\n+    finally:\n+        pass  # Keep pool alive for reuse\n+\n+@asynccontextmanager\n+async def get_thread_pool():\n+    \"\"\"Context manager for thread pool.\"\"\"\n+    global thread_pool\n+    if thread_pool is None:\n+        thread_pool = ThreadPoolExecutor(max_workers=scale_config.thread_pool_workers)\n+    try:\n+        yield thread_pool\n+    finally:\n+        pass  # Keep pool alive for reuse\n+\n+class LoadBalancer:\n+    \"\"\"Load balancer for distributing work across workers.\"\"\"\n+\n+    def __init__(self, max_workers: Optional[int] = None):\n+        self.max_workers = max_workers or scale_config.max_workers\n+        self.worker_loads: Dict[str, int] = {}\n+        self.worker_queues: Dict[str, asyncio.Queue] = {}\n+        self._lock = asyncio.Lock()\n+\n+    async def register_worker(self, worker_id: str):\n+        \"\"\"Register a new worker.\"\"\"\n+        async with self._lock:\n+            self.worker_loads[worker_id] = 0\n+            self.worker_queues[worker_id] = asyncio.Queue()\n+\n+    async def get_least_loaded_worker(self) -> Optional[str]:\n+        \"\"\"Get the worker with the least load.\"\"\"\n+        async with self._lock:\n+            if not self.worker_loads:\n+                return None\n+            return min(self.worker_loads.keys(), key=lambda k: self.worker_loads[k])\n+\n+    async def assign_task(self, worker_id: str, task: Any):\n+        \"\"\"Assign a task to a worker.\"\"\"\n+        async with self._lock:\n+            if worker_id in self.worker_queues:\n+                await self.worker_queues[worker_id].put(task)\n+                self.worker_loads[worker_id] += 1\n+\n+    async def complete_task(self, worker_id: str):\n+        \"\"\"Mark a task as completed for a worker.\"\"\"\n+        async with self._lock:\n+            if worker_id in self.worker_loads:\n+                self.worker_loads[worker_id] = max(0, self.worker_loads[worker_id] - 1)\n+\n+# Global load balancer\n+load_balancer = LoadBalancer()\n+\n+async def distribute_workload(items: List[Any], processor_func: Callable, chunk_size: Optional[int] = None) -> List[Any]:\n+    \"\"\"Distribute workload across available workers.\"\"\"\n+    if chunk_size is None:\n+        chunk_size = scale_config.chunk_size\n+\n+    results = []\n+    semaphore = asyncio.Semaphore(scale_config.max_concurrent_tasks)\n+\n+    async def process_chunk(chunk: List[Any]) -> List[Any]:\n+        \"\"\"Process a chunk of items.\"\"\"\n+        async with semaphore:\n+            try:\n+                if asyncio.iscoroutinefunction(processor_func):\n+                    return await processor_func(chunk)\n+                else:\n+                    # Use thread pool for sync functions\n+                    async with get_thread_pool() as pool:\n+                        loop = asyncio.get_event_loop()\n+                        return await loop.run_in_executor(pool, processor_func, chunk)\n+            except Exception as e:\n+                logger.error(f\"Error processing chunk: {e}\")\n+                return []\n+\n+    # Create tasks for each chunk\n+    tasks = []\n+    for i in range(0, len(items), chunk_size):\n+        chunk = items[i:i + chunk_size]\n+        task = asyncio.create_task(process_chunk(chunk))\n+        tasks.append(task)\n+\n+    # Wait for all tasks to complete\n+    completed_results = await asyncio.gather(*tasks, return_exceptions=True)\n+\n+    # Collect results\n+    for result in completed_results:\n+        if isinstance(result, Exception):\n+            logger.error(f\"Task failed: {result}\")\n+        elif isinstance(result, list):\n+            results.extend(result)\n+\n+    return results\n+\n+async def parallel_findings_processing(state: GraphState, findings: List[Dict[str, Any]]) -> GraphState:\n+    \"\"\"Process findings in parallel across multiple workers.\"\"\"\n+    start_time = time.time()\n+    state['current_stage'] = 'parallel_processing'\n+\n+    try:\n+        # Split findings into chunks for parallel processing\n+        chunk_size = min(len(findings) // scale_config.max_workers + 1, scale_config.chunk_size)\n+\n+        async def process_findings_chunk(chunk: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n+            \"\"\"Process a chunk of findings.\"\"\"\n+            processed = []\n+            for finding in chunk:\n+                try:\n+                    # Apply enrichment and correlation logic\n+                    processed_finding = finding.copy()\n+\n+                    # Add processing metadata\n+                    processed_finding['processed_by'] = f\"worker_{hash(str(finding)) % scale_config.max_workers}\"\n+                    processed_finding['processing_timestamp'] = datetime.now().isoformat()\n+\n+                    processed.append(processed_finding)\n+\n+                except Exception as e:\n+                    logger.warning(f\"Failed to process finding {finding.get('id')}: {e}\")\n+                    processed.append(finding)  # Return original if processing fails\n+\n+            return processed\n+\n+        # Distribute processing across workers\n+        processed_findings = await distribute_workload(findings, process_findings_chunk, chunk_size)\n+\n+        state['enriched_findings'] = processed_findings\n+\n+        # Update metrics\n+        state.setdefault('metrics', {})['parallel_processing_duration'] = time.time() - start_time\n+        state.setdefault('metrics', {})['chunks_processed'] = len(range(0, len(findings), chunk_size))\n+        state.setdefault('metrics', {})['workers_utilized'] = scale_config.max_workers\n+\n+    except Exception as e:\n+        logger.error(f\"Parallel processing failed: {e}\")\n+        _append_warning(state, 'graph', 'parallel_processing', str(e))\n+        if 'enriched_findings' not in state:\n+            state['enriched_findings'] = findings\n+\n+    return state\n+\n+async def horizontal_scaling_router(state: GraphState) -> str:\n+    \"\"\"Route to appropriate processing strategy based on load.\"\"\"\n+    findings_count = len(state.get('raw_findings', []))\n+    current_load = len(asyncio.all_tasks())\n+\n+    # High load conditions\n+    if findings_count > 10000 or current_load > scale_config.max_concurrent_tasks:\n+        return \"parallel_processing\"\n+\n+    # Medium load - use batch processing\n+    elif findings_count > 1000:\n+        return \"batch_processing\"\n+\n+    # Low load - use standard processing\n+    else:\n+        return \"standard_processing\"\n+\n+async def adaptive_batch_sizing(state: GraphState) -> int:\n+    \"\"\"Dynamically adjust batch size based on system load and data characteristics.\"\"\"\n+    findings_count = len(state.get('raw_findings', []))\n+    system_load = len(asyncio.all_tasks())\n+\n+    # Base batch size\n+    base_size = perf_config.batch_size\n+\n+    # Adjust based on system load\n+    if system_load > scale_config.max_concurrent_tasks * 0.8:\n+        # High load - smaller batches\n+        adjusted_size = max(10, base_size // 4)\n+    elif system_load > scale_config.max_concurrent_tasks * 0.5:\n+        # Medium load - slightly smaller batches\n+        adjusted_size = max(25, base_size // 2)\n+    else:\n+        # Low load - can use larger batches\n+        adjusted_size = min(500, base_size * 2)\n+\n+    # Adjust based on data size\n+    if findings_count > 50000:\n+        adjusted_size = min(adjusted_size, 200)  # Smaller batches for very large datasets\n+    elif findings_count < 100:\n+        adjusted_size = max(adjusted_size, 50)  # Larger batches for small datasets\n+\n+    return adjusted_size\n+\n+class WorkerPool:\n+    \"\"\"Pool of worker processes for distributed processing.\"\"\"\n+\n+    def __init__(self, num_workers: Optional[int] = None):\n+        self.num_workers = num_workers or scale_config.max_workers\n+        self.workers: List[multiprocessing.Process] = []\n+        self.task_queue = multiprocessing.Queue()\n+        self.result_queue = multiprocessing.Queue()\n+        self._running = False\n+\n+    def start_workers(self):\n+        \"\"\"Start worker processes.\"\"\"\n+        self._running = True\n+        for i in range(self.num_workers):\n+            worker = multiprocessing.Process(\n+                target=self._worker_process,\n+                args=(i, self.task_queue, self.result_queue)\n+            )\n+            worker.start()\n+            self.workers.append(worker)\n+\n+    def stop_workers(self):\n+        \"\"\"Stop all worker processes.\"\"\"\n+        self._running = False\n+        for _ in self.workers:\n+            self.task_queue.put(None)  # Poison pill\n+\n+        for worker in self.workers:\n+            worker.join(timeout=5)\n+            if worker.is_alive():\n+                worker.terminate()\n+\n+    def submit_task(self, task: Any):\n+        \"\"\"Submit a task to the worker pool.\"\"\"\n+        if self._running:\n+            self.task_queue.put(task)\n+\n+    def get_result(self, timeout: float = 1.0) -> Optional[Any]:\n+        \"\"\"Get a result from the worker pool.\"\"\"\n+        try:\n+            return self.result_queue.get(timeout=timeout)\n+        except:\n+            return None\n+\n+    @staticmethod\n+    def _worker_process(worker_id: int, task_queue: multiprocessing.Queue, result_queue: multiprocessing.Queue):\n+        \"\"\"Worker process function.\"\"\"\n+        logger.info(f\"Worker {worker_id} started\")\n+\n+        while True:\n+            try:\n+                task = task_queue.get(timeout=1)\n+                if task is None:  # Poison pill\n+                    break\n+\n+                # Process the task (placeholder - implement specific processing logic)\n+                result = f\"Processed by worker {worker_id}: {task}\"\n+                result_queue.put(result)\n+\n+            except Exception as e:\n+                logger.error(f\"Worker {worker_id} error: {e}\")\n+                break\n+\n+        logger.info(f\"Worker {worker_id} stopped\")\n+\n+# Global worker pool\n+worker_pool: Optional[WorkerPool] = None\n+\n+async def initialize_scalability():\n+    \"\"\"Initialize scalability components.\"\"\"\n+    global worker_pool\n+\n+    if scale_config.distributed_mode and worker_pool is None:\n+        worker_pool = WorkerPool()\n+        worker_pool.start_workers()\n+        logger.info(f\"Started {scale_config.max_workers} worker processes\")\n+\n+async def shutdown_scalability():\n+    \"\"\"Shutdown scalability components.\"\"\"\n+    global worker_pool\n+\n+    if worker_pool:\n+        worker_pool.stop_workers()\n+        worker_pool = None\n+        logger.info(\"Stopped worker processes\")\n+\n+# Import time for metrics\n+import time\n+from datetime import datetime\n+\n+def _append_warning(state: GraphState, module: str, stage: str, error: str, hint: str | None = None):\n+    \"\"\"Enhanced warning appender with performance tracking.\"\"\"\n+    wl = state.setdefault('warnings', [])\n+    error_entry = {\n+        'module': module,\n+        'stage': stage,\n+        'error': error,\n+        'hint': hint,\n+        'timestamp': datetime.now().isoformat(),\n+        'session_id': state.get('session_id', 'unknown'),\n+        'performance_impact': 'low'  # Can be upgraded based on error type\n+    }\n+    wl.append(error_entry)\n+\n+    # Track in errors list for better visibility\n+    errors = state.setdefault('errors', [])\n+    errors.append(error_entry)\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -0,0 +1,342 @@\n",
        "+from __future__ import annotations\n",
        "+\"\"\"Scalability enhancements for LangGraph processing.\n",
        "+\n",
        "+This module provides horizontal scaling capabilities and distributed processing\n",
        "+patterns to handle increased load efficiently.\n",
        "+\"\"\"\n",
        "+\n",
        "+from typing import Any, List, Dict, Optional, Callable, AsyncIterator\n",
        "+import asyncio\n",
        "+import logging\n",
        "+import os\n",
        "+from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
        "+from dataclasses import dataclass, field\n",
        "+import multiprocessing\n",
        "+import threading\n",
        "+from contextlib import asynccontextmanager\n",
        "+\n",
        "+from .graph import GraphState\n",
        "+from .graph_nodes_performance import PerformanceConfig, perf_config\n",
        "+\n",
        "+logger = logging.getLogger(__name__)\n",
        "+\n",
        "+# Scalability configuration\n",
        "+@dataclass\n",
        "+class ScalabilityConfig:\n",
        "+    \"\"\"Configuration for scalability enhancements.\"\"\"\n",
        "+    max_workers: int = multiprocessing.cpu_count()\n",
        "+    process_pool_workers: int = max(2, multiprocessing.cpu_count() // 2)\n",
        "+    thread_pool_workers: int = min(10, multiprocessing.cpu_count() * 2)\n",
        "+    chunk_size: int = 1000\n",
        "+    max_concurrent_tasks: int = 50\n",
        "+    distributed_mode: bool = False\n",
        "+    worker_timeout: int = 300\n",
        "+    load_balancing_enabled: bool = True\n",
        "+\n",
        "+# Global scalability config\n",
        "+scale_config = ScalabilityConfig()\n",
        "+\n",
        "+# Process pool for CPU-intensive tasks\n",
        "+process_pool: Optional[ProcessPoolExecutor] = None\n",
        "+thread_pool: Optional[ThreadPoolExecutor] = None\n",
        "+\n",
        "+@asynccontextmanager\n",
        "+async def get_process_pool():\n",
        "+    \"\"\"Context manager for process pool.\"\"\"\n",
        "+    global process_pool\n",
        "+    if process_pool is None:\n",
        "+        process_pool = ProcessPoolExecutor(max_workers=scale_config.process_pool_workers)\n",
        "+    try:\n",
        "+        yield process_pool\n",
        "+    finally:\n",
        "+        pass  # Keep pool alive for reuse\n",
        "+\n",
        "+@asynccontextmanager\n",
        "+async def get_thread_pool():\n",
        "+    \"\"\"Context manager for thread pool.\"\"\"\n",
        "+    global thread_pool\n",
        "+    if thread_pool is None:\n",
        "+        thread_pool = ThreadPoolExecutor(max_workers=scale_config.thread_pool_workers)\n",
        "+    try:\n",
        "+        yield thread_pool\n",
        "+    finally:\n",
        "+        pass  # Keep pool alive for reuse\n",
        "+\n",
        "+class LoadBalancer:\n",
        "+    \"\"\"Load balancer for distributing work across workers.\"\"\"\n",
        "+\n",
        "+    def __init__(self, max_workers: Optional[int] = None):\n",
        "+        self.max_workers = max_workers or scale_config.max_workers\n",
        "+        self.worker_loads: Dict[str, int] = {}\n",
        "+        self.worker_queues: Dict[str, asyncio.Queue] = {}\n",
        "+        self._lock = asyncio.Lock()\n",
        "+\n",
        "+    async def register_worker(self, worker_id: str):\n",
        "+        \"\"\"Register a new worker.\"\"\"\n",
        "+        async with self._lock:\n",
        "+            self.worker_loads[worker_id] = 0\n",
        "+            self.worker_queues[worker_id] = asyncio.Queue()\n",
        "+\n",
        "+    async def get_least_loaded_worker(self) -> Optional[str]:\n",
        "+        \"\"\"Get the worker with the least load.\"\"\"\n",
        "+        async with self._lock:\n",
        "+            if not self.worker_loads:\n",
        "+                return None\n",
        "+            return min(self.worker_loads.keys(), key=lambda k: self.worker_loads[k])\n",
        "+\n",
        "+    async def assign_task(self, worker_id: str, task: Any):\n",
        "+        \"\"\"Assign a task to a worker.\"\"\"\n",
        "+        async with self._lock:\n",
        "+            if worker_id in self.worker_queues:\n",
        "+                await self.worker_queues[worker_id].put(task)\n",
        "+                self.worker_loads[worker_id] += 1\n",
        "+\n",
        "+    async def complete_task(self, worker_id: str):\n",
        "+        \"\"\"Mark a task as completed for a worker.\"\"\"\n",
        "+        async with self._lock:\n",
        "+            if worker_id in self.worker_loads:\n",
        "+                self.worker_loads[worker_id] = max(0, self.worker_loads[worker_id] - 1)\n",
        "+\n",
        "+# Global load balancer\n",
        "+load_balancer = LoadBalancer()\n",
        "+\n",
        "+async def distribute_workload(items: List[Any], processor_func: Callable, chunk_size: Optional[int] = None) -> List[Any]:\n",
        "+    \"\"\"Distribute workload across available workers.\"\"\"\n",
        "+    if chunk_size is None:\n",
        "+        chunk_size = scale_config.chunk_size\n",
        "+\n",
        "+    results = []\n",
        "+    semaphore = asyncio.Semaphore(scale_config.max_concurrent_tasks)\n",
        "+\n",
        "+    async def process_chunk(chunk: List[Any]) -> List[Any]:\n",
        "+        \"\"\"Process a chunk of items.\"\"\"\n",
        "+        async with semaphore:\n",
        "+            try:\n",
        "+                if asyncio.iscoroutinefunction(processor_func):\n",
        "+                    return await processor_func(chunk)\n",
        "+                else:\n",
        "+                    # Use thread pool for sync functions\n",
        "+                    async with get_thread_pool() as pool:\n",
        "+                        loop = asyncio.get_event_loop()\n",
        "+                        return await loop.run_in_executor(pool, processor_func, chunk)\n",
        "+            except Exception as e:\n",
        "+                logger.error(f\"Error processing chunk: {e}\")\n",
        "+                return []\n",
        "+\n",
        "+    # Create tasks for each chunk\n",
        "+    tasks = []\n",
        "+    for i in range(0, len(items), chunk_size):\n",
        "+        chunk = items[i:i + chunk_size]\n",
        "+        task = asyncio.create_task(process_chunk(chunk))\n",
        "+        tasks.append(task)\n",
        "+\n",
        "+    # Wait for all tasks to complete\n",
        "+    completed_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "+\n",
        "+    # Collect results\n",
        "+    for result in completed_results:\n",
        "+        if isinstance(result, Exception):\n",
        "+            logger.error(f\"Task failed: {result}\")\n",
        "+        elif isinstance(result, list):\n",
        "+            results.extend(result)\n",
        "+\n",
        "+    return results\n",
        "+\n",
        "+async def parallel_findings_processing(state: GraphState, findings: List[Dict[str, Any]]) -> GraphState:\n",
        "+    \"\"\"Process findings in parallel across multiple workers.\"\"\"\n",
        "+    start_time = time.time()\n",
        "+    state['current_stage'] = 'parallel_processing'\n",
        "+\n",
        "+    try:\n",
        "+        # Split findings into chunks for parallel processing\n",
        "+        chunk_size = min(len(findings) // scale_config.max_workers + 1, scale_config.chunk_size)\n",
        "+\n",
        "+        async def process_findings_chunk(chunk: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "+            \"\"\"Process a chunk of findings.\"\"\"\n",
        "+            processed = []\n",
        "+            for finding in chunk:\n",
        "+                try:\n",
        "+                    # Apply enrichment and correlation logic\n",
        "+                    processed_finding = finding.copy()\n",
        "+\n",
        "+                    # Add processing metadata\n",
        "+                    processed_finding['processed_by'] = f\"worker_{hash(str(finding)) % scale_config.max_workers}\"\n",
        "+                    processed_finding['processing_timestamp'] = datetime.now().isoformat()\n",
        "+\n",
        "+                    processed.append(processed_finding)\n",
        "+\n",
        "+                except Exception as e:\n",
        "+                    logger.warning(f\"Failed to process finding {finding.get('id')}: {e}\")\n",
        "+                    processed.append(finding)  # Return original if processing fails\n",
        "+\n",
        "+            return processed\n",
        "+\n",
        "+        # Distribute processing across workers\n",
        "+        processed_findings = await distribute_workload(findings, process_findings_chunk, chunk_size)\n",
        "+\n",
        "+        state['enriched_findings'] = processed_findings\n",
        "+\n",
        "+        # Update metrics\n",
        "+        state.setdefault('metrics', {})['parallel_processing_duration'] = time.time() - start_time\n",
        "+        state.setdefault('metrics', {})['chunks_processed'] = len(range(0, len(findings), chunk_size))\n",
        "+        state.setdefault('metrics', {})['workers_utilized'] = scale_config.max_workers\n",
        "+\n",
        "+    except Exception as e:\n",
        "+        logger.error(f\"Parallel processing failed: {e}\")\n",
        "+        _append_warning(state, 'graph', 'parallel_processing', str(e))\n",
        "+        if 'enriched_findings' not in state:\n",
        "+            state['enriched_findings'] = findings\n",
        "+\n",
        "+    return state\n",
        "+\n",
        "+async def horizontal_scaling_router(state: GraphState) -> str:\n",
        "+    \"\"\"Route to appropriate processing strategy based on load.\"\"\"\n",
        "+    findings_count = len(state.get('raw_findings', []))\n",
        "+    current_load = len(asyncio.all_tasks())\n",
        "+\n",
        "+    # High load conditions\n",
        "+    if findings_count > 10000 or current_load > scale_config.max_concurrent_tasks:\n",
        "+        return \"parallel_processing\"\n",
        "+\n",
        "+    # Medium load - use batch processing\n",
        "+    elif findings_count > 1000:\n",
        "+        return \"batch_processing\"\n",
        "+\n",
        "+    # Low load - use standard processing\n",
        "+    else:\n",
        "+        return \"standard_processing\"\n",
        "+\n",
        "+async def adaptive_batch_sizing(state: GraphState) -> int:\n",
        "+    \"\"\"Dynamically adjust batch size based on system load and data characteristics.\"\"\"\n",
        "+    findings_count = len(state.get('raw_findings', []))\n",
        "+    system_load = len(asyncio.all_tasks())\n",
        "+\n",
        "+    # Base batch size\n",
        "+    base_size = perf_config.batch_size\n",
        "+\n",
        "+    # Adjust based on system load\n",
        "+    if system_load > scale_config.max_concurrent_tasks * 0.8:\n",
        "+        # High load - smaller batches\n",
        "+        adjusted_size = max(10, base_size // 4)\n",
        "+    elif system_load > scale_config.max_concurrent_tasks * 0.5:\n",
        "+        # Medium load - slightly smaller batches\n",
        "+        adjusted_size = max(25, base_size // 2)\n",
        "+    else:\n",
        "+        # Low load - can use larger batches\n",
        "+        adjusted_size = min(500, base_size * 2)\n",
        "+\n",
        "+    # Adjust based on data size\n",
        "+    if findings_count > 50000:\n",
        "+        adjusted_size = min(adjusted_size, 200)  # Smaller batches for very large datasets\n",
        "+    elif findings_count < 100:\n",
        "+        adjusted_size = max(adjusted_size, 50)  # Larger batches for small datasets\n",
        "+\n",
        "+    return adjusted_size\n",
        "+\n",
        "+class WorkerPool:\n",
        "+    \"\"\"Pool of worker processes for distributed processing.\"\"\"\n",
        "+\n",
        "+    def __init__(self, num_workers: Optional[int] = None):\n",
        "+        self.num_workers = num_workers or scale_config.max_workers\n",
        "+        self.workers: List[multiprocessing.Process] = []\n",
        "+        self.task_queue = multiprocessing.Queue()\n",
        "+        self.result_queue = multiprocessing.Queue()\n",
        "+        self._running = False\n",
        "+\n",
        "+    def start_workers(self):\n",
        "+        \"\"\"Start worker processes.\"\"\"\n",
        "+        self._running = True\n",
        "+        for i in range(self.num_workers):\n",
        "+            worker = multiprocessing.Process(\n",
        "+                target=self._worker_process,\n",
        "+                args=(i, self.task_queue, self.result_queue)\n",
        "+            )\n",
        "+            worker.start()\n",
        "+            self.workers.append(worker)\n",
        "+\n",
        "+    def stop_workers(self):\n",
        "+        \"\"\"Stop all worker processes.\"\"\"\n",
        "+        self._running = False\n",
        "+        for _ in self.workers:\n",
        "+            self.task_queue.put(None)  # Poison pill\n",
        "+\n",
        "+        for worker in self.workers:\n",
        "+            worker.join(timeout=5)\n",
        "+            if worker.is_alive():\n",
        "+                worker.terminate()\n",
        "+\n",
        "+    def submit_task(self, task: Any):\n",
        "+        \"\"\"Submit a task to the worker pool.\"\"\"\n",
        "+        if self._running:\n",
        "+            self.task_queue.put(task)\n",
        "+\n",
        "+    def get_result(self, timeout: float = 1.0) -> Optional[Any]:\n",
        "+        \"\"\"Get a result from the worker pool.\"\"\"\n",
        "+        try:\n",
        "+            return self.result_queue.get(timeout=timeout)\n",
        "+        except:\n",
        "+            return None\n",
        "+\n",
        "+    @staticmethod\n",
        "+    def _worker_process(worker_id: int, task_queue: multiprocessing.Queue, result_queue: multiprocessing.Queue):\n",
        "+        \"\"\"Worker process function.\"\"\"\n",
        "+        logger.info(f\"Worker {worker_id} started\")\n",
        "+\n",
        "+        while True:\n",
        "+            try:\n",
        "+                task = task_queue.get(timeout=1)\n",
        "+                if task is None:  # Poison pill\n",
        "+                    break\n",
        "+\n",
        "+                # Process the task (placeholder - implement specific processing logic)\n",
        "+                result = f\"Processed by worker {worker_id}: {task}\"\n",
        "+                result_queue.put(result)\n",
        "+\n",
        "+            except Exception as e:\n",
        "+                logger.error(f\"Worker {worker_id} error: {e}\")\n",
        "+                break\n",
        "+\n",
        "+        logger.info(f\"Worker {worker_id} stopped\")\n",
        "+\n",
        "+# Global worker pool\n",
        "+worker_pool: Optional[WorkerPool] = None\n",
        "+\n",
        "+async def initialize_scalability():\n",
        "+    \"\"\"Initialize scalability components.\"\"\"\n",
        "+    global worker_pool\n",
        "+\n",
        "+    if scale_config.distributed_mode and worker_pool is None:\n",
        "+        worker_pool = WorkerPool()\n",
        "+        worker_pool.start_workers()\n",
        "+        logger.info(f\"Started {scale_config.max_workers} worker processes\")\n",
        "+\n",
        "+async def shutdown_scalability():\n",
        "+    \"\"\"Shutdown scalability components.\"\"\"\n",
        "+    global worker_pool\n",
        "+\n",
        "+    if worker_pool:\n",
        "+        worker_pool.stop_workers()\n",
        "+        worker_pool = None\n",
        "+        logger.info(\"Stopped worker processes\")\n",
        "+\n",
        "+# Import time for metrics\n",
        "+import time\n",
        "+from datetime import datetime\n",
        "+\n",
        "+def _append_warning(state: GraphState, module: str, stage: str, error: str, hint: str | None = None):\n",
        "+    \"\"\"Enhanced warning appender with performance tracking.\"\"\"\n",
        "+    wl = state.setdefault('warnings', [])\n",
        "+    error_entry = {\n",
        "+        'module': module,\n",
        "+        'stage': stage,\n",
        "+        'error': error,\n",
        "+        'hint': hint,\n",
        "+        'timestamp': datetime.now().isoformat(),\n",
        "+        'session_id': state.get('session_id', 'unknown'),\n",
        "+        'performance_impact': 'low'  # Can be upgraded based on error type\n",
        "+    }\n",
        "+    wl.append(error_entry)\n",
        "+\n",
        "+    # Track in errors list for better visibility\n",
        "+    errors = state.setdefault('errors', [])\n",
        "+    errors.append(error_entry)\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "agent/legacy/knowledge.py",
      "status": "added",
      "additions": 4,
      "deletions": 0,
      "patch": "@@ -0,0 +1,4 @@\n+\"\"\"\n+Legacy knowledge module - imports functions from main knowledge directory for backward compatibility.\n+\"\"\"\n+from ..knowledge import apply_external_knowledge\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -0,0 +1,4 @@\n",
        "+\"\"\"\n",
        "+Legacy knowledge module - imports functions from main knowledge directory for backward compatibility.\n",
        "+\"\"\"\n",
        "+from ..knowledge import apply_external_knowledge\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "agent/legacy/llm_provider.py",
      "status": "added",
      "additions": 4,
      "deletions": 0,
      "patch": "@@ -0,0 +1,4 @@\n+\"\"\"\n+Legacy llm_provider module - imports functions from main llm_provider directory for backward compatibility.\n+\"\"\"\n+from ..llm_provider import get_llm_provider\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -0,0 +1,4 @@\n",
        "+\"\"\"\n",
        "+Legacy llm_provider module - imports functions from main llm_provider directory for backward compatibility.\n",
        "+\"\"\"\n",
        "+from ..llm_provider import get_llm_provider\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "agent/legacy/models.py",
      "status": "added",
      "additions": 14,
      "deletions": 0,
      "patch": "@@ -0,0 +1,14 @@\n+\"\"\"\n+Legacy models module - imports models from main agent directory for backward compatibility.\n+\"\"\"\n+from ..models import (\n+    AgentState,\n+    Report,\n+    Finding,\n+    EnrichedOutput,\n+    ActionItem,\n+    ScannerResult,\n+    MultiHostCorrelation,\n+    Correlation,\n+    AgentWarning\n+)\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -0,0 +1,14 @@\n",
        "+\"\"\"\n",
        "+Legacy models module - imports models from main agent directory for backward compatibility.\n",
        "+\"\"\"\n",
        "+from ..models import (\n",
        "+    AgentState,\n",
        "+    Report,\n",
        "+    Finding,\n",
        "+    EnrichedOutput,\n",
        "+    ActionItem,\n",
        "+    ScannerResult,\n",
        "+    MultiHostCorrelation,\n",
        "+    Correlation,\n",
        "+    AgentWarning\n",
        "+)\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "agent/legacy/pipeline.py",
      "status": "added",
      "additions": 1265,
      "deletions": 0,
      "patch": "@@ -0,0 +1,1265 @@\n+from __future__ import annotations\n+import json, hashlib, os, uuid\n+import json as _json\n+from pathlib import Path\n+from typing import List\n+from .models import AgentState, Report, Finding, EnrichedOutput, ActionItem, ScannerResult, MultiHostCorrelation, Correlation, AgentWarning\n+from .knowledge import apply_external_knowledge\n+from .rules import Correlator, DEFAULT_RULES\n+from .reduction import reduce_all\n+from .llm_provider import get_llm_provider\n+from .data_governance import get_data_governor\n+from .baseline import BaselineStore\n+from .metrics import get_metrics_collector\n+from .canonicalize import canonicalize_enriched_output_dict\n+from .risk import compute_risk, load_persistent_weights\n+from .calibration import apply_probability\n+from .graph_analysis import annotate_and_summarize\n+from .endpoint_classification import classify as classify_host_role\n+from .executors import hash_binary, query_package_manager\n+from .integrity import sha256_file, verify_file\n+from .audit import log_stage, hash_text\n+import yaml\n+from .baseline import process_feature_vector\n+import yaml as _yaml\n+import uuid as _uuid\n+from .config import load_config\n+import concurrent.futures\n+\n+# -----------------\n+# Internal helpers (risk recomputation & error logging)\n+# -----------------\n+\n+def _recompute_finding_risk(f: Finding):\n+    \"\"\"Recompute risk fields after any risk_subscores mutation.\n+    Safe no-op if subscores absent; logs errors instead of raising.\"\"\"\n+    try:\n+        subs = getattr(f, 'risk_subscores', None)\n+        if not subs:\n+            return\n+        weights = load_persistent_weights()\n+        score, raw = compute_risk(subs, weights)\n+        f.risk_score = score\n+        f.risk_total = score\n+        subs[\"_raw_weighted_sum\"] = round(raw, 3)\n+        f.probability_actionable = apply_probability(raw)\n+    except (ValueError, TypeError) as e:  # expected computation issues\n+        try:\n+            log_stage('risk_recompute_error', error=str(e), type=type(e).__name__)\n+        except Exception:\n+            pass\n+    except Exception as e:  # unexpected\n+        try:\n+            log_stage('risk_recompute_error_unexpected', error=str(e), type=type(e).__name__)\n+        except Exception:\n+            pass\n+\n+def _log_error(stage: str, e: Exception, state: AgentState | None = None, module: str = 'pipeline', severity: str = 'warning', hint: str | None = None):\n+    if state is not None:\n+        try:\n+            state.agent_warnings.append(AgentWarning(module=module, stage=stage, error_type=type(e).__name__, message=str(e), severity=severity, hint=hint).model_dump())\n+        except Exception:\n+            pass\n+    try:\n+        log_stage(f'{stage}_error', error=str(e), type=type(e).__name__)\n+    except Exception:\n+        pass\n+\n+# Node functions (imperative; future step: convert to LangGraph graph)\n+\n+def load_report(state: AgentState, path: Path) -> AgentState:\n+    \"\"\"Securely load and parse the raw JSON report.\n+\n+    Hardening steps:\n+    1. Enforce maximum size (default 5 MB, override via AGENT_MAX_REPORT_MB env).\n+    2. Read bytes then decode strictly as UTF-8 (reject invalid sequences).\n+    3. Canonicalize newlines to '\\n' before JSON parsing to avoid platform variance.\n+    \"\"\"\n+    max_mb_env = os.environ.get('AGENT_MAX_REPORT_MB')\n+    try:\n+        max_mb = int(max_mb_env) if max_mb_env else 5\n+    except ValueError:\n+        max_mb = 5\n+    mc = get_metrics_collector()\n+    with mc.time_stage('load_report.read_bytes'):\n+        raw_bytes = Path(path).read_bytes()\n+    size_mb = len(raw_bytes) / (1024 * 1024)\n+    if size_mb > max_mb:\n+        raise ValueError(f\"Report size {size_mb:.2f} MB exceeds maximum size {max_mb} MB\")\n+    try:\n+        text = raw_bytes.decode('utf-8', errors='strict')\n+    except UnicodeDecodeError as e:\n+        raise ValueError(f\"Report is not valid UTF-8: {e}\") from e\n+    # Canonicalize newlines (CRLF, CR -> LF)\n+    if '\\r' in text:\n+        text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n+    try:\n+        with mc.time_stage('load_report.json_parse'):\n+            data = json.loads(text)\n+    except json.JSONDecodeError as e:\n+        raise ValueError(f\"Report JSON parse error: {e}\") from e\n+    state.raw_report = data\n+    # Normalize risk naming migration (base_severity_score -> risk_score) BEFORE schema validation.\n+    # The C++ core now emits base_severity_score only. Downstream Python pipeline still expects\n+    # risk_score. We inject risk_score where missing for backward compatibility and retain the\n+    # original field (mapped into Finding.base_severity_score by pydantic if present).\n+    try:\n+        results = data.get('results') if isinstance(data, dict) else None\n+        if isinstance(results, list):\n+            for sr in results:\n+                findings = sr.get('findings') if isinstance(sr, dict) else None\n+                if not isinstance(findings, list):\n+                    continue\n+                for f in findings:\n+                    if not isinstance(f, dict):\n+                        continue\n+                    # If legacy risk_score missing but new base_severity_score present, copy.\n+                    if 'risk_score' not in f and 'base_severity_score' in f:\n+                        try:\n+                            f['risk_score'] = int(f.get('base_severity_score') or 0)\n+                        except (TypeError, ValueError):\n+                            f['risk_score'] = 0\n+                    # If both present but divergent (shouldn't happen), prefer explicit risk_score and log later.\n+                    # risk_total duplication if absent\n+                    if 'risk_total' not in f and 'risk_score' in f:\n+                        f['risk_total'] = f['risk_score']\n+    except Exception as norm_e:\n+        # Non-fatal; proceed to validation which may still fail with clearer message.\n+        try:\n+            log_stage('load_report.normalization_warning', error=str(norm_e), type=type(norm_e).__name__)\n+        except Exception:\n+            pass\n+    try:\n+        with mc.time_stage('load_report.validate'):\n+            state.report = Report.model_validate(data)\n+    except Exception as e:\n+        raise ValueError(f\"Report schema validation failed: {e}\") from e\n+    return state\n+\n+\n+def augment(state: AgentState) -> AgentState:\n+    \"\"\"Derive host_id, scan_id, finding categories & basic tags without modifying core C++ schema.\n+    host_id: stable hash of hostname (and kernel if present) unless provided.\n+    scan_id: random uuid4 hex per run.\n+    category: inferred from scanner name.\n+    tags: severity, scanner, plus simple heuristics (network_port, suid, module, kernel_param).\n+    risk_subscores: placeholder computation (impact/exposure/anomaly/confidence) using existing fields only.\n+    \"\"\"\n+    if not state.report:\n+        return state\n+    meta_raw = state.raw_report.get(\"meta\", {}) if state.raw_report else {}\n+    hostname = meta_raw.get(\"hostname\", \"unknown\")\n+    kernel = meta_raw.get(\"kernel\", \"\")\n+    # Derive host_id if absent\n+    if not state.report.meta.host_id:\n+        h = hashlib.sha256()\n+        h.update(hostname.encode())\n+        h.update(b\"|\")\n+        h.update(kernel.encode())\n+        state.report.meta.host_id = h.hexdigest()[:32]\n+    # Always assign a fresh scan_id (caller can override later if desired)\n+    state.report.meta.scan_id = uuid.uuid4().hex\n+    # Category mapping table\n+    cat_map = {\n+        \"process\": \"process\",\n+        \"network\": \"network_socket\",\n+        \"kernel_params\": \"kernel_param\",\n+        \"kernel_modules\": \"kernel_module\",\n+        \"modules\": \"kernel_module\",\n+        \"world_writable\": \"filesystem\",\n+        \"suid\": \"privilege_escalation_surface\",\n+        \"ioc\": \"ioc\",\n+        \"mac\": \"mac\",\n+        \"integrity\": \"integrity\",\n+        \"rules\": \"rule_enrichment\"\n+    }\n+    # Policy multipliers for impact based on category/policy nature\n+    policy_multiplier = {\n+        \"ioc\": 2.0,\n+        \"privilege_escalation_surface\": 1.5,\n+        \"network_socket\": 1.3,\n+        \"kernel_module\": 1.2,\n+        \"kernel_param\": 1.1,\n+    }\n+    severity_base = {\"info\":1, \"low\":2, \"medium\":3, \"high\":4, \"critical\":5, \"error\":4}\n+    # Iterate findings to enrich\n+    if not state.report or not state.report.results:\n+        return state\n+    # Apply external knowledge dictionaries after base tagging\n+    # First pass host role classification prerequisites (we need basic tags/listeners etc) so classification after loop\n+    mc = get_metrics_collector()\n+    with mc.time_stage('augment.iter_findings'):\n+        for sr in state.report.results:\n+            inferred_cat = cat_map.get(sr.scanner.lower(), sr.scanner.lower())\n+            for finding in sr.findings:\n+                if not finding.category:\n+                    finding.category = inferred_cat\n+                # Base tags\n+                base_tags = {f\"scanner:{sr.scanner}\", f\"severity:{finding.severity}\"}\n+                # Heuristic tags\n+                md = finding.metadata\n+                if md.get(\"port\"): base_tags.add(\"network_port\")\n+                if md.get(\"state\") == \"LISTEN\": base_tags.add(\"listening\")\n+                if md.get(\"suid\") == \"true\": base_tags.add(\"suid\")\n+                if md.get(\"module\"): base_tags.add(\"module\")\n+                if md.get(\"sysctl_key\"): base_tags.add(\"kernel_param\")\n+                if not finding.tags:\n+                    finding.tags = list(sorted(base_tags))\n+                else:\n+                    # merge preserving existing list order\n+                    existing = set(finding.tags)\n+                    for t in sorted(base_tags):\n+                        if t not in existing:\n+                            finding.tags.append(t)\n+                # Structured risk subscores initialization\n+                if not finding.risk_subscores:\n+                    exposure = 0.0\n+                    if any(t in finding.tags for t in [\"listening\",\"suid\",\"routing\",\"nat\"]):\n+                        # exposure scoring additive, clamp later\n+                        if \"listening\" in finding.tags: exposure += 1.0\n+                        if \"suid\" in finding.tags: exposure += 1.0\n+                        if any(t.startswith(\"network_port\") for t in finding.tags): exposure += 0.5\n+                        if \"routing\" in finding.tags: exposure += 0.5\n+                        if \"nat\" in finding.tags: exposure += 0.5\n+                    cat_key = finding.category or inferred_cat or \"unknown\"\n+                    impact = float(severity_base.get(finding.severity,1)) * policy_multiplier.get(cat_key,1.0)\n+                    anomaly = 0.0  # baseline stage will add weights\n+                    confidence = 1.0  # default; heuristic rules may lower\n+                    finding.risk_subscores = {\n+                        \"impact\": round(impact,2),\n+                        \"exposure\": round(min(exposure,3.0),2),\n+                        \"anomaly\": anomaly,\n+                        \"confidence\": confidence\n+                    }\n+    # Host role classification (second pass after initial tagging so we can count listeners etc.)\n+    if state.report:\n+        role, role_signals = classify_host_role(state.report)\n+        for sr in state.report.results:\n+            for f in sr.findings:\n+                f.host_role = role\n+                if not f.host_role_rationale:\n+                    f.host_role_rationale = role_signals\n+                if f.category == 'kernel_param' and f.metadata.get('sysctl_key') == 'net.ipv4.ip_forward' and f.risk_subscores:\n+                    impact_changed = False\n+                    if role in {'lightweight_router','container_host'}:\n+                        new_imp = round(max(0.5, f.risk_subscores['impact'] * 0.6),2)\n+                        if new_imp != f.risk_subscores['impact']:\n+                            f.risk_subscores['impact'] = new_imp; impact_changed = True\n+                        note = f\"host_role {role} => ip_forward normalized (impact adjusted)\"\n+                    elif role in {'workstation','dev_workstation'}:\n+                        new_imp = round(min(10.0, f.risk_subscores['impact'] * 1.2 + 0.5),2)\n+                        if new_imp != f.risk_subscores['impact']:\n+                            f.risk_subscores['impact'] = new_imp; impact_changed = True\n+                        note = f\"host_role {role} => ip_forward unusual (impact raised)\"\n+                    else:\n+                        note = None\n+                    if note:\n+                        if f.rationale:\n+                            f.rationale.append(note)\n+                        else:\n+                            f.rationale = [note]\n+                    if impact_changed:\n+                        _recompute_finding_risk(f)\n+    # Initial risk computation for findings lacking risk_score\n+    if state.report:\n+        with mc.time_stage('augment.risk_recompute_initial'):\n+            for sr in state.report.results:\n+                for finding in sr.findings:\n+                    if finding.risk_subscores and finding.risk_score is None:\n+                        _recompute_finding_risk(finding)\n+    return state\n+\n+\n+def correlate(state: AgentState) -> AgentState:\n+    # Add external knowledge enrichment pass before correlation (ensures knowledge tags in correlations)\n+    mc = get_metrics_collector()\n+    with mc.time_stage('knowledge.enrichment'):\n+        state = apply_external_knowledge(state)\n+    all_findings: List[Finding] = []\n+    if not state.report:\n+        return state\n+    for r in state.report.results:\n+        for finding in r.findings:\n+            all_findings.append(finding)\n+    cfg = load_config()\n+    # Merge default + rule dirs (dedupe by id keeping first)\n+    from .rules import load_rules_dir, DEFAULT_RULES\n+    merged = []\n+    seen = set()\n+    for rd in (cfg.paths.rule_dirs or []):\n+        for rule in load_rules_dir(rd):\n+            rid = rule.get('id')\n+            if rid and rid in seen: continue\n+            merged.append(rule); seen.add(rid)\n+    for rule in DEFAULT_RULES:\n+        rid = rule.get('id')\n+        if rid and rid in seen: continue\n+        merged.append(rule); seen.add(rid)\n+    correlator = Correlator(merged)\n+    with mc.time_stage('correlate.apply_rules'):\n+        state.correlations = correlator.apply(all_findings)\n+    mc.incr('correlate.rules_loaded', len(merged))\n+    mc.incr('correlate.correlations', len(state.correlations))\n+    # back-reference correlation ids (simple example: attach first correlation id)\n+    corr_map = {}\n+    for c in state.correlations:\n+        for fid in c.related_finding_ids:\n+            corr_map.setdefault(fid, []).append(c.id)\n+    for finding in all_findings:\n+        finding.correlation_refs = corr_map.get(finding.id, [])\n+    return state\n+\n+def integrate_compliance(state: AgentState) -> AgentState:\n+    \"\"\"Extract compliance summary/gaps from raw report (if present) and surface in metrics for downstream summarization.\n+    Adds keys:\n+      metrics.compliance_summary.<standard> = {passed, failed, score, total_controls}\n+      metrics.compliance_gap_count\n+      metrics.compliance_gaps (first N gap dicts)\n+    \"\"\"\n+    if not state.report or not state.raw_report:\n+        return state\n+    meta = state.raw_report\n+    comp_sum = meta.get('compliance_summary') or {}\n+    gaps = meta.get('compliance_gaps') or []\n+    # Enrich gaps with standardized severity normalization & richer remediation hints if minimal\n+    if gaps:\n+        # Static mapping (could later externalize) control_id/keyword -> remediation & severity normalization\n+        remediation_map = {\n+            '2.2.4': 'Baseline and harden system services; disable or remove unused services.',\n+            '164.312(e)': 'Ensure transmission security: enforce TLS 1.2+, disable weak ciphers, encrypt PHI in transit.',\n+            '164.308(a)(1)': 'Implement risk management processes; document risk analysis and ongoing monitoring.',\n+            'ID.AM-01': 'Maintain accurate asset inventory (automated discovery + periodic reconciliation).',\n+            'PR.AC-01': 'Centralize access control; enforce MFA for privileged accounts.',\n+            'PR.DS-01': 'Encrypt sensitive data at rest with strong algorithms and manage keys securely.'\n+        }\n+        sev_order = {'info':0,'low':1,'medium':2,'moderate':2,'high':3,'critical':4}\n+        # Normalize severities and backfill remediation_hints\n+        for g in gaps:\n+            cid = str(g.get('control_id') or '')\n+            # severity normalization\n+            sev = (g.get('severity') or '').lower()\n+            if sev and sev not in sev_order:\n+                # map alternative labels\n+                if sev in {'moderate'}:\n+                    g['severity'] = 'medium'\n+            # add mapped remediation if existing is missing or too short\n+            hint = g.get('remediation_hint') or ''\n+            if len(hint.strip()) < 12:\n+                mapped = remediation_map.get(cid)\n+                if mapped:\n+                    g['remediation_hint'] = mapped\n+    # Attach into summaries.metrics (create if absent)\n+    if not state.summaries:\n+        from .models import Summaries\n+        state.summaries = Summaries(metrics={})\n+    metrics = state.summaries.metrics or {}\n+    comp_export = {}\n+    for std, vals in comp_sum.items():\n+        # filter numeric fields\n+        comp_export[std] = {k: vals.get(k) for k in ['passed','failed','score','total_controls','not_applicable'] if k in vals}\n+    if comp_export:\n+        metrics['compliance_summary'] = comp_export\n+    if gaps:\n+        metrics['compliance_gap_count'] = len(gaps)\n+        # only include first 50 to cap size\n+        metrics['compliance_gaps'] = gaps[:50]\n+    state.summaries.metrics = metrics\n+    return state\n+\n+\n+def baseline_rarity(state: AgentState, baseline_path: Path = Path(\"agent_baseline.db\")) -> AgentState:\n+    \"\"\"Update findings with rarity/anomaly score based on baseline store.\n+    New finding => anomaly=1.0, existing => anomaly decays toward 0.\n+    Exposure + impact remain unchanged; recalculates composite risk_score (simple formula for now).\n+    \"\"\"\n+    if not state.report:\n+        return state\n+    import os as _os\n+    env_path = _os.environ.get('AGENT_BASELINE_DB')\n+    if env_path:\n+        baseline_path = Path(env_path)\n+    store = BaselineStore(baseline_path)\n+    host_id = state.report.meta.host_id or \"unknown_host\"\n+    all_pairs = []\n+    all_findings: List[Finding] = []\n+    for sr in state.report.results:\n+        for f in sr.findings:\n+            all_pairs.append((sr.scanner, f))\n+            all_findings.append(f)\n+    deltas = store.update_and_diff(host_id, all_pairs)\n+    # Map back anomaly score\n+    for sr in state.report.results:\n+        for finding in sr.findings:\n+            from .baseline import hashlib_sha\n+            h = finding.identity_hash()\n+            comp = hashlib_sha(sr.scanner, h)\n+            d = deltas.get(comp)\n+            if not finding.risk_subscores:\n+                continue\n+            # Anomaly weighting: new +2, existing changed +1 else decay\n+            rationale_bits = []\n+            if d:\n+                if d[\"status\"] == \"new\":\n+                    finding.risk_subscores[\"anomaly\"] = 2.0\n+                    if \"baseline:new\" not in finding.tags:\n+                        finding.tags.append(\"baseline:new\")\n+                    finding.baseline_status = \"new\"\n+                    rationale_bits.append(\"new finding (anomaly +2)\")\n+                else:\n+                    prev = d.get(\"prev_seen_count\", 1)\n+                    # Changed vs stable heuristic: if prev_seen_count just incremented from 1->2 treat as +1 else decay\n+                    if prev <= 2:\n+                        finding.risk_subscores[\"anomaly\"] = 1.0\n+                        finding.baseline_status = \"recent\"\n+                        rationale_bits.append(\"recent finding (anomaly +1)\")\n+                    else:\n+                        finding.risk_subscores[\"anomaly\"] = round(max(0.1, 1.0 / (prev)), 2)\n+                        finding.baseline_status = \"existing\"\n+                        rationale_bits.append(f\"established finding (anomaly {finding.risk_subscores['anomaly']})\")\n+                        if prev >= 5:\n+                            # Very common => tag and downweight anomaly further contextually\n+                            if 'baseline:common' not in finding.tags:\n+                                finding.tags.append('baseline:common')\n+                            rationale_bits.append('very common baseline occurrence')\n+            # Confidence adjustment (placeholder): if only pattern-based IOC (tag contains 'ioc-pattern') lower confidence\n+            if any(t.startswith(\"ioc-pattern\") for t in finding.tags):\n+                finding.risk_subscores[\"confidence\"] = min(finding.risk_subscores.get(\"confidence\",1.0), 0.7)\n+                rationale_bits.append(\"heuristic IOC pattern (confidence down)\")\n+            # Calibrated risk using weights\n+            weights = load_persistent_weights()\n+            score, raw = compute_risk(finding.risk_subscores, weights)\n+            finding.risk_score = score\n+            finding.risk_subscores[\"_raw_weighted_sum\"] = round(raw,3)\n+            finding.probability_actionable = apply_probability(raw)\n+            # Impact & exposure rationale\n+            impact = finding.risk_subscores.get(\"impact\")\n+            exposure = finding.risk_subscores.get(\"exposure\")\n+            rationale_bits.insert(0, f\"impact={impact}\")\n+            rationale_bits.insert(1, f\"exposure={exposure}\")\n+            if not finding.rationale:\n+                finding.rationale = rationale_bits\n+            else:\n+                finding.rationale.extend(rationale_bits)\n+            finding.risk_total = finding.risk_score\n+            # Log calibration observation (raw sum) for future supervised tuning\n+            if state.report and state.report.meta and state.report.meta.scan_id:\n+                comp_hash = comp  # composite hash from earlier\n+                try:\n+                    store.log_calibration_observation(host_id, state.report.meta.scan_id, comp_hash, raw)\n+                except Exception as e:\n+                    _log_error('calibration_observe', e)\n+    return state\n+\n+\n+def process_novelty(state: AgentState, baseline_path: Path = Path(\"agent_baseline.db\"), distance_threshold: float | None = None, anomaly_boost: float = 1.5) -> AgentState:\n+    \"\"\"Assign lightweight embedding-based novelty for process findings.\n+    Uses config threshold if distance_threshold not provided.\n+    Parallelizes feature vector computation if configured (CPU-bound hashing/light transforms).\"\"\"\n+    if not state.report:\n+        return state\n+    cfg = load_config()\n+    if distance_threshold is None:\n+        # Fallback to configured threshold; if missing or None, choose conservative default 1.0\n+        dt_cfg = getattr(cfg.thresholds, 'process_novelty_distance', None)\n+        distance_threshold = float(dt_cfg) if dt_cfg is not None else 1.0\n+    env_path = os.environ.get('AGENT_BASELINE_DB')\n+    if env_path:\n+        baseline_path = Path(env_path)\n+    store = BaselineStore(baseline_path)\n+    host_id = state.report.meta.host_id or \"unknown_host\"\n+    # Collect candidate findings\n+    candidates: List[Finding] = []\n+    for sr in state.report.results:\n+        if sr.scanner.lower() != 'process':\n+            continue\n+        for f in sr.findings:\n+            candidates.append(f)\n+    if not candidates:\n+        return state\n+    # Pre-compute feature vectors (parallel if enabled)\n+    vecs: dict[str, list[float]] = {}\n+    def _build_vec(f: Finding):\n+        cmd = f.metadata.get('cmdline') or f.title or f.metadata.get('process') or ''\n+        return f.id, process_feature_vector(cmd)\n+    if cfg.performance.parallel_baseline and len(candidates) > 4:\n+        with concurrent.futures.ThreadPoolExecutor(max_workers=cfg.performance.workers) as ex:\n+            for fid, v in ex.map(_build_vec, candidates):\n+                vecs[fid] = v\n+    else:\n+        for f in candidates:\n+            fid, v = _build_vec(f)\n+            vecs[fid] = v\n+    for f in candidates:\n+        vec = vecs.get(f.id)\n+        if vec is None:\n+            continue  # no vector computed\n+        cid, dist, is_new = store.assign_process_vector(host_id, vec, distance_threshold=float(distance_threshold))\n+        if is_new or dist > float(distance_threshold):\n+            if 'process_novel' not in f.tags:\n+                f.tags.append('process_novel')\n+            if f.risk_subscores:\n+                prev = f.risk_subscores.get('anomaly', 0.0)\n+                from .risk import CAPS\n+                cap = CAPS.get('anomaly', 2.0)\n+                new_anom = round(min(prev + anomaly_boost, cap),2)\n+                if new_anom != prev:\n+                    f.risk_subscores['anomaly'] = new_anom\n+                    _recompute_finding_risk(f)\n+            rationale_note = f\"novel process cluster (cid={cid} dist={dist:.2f})\"\n+            if f.rationale:\n+                f.rationale.append(rationale_note)\n+            else:\n+                f.rationale = [rationale_note]\n+        else:\n+            if dist > float(distance_threshold) * 0.8:\n+                note = f\"near-novel process (cid={cid} dist={dist:.2f})\"\n+                if f.rationale:\n+                    f.rationale.append(note)\n+                else:\n+                    f.rationale = [note]\n+    return state\n+\n+\n+def sequence_correlation(state: AgentState) -> AgentState:\n+    \"\"\"Detect suspicious temporal sequences inside a single scan.\n+    Current heuristic pattern:\n+      1. New SUID binary (tag baseline:new + suid) appears\n+      2. net.ipv4.ip_forward kernel param enabled (value=1) in same scan after the SUID finding order.\n+    If both occur, emit synthetic correlation with tag sequence_anomaly.\n+    Ordering proxy: we use appearance order in report results since per-scanner timestamps absent.\n+    \"\"\"\n+    if not state.report:\n+        return state\n+    # Flatten findings preserving order\n+    ordered: List[Finding] = []\n+    for r in state.report.results:\n+        for f in r.findings:\n+            ordered.append(f)\n+    suid_indices = []\n+    ip_forward_indices = []\n+    for idx, f in enumerate(ordered):\n+        if 'suid' in (f.tags or []) and any(t == 'baseline:new' for t in (f.tags or [])):\n+            suid_indices.append((idx, f))\n+        if f.category == 'kernel_param' and f.metadata.get('sysctl_key') == 'net.ipv4.ip_forward':\n+            val = str(f.metadata.get('value') or f.metadata.get('desired') or f.metadata.get('current') or '')\n+            if val in {'1','true','enabled'}:\n+                ip_forward_indices.append((idx, f))\n+    if suid_indices and ip_forward_indices:\n+        # Check if any suid index precedes any ip_forward index\n+        trigger_pairs = [(s,i) for (s,_) in suid_indices for (i,_) in ip_forward_indices if s < i]\n+        if trigger_pairs:\n+            # Build correlation referencing the involved findings (limit to first few to bound size)\n+            related = []\n+            for (s_idx, s_f) in suid_indices[:3]:\n+                related.append(s_f.id)\n+            for (i_idx, i_f) in ip_forward_indices[:2]:\n+                related.append(i_f.id)\n+            # Avoid duplicate correlation creation\n+            already = any(c.related_finding_ids == related and 'sequence_anomaly' in (c.tags or []) for c in state.correlations)\n+            if not already:\n+                # Deterministic ID: sequence_anom_<n>\n+                existing = [c for c in state.correlations if 'sequence_anomaly' in (c.tags or []) and c.id.startswith('sequence_anom_')]\n+                corr_id = f'sequence_anom_{len(existing)+1}'\n+                corr = Correlation(\n+                    id=corr_id,\n+                    title='Suspicious Sequence: New SUID followed by IP forwarding enabled',\n+                    rationale='Heuristic: newly introduced SUID binary preceded enabling IP forwarding in same scan',\n+                    related_finding_ids=related,\n+                    risk_score_delta=8,\n+                    tags=['sequence_anomaly','routing','privilege_escalation_surface'],\n+                    severity='high'\n+                )\n+                state.correlations.append(corr)\n+                # Back-reference on findings\n+                for f in ordered:\n+                    if f.id in related:\n+                        if corr.id not in f.correlation_refs:\n+                            f.correlation_refs.append(corr.id)\n+    return state\n+\n+\n+def reduce(state: AgentState) -> AgentState:\n+    if not state.report:\n+        return state\n+    all_findings: List[Finding] = [f for r in state.report.results for f in r.findings]\n+    mc = get_metrics_collector()\n+    with mc.time_stage('reduce.reduce_all'):\n+        state.reductions = reduce_all(all_findings)\n+    return state\n+\n+\n+def summarize(state: AgentState) -> AgentState:\n+    client = get_llm_provider()\n+    governor = get_data_governor()\n+    cfg = load_config()\n+    threshold = cfg.thresholds.summarization_risk_sum\n+    high_med_sum = 0\n+    new_found = False\n+    all_findings = [f for r in state.report.results for f in r.findings] if state.report else []\n+    for f in all_findings:\n+        sev = f.severity.lower()\n+        if sev in {\"medium\",\"high\",\"critical\"}:\n+            # Exclude operational_error pseudo-findings from security risk aggregation\n+            if not getattr(f, 'operational_error', False):\n+                high_med_sum += (f.risk_total or f.risk_score or 0)\n+        if any(t == 'baseline:new' for t in (f.tags or [])):\n+            new_found = True\n+    skip = (high_med_sum < threshold) and (not new_found)\n+    prev = getattr(state, 'summaries', None)\n+    # Redact inputs before passing to provider (governance enforcement)\n+    red_reductions = governor.redact_for_llm(state.reductions)\n+    red_correlations = [governor.redact_for_llm(c) for c in state.correlations]\n+    red_actions = [governor.redact_for_llm(a) for a in state.actions]\n+    mc = get_metrics_collector()\n+    with mc.time_stage('summarize.llm'):\n+        state.summaries = client.summarize(red_reductions, red_correlations, red_actions, skip=skip, previous=prev, skip_reason=\"low_risk_no_change\" if skip else None, baseline_context=None)\n+    # Attach token accounting snapshot for future cost modeling\n+    try:\n+        # Token accounting model removed / deprecated; inline dict if needed in future.\n+        m = state.summaries.metrics or {}\n+        state.summaries.metrics = state.summaries.metrics or {}\n+        state.summaries.metrics['token_accounting'] = {\n+            'prompt_tokens': m.get('tokens_prompt',0),\n+            'completion_tokens': m.get('tokens_completion',0),\n+            'cached': bool(m.get('skipped')),\n+            'unit_cost_prompt': float(os.environ.get('AGENT_COST_PROMPT_PER_1K', '0') or 0)/1000.0,\n+            'unit_cost_completion': float(os.environ.get('AGENT_COST_COMPLETION_PER_1K','0') or 0)/1000.0,\n+        }\n+        state.summaries.metrics['estimated_cost'] = round(\n+            state.summaries.metrics['token_accounting']['prompt_tokens'] * state.summaries.metrics['token_accounting']['unit_cost_prompt'] +\n+            state.summaries.metrics['token_accounting']['completion_tokens'] * state.summaries.metrics['token_accounting']['unit_cost_completion'], 6\n+        )\n+    except Exception:\n+        pass\n+    # Scrub narrative fields prior to persistence/output\n+    state.summaries = governor.redact_output_narratives(state.summaries)\n+    # ATT&CK coverage computation\n+    try:\n+        mapping = _load_attack_mapping()\n+        if state.report and state.summaries:\n+            covered = {}\n+            all_tags = set()\n+            for r in state.report.results:\n+                for f in r.findings:\n+                    for t in (f.tags or []):\n+                        all_tags.add(t)\n+            for c in state.correlations:\n+                for t in (c.tags or []):\n+                    all_tags.add(t)\n+            techniques = set()\n+            tag_hits = {}\n+            for tag in all_tags:\n+                techs = mapping.get(tag)\n+                if not techs:\n+                    continue\n+                if isinstance(techs, list):\n+                    for tid in techs:\n+                        techniques.add(tid)\n+                elif isinstance(techs, str):\n+                    techniques.add(techs)\n+                tag_hits[tag] = techs\n+            state.summaries.attack_coverage = {\n+                'technique_count': len(techniques),\n+                'techniques': sorted(techniques),\n+                'tag_hits': tag_hits\n+            }\n+    except Exception as e:\n+        _log_error('attack_coverage', e, state)\n+    # Experimental causal hypotheses\n+    try:\n+        if state.summaries:\n+            state.summaries.causal_hypotheses = generate_causal_hypotheses(state)\n+    except Exception as e:\n+        _log_error('causal_hypotheses', e, state)\n+    return state\n+\n+\n+def actions(state: AgentState) -> AgentState:\n+    items: List[ActionItem] = []\n+    # Simple deterministic mapping examples\n+    mc = get_metrics_collector()\n+    with mc.time_stage('actions.build'):\n+        for c in state.correlations:\n+            if \"routing\" in c.tags:\n+                items.append(ActionItem(priority=len(items)+1, action=\"Confirm intent for routing/NAT configuration; disable if not required.\", correlation_refs=[c.id]))\n+    # Baseline noise example: if many SUID unexpected\n+    if state.reductions.suid_summary and state.reductions.suid_summary.get(\"unexpected_suid\"):\n+        items.append(ActionItem(priority=len(items)+1, action=\"Review unexpected SUID binaries; remove SUID bit if unnecessary.\", correlation_refs=[]))\n+    mc.incr('actions.count', len(items))\n+    state.actions = items\n+    return state\n+\n+\n+def build_output(state: AgentState, raw_path: Path) -> EnrichedOutput:\n+    sha = hashlib.sha256(Path(raw_path).read_bytes()).hexdigest() if raw_path.exists() else None\n+    integrity_status = None\n+    try:\n+        # If verification key provided via env, attempt signature verification\n+        vkey = os.environ.get('AGENT_VERIFY_KEY_B64')\n+        if raw_path.exists():\n+            if vkey:\n+                integrity_status = verify_file(raw_path, vkey)\n+            else:\n+                # minimal status with sha only\n+                integrity_status = {'sha256_actual': sha}\n+    except Exception as e:\n+        _log_error('integrity_verify', e, state, severity='error', hint='Check signature key / file permissions')\n+        integrity_status = {'sha256_actual': sha, 'error': 'integrity_check_failed'}\n+    flat_findings = []\n+    if state.report:\n+        for r in state.report.results:\n+            flat_findings.extend(r.findings)\n+    # Correlation graph metrics\n+    mc = get_metrics_collector()\n+    with mc.time_stage('graph.annotate'):\n+        graph_meta = annotate_and_summarize(state)\n+    out = EnrichedOutput(\n+        correlations=state.correlations,\n+        reductions=state.reductions,\n+        summaries=state.summaries,\n+        actions=state.actions,\n+        raw_reference=sha,\n+        enriched_findings=flat_findings,\n+        correlation_graph=graph_meta if graph_meta else None,\n+        followups=state.followups if state.followups else None,\n+    enrichment_results=state.enrichment_results or None,\n+    multi_host_correlation=state.multi_host_correlation or None,\n+    integrity=integrity_status\n+    )\n+    if out.enrichment_results is None:\n+        out.enrichment_results = {}\n+    if state.agent_warnings:\n+        out.enrichment_results['agent_warnings'] = state.agent_warnings\n+    # Performance metrics and baseline regression detection\n+    perf_snap = mc.snapshot()\n+    baseline_path = os.environ.get('AGENT_PERF_BASELINE_PATH', 'artifacts/perf_baseline.json')\n+    threshold_env = os.environ.get('AGENT_PERF_REGRESSION_PCT', '30')\n+    try:\n+        threshold = max(0.0, float(threshold_env))/100.0\n+    except ValueError:\n+        threshold = 0.30\n+    from .metrics import MetricsCollector\n+    base = MetricsCollector.load_baseline(baseline_path)\n+    regressions = MetricsCollector.compare_to_baseline(perf_snap, base or {}, threshold) if base else []\n+    perf_snap['baseline_regressions'] = regressions\n+    perf_snap['baseline_threshold_pct'] = threshold*100\n+    out.enrichment_results['perf'] = perf_snap\n+    # Save current snapshot as new baseline (rolling)\n+    MetricsCollector.save_baseline(baseline_path, perf_snap)\n+    # Surface summary perf metrics\n+    if out.summaries:\n+        totals = perf_snap.get('durations', {})\n+        total_time = sum(v.get('total',0) for v in totals.values())\n+        slowest = None\n+        slowest_time = -1\n+        for stage, stats in totals.items():\n+            if stats.get('total',0) > slowest_time:\n+                slowest = stage\n+                slowest_time = stats.get('total',0)\n+        metrics_map = out.summaries.metrics or {}\n+        metrics_map.update({\n+            'perf.total_ms': total_time,\n+            'perf.slowest_stage': slowest,\n+            'perf.slowest_ms': slowest_time,\n+            'perf.regression_count': len(regressions)\n+        })\n+        out.summaries.metrics = metrics_map\n+    # Populate meta.analytics (INT-OBS-001) with concise summary (avoid large arrays)\n+    try:\n+        if state.report and state.report.meta:\n+            if not state.report.meta.analytics:\n+                state.report.meta.analytics = {}\n+            # Summarize durations\n+            dur_summary = {k: { 'avg_ms': round(v.get('avg',0),2), 'total_ms': v.get('total',0), 'count': v.get('count',0)} for k,v in perf_snap.get('durations', {}).items()}\n+            state.report.meta.analytics.update({\n+                'performance': {\n+                    'durations': dur_summary,\n+                    'counters': perf_snap.get('counters', {}),\n+                    'regressions': perf_snap.get('baseline_regressions', []),\n+                    'regression_threshold_pct': perf_snap.get('baseline_threshold_pct')\n+                }\n+            })\n+    except Exception:\n+        pass\n+    # Apply deterministic canonical ordering to entire output\n+    try:\n+        out_dict = out.model_dump()\n+        canon = canonicalize_enriched_output_dict(out_dict)\n+        from .models import EnrichedOutput as _EO\n+        out = _EO(**canon)\n+    except Exception:\n+        # On failure, fall back to original out\n+        pass\n+    return out\n+\n+\n+# -----------------\n+# Optional External Corpus Integration (Hugging Face datasets)\n+# -----------------\n+\n+def _augment_with_corpus_insights(state: AgentState):\n+    \"\"\"Optionally load external cybersecurity corpora (if token & pandas available) to\n+    attach high-level corpus metrics to summaries.metrics for adaptive reasoning.\n+\n+    Controlled by env AGENT_LOAD_HF_CORPUS=1. Lightweight: only counts / sample hash.\n+    Avoids loading if already present in metrics. Silent (logs on error).\"\"\"\n+    if not os.environ.get('AGENT_LOAD_HF_CORPUS'):\n+        return state\n+    try:\n+        from . import hf_loader  # lazy import\n+    except Exception as e:  # module absent\n+        _log_error('corpus_import', e)\n+        return state\n+    try:\n+        jsonl_df = hf_loader.load_cybersec_jsonl()\n+        parquet_df = hf_loader.load_cybersec_parquet()\n+        j_rows = int(len(jsonl_df)) if jsonl_df is not None else None\n+        p_rows = int(len(parquet_df)) if parquet_df is not None else None\n+        # Minimal content fingerprint (no sensitive data): column name hash\n+        import hashlib as _hl\n+        def _col_fprint(df):\n+            if df is None: return None\n+            h = _hl.sha256()\n+            for c in sorted(df.columns):\n+                h.update(c.encode())\n+            return h.hexdigest()[:16]\n+        metrics_add = {\n+            'corpus.jsonl_rows': j_rows,\n+            'corpus.parquet_rows': p_rows,\n+            'corpus.jsonl_schema_fprint': _col_fprint(jsonl_df),\n+            'corpus.parquet_schema_fprint': _col_fprint(parquet_df)\n+        }\n+        if state.summaries:\n+            base = state.summaries.metrics or {}\n+            # Do not overwrite existing keys unless None\n+            for k,v in metrics_add.items():\n+                if k not in base or base[k] is None:\n+                    base[k] = v\n+            state.summaries.metrics = base\n+    except Exception as e:\n+        _log_error('corpus_insights', e)\n+    return state\n+\n+\n+def generate_causal_hypotheses(state: AgentState, max_hypotheses: int = 3) -> list[dict]:\n+    \"\"\"Generate speculative causal hypotheses from correlations & findings.\n+    Heuristics only (deterministic):\n+      - sequence_anomaly => privilege escalation chain.\n+      - module_propagation => lateral movement via module.\n+      - presence of metric_drift finding + routing correlation => config change root cause.\n+    Mark all as speculative with low confidence.\n+    \"\"\"\n+    hyps = []\n+    for c in state.correlations:\n+        if 'sequence_anomaly' in c.tags:\n+            hyps.append({\n+                'id': f\"hyp_{len(hyps)+1}\",\n+                'summary': 'Potential privilege escalation chain (new SUID then IP forwarding)',\n+                'rationale': [c.rationale],\n+                'confidence': 'low',\n+                'speculative': True\n+            })\n+        if 'module_propagation' in c.tags:\n+            hyps.append({\n+                'id': f\"hyp_{len(hyps)+1}\",\n+                'summary': 'Possible lateral movement via near-simultaneous kernel module deployment',\n+                'rationale': [c.rationale or 'simultaneous module emergence across hosts'],\n+                'confidence': 'low',\n+                'speculative': True\n+            })\n+    drift_present = any('metric_drift' in (f.tags or []) for r in (state.report.results if state.report else []) for f in r.findings)\n+    routing_corr = any('routing' in c.tags for c in state.correlations)\n+    if drift_present and routing_corr:\n+        hyps.append({\n+            'id': f\"hyp_{len(hyps)+1}\",\n+            'summary': 'Configuration change likely triggered routing and risk metric drift',\n+            'rationale': ['metric drift finding plus routing-related correlation(s)'],\n+            'confidence': 'low',\n+            'speculative': True\n+        })\n+    # Deduplicate by summary, cap\n+    out = []\n+    seen = set()\n+    for h in hyps:\n+        if h['summary'] in seen: continue\n+        seen.add(h['summary'])\n+        out.append(h)\n+        if len(out) >= max_hypotheses:\n+            break\n+    return out\n+\n+\n+def _load_attack_mapping(path: Path = Path('agent/attack_mapping.yaml')) -> dict:\n+    try:\n+        if not path.exists():\n+            return {}\n+        data = _yaml.safe_load(path.read_text()) or {}\n+        return data\n+    except Exception:\n+        return {}\n+\n+\n+def run_pipeline(report_path: Path) -> EnrichedOutput:\n+    state = AgentState()\n+    state = load_report(state, report_path)\n+    try:\n+        log_stage('load_report', file=str(report_path), sha256=hashlib.sha256(report_path.read_bytes()).hexdigest())\n+    except Exception as e:\n+        _log_error('load_report_log', e)\n+    state = augment(state)\n+    state = integrate_compliance(state)\n+    # Policy enforcement (denylist executable paths)\n+    try:\n+        state = apply_policy(state)\n+        log_stage('policy_enforce')\n+    except Exception as e:\n+        _log_error('policy_enforce', e)\n+    try:\n+        log_stage('augment', findings=sum(len(r.findings) for r in (state.report.results if state.report else [])))\n+    except Exception as e:\n+        _log_error('augment_log', e)\n+    state = correlate(state)\n+    try:\n+        log_stage('correlate', correlations=len(state.correlations))\n+    except Exception as e:\n+        _log_error('correlate_log', e)\n+    # Temporal sequence correlations\n+    try:\n+        state = sequence_correlation(state)\n+        log_stage('sequence_correlation')\n+    except Exception as e:\n+        _log_error('sequence_correlation', e)\n+    state = baseline_rarity(state)\n+    try:\n+        log_stage('baseline_rarity')\n+    except Exception as e:\n+        _log_error('baseline_rarity_log', e)\n+    # Embedding-based process novelty\n+    try:\n+        state = process_novelty(state)\n+        log_stage('process_novelty')\n+    except Exception as e:\n+        _log_error('process_novelty', e)\n+    # Metric drift detection: derive simple metrics and record; synthesize findings if z>|threshold|\n+    if state.report and state.report.meta and state.report.meta.host_id and state.report.meta.scan_id:\n+        host_id = state.report.meta.host_id\n+        scan_id = state.report.meta.scan_id\n+        # Derive metrics (can expand later). Examples:\n+        # 1. total findings\n+        # 2. high severity count\n+        # 3. sum risk_total of medium+ severity\n+        all_findings = [f for r in state.report.results for f in r.findings]\n+        total_findings = len(all_findings)\n+        high_count = sum(1 for f in all_findings if f.severity.lower() in {\"high\",\"critical\"})\n+        med_hi_risk_sum = sum((f.risk_total or f.risk_score or 0) for f in all_findings if f.severity.lower() in {\"medium\",\"high\",\"critical\"} and not getattr(f, 'operational_error', False))\n+        metrics = {\n+            'finding.count.total': float(total_findings),\n+            'finding.count.high': float(high_count),\n+            'risk.sum.medium_high': float(med_hi_risk_sum)\n+        }\n+        store = BaselineStore(Path(\"agent_baseline.db\"))\n+        metric_stats = store.record_metrics(host_id, scan_id, metrics, history_limit=10)\n+        # Lower initial thresholds: if history_n >=2 compute z; threshold 2.5; if no std yet use simple delta heuristic\n+        drift_threshold = 2.5\n+        drift_findings = []\n+        for mname, stats in metric_stats.items():\n+            z = stats.get('z')\n+            hist_n = stats.get('history_n',0)\n+            # Accept drift if enough history for z OR early large delta vs mean when hist>=2\n+            trigger = False\n+            if z is not None and hist_n >= 2 and abs(z) >= drift_threshold:\n+                trigger = True\n+            elif z is None and hist_n >= 2 and stats.get('mean') is not None:\n+                mean = stats['mean']; val = stats['value']\n+                # simple 100% increase heuristic\n+                if mean and (val - mean)/mean >= 1.0:\n+                    trigger = True\n+            if trigger:\n+                # Build synthetic finding\n+                fid = f\"metric:{mname}:drift\"\n+                z_for_sev = abs(z) if z is not None else 0\n+                from .risk import CAPS\n+                anomaly_cap = CAPS.get('anomaly', 2.0)\n+                raw_anom = (abs(z)/3.0) if z is not None else 1.0\n+                anomaly_val = min(anomaly_cap, raw_anom)\n+                drift = Finding(\n+                    id=fid,\n+                    title=\"Metric Drift Detected\",\n+                    severity=\"medium\" if z_for_sev < 5 else \"high\",\n+                    risk_score=0,\n+                    metadata={'metric': mname, 'z': z, 'value': stats['value'], 'mean': stats['mean'], 'std': stats['std']},\n+                    category='telemetry',\n+                    tags=['synthetic','metric_drift'],\n+                    risk_subscores={'impact': 3.0, 'exposure': 0.0, 'anomaly': anomaly_val, 'confidence': 0.9},\n+                    baseline_status='new',\n+                    metric_drift=stats\n+                )\n+                # compute risk for synthetic\n+                weights = load_persistent_weights()\n+                # risk_subscores is always set above\n+                score, raw = compute_risk(drift.risk_subscores or {}, weights)\n+                drift.risk_score = score\n+                drift.risk_total = score\n+                drift.probability_actionable = apply_probability(raw)\n+                if z is not None:\n+                    drift.rationale = [f\"metric {mname} drift z={z:.2f} value={stats['value']} mean={stats['mean']:.2f} std={stats['std']:.2f}\"]\n+                else:\n+                    drift.rationale = [f\"metric {mname} early drift value={stats['value']} mean={stats['mean']:.2f} (delta >=100%)\"]\n+                drift_findings.append(drift)\n+        if drift_findings:\n+            # Append to a synthetic scanner result so downstream logic sees them\n+            sr = ScannerResult(scanner='metric_drift', finding_count=len(drift_findings), findings=drift_findings)\n+            state.report.results.append(sr)\n+    state = reduce(state)\n+    try:\n+        log_stage('reduce', top_findings=len(state.reductions.top_findings))\n+    except Exception as e:\n+        _log_error('reduce_log', e)\n+    state = actions(state)\n+    try:\n+        log_stage('actions', actions=len(state.actions))\n+    except Exception as e:\n+        _log_error('actions_log', e)\n+    # Cross-host anomaly: module simultaneous emergence\n+    try:\n+        if state.report and state.report.results:\n+            store = BaselineStore(Path(os.environ.get('AGENT_BASELINE_DB','agent_baseline.db')))\n+            recent = store.recent_module_first_seen(within_seconds=86400)\n+            threshold = int(os.environ.get('PROPAGATION_HOST_THRESHOLD','3'))\n+            current_host = state.report.meta.host_id if state.report.meta else None  # reserved for future filtering\n+            for module, hosts in recent.items():\n+                if len(hosts) >= threshold:\n+                    corr = MultiHostCorrelation(type='module_propagation', key=module, host_ids=hosts, rationale=f\"Module '{module}' first appeared on {len(hosts)} hosts within 24h window\")\n+                    state.multi_host_correlation.append(corr)\n+                    fid = f\"multi_host_module:{module}\"\n+                    synth = Finding(\n+                        id=fid,\n+                        title=f\"Potential Propagation: module {module}\",\n+                        severity='medium',\n+                        risk_score=0,\n+                        metadata={'module': module, 'host_cluster_size': len(hosts)},\n+                        category='cross_host',\n+                        tags=['synthetic','cross_host','module_propagation'],\n+                        risk_subscores={'impact': 5.0, 'exposure': 0.0, 'anomaly': min(1.5, (__import__('agent.risk', fromlist=['CAPS']).CAPS.get('anomaly',2.0))), 'confidence': 0.8},\n+                        baseline_status='new'\n+                    )\n+                    _recompute_finding_risk(synth)\n+                    synth.rationale = [f\"Module simultaneously observed on {len(hosts)} hosts (>= {threshold})\"]\n+                    added = False\n+                    for sr in state.report.results:\n+                        if sr.scanner == 'multi_host':\n+                            sr.findings.append(synth)\n+                            sr.finding_count += 1\n+                            added = True\n+                            break\n+                    if not added:\n+                        state.report.results.append(ScannerResult(scanner='multi_host', finding_count=1, findings=[synth]))\n+    except Exception as e:\n+        _log_error('multi_host_correlation', e)\n+    # Follow-up planning/execution (deterministic gate)\n+    # Criteria: finding tagged ioc:development-tool and not allowlisted\n+    if state.report:\n+        for r in state.report.results:\n+            for f in r.findings:\n+                follow = False\n+                # Heuristic: mark certain IOC executables as development tools\n+                exe_path = f.metadata.get('exe') or ''\n+                if exe_path and any(tok in exe_path for tok in ['cpptools','python-env-tools']):\n+                    if 'ioc:development-tool' not in f.tags:\n+                        f.tags.append('ioc:development-tool')\n+                if any(t == 'ioc:development-tool' for t in (f.tags or [])) and not f.allowlist_reason:\n+                    follow = True\n+                if r.scanner.lower() == 'suid' and f.metadata.get('path'):\n+                    follow = True\n+                if follow:\n+                    plan = [\"hash_binary\", \"query_package_manager\"]\n+                    results = {}\n+                    bin_path = f.metadata.get('exe') or f.metadata.get('path')\n+                    if bin_path:\n+                        results['hash_binary'] = hash_binary(bin_path)\n+                        results['query_package_manager'] = query_package_manager(bin_path)\n+                    from .models import FollowupResult\n+                    state.followups.append(FollowupResult(finding_id=f.id, plan=plan, results=results))\n+                    try:\n+                        log_stage('followup_execute', finding_id=f.id, plan=\";\".join(plan))\n+                    except Exception as e:\n+                        _log_error('followup_execute', e)\n+    # Post follow-up aggregation / severity adjustments\n+    if state.followups and state.report:\n+        # Load trusted manifest\n+        trust_path = Path(__file__).parent / 'knowledge' / 'trusted_binaries.yaml'\n+        trusted = {}\n+        if trust_path.exists():\n+            try:\n+                trusted = yaml.safe_load(trust_path.read_text()) or {}\n+            except Exception as e:\n+                _log_error('trusted_manifest_load', e)\n+                trusted = {}\n+        trust_map = trusted.get('trusted', {})\n+        report_results = state.report.results or []\n+        # Build hash->(tool_key, downgrade) index for faster match & robustness\n+        hash_index = {}\n+        for tk, meta in trust_map.items():\n+            for hv in meta.get('sha256', []) or []:\n+                hash_index[hv] = (tk, meta.get('downgrade_severity_to'))\n+        for fu in state.followups:\n+            fobj = None\n+            for r in report_results:\n+                for f in r.findings:\n+                    if f.id == fu.finding_id:\n+                        fobj = f\n+                        break\n+                if fobj:\n+                    break\n+            state.enrichment_results.setdefault(fu.finding_id, fu.results)\n+            # Evaluate trust\n+            hdata = fu.results.get('hash_binary') or {}\n+            sha = hdata.get('sha256')\n+            if fobj and sha:\n+                # First, direct hash index lookup\n+                trust_entry = hash_index.get(sha)\n+                tool_key = None\n+                downgrade = None\n+                if trust_entry:\n+                    tool_key, downgrade = trust_entry\n+                else:\n+                    # Fallback heuristic by tool key substring present in path/title\n+                    title_lower = fobj.title.lower()\n+                    path_val = fobj.metadata.get('exe') or ''\n+                    if 'cpptools' in title_lower or 'cpptools' in path_val:\n+                        tool_key = 'cpptools'\n+                        meta = trust_map.get(tool_key) or {}\n+                        if sha in (meta.get('sha256') or []):\n+                            downgrade = meta.get('downgrade_severity_to')\n+                if tool_key and downgrade and downgrade != fobj.severity:\n+                    old = fobj.severity\n+                    fobj.severity = downgrade\n+                    if fobj.rationale:\n+                        fobj.rationale.append(f\"trusted binary hash matched ({tool_key}); severity {old}->{downgrade}\")\n+                    else:\n+                        fobj.rationale = [f\"trusted binary hash matched ({tool_key}); severity {old}->{downgrade}\"]\n+                    if 'trusted_binary' not in (fobj.tags or []):\n+                        fobj.tags.append('trusted_binary')\n+                    # Also add/update severity: tag list (not removing old to preserve provenance)\n+                    sev_tag = f\"severity:{downgrade}\"\n+                    if fobj.tags and sev_tag not in fobj.tags:\n+                        fobj.tags.append(sev_tag)\n+                    # Recompute risk if impact/exposure might depend on severity externally later\n+                    _recompute_finding_risk(fobj)\n+    state = summarize(state)\n+    # Optional external corpus insights after summaries produced\n+    state = _augment_with_corpus_insights(state)\n+    try:\n+        metrics = (state.summaries.metrics if state.summaries else {}) or {}\n+        log_stage('summarize', tokens_prompt=metrics.get('tokens_prompt'), tokens_completion=metrics.get('tokens_completion'))\n+    except Exception as e:\n+        _log_error('summarize_log', e)\n+    return build_output(state, report_path)\n+\n+\n+# -----------------\n+# Policy Layer\n+# -----------------\n+\n+APPROVED_DEFAULT = [\"/bin\",\"/usr/bin\",\"/usr/local/bin\",\"/sbin\",\"/usr/sbin\",\"/opt/trusted\"]\n+SEVERITY_ORDER = [\"info\",\"low\",\"medium\",\"high\",\"critical\"]\n+\n+def _load_policy_allowlist() -> set[str]:\n+    import yaml\n+    paths: set[str] = set()\n+    # Config-based allowlist\n+    try:\n+        cfg = load_config()\n+        for p in cfg.paths.policy_allowlist:\n+            paths.add(p)\n+    except Exception:\n+        pass\n+    # File allowlist\n+    allow_file = Path('policy_allowlist.yaml')\n+    if allow_file.exists():\n+        try:\n+            data = yaml.safe_load(allow_file.read_text()) or {}\n+            for p in (data.get('allow_executables') or []):\n+                if isinstance(p, str):\n+                    paths.add(p)\n+        except Exception as e:\n+            _log_error('policy_allowlist_load', e)\n+    # Env variable (colon separated)\n+    import os as _os\n+    env_list = _os.environ.get('AGENT_POLICY_ALLOWLIST','')\n+    for part in env_list.split(':'):\n+        part = part.strip()\n+        if part:\n+            paths.add(part)\n+    return paths\n+\n+def _approved_dirs() -> list[str]:\n+    import os as _os\n+    env_dirs = _os.environ.get('AGENT_APPROVED_DIRS')\n+    if env_dirs:\n+        return [d for d in (p.strip() for p in env_dirs.split(':')) if d]\n+    return APPROVED_DEFAULT\n+\n+def apply_policy(state: AgentState) -> AgentState:\n+    if not state.report:\n+        return state\n+    allow = _load_policy_allowlist()\n+    approved = _approved_dirs()\n+    # Resolve approved dirs to absolute canonical paths\n+    approved_real = []\n+    for d in approved:\n+        try:\n+            approved_real.append(str(Path(d).resolve()))\n+        except Exception:\n+            approved_real.append(d)\n+    for sr in state.report.results:\n+        for f in sr.findings:\n+            exe = f.metadata.get('exe') if isinstance(f.metadata, dict) else None\n+            if not exe:\n+                continue\n+            # Already allowlisted\n+            if exe in allow:\n+                continue\n+            try:\n+                exe_real = str(Path(exe).resolve())\n+            except Exception:\n+                exe_real = exe\n+            in_approved = False\n+            for d in approved_real:\n+                try:\n+                    if os.path.commonpath([exe_real, d]) == d:\n+                        in_approved = True\n+                        break\n+                except Exception:\n+                    continue\n+            if not in_approved:\n+                # Escalate severity to at least high unless already critical\n+                try:\n+                    sev_idx = SEVERITY_ORDER.index(f.severity.lower()) if f.severity else 0\n+                except ValueError:\n+                    sev_idx = 0\n+                target = 'high'\n+                if f.severity.lower() != 'critical' and f.severity.lower() != target:\n+                    # Only raise if below target\n+                    if SEVERITY_ORDER.index(target) > sev_idx:\n+                        old = f.severity\n+                        f.severity = target\n+                        f.severity_source = 'policy'\n+                        if f.rationale:\n+                            f.rationale.append(f\"policy escalation: executable outside approved dirs ({exe})\")\n+                        else:\n+                            f.rationale = [f\"policy escalation: executable outside approved dirs ({exe})\"]\n+                        if f.tags is not None:\n+                            if 'policy:denied_path' not in f.tags:\n+                                f.tags.append('policy:denied_path')\n+                            sev_tag = f'severity:{target}'\n+                            if sev_tag not in f.tags:\n+                                f.tags.append(sev_tag)\n+                # risk_subscores impact bump (optional)\n+                if f.risk_subscores:\n+                    new_imp = min(10.0, (f.risk_subscores.get('impact',0)+1.0))\n+                    if new_imp != f.risk_subscores.get('impact'):\n+                        f.risk_subscores['impact'] = new_imp\n+                        _recompute_finding_risk(f)\n+    return state\n+",
      "patch_lines": [
        "@@ -0,0 +1,1265 @@\n",
        "+from __future__ import annotations\n",
        "+import json, hashlib, os, uuid\n",
        "+import json as _json\n",
        "+from pathlib import Path\n",
        "+from typing import List\n",
        "+from .models import AgentState, Report, Finding, EnrichedOutput, ActionItem, ScannerResult, MultiHostCorrelation, Correlation, AgentWarning\n",
        "+from .knowledge import apply_external_knowledge\n",
        "+from .rules import Correlator, DEFAULT_RULES\n",
        "+from .reduction import reduce_all\n",
        "+from .llm_provider import get_llm_provider\n",
        "+from .data_governance import get_data_governor\n",
        "+from .baseline import BaselineStore\n",
        "+from .metrics import get_metrics_collector\n",
        "+from .canonicalize import canonicalize_enriched_output_dict\n",
        "+from .risk import compute_risk, load_persistent_weights\n",
        "+from .calibration import apply_probability\n",
        "+from .graph_analysis import annotate_and_summarize\n",
        "+from .endpoint_classification import classify as classify_host_role\n",
        "+from .executors import hash_binary, query_package_manager\n",
        "+from .integrity import sha256_file, verify_file\n",
        "+from .audit import log_stage, hash_text\n",
        "+import yaml\n",
        "+from .baseline import process_feature_vector\n",
        "+import yaml as _yaml\n",
        "+import uuid as _uuid\n",
        "+from .config import load_config\n",
        "+import concurrent.futures\n",
        "+\n",
        "+# -----------------\n",
        "+# Internal helpers (risk recomputation & error logging)\n",
        "+# -----------------\n",
        "+\n",
        "+def _recompute_finding_risk(f: Finding):\n",
        "+    \"\"\"Recompute risk fields after any risk_subscores mutation.\n",
        "+    Safe no-op if subscores absent; logs errors instead of raising.\"\"\"\n",
        "+    try:\n",
        "+        subs = getattr(f, 'risk_subscores', None)\n",
        "+        if not subs:\n",
        "+            return\n",
        "+        weights = load_persistent_weights()\n",
        "+        score, raw = compute_risk(subs, weights)\n",
        "+        f.risk_score = score\n",
        "+        f.risk_total = score\n",
        "+        subs[\"_raw_weighted_sum\"] = round(raw, 3)\n",
        "+        f.probability_actionable = apply_probability(raw)\n",
        "+    except (ValueError, TypeError) as e:  # expected computation issues\n",
        "+        try:\n",
        "+            log_stage('risk_recompute_error', error=str(e), type=type(e).__name__)\n",
        "+        except Exception:\n",
        "+            pass\n",
        "+    except Exception as e:  # unexpected\n",
        "+        try:\n",
        "+            log_stage('risk_recompute_error_unexpected', error=str(e), type=type(e).__name__)\n",
        "+        except Exception:\n",
        "+            pass\n",
        "+\n",
        "+def _log_error(stage: str, e: Exception, state: AgentState | None = None, module: str = 'pipeline', severity: str = 'warning', hint: str | None = None):\n",
        "+    if state is not None:\n",
        "+        try:\n",
        "+            state.agent_warnings.append(AgentWarning(module=module, stage=stage, error_type=type(e).__name__, message=str(e), severity=severity, hint=hint).model_dump())\n",
        "+        except Exception:\n",
        "+            pass\n",
        "+    try:\n",
        "+        log_stage(f'{stage}_error', error=str(e), type=type(e).__name__)\n",
        "+    except Exception:\n",
        "+        pass\n",
        "+\n",
        "+# Node functions (imperative; future step: convert to LangGraph graph)\n",
        "+\n",
        "+def load_report(state: AgentState, path: Path) -> AgentState:\n",
        "+    \"\"\"Securely load and parse the raw JSON report.\n",
        "+\n",
        "+    Hardening steps:\n",
        "+    1. Enforce maximum size (default 5 MB, override via AGENT_MAX_REPORT_MB env).\n",
        "+    2. Read bytes then decode strictly as UTF-8 (reject invalid sequences).\n",
        "+    3. Canonicalize newlines to '\\n' before JSON parsing to avoid platform variance.\n",
        "+    \"\"\"\n",
        "+    max_mb_env = os.environ.get('AGENT_MAX_REPORT_MB')\n",
        "+    try:\n",
        "+        max_mb = int(max_mb_env) if max_mb_env else 5\n",
        "+    except ValueError:\n",
        "+        max_mb = 5\n",
        "+    mc = get_metrics_collector()\n",
        "+    with mc.time_stage('load_report.read_bytes'):\n",
        "+        raw_bytes = Path(path).read_bytes()\n",
        "+    size_mb = len(raw_bytes) / (1024 * 1024)\n",
        "+    if size_mb > max_mb:\n",
        "+        raise ValueError(f\"Report size {size_mb:.2f} MB exceeds maximum size {max_mb} MB\")\n",
        "+    try:\n",
        "+        text = raw_bytes.decode('utf-8', errors='strict')\n",
        "+    except UnicodeDecodeError as e:\n",
        "+        raise ValueError(f\"Report is not valid UTF-8: {e}\") from e\n",
        "+    # Canonicalize newlines (CRLF, CR -> LF)\n",
        "+    if '\\r' in text:\n",
        "+        text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
        "+    try:\n",
        "+        with mc.time_stage('load_report.json_parse'):\n",
        "+            data = json.loads(text)\n",
        "+    except json.JSONDecodeError as e:\n",
        "+        raise ValueError(f\"Report JSON parse error: {e}\") from e\n",
        "+    state.raw_report = data\n",
        "+    # Normalize risk naming migration (base_severity_score -> risk_score) BEFORE schema validation.\n",
        "+    # The C++ core now emits base_severity_score only. Downstream Python pipeline still expects\n",
        "+    # risk_score. We inject risk_score where missing for backward compatibility and retain the\n",
        "+    # original field (mapped into Finding.base_severity_score by pydantic if present).\n",
        "+    try:\n",
        "+        results = data.get('results') if isinstance(data, dict) else None\n",
        "+        if isinstance(results, list):\n",
        "+            for sr in results:\n",
        "+                findings = sr.get('findings') if isinstance(sr, dict) else None\n",
        "+                if not isinstance(findings, list):\n",
        "+                    continue\n",
        "+                for f in findings:\n",
        "+                    if not isinstance(f, dict):\n",
        "+                        continue\n",
        "+                    # If legacy risk_score missing but new base_severity_score present, copy.\n",
        "+                    if 'risk_score' not in f and 'base_severity_score' in f:\n",
        "+                        try:\n",
        "+                            f['risk_score'] = int(f.get('base_severity_score') or 0)\n",
        "+                        except (TypeError, ValueError):\n",
        "+                            f['risk_score'] = 0\n",
        "+                    # If both present but divergent (shouldn't happen), prefer explicit risk_score and log later.\n",
        "+                    # risk_total duplication if absent\n",
        "+                    if 'risk_total' not in f and 'risk_score' in f:\n",
        "+                        f['risk_total'] = f['risk_score']\n",
        "+    except Exception as norm_e:\n",
        "+        # Non-fatal; proceed to validation which may still fail with clearer message.\n",
        "+        try:\n",
        "+            log_stage('load_report.normalization_warning', error=str(norm_e), type=type(norm_e).__name__)\n",
        "+        except Exception:\n",
        "+            pass\n",
        "+    try:\n",
        "+        with mc.time_stage('load_report.validate'):\n",
        "+            state.report = Report.model_validate(data)\n",
        "+    except Exception as e:\n",
        "+        raise ValueError(f\"Report schema validation failed: {e}\") from e\n",
        "+    return state\n",
        "+\n",
        "+\n",
        "+def augment(state: AgentState) -> AgentState:\n",
        "+    \"\"\"Derive host_id, scan_id, finding categories & basic tags without modifying core C++ schema.\n",
        "+    host_id: stable hash of hostname (and kernel if present) unless provided.\n",
        "+    scan_id: random uuid4 hex per run.\n",
        "+    category: inferred from scanner name.\n",
        "+    tags: severity, scanner, plus simple heuristics (network_port, suid, module, kernel_param).\n",
        "+    risk_subscores: placeholder computation (impact/exposure/anomaly/confidence) using existing fields only.\n",
        "+    \"\"\"\n",
        "+    if not state.report:\n",
        "+        return state\n",
        "+    meta_raw = state.raw_report.get(\"meta\", {}) if state.raw_report else {}\n",
        "+    hostname = meta_raw.get(\"hostname\", \"unknown\")\n",
        "+    kernel = meta_raw.get(\"kernel\", \"\")\n",
        "+    # Derive host_id if absent\n",
        "+    if not state.report.meta.host_id:\n",
        "+        h = hashlib.sha256()\n",
        "+        h.update(hostname.encode())\n",
        "+        h.update(b\"|\")\n",
        "+        h.update(kernel.encode())\n",
        "+        state.report.meta.host_id = h.hexdigest()[:32]\n",
        "+    # Always assign a fresh scan_id (caller can override later if desired)\n",
        "+    state.report.meta.scan_id = uuid.uuid4().hex\n",
        "+    # Category mapping table\n",
        "+    cat_map = {\n",
        "+        \"process\": \"process\",\n",
        "+        \"network\": \"network_socket\",\n",
        "+        \"kernel_params\": \"kernel_param\",\n",
        "+        \"kernel_modules\": \"kernel_module\",\n",
        "+        \"modules\": \"kernel_module\",\n",
        "+        \"world_writable\": \"filesystem\",\n",
        "+        \"suid\": \"privilege_escalation_surface\",\n",
        "+        \"ioc\": \"ioc\",\n",
        "+        \"mac\": \"mac\",\n",
        "+        \"integrity\": \"integrity\",\n",
        "+        \"rules\": \"rule_enrichment\"\n",
        "+    }\n",
        "+    # Policy multipliers for impact based on category/policy nature\n",
        "+    policy_multiplier = {\n",
        "+        \"ioc\": 2.0,\n",
        "+        \"privilege_escalation_surface\": 1.5,\n",
        "+        \"network_socket\": 1.3,\n",
        "+        \"kernel_module\": 1.2,\n",
        "+        \"kernel_param\": 1.1,\n",
        "+    }\n",
        "+    severity_base = {\"info\":1, \"low\":2, \"medium\":3, \"high\":4, \"critical\":5, \"error\":4}\n",
        "+    # Iterate findings to enrich\n",
        "+    if not state.report or not state.report.results:\n",
        "+        return state\n",
        "+    # Apply external knowledge dictionaries after base tagging\n",
        "+    # First pass host role classification prerequisites (we need basic tags/listeners etc) so classification after loop\n",
        "+    mc = get_metrics_collector()\n",
        "+    with mc.time_stage('augment.iter_findings'):\n",
        "+        for sr in state.report.results:\n",
        "+            inferred_cat = cat_map.get(sr.scanner.lower(), sr.scanner.lower())\n",
        "+            for finding in sr.findings:\n",
        "+                if not finding.category:\n",
        "+                    finding.category = inferred_cat\n",
        "+                # Base tags\n",
        "+                base_tags = {f\"scanner:{sr.scanner}\", f\"severity:{finding.severity}\"}\n",
        "+                # Heuristic tags\n",
        "+                md = finding.metadata\n",
        "+                if md.get(\"port\"): base_tags.add(\"network_port\")\n",
        "+                if md.get(\"state\") == \"LISTEN\": base_tags.add(\"listening\")\n",
        "+                if md.get(\"suid\") == \"true\": base_tags.add(\"suid\")\n",
        "+                if md.get(\"module\"): base_tags.add(\"module\")\n",
        "+                if md.get(\"sysctl_key\"): base_tags.add(\"kernel_param\")\n",
        "+                if not finding.tags:\n",
        "+                    finding.tags = list(sorted(base_tags))\n",
        "+                else:\n",
        "+                    # merge preserving existing list order\n",
        "+                    existing = set(finding.tags)\n",
        "+                    for t in sorted(base_tags):\n",
        "+                        if t not in existing:\n",
        "+                            finding.tags.append(t)\n",
        "+                # Structured risk subscores initialization\n",
        "+                if not finding.risk_subscores:\n",
        "+                    exposure = 0.0\n",
        "+                    if any(t in finding.tags for t in [\"listening\",\"suid\",\"routing\",\"nat\"]):\n",
        "+                        # exposure scoring additive, clamp later\n",
        "+                        if \"listening\" in finding.tags: exposure += 1.0\n",
        "+                        if \"suid\" in finding.tags: exposure += 1.0\n",
        "+                        if any(t.startswith(\"network_port\") for t in finding.tags): exposure += 0.5\n",
        "+                        if \"routing\" in finding.tags: exposure += 0.5\n",
        "+                        if \"nat\" in finding.tags: exposure += 0.5\n",
        "+                    cat_key = finding.category or inferred_cat or \"unknown\"\n",
        "+                    impact = float(severity_base.get(finding.severity,1)) * policy_multiplier.get(cat_key,1.0)\n",
        "+                    anomaly = 0.0  # baseline stage will add weights\n",
        "+                    confidence = 1.0  # default; heuristic rules may lower\n",
        "+                    finding.risk_subscores = {\n",
        "+                        \"impact\": round(impact,2),\n",
        "+                        \"exposure\": round(min(exposure,3.0),2),\n",
        "+                        \"anomaly\": anomaly,\n",
        "+                        \"confidence\": confidence\n",
        "+                    }\n",
        "+    # Host role classification (second pass after initial tagging so we can count listeners etc.)\n",
        "+    if state.report:\n",
        "+        role, role_signals = classify_host_role(state.report)\n",
        "+        for sr in state.report.results:\n",
        "+            for f in sr.findings:\n",
        "+                f.host_role = role\n",
        "+                if not f.host_role_rationale:\n",
        "+                    f.host_role_rationale = role_signals\n",
        "+                if f.category == 'kernel_param' and f.metadata.get('sysctl_key') == 'net.ipv4.ip_forward' and f.risk_subscores:\n",
        "+                    impact_changed = False\n",
        "+                    if role in {'lightweight_router','container_host'}:\n",
        "+                        new_imp = round(max(0.5, f.risk_subscores['impact'] * 0.6),2)\n",
        "+                        if new_imp != f.risk_subscores['impact']:\n",
        "+                            f.risk_subscores['impact'] = new_imp; impact_changed = True\n",
        "+                        note = f\"host_role {role} => ip_forward normalized (impact adjusted)\"\n",
        "+                    elif role in {'workstation','dev_workstation'}:\n",
        "+                        new_imp = round(min(10.0, f.risk_subscores['impact'] * 1.2 + 0.5),2)\n",
        "+                        if new_imp != f.risk_subscores['impact']:\n",
        "+                            f.risk_subscores['impact'] = new_imp; impact_changed = True\n",
        "+                        note = f\"host_role {role} => ip_forward unusual (impact raised)\"\n",
        "+                    else:\n",
        "+                        note = None\n",
        "+                    if note:\n",
        "+                        if f.rationale:\n",
        "+                            f.rationale.append(note)\n",
        "+                        else:\n",
        "+                            f.rationale = [note]\n",
        "+                    if impact_changed:\n",
        "+                        _recompute_finding_risk(f)\n",
        "+    # Initial risk computation for findings lacking risk_score\n",
        "+    if state.report:\n",
        "+        with mc.time_stage('augment.risk_recompute_initial'):\n",
        "+            for sr in state.report.results:\n",
        "+                for finding in sr.findings:\n",
        "+                    if finding.risk_subscores and finding.risk_score is None:\n",
        "+                        _recompute_finding_risk(finding)\n",
        "+    return state\n",
        "+\n",
        "+\n",
        "+def correlate(state: AgentState) -> AgentState:\n",
        "+    # Add external knowledge enrichment pass before correlation (ensures knowledge tags in correlations)\n",
        "+    mc = get_metrics_collector()\n",
        "+    with mc.time_stage('knowledge.enrichment'):\n",
        "+        state = apply_external_knowledge(state)\n",
        "+    all_findings: List[Finding] = []\n",
        "+    if not state.report:\n",
        "+        return state\n",
        "+    for r in state.report.results:\n",
        "+        for finding in r.findings:\n",
        "+            all_findings.append(finding)\n",
        "+    cfg = load_config()\n",
        "+    # Merge default + rule dirs (dedupe by id keeping first)\n",
        "+    from .rules import load_rules_dir, DEFAULT_RULES\n",
        "+    merged = []\n",
        "+    seen = set()\n",
        "+    for rd in (cfg.paths.rule_dirs or []):\n",
        "+        for rule in load_rules_dir(rd):\n",
        "+            rid = rule.get('id')\n",
        "+            if rid and rid in seen: continue\n",
        "+            merged.append(rule); seen.add(rid)\n",
        "+    for rule in DEFAULT_RULES:\n",
        "+        rid = rule.get('id')\n",
        "+        if rid and rid in seen: continue\n",
        "+        merged.append(rule); seen.add(rid)\n",
        "+    correlator = Correlator(merged)\n",
        "+    with mc.time_stage('correlate.apply_rules'):\n",
        "+        state.correlations = correlator.apply(all_findings)\n",
        "+    mc.incr('correlate.rules_loaded', len(merged))\n",
        "+    mc.incr('correlate.correlations', len(state.correlations))\n",
        "+    # back-reference correlation ids (simple example: attach first correlation id)\n",
        "+    corr_map = {}\n",
        "+    for c in state.correlations:\n",
        "+        for fid in c.related_finding_ids:\n",
        "+            corr_map.setdefault(fid, []).append(c.id)\n",
        "+    for finding in all_findings:\n",
        "+        finding.correlation_refs = corr_map.get(finding.id, [])\n",
        "+    return state\n",
        "+\n",
        "+def integrate_compliance(state: AgentState) -> AgentState:\n",
        "+    \"\"\"Extract compliance summary/gaps from raw report (if present) and surface in metrics for downstream summarization.\n",
        "+    Adds keys:\n",
        "+      metrics.compliance_summary.<standard> = {passed, failed, score, total_controls}\n",
        "+      metrics.compliance_gap_count\n",
        "+      metrics.compliance_gaps (first N gap dicts)\n",
        "+    \"\"\"\n",
        "+    if not state.report or not state.raw_report:\n",
        "+        return state\n",
        "+    meta = state.raw_report\n",
        "+    comp_sum = meta.get('compliance_summary') or {}\n",
        "+    gaps = meta.get('compliance_gaps') or []\n",
        "+    # Enrich gaps with standardized severity normalization & richer remediation hints if minimal\n",
        "+    if gaps:\n",
        "+        # Static mapping (could later externalize) control_id/keyword -> remediation & severity normalization\n",
        "+        remediation_map = {\n",
        "+            '2.2.4': 'Baseline and harden system services; disable or remove unused services.',\n",
        "+            '164.312(e)': 'Ensure transmission security: enforce TLS 1.2+, disable weak ciphers, encrypt PHI in transit.',\n",
        "+            '164.308(a)(1)': 'Implement risk management processes; document risk analysis and ongoing monitoring.',\n",
        "+            'ID.AM-01': 'Maintain accurate asset inventory (automated discovery + periodic reconciliation).',\n",
        "+            'PR.AC-01': 'Centralize access control; enforce MFA for privileged accounts.',\n",
        "+            'PR.DS-01': 'Encrypt sensitive data at rest with strong algorithms and manage keys securely.'\n",
        "+        }\n",
        "+        sev_order = {'info':0,'low':1,'medium':2,'moderate':2,'high':3,'critical':4}\n",
        "+        # Normalize severities and backfill remediation_hints\n",
        "+        for g in gaps:\n",
        "+            cid = str(g.get('control_id') or '')\n",
        "+            # severity normalization\n",
        "+            sev = (g.get('severity') or '').lower()\n",
        "+            if sev and sev not in sev_order:\n",
        "+                # map alternative labels\n",
        "+                if sev in {'moderate'}:\n",
        "+                    g['severity'] = 'medium'\n",
        "+            # add mapped remediation if existing is missing or too short\n",
        "+            hint = g.get('remediation_hint') or ''\n",
        "+            if len(hint.strip()) < 12:\n",
        "+                mapped = remediation_map.get(cid)\n",
        "+                if mapped:\n",
        "+                    g['remediation_hint'] = mapped\n",
        "+    # Attach into summaries.metrics (create if absent)\n",
        "+    if not state.summaries:\n",
        "+        from .models import Summaries\n",
        "+        state.summaries = Summaries(metrics={})\n",
        "+    metrics = state.summaries.metrics or {}\n",
        "+    comp_export = {}\n",
        "+    for std, vals in comp_sum.items():\n",
        "+        # filter numeric fields\n",
        "+        comp_export[std] = {k: vals.get(k) for k in ['passed','failed','score','total_controls','not_applicable'] if k in vals}\n",
        "+    if comp_export:\n",
        "+        metrics['compliance_summary'] = comp_export\n",
        "+    if gaps:\n",
        "+        metrics['compliance_gap_count'] = len(gaps)\n",
        "+        # only include first 50 to cap size\n",
        "+        metrics['compliance_gaps'] = gaps[:50]\n",
        "+    state.summaries.metrics = metrics\n",
        "+    return state\n",
        "+\n",
        "+\n",
        "+def baseline_rarity(state: AgentState, baseline_path: Path = Path(\"agent_baseline.db\")) -> AgentState:\n",
        "+    \"\"\"Update findings with rarity/anomaly score based on baseline store.\n",
        "+    New finding => anomaly=1.0, existing => anomaly decays toward 0.\n",
        "+    Exposure + impact remain unchanged; recalculates composite risk_score (simple formula for now).\n",
        "+    \"\"\"\n",
        "+    if not state.report:\n",
        "+        return state\n",
        "+    import os as _os\n",
        "+    env_path = _os.environ.get('AGENT_BASELINE_DB')\n",
        "+    if env_path:\n",
        "+        baseline_path = Path(env_path)\n",
        "+    store = BaselineStore(baseline_path)\n",
        "+    host_id = state.report.meta.host_id or \"unknown_host\"\n",
        "+    all_pairs = []\n",
        "+    all_findings: List[Finding] = []\n",
        "+    for sr in state.report.results:\n",
        "+        for f in sr.findings:\n",
        "+            all_pairs.append((sr.scanner, f))\n",
        "+            all_findings.append(f)\n",
        "+    deltas = store.update_and_diff(host_id, all_pairs)\n",
        "+    # Map back anomaly score\n",
        "+    for sr in state.report.results:\n",
        "+        for finding in sr.findings:\n",
        "+            from .baseline import hashlib_sha\n",
        "+            h = finding.identity_hash()\n",
        "+            comp = hashlib_sha(sr.scanner, h)\n",
        "+            d = deltas.get(comp)\n",
        "+            if not finding.risk_subscores:\n",
        "+                continue\n",
        "+            # Anomaly weighting: new +2, existing changed +1 else decay\n",
        "+            rationale_bits = []\n",
        "+            if d:\n",
        "+                if d[\"status\"] == \"new\":\n",
        "+                    finding.risk_subscores[\"anomaly\"] = 2.0\n",
        "+                    if \"baseline:new\" not in finding.tags:\n",
        "+                        finding.tags.append(\"baseline:new\")\n",
        "+                    finding.baseline_status = \"new\"\n",
        "+                    rationale_bits.append(\"new finding (anomaly +2)\")\n",
        "+                else:\n",
        "+                    prev = d.get(\"prev_seen_count\", 1)\n",
        "+                    # Changed vs stable heuristic: if prev_seen_count just incremented from 1->2 treat as +1 else decay\n",
        "+                    if prev <= 2:\n",
        "+                        finding.risk_subscores[\"anomaly\"] = 1.0\n",
        "+                        finding.baseline_status = \"recent\"\n",
        "+                        rationale_bits.append(\"recent finding (anomaly +1)\")\n",
        "+                    else:\n",
        "+                        finding.risk_subscores[\"anomaly\"] = round(max(0.1, 1.0 / (prev)), 2)\n",
        "+                        finding.baseline_status = \"existing\"\n",
        "+                        rationale_bits.append(f\"established finding (anomaly {finding.risk_subscores['anomaly']})\")\n",
        "+                        if prev >= 5:\n",
        "+                            # Very common => tag and downweight anomaly further contextually\n",
        "+                            if 'baseline:common' not in finding.tags:\n",
        "+                                finding.tags.append('baseline:common')\n",
        "+                            rationale_bits.append('very common baseline occurrence')\n",
        "+            # Confidence adjustment (placeholder): if only pattern-based IOC (tag contains 'ioc-pattern') lower confidence\n",
        "+            if any(t.startswith(\"ioc-pattern\") for t in finding.tags):\n",
        "+                finding.risk_subscores[\"confidence\"] = min(finding.risk_subscores.get(\"confidence\",1.0), 0.7)\n",
        "+                rationale_bits.append(\"heuristic IOC pattern (confidence down)\")\n",
        "+            # Calibrated risk using weights\n",
        "+            weights = load_persistent_weights()\n",
        "+            score, raw = compute_risk(finding.risk_subscores, weights)\n",
        "+            finding.risk_score = score\n",
        "+            finding.risk_subscores[\"_raw_weighted_sum\"] = round(raw,3)\n",
        "+            finding.probability_actionable = apply_probability(raw)\n",
        "+            # Impact & exposure rationale\n",
        "+            impact = finding.risk_subscores.get(\"impact\")\n",
        "+            exposure = finding.risk_subscores.get(\"exposure\")\n",
        "+            rationale_bits.insert(0, f\"impact={impact}\")\n",
        "+            rationale_bits.insert(1, f\"exposure={exposure}\")\n",
        "+            if not finding.rationale:\n",
        "+                finding.rationale = rationale_bits\n",
        "+            else:\n",
        "+                finding.rationale.extend(rationale_bits)\n",
        "+            finding.risk_total = finding.risk_score\n",
        "+            # Log calibration observation (raw sum) for future supervised tuning\n",
        "+            if state.report and state.report.meta and state.report.meta.scan_id:\n",
        "+                comp_hash = comp  # composite hash from earlier\n",
        "+                try:\n",
        "+                    store.log_calibration_observation(host_id, state.report.meta.scan_id, comp_hash, raw)\n",
        "+                except Exception as e:\n",
        "+                    _log_error('calibration_observe', e)\n",
        "+    return state\n",
        "+\n",
        "+\n",
        "+def process_novelty(state: AgentState, baseline_path: Path = Path(\"agent_baseline.db\"), distance_threshold: float | None = None, anomaly_boost: float = 1.5) -> AgentState:\n",
        "+    \"\"\"Assign lightweight embedding-based novelty for process findings.\n",
        "+    Uses config threshold if distance_threshold not provided.\n",
        "+    Parallelizes feature vector computation if configured (CPU-bound hashing/light transforms).\"\"\"\n",
        "+    if not state.report:\n",
        "+        return state\n",
        "+    cfg = load_config()\n",
        "+    if distance_threshold is None:\n",
        "+        # Fallback to configured threshold; if missing or None, choose conservative default 1.0\n",
        "+        dt_cfg = getattr(cfg.thresholds, 'process_novelty_distance', None)\n",
        "+        distance_threshold = float(dt_cfg) if dt_cfg is not None else 1.0\n",
        "+    env_path = os.environ.get('AGENT_BASELINE_DB')\n",
        "+    if env_path:\n",
        "+        baseline_path = Path(env_path)\n",
        "+    store = BaselineStore(baseline_path)\n",
        "+    host_id = state.report.meta.host_id or \"unknown_host\"\n",
        "+    # Collect candidate findings\n",
        "+    candidates: List[Finding] = []\n",
        "+    for sr in state.report.results:\n",
        "+        if sr.scanner.lower() != 'process':\n",
        "+            continue\n",
        "+        for f in sr.findings:\n",
        "+            candidates.append(f)\n",
        "+    if not candidates:\n",
        "+        return state\n",
        "+    # Pre-compute feature vectors (parallel if enabled)\n",
        "+    vecs: dict[str, list[float]] = {}\n",
        "+    def _build_vec(f: Finding):\n",
        "+        cmd = f.metadata.get('cmdline') or f.title or f.metadata.get('process') or ''\n",
        "+        return f.id, process_feature_vector(cmd)\n",
        "+    if cfg.performance.parallel_baseline and len(candidates) > 4:\n",
        "+        with concurrent.futures.ThreadPoolExecutor(max_workers=cfg.performance.workers) as ex:\n",
        "+            for fid, v in ex.map(_build_vec, candidates):\n",
        "+                vecs[fid] = v\n",
        "+    else:\n",
        "+        for f in candidates:\n",
        "+            fid, v = _build_vec(f)\n",
        "+            vecs[fid] = v\n",
        "+    for f in candidates:\n",
        "+        vec = vecs.get(f.id)\n",
        "+        if vec is None:\n",
        "+            continue  # no vector computed\n",
        "+        cid, dist, is_new = store.assign_process_vector(host_id, vec, distance_threshold=float(distance_threshold))\n",
        "+        if is_new or dist > float(distance_threshold):\n",
        "+            if 'process_novel' not in f.tags:\n",
        "+                f.tags.append('process_novel')\n",
        "+            if f.risk_subscores:\n",
        "+                prev = f.risk_subscores.get('anomaly', 0.0)\n",
        "+                from .risk import CAPS\n",
        "+                cap = CAPS.get('anomaly', 2.0)\n",
        "+                new_anom = round(min(prev + anomaly_boost, cap),2)\n",
        "+                if new_anom != prev:\n",
        "+                    f.risk_subscores['anomaly'] = new_anom\n",
        "+                    _recompute_finding_risk(f)\n",
        "+            rationale_note = f\"novel process cluster (cid={cid} dist={dist:.2f})\"\n",
        "+            if f.rationale:\n",
        "+                f.rationale.append(rationale_note)\n",
        "+            else:\n",
        "+                f.rationale = [rationale_note]\n",
        "+        else:\n",
        "+            if dist > float(distance_threshold) * 0.8:\n",
        "+                note = f\"near-novel process (cid={cid} dist={dist:.2f})\"\n",
        "+                if f.rationale:\n",
        "+                    f.rationale.append(note)\n",
        "+                else:\n",
        "+                    f.rationale = [note]\n",
        "+    return state\n",
        "+\n",
        "+\n",
        "+def sequence_correlation(state: AgentState) -> AgentState:\n",
        "+    \"\"\"Detect suspicious temporal sequences inside a single scan.\n",
        "+    Current heuristic pattern:\n",
        "+      1. New SUID binary (tag baseline:new + suid) appears\n",
        "+      2. net.ipv4.ip_forward kernel param enabled (value=1) in same scan after the SUID finding order.\n",
        "+    If both occur, emit synthetic correlation with tag sequence_anomaly.\n",
        "+    Ordering proxy: we use appearance order in report results since per-scanner timestamps absent.\n",
        "+    \"\"\"\n",
        "+    if not state.report:\n",
        "+        return state\n",
        "+    # Flatten findings preserving order\n",
        "+    ordered: List[Finding] = []\n",
        "+    for r in state.report.results:\n",
        "+        for f in r.findings:\n",
        "+            ordered.append(f)\n",
        "+    suid_indices = []\n",
        "+    ip_forward_indices = []\n",
        "+    for idx, f in enumerate(ordered):\n",
        "+        if 'suid' in (f.tags or []) and any(t == 'baseline:new' for t in (f.tags or [])):\n",
        "+            suid_indices.append((idx, f))\n",
        "+        if f.category == 'kernel_param' and f.metadata.get('sysctl_key') == 'net.ipv4.ip_forward':\n",
        "+            val = str(f.metadata.get('value') or f.metadata.get('desired') or f.metadata.get('current') or '')\n",
        "+            if val in {'1','true','enabled'}:\n",
        "+                ip_forward_indices.append((idx, f))\n",
        "+    if suid_indices and ip_forward_indices:\n",
        "+        # Check if any suid index precedes any ip_forward index\n",
        "+        trigger_pairs = [(s,i) for (s,_) in suid_indices for (i,_) in ip_forward_indices if s < i]\n",
        "+        if trigger_pairs:\n",
        "+            # Build correlation referencing the involved findings (limit to first few to bound size)\n",
        "+            related = []\n",
        "+            for (s_idx, s_f) in suid_indices[:3]:\n",
        "+                related.append(s_f.id)\n",
        "+            for (i_idx, i_f) in ip_forward_indices[:2]:\n",
        "+                related.append(i_f.id)\n",
        "+            # Avoid duplicate correlation creation\n",
        "+            already = any(c.related_finding_ids == related and 'sequence_anomaly' in (c.tags or []) for c in state.correlations)\n",
        "+            if not already:\n",
        "+                # Deterministic ID: sequence_anom_<n>\n",
        "+                existing = [c for c in state.correlations if 'sequence_anomaly' in (c.tags or []) and c.id.startswith('sequence_anom_')]\n",
        "+                corr_id = f'sequence_anom_{len(existing)+1}'\n",
        "+                corr = Correlation(\n",
        "+                    id=corr_id,\n",
        "+                    title='Suspicious Sequence: New SUID followed by IP forwarding enabled',\n",
        "+                    rationale='Heuristic: newly introduced SUID binary preceded enabling IP forwarding in same scan',\n",
        "+                    related_finding_ids=related,\n",
        "+                    risk_score_delta=8,\n",
        "+                    tags=['sequence_anomaly','routing','privilege_escalation_surface'],\n",
        "+                    severity='high'\n",
        "+                )\n",
        "+                state.correlations.append(corr)\n",
        "+                # Back-reference on findings\n",
        "+                for f in ordered:\n",
        "+                    if f.id in related:\n",
        "+                        if corr.id not in f.correlation_refs:\n",
        "+                            f.correlation_refs.append(corr.id)\n",
        "+    return state\n",
        "+\n",
        "+\n",
        "+def reduce(state: AgentState) -> AgentState:\n",
        "+    if not state.report:\n",
        "+        return state\n",
        "+    all_findings: List[Finding] = [f for r in state.report.results for f in r.findings]\n",
        "+    mc = get_metrics_collector()\n",
        "+    with mc.time_stage('reduce.reduce_all'):\n",
        "+        state.reductions = reduce_all(all_findings)\n",
        "+    return state\n",
        "+\n",
        "+\n",
        "+def summarize(state: AgentState) -> AgentState:\n",
        "+    client = get_llm_provider()\n",
        "+    governor = get_data_governor()\n",
        "+    cfg = load_config()\n",
        "+    threshold = cfg.thresholds.summarization_risk_sum\n",
        "+    high_med_sum = 0\n",
        "+    new_found = False\n",
        "+    all_findings = [f for r in state.report.results for f in r.findings] if state.report else []\n",
        "+    for f in all_findings:\n",
        "+        sev = f.severity.lower()\n",
        "+        if sev in {\"medium\",\"high\",\"critical\"}:\n",
        "+            # Exclude operational_error pseudo-findings from security risk aggregation\n",
        "+            if not getattr(f, 'operational_error', False):\n",
        "+                high_med_sum += (f.risk_total or f.risk_score or 0)\n",
        "+        if any(t == 'baseline:new' for t in (f.tags or [])):\n",
        "+            new_found = True\n",
        "+    skip = (high_med_sum < threshold) and (not new_found)\n",
        "+    prev = getattr(state, 'summaries', None)\n",
        "+    # Redact inputs before passing to provider (governance enforcement)\n",
        "+    red_reductions = governor.redact_for_llm(state.reductions)\n",
        "+    red_correlations = [governor.redact_for_llm(c) for c in state.correlations]\n",
        "+    red_actions = [governor.redact_for_llm(a) for a in state.actions]\n",
        "+    mc = get_metrics_collector()\n",
        "+    with mc.time_stage('summarize.llm'):\n",
        "+        state.summaries = client.summarize(red_reductions, red_correlations, red_actions, skip=skip, previous=prev, skip_reason=\"low_risk_no_change\" if skip else None, baseline_context=None)\n",
        "+    # Attach token accounting snapshot for future cost modeling\n",
        "+    try:\n",
        "+        # Token accounting model removed / deprecated; inline dict if needed in future.\n",
        "+        m = state.summaries.metrics or {}\n",
        "+        state.summaries.metrics = state.summaries.metrics or {}\n",
        "+        state.summaries.metrics['token_accounting'] = {\n",
        "+            'prompt_tokens': m.get('tokens_prompt',0),\n",
        "+            'completion_tokens': m.get('tokens_completion',0),\n",
        "+            'cached': bool(m.get('skipped')),\n",
        "+            'unit_cost_prompt': float(os.environ.get('AGENT_COST_PROMPT_PER_1K', '0') or 0)/1000.0,\n",
        "+            'unit_cost_completion': float(os.environ.get('AGENT_COST_COMPLETION_PER_1K','0') or 0)/1000.0,\n",
        "+        }\n",
        "+        state.summaries.metrics['estimated_cost'] = round(\n",
        "+            state.summaries.metrics['token_accounting']['prompt_tokens'] * state.summaries.metrics['token_accounting']['unit_cost_prompt'] +\n",
        "+            state.summaries.metrics['token_accounting']['completion_tokens'] * state.summaries.metrics['token_accounting']['unit_cost_completion'], 6\n",
        "+        )\n",
        "+    except Exception:\n",
        "+        pass\n",
        "+    # Scrub narrative fields prior to persistence/output\n",
        "+    state.summaries = governor.redact_output_narratives(state.summaries)\n",
        "+    # ATT&CK coverage computation\n",
        "+    try:\n",
        "+        mapping = _load_attack_mapping()\n",
        "+        if state.report and state.summaries:\n",
        "+            covered = {}\n",
        "+            all_tags = set()\n",
        "+            for r in state.report.results:\n",
        "+                for f in r.findings:\n",
        "+                    for t in (f.tags or []):\n",
        "+                        all_tags.add(t)\n",
        "+            for c in state.correlations:\n",
        "+                for t in (c.tags or []):\n",
        "+                    all_tags.add(t)\n",
        "+            techniques = set()\n",
        "+            tag_hits = {}\n",
        "+            for tag in all_tags:\n",
        "+                techs = mapping.get(tag)\n",
        "+                if not techs:\n",
        "+                    continue\n",
        "+                if isinstance(techs, list):\n",
        "+                    for tid in techs:\n",
        "+                        techniques.add(tid)\n",
        "+                elif isinstance(techs, str):\n",
        "+                    techniques.add(techs)\n",
        "+                tag_hits[tag] = techs\n",
        "+            state.summaries.attack_coverage = {\n",
        "+                'technique_count': len(techniques),\n",
        "+                'techniques': sorted(techniques),\n",
        "+                'tag_hits': tag_hits\n",
        "+            }\n",
        "+    except Exception as e:\n",
        "+        _log_error('attack_coverage', e, state)\n",
        "+    # Experimental causal hypotheses\n",
        "+    try:\n",
        "+        if state.summaries:\n",
        "+            state.summaries.causal_hypotheses = generate_causal_hypotheses(state)\n",
        "+    except Exception as e:\n",
        "+        _log_error('causal_hypotheses', e, state)\n",
        "+    return state\n",
        "+\n",
        "+\n",
        "+def actions(state: AgentState) -> AgentState:\n",
        "+    items: List[ActionItem] = []\n",
        "+    # Simple deterministic mapping examples\n",
        "+    mc = get_metrics_collector()\n",
        "+    with mc.time_stage('actions.build'):\n",
        "+        for c in state.correlations:\n",
        "+            if \"routing\" in c.tags:\n",
        "+                items.append(ActionItem(priority=len(items)+1, action=\"Confirm intent for routing/NAT configuration; disable if not required.\", correlation_refs=[c.id]))\n",
        "+    # Baseline noise example: if many SUID unexpected\n",
        "+    if state.reductions.suid_summary and state.reductions.suid_summary.get(\"unexpected_suid\"):\n",
        "+        items.append(ActionItem(priority=len(items)+1, action=\"Review unexpected SUID binaries; remove SUID bit if unnecessary.\", correlation_refs=[]))\n",
        "+    mc.incr('actions.count', len(items))\n",
        "+    state.actions = items\n",
        "+    return state\n",
        "+\n",
        "+\n",
        "+def build_output(state: AgentState, raw_path: Path) -> EnrichedOutput:\n",
        "+    sha = hashlib.sha256(Path(raw_path).read_bytes()).hexdigest() if raw_path.exists() else None\n",
        "+    integrity_status = None\n",
        "+    try:\n",
        "+        # If verification key provided via env, attempt signature verification\n",
        "+        vkey = os.environ.get('AGENT_VERIFY_KEY_B64')\n",
        "+        if raw_path.exists():\n",
        "+            if vkey:\n",
        "+                integrity_status = verify_file(raw_path, vkey)\n",
        "+            else:\n",
        "+                # minimal status with sha only\n",
        "+                integrity_status = {'sha256_actual': sha}\n",
        "+    except Exception as e:\n",
        "+        _log_error('integrity_verify', e, state, severity='error', hint='Check signature key / file permissions')\n",
        "+        integrity_status = {'sha256_actual': sha, 'error': 'integrity_check_failed'}\n",
        "+    flat_findings = []\n",
        "+    if state.report:\n",
        "+        for r in state.report.results:\n",
        "+            flat_findings.extend(r.findings)\n",
        "+    # Correlation graph metrics\n",
        "+    mc = get_metrics_collector()\n",
        "+    with mc.time_stage('graph.annotate'):\n",
        "+        graph_meta = annotate_and_summarize(state)\n",
        "+    out = EnrichedOutput(\n",
        "+        correlations=state.correlations,\n",
        "+        reductions=state.reductions,\n",
        "+        summaries=state.summaries,\n",
        "+        actions=state.actions,\n",
        "+        raw_reference=sha,\n",
        "+        enriched_findings=flat_findings,\n",
        "+        correlation_graph=graph_meta if graph_meta else None,\n",
        "+        followups=state.followups if state.followups else None,\n",
        "+    enrichment_results=state.enrichment_results or None,\n",
        "+    multi_host_correlation=state.multi_host_correlation or None,\n",
        "+    integrity=integrity_status\n",
        "+    )\n",
        "+    if out.enrichment_results is None:\n",
        "+        out.enrichment_results = {}\n",
        "+    if state.agent_warnings:\n",
        "+        out.enrichment_results['agent_warnings'] = state.agent_warnings\n",
        "+    # Performance metrics and baseline regression detection\n",
        "+    perf_snap = mc.snapshot()\n",
        "+    baseline_path = os.environ.get('AGENT_PERF_BASELINE_PATH', 'artifacts/perf_baseline.json')\n",
        "+    threshold_env = os.environ.get('AGENT_PERF_REGRESSION_PCT', '30')\n",
        "+    try:\n",
        "+        threshold = max(0.0, float(threshold_env))/100.0\n",
        "+    except ValueError:\n",
        "+        threshold = 0.30\n",
        "+    from .metrics import MetricsCollector\n",
        "+    base = MetricsCollector.load_baseline(baseline_path)\n",
        "+    regressions = MetricsCollector.compare_to_baseline(perf_snap, base or {}, threshold) if base else []\n",
        "+    perf_snap['baseline_regressions'] = regressions\n",
        "+    perf_snap['baseline_threshold_pct'] = threshold*100\n",
        "+    out.enrichment_results['perf'] = perf_snap\n",
        "+    # Save current snapshot as new baseline (rolling)\n",
        "+    MetricsCollector.save_baseline(baseline_path, perf_snap)\n",
        "+    # Surface summary perf metrics\n",
        "+    if out.summaries:\n",
        "+        totals = perf_snap.get('durations', {})\n",
        "+        total_time = sum(v.get('total',0) for v in totals.values())\n",
        "+        slowest = None\n",
        "+        slowest_time = -1\n",
        "+        for stage, stats in totals.items():\n",
        "+            if stats.get('total',0) > slowest_time:\n",
        "+                slowest = stage\n",
        "+                slowest_time = stats.get('total',0)\n",
        "+        metrics_map = out.summaries.metrics or {}\n",
        "+        metrics_map.update({\n",
        "+            'perf.total_ms': total_time,\n",
        "+            'perf.slowest_stage': slowest,\n",
        "+            'perf.slowest_ms': slowest_time,\n",
        "+            'perf.regression_count': len(regressions)\n",
        "+        })\n",
        "+        out.summaries.metrics = metrics_map\n",
        "+    # Populate meta.analytics (INT-OBS-001) with concise summary (avoid large arrays)\n",
        "+    try:\n",
        "+        if state.report and state.report.meta:\n",
        "+            if not state.report.meta.analytics:\n",
        "+                state.report.meta.analytics = {}\n",
        "+            # Summarize durations\n",
        "+            dur_summary = {k: { 'avg_ms': round(v.get('avg',0),2), 'total_ms': v.get('total',0), 'count': v.get('count',0)} for k,v in perf_snap.get('durations', {}).items()}\n",
        "+            state.report.meta.analytics.update({\n",
        "+                'performance': {\n",
        "+                    'durations': dur_summary,\n",
        "+                    'counters': perf_snap.get('counters', {}),\n",
        "+                    'regressions': perf_snap.get('baseline_regressions', []),\n",
        "+                    'regression_threshold_pct': perf_snap.get('baseline_threshold_pct')\n",
        "+                }\n",
        "+            })\n",
        "+    except Exception:\n",
        "+        pass\n",
        "+    # Apply deterministic canonical ordering to entire output\n",
        "+    try:\n",
        "+        out_dict = out.model_dump()\n",
        "+        canon = canonicalize_enriched_output_dict(out_dict)\n",
        "+        from .models import EnrichedOutput as _EO\n",
        "+        out = _EO(**canon)\n",
        "+    except Exception:\n",
        "+        # On failure, fall back to original out\n",
        "+        pass\n",
        "+    return out\n",
        "+\n",
        "+\n",
        "+# -----------------\n",
        "+# Optional External Corpus Integration (Hugging Face datasets)\n",
        "+# -----------------\n",
        "+\n",
        "+def _augment_with_corpus_insights(state: AgentState):\n",
        "+    \"\"\"Optionally load external cybersecurity corpora (if token & pandas available) to\n",
        "+    attach high-level corpus metrics to summaries.metrics for adaptive reasoning.\n",
        "+\n",
        "+    Controlled by env AGENT_LOAD_HF_CORPUS=1. Lightweight: only counts / sample hash.\n",
        "+    Avoids loading if already present in metrics. Silent (logs on error).\"\"\"\n",
        "+    if not os.environ.get('AGENT_LOAD_HF_CORPUS'):\n",
        "+        return state\n",
        "+    try:\n",
        "+        from . import hf_loader  # lazy import\n",
        "+    except Exception as e:  # module absent\n",
        "+        _log_error('corpus_import', e)\n",
        "+        return state\n",
        "+    try:\n",
        "+        jsonl_df = hf_loader.load_cybersec_jsonl()\n",
        "+        parquet_df = hf_loader.load_cybersec_parquet()\n",
        "+        j_rows = int(len(jsonl_df)) if jsonl_df is not None else None\n",
        "+        p_rows = int(len(parquet_df)) if parquet_df is not None else None\n",
        "+        # Minimal content fingerprint (no sensitive data): column name hash\n",
        "+        import hashlib as _hl\n",
        "+        def _col_fprint(df):\n",
        "+            if df is None: return None\n",
        "+            h = _hl.sha256()\n",
        "+            for c in sorted(df.columns):\n",
        "+                h.update(c.encode())\n",
        "+            return h.hexdigest()[:16]\n",
        "+        metrics_add = {\n",
        "+            'corpus.jsonl_rows': j_rows,\n",
        "+            'corpus.parquet_rows': p_rows,\n",
        "+            'corpus.jsonl_schema_fprint': _col_fprint(jsonl_df),\n",
        "+            'corpus.parquet_schema_fprint': _col_fprint(parquet_df)\n",
        "+        }\n",
        "+        if state.summaries:\n",
        "+            base = state.summaries.metrics or {}\n",
        "+            # Do not overwrite existing keys unless None\n",
        "+            for k,v in metrics_add.items():\n",
        "+                if k not in base or base[k] is None:\n",
        "+                    base[k] = v\n",
        "+            state.summaries.metrics = base\n",
        "+    except Exception as e:\n",
        "+        _log_error('corpus_insights', e)\n",
        "+    return state\n",
        "+\n",
        "+\n",
        "+def generate_causal_hypotheses(state: AgentState, max_hypotheses: int = 3) -> list[dict]:\n",
        "+    \"\"\"Generate speculative causal hypotheses from correlations & findings.\n",
        "+    Heuristics only (deterministic):\n",
        "+      - sequence_anomaly => privilege escalation chain.\n",
        "+      - module_propagation => lateral movement via module.\n",
        "+      - presence of metric_drift finding + routing correlation => config change root cause.\n",
        "+    Mark all as speculative with low confidence.\n",
        "+    \"\"\"\n",
        "+    hyps = []\n",
        "+    for c in state.correlations:\n",
        "+        if 'sequence_anomaly' in c.tags:\n",
        "+            hyps.append({\n",
        "+                'id': f\"hyp_{len(hyps)+1}\",\n",
        "+                'summary': 'Potential privilege escalation chain (new SUID then IP forwarding)',\n",
        "+                'rationale': [c.rationale],\n",
        "+                'confidence': 'low',\n",
        "+                'speculative': True\n",
        "+            })\n",
        "+        if 'module_propagation' in c.tags:\n",
        "+            hyps.append({\n",
        "+                'id': f\"hyp_{len(hyps)+1}\",\n",
        "+                'summary': 'Possible lateral movement via near-simultaneous kernel module deployment',\n",
        "+                'rationale': [c.rationale or 'simultaneous module emergence across hosts'],\n",
        "+                'confidence': 'low',\n",
        "+                'speculative': True\n",
        "+            })\n",
        "+    drift_present = any('metric_drift' in (f.tags or []) for r in (state.report.results if state.report else []) for f in r.findings)\n",
        "+    routing_corr = any('routing' in c.tags for c in state.correlations)\n",
        "+    if drift_present and routing_corr:\n",
        "+        hyps.append({\n",
        "+            'id': f\"hyp_{len(hyps)+1}\",\n",
        "+            'summary': 'Configuration change likely triggered routing and risk metric drift',\n",
        "+            'rationale': ['metric drift finding plus routing-related correlation(s)'],\n",
        "+            'confidence': 'low',\n",
        "+            'speculative': True\n",
        "+        })\n",
        "+    # Deduplicate by summary, cap\n",
        "+    out = []\n",
        "+    seen = set()\n",
        "+    for h in hyps:\n",
        "+        if h['summary'] in seen: continue\n",
        "+        seen.add(h['summary'])\n",
        "+        out.append(h)\n",
        "+        if len(out) >= max_hypotheses:\n",
        "+            break\n",
        "+    return out\n",
        "+\n",
        "+\n",
        "+def _load_attack_mapping(path: Path = Path('agent/attack_mapping.yaml')) -> dict:\n",
        "+    try:\n",
        "+        if not path.exists():\n",
        "+            return {}\n",
        "+        data = _yaml.safe_load(path.read_text()) or {}\n",
        "+        return data\n",
        "+    except Exception:\n",
        "+        return {}\n",
        "+\n",
        "+\n",
        "+def run_pipeline(report_path: Path) -> EnrichedOutput:\n",
        "+    state = AgentState()\n",
        "+    state = load_report(state, report_path)\n",
        "+    try:\n",
        "+        log_stage('load_report', file=str(report_path), sha256=hashlib.sha256(report_path.read_bytes()).hexdigest())\n",
        "+    except Exception as e:\n",
        "+        _log_error('load_report_log', e)\n",
        "+    state = augment(state)\n",
        "+    state = integrate_compliance(state)\n",
        "+    # Policy enforcement (denylist executable paths)\n",
        "+    try:\n",
        "+        state = apply_policy(state)\n",
        "+        log_stage('policy_enforce')\n",
        "+    except Exception as e:\n",
        "+        _log_error('policy_enforce', e)\n",
        "+    try:\n",
        "+        log_stage('augment', findings=sum(len(r.findings) for r in (state.report.results if state.report else [])))\n",
        "+    except Exception as e:\n",
        "+        _log_error('augment_log', e)\n",
        "+    state = correlate(state)\n",
        "+    try:\n",
        "+        log_stage('correlate', correlations=len(state.correlations))\n",
        "+    except Exception as e:\n",
        "+        _log_error('correlate_log', e)\n",
        "+    # Temporal sequence correlations\n",
        "+    try:\n",
        "+        state = sequence_correlation(state)\n",
        "+        log_stage('sequence_correlation')\n",
        "+    except Exception as e:\n",
        "+        _log_error('sequence_correlation', e)\n",
        "+    state = baseline_rarity(state)\n",
        "+    try:\n",
        "+        log_stage('baseline_rarity')\n",
        "+    except Exception as e:\n",
        "+        _log_error('baseline_rarity_log', e)\n",
        "+    # Embedding-based process novelty\n",
        "+    try:\n",
        "+        state = process_novelty(state)\n",
        "+        log_stage('process_novelty')\n",
        "+    except Exception as e:\n",
        "+        _log_error('process_novelty', e)\n",
        "+    # Metric drift detection: derive simple metrics and record; synthesize findings if z>|threshold|\n",
        "+    if state.report and state.report.meta and state.report.meta.host_id and state.report.meta.scan_id:\n",
        "+        host_id = state.report.meta.host_id\n",
        "+        scan_id = state.report.meta.scan_id\n",
        "+        # Derive metrics (can expand later). Examples:\n",
        "+        # 1. total findings\n",
        "+        # 2. high severity count\n",
        "+        # 3. sum risk_total of medium+ severity\n",
        "+        all_findings = [f for r in state.report.results for f in r.findings]\n",
        "+        total_findings = len(all_findings)\n",
        "+        high_count = sum(1 for f in all_findings if f.severity.lower() in {\"high\",\"critical\"})\n",
        "+        med_hi_risk_sum = sum((f.risk_total or f.risk_score or 0) for f in all_findings if f.severity.lower() in {\"medium\",\"high\",\"critical\"} and not getattr(f, 'operational_error', False))\n",
        "+        metrics = {\n",
        "+            'finding.count.total': float(total_findings),\n",
        "+            'finding.count.high': float(high_count),\n",
        "+            'risk.sum.medium_high': float(med_hi_risk_sum)\n",
        "+        }\n",
        "+        store = BaselineStore(Path(\"agent_baseline.db\"))\n",
        "+        metric_stats = store.record_metrics(host_id, scan_id, metrics, history_limit=10)\n",
        "+        # Lower initial thresholds: if history_n >=2 compute z; threshold 2.5; if no std yet use simple delta heuristic\n",
        "+        drift_threshold = 2.5\n",
        "+        drift_findings = []\n",
        "+        for mname, stats in metric_stats.items():\n",
        "+            z = stats.get('z')\n",
        "+            hist_n = stats.get('history_n',0)\n",
        "+            # Accept drift if enough history for z OR early large delta vs mean when hist>=2\n",
        "+            trigger = False\n",
        "+            if z is not None and hist_n >= 2 and abs(z) >= drift_threshold:\n",
        "+                trigger = True\n",
        "+            elif z is None and hist_n >= 2 and stats.get('mean') is not None:\n",
        "+                mean = stats['mean']; val = stats['value']\n",
        "+                # simple 100% increase heuristic\n",
        "+                if mean and (val - mean)/mean >= 1.0:\n",
        "+                    trigger = True\n",
        "+            if trigger:\n",
        "+                # Build synthetic finding\n",
        "+                fid = f\"metric:{mname}:drift\"\n",
        "+                z_for_sev = abs(z) if z is not None else 0\n",
        "+                from .risk import CAPS\n",
        "+                anomaly_cap = CAPS.get('anomaly', 2.0)\n",
        "+                raw_anom = (abs(z)/3.0) if z is not None else 1.0\n",
        "+                anomaly_val = min(anomaly_cap, raw_anom)\n",
        "+                drift = Finding(\n",
        "+                    id=fid,\n",
        "+                    title=\"Metric Drift Detected\",\n",
        "+                    severity=\"medium\" if z_for_sev < 5 else \"high\",\n",
        "+                    risk_score=0,\n",
        "+                    metadata={'metric': mname, 'z': z, 'value': stats['value'], 'mean': stats['mean'], 'std': stats['std']},\n",
        "+                    category='telemetry',\n",
        "+                    tags=['synthetic','metric_drift'],\n",
        "+                    risk_subscores={'impact': 3.0, 'exposure': 0.0, 'anomaly': anomaly_val, 'confidence': 0.9},\n",
        "+                    baseline_status='new',\n",
        "+                    metric_drift=stats\n",
        "+                )\n",
        "+                # compute risk for synthetic\n",
        "+                weights = load_persistent_weights()\n",
        "+                # risk_subscores is always set above\n",
        "+                score, raw = compute_risk(drift.risk_subscores or {}, weights)\n",
        "+                drift.risk_score = score\n",
        "+                drift.risk_total = score\n",
        "+                drift.probability_actionable = apply_probability(raw)\n",
        "+                if z is not None:\n",
        "+                    drift.rationale = [f\"metric {mname} drift z={z:.2f} value={stats['value']} mean={stats['mean']:.2f} std={stats['std']:.2f}\"]\n",
        "+                else:\n",
        "+                    drift.rationale = [f\"metric {mname} early drift value={stats['value']} mean={stats['mean']:.2f} (delta >=100%)\"]\n",
        "+                drift_findings.append(drift)\n",
        "+        if drift_findings:\n",
        "+            # Append to a synthetic scanner result so downstream logic sees them\n",
        "+            sr = ScannerResult(scanner='metric_drift', finding_count=len(drift_findings), findings=drift_findings)\n",
        "+            state.report.results.append(sr)\n",
        "+    state = reduce(state)\n",
        "+    try:\n",
        "+        log_stage('reduce', top_findings=len(state.reductions.top_findings))\n",
        "+    except Exception as e:\n",
        "+        _log_error('reduce_log', e)\n",
        "+    state = actions(state)\n",
        "+    try:\n",
        "+        log_stage('actions', actions=len(state.actions))\n",
        "+    except Exception as e:\n",
        "+        _log_error('actions_log', e)\n",
        "+    # Cross-host anomaly: module simultaneous emergence\n",
        "+    try:\n",
        "+        if state.report and state.report.results:\n",
        "+            store = BaselineStore(Path(os.environ.get('AGENT_BASELINE_DB','agent_baseline.db')))\n",
        "+            recent = store.recent_module_first_seen(within_seconds=86400)\n",
        "+            threshold = int(os.environ.get('PROPAGATION_HOST_THRESHOLD','3'))\n",
        "+            current_host = state.report.meta.host_id if state.report.meta else None  # reserved for future filtering\n",
        "+            for module, hosts in recent.items():\n",
        "+                if len(hosts) >= threshold:\n",
        "+                    corr = MultiHostCorrelation(type='module_propagation', key=module, host_ids=hosts, rationale=f\"Module '{module}' first appeared on {len(hosts)} hosts within 24h window\")\n",
        "+                    state.multi_host_correlation.append(corr)\n",
        "+                    fid = f\"multi_host_module:{module}\"\n",
        "+                    synth = Finding(\n",
        "+                        id=fid,\n",
        "+                        title=f\"Potential Propagation: module {module}\",\n",
        "+                        severity='medium',\n",
        "+                        risk_score=0,\n",
        "+                        metadata={'module': module, 'host_cluster_size': len(hosts)},\n",
        "+                        category='cross_host',\n",
        "+                        tags=['synthetic','cross_host','module_propagation'],\n",
        "+                        risk_subscores={'impact': 5.0, 'exposure': 0.0, 'anomaly': min(1.5, (__import__('agent.risk', fromlist=['CAPS']).CAPS.get('anomaly',2.0))), 'confidence': 0.8},\n",
        "+                        baseline_status='new'\n",
        "+                    )\n",
        "+                    _recompute_finding_risk(synth)\n",
        "+                    synth.rationale = [f\"Module simultaneously observed on {len(hosts)} hosts (>= {threshold})\"]\n",
        "+                    added = False\n",
        "+                    for sr in state.report.results:\n",
        "+                        if sr.scanner == 'multi_host':\n",
        "+                            sr.findings.append(synth)\n",
        "+                            sr.finding_count += 1\n",
        "+                            added = True\n",
        "+                            break\n",
        "+                    if not added:\n",
        "+                        state.report.results.append(ScannerResult(scanner='multi_host', finding_count=1, findings=[synth]))\n",
        "+    except Exception as e:\n",
        "+        _log_error('multi_host_correlation', e)\n",
        "+    # Follow-up planning/execution (deterministic gate)\n",
        "+    # Criteria: finding tagged ioc:development-tool and not allowlisted\n",
        "+    if state.report:\n",
        "+        for r in state.report.results:\n",
        "+            for f in r.findings:\n",
        "+                follow = False\n",
        "+                # Heuristic: mark certain IOC executables as development tools\n",
        "+                exe_path = f.metadata.get('exe') or ''\n",
        "+                if exe_path and any(tok in exe_path for tok in ['cpptools','python-env-tools']):\n",
        "+                    if 'ioc:development-tool' not in f.tags:\n",
        "+                        f.tags.append('ioc:development-tool')\n",
        "+                if any(t == 'ioc:development-tool' for t in (f.tags or [])) and not f.allowlist_reason:\n",
        "+                    follow = True\n",
        "+                if r.scanner.lower() == 'suid' and f.metadata.get('path'):\n",
        "+                    follow = True\n",
        "+                if follow:\n",
        "+                    plan = [\"hash_binary\", \"query_package_manager\"]\n",
        "+                    results = {}\n",
        "+                    bin_path = f.metadata.get('exe') or f.metadata.get('path')\n",
        "+                    if bin_path:\n",
        "+                        results['hash_binary'] = hash_binary(bin_path)\n",
        "+                        results['query_package_manager'] = query_package_manager(bin_path)\n",
        "+                    from .models import FollowupResult\n",
        "+                    state.followups.append(FollowupResult(finding_id=f.id, plan=plan, results=results))\n",
        "+                    try:\n",
        "+                        log_stage('followup_execute', finding_id=f.id, plan=\";\".join(plan))\n",
        "+                    except Exception as e:\n",
        "+                        _log_error('followup_execute', e)\n",
        "+    # Post follow-up aggregation / severity adjustments\n",
        "+    if state.followups and state.report:\n",
        "+        # Load trusted manifest\n",
        "+        trust_path = Path(__file__).parent / 'knowledge' / 'trusted_binaries.yaml'\n",
        "+        trusted = {}\n",
        "+        if trust_path.exists():\n",
        "+            try:\n",
        "+                trusted = yaml.safe_load(trust_path.read_text()) or {}\n",
        "+            except Exception as e:\n",
        "+                _log_error('trusted_manifest_load', e)\n",
        "+                trusted = {}\n",
        "+        trust_map = trusted.get('trusted', {})\n",
        "+        report_results = state.report.results or []\n",
        "+        # Build hash->(tool_key, downgrade) index for faster match & robustness\n",
        "+        hash_index = {}\n",
        "+        for tk, meta in trust_map.items():\n",
        "+            for hv in meta.get('sha256', []) or []:\n",
        "+                hash_index[hv] = (tk, meta.get('downgrade_severity_to'))\n",
        "+        for fu in state.followups:\n",
        "+            fobj = None\n",
        "+            for r in report_results:\n",
        "+                for f in r.findings:\n",
        "+                    if f.id == fu.finding_id:\n",
        "+                        fobj = f\n",
        "+                        break\n",
        "+                if fobj:\n",
        "+                    break\n",
        "+            state.enrichment_results.setdefault(fu.finding_id, fu.results)\n",
        "+            # Evaluate trust\n",
        "+            hdata = fu.results.get('hash_binary') or {}\n",
        "+            sha = hdata.get('sha256')\n",
        "+            if fobj and sha:\n",
        "+                # First, direct hash index lookup\n",
        "+                trust_entry = hash_index.get(sha)\n",
        "+                tool_key = None\n",
        "+                downgrade = None\n",
        "+                if trust_entry:\n",
        "+                    tool_key, downgrade = trust_entry\n",
        "+                else:\n",
        "+                    # Fallback heuristic by tool key substring present in path/title\n",
        "+                    title_lower = fobj.title.lower()\n",
        "+                    path_val = fobj.metadata.get('exe') or ''\n",
        "+                    if 'cpptools' in title_lower or 'cpptools' in path_val:\n",
        "+                        tool_key = 'cpptools'\n",
        "+                        meta = trust_map.get(tool_key) or {}\n",
        "+                        if sha in (meta.get('sha256') or []):\n",
        "+                            downgrade = meta.get('downgrade_severity_to')\n",
        "+                if tool_key and downgrade and downgrade != fobj.severity:\n",
        "+                    old = fobj.severity\n",
        "+                    fobj.severity = downgrade\n",
        "+                    if fobj.rationale:\n",
        "+                        fobj.rationale.append(f\"trusted binary hash matched ({tool_key}); severity {old}->{downgrade}\")\n",
        "+                    else:\n",
        "+                        fobj.rationale = [f\"trusted binary hash matched ({tool_key}); severity {old}->{downgrade}\"]\n",
        "+                    if 'trusted_binary' not in (fobj.tags or []):\n",
        "+                        fobj.tags.append('trusted_binary')\n",
        "+                    # Also add/update severity: tag list (not removing old to preserve provenance)\n",
        "+                    sev_tag = f\"severity:{downgrade}\"\n",
        "+                    if fobj.tags and sev_tag not in fobj.tags:\n",
        "+                        fobj.tags.append(sev_tag)\n",
        "+                    # Recompute risk if impact/exposure might depend on severity externally later\n",
        "+                    _recompute_finding_risk(fobj)\n",
        "+    state = summarize(state)\n",
        "+    # Optional external corpus insights after summaries produced\n",
        "+    state = _augment_with_corpus_insights(state)\n",
        "+    try:\n",
        "+        metrics = (state.summaries.metrics if state.summaries else {}) or {}\n",
        "+        log_stage('summarize', tokens_prompt=metrics.get('tokens_prompt'), tokens_completion=metrics.get('tokens_completion'))\n",
        "+    except Exception as e:\n",
        "+        _log_error('summarize_log', e)\n",
        "+    return build_output(state, report_path)\n",
        "+\n",
        "+\n",
        "+# -----------------\n",
        "+# Policy Layer\n",
        "+# -----------------\n",
        "+\n",
        "+APPROVED_DEFAULT = [\"/bin\",\"/usr/bin\",\"/usr/local/bin\",\"/sbin\",\"/usr/sbin\",\"/opt/trusted\"]\n",
        "+SEVERITY_ORDER = [\"info\",\"low\",\"medium\",\"high\",\"critical\"]\n",
        "+\n",
        "+def _load_policy_allowlist() -> set[str]:\n",
        "+    import yaml\n",
        "+    paths: set[str] = set()\n",
        "+    # Config-based allowlist\n",
        "+    try:\n",
        "+        cfg = load_config()\n",
        "+        for p in cfg.paths.policy_allowlist:\n",
        "+            paths.add(p)\n",
        "+    except Exception:\n",
        "+        pass\n",
        "+    # File allowlist\n",
        "+    allow_file = Path('policy_allowlist.yaml')\n",
        "+    if allow_file.exists():\n",
        "+        try:\n",
        "+            data = yaml.safe_load(allow_file.read_text()) or {}\n",
        "+            for p in (data.get('allow_executables') or []):\n",
        "+                if isinstance(p, str):\n",
        "+                    paths.add(p)\n",
        "+        except Exception as e:\n",
        "+            _log_error('policy_allowlist_load', e)\n",
        "+    # Env variable (colon separated)\n",
        "+    import os as _os\n",
        "+    env_list = _os.environ.get('AGENT_POLICY_ALLOWLIST','')\n",
        "+    for part in env_list.split(':'):\n",
        "+        part = part.strip()\n",
        "+        if part:\n",
        "+            paths.add(part)\n",
        "+    return paths\n",
        "+\n",
        "+def _approved_dirs() -> list[str]:\n",
        "+    import os as _os\n",
        "+    env_dirs = _os.environ.get('AGENT_APPROVED_DIRS')\n",
        "+    if env_dirs:\n",
        "+        return [d for d in (p.strip() for p in env_dirs.split(':')) if d]\n",
        "+    return APPROVED_DEFAULT\n",
        "+\n",
        "+def apply_policy(state: AgentState) -> AgentState:\n",
        "+    if not state.report:\n",
        "+        return state\n",
        "+    allow = _load_policy_allowlist()\n",
        "+    approved = _approved_dirs()\n",
        "+    # Resolve approved dirs to absolute canonical paths\n",
        "+    approved_real = []\n",
        "+    for d in approved:\n",
        "+        try:\n",
        "+            approved_real.append(str(Path(d).resolve()))\n",
        "+        except Exception:\n",
        "+            approved_real.append(d)\n",
        "+    for sr in state.report.results:\n",
        "+        for f in sr.findings:\n",
        "+            exe = f.metadata.get('exe') if isinstance(f.metadata, dict) else None\n",
        "+            if not exe:\n",
        "+                continue\n",
        "+            # Already allowlisted\n",
        "+            if exe in allow:\n",
        "+                continue\n",
        "+            try:\n",
        "+                exe_real = str(Path(exe).resolve())\n",
        "+            except Exception:\n",
        "+                exe_real = exe\n",
        "+            in_approved = False\n",
        "+            for d in approved_real:\n",
        "+                try:\n",
        "+                    if os.path.commonpath([exe_real, d]) == d:\n",
        "+                        in_approved = True\n",
        "+                        break\n",
        "+                except Exception:\n",
        "+                    continue\n",
        "+            if not in_approved:\n",
        "+                # Escalate severity to at least high unless already critical\n",
        "+                try:\n",
        "+                    sev_idx = SEVERITY_ORDER.index(f.severity.lower()) if f.severity else 0\n",
        "+                except ValueError:\n",
        "+                    sev_idx = 0\n",
        "+                target = 'high'\n",
        "+                if f.severity.lower() != 'critical' and f.severity.lower() != target:\n",
        "+                    # Only raise if below target\n",
        "+                    if SEVERITY_ORDER.index(target) > sev_idx:\n",
        "+                        old = f.severity\n",
        "+                        f.severity = target\n",
        "+                        f.severity_source = 'policy'\n",
        "+                        if f.rationale:\n",
        "+                            f.rationale.append(f\"policy escalation: executable outside approved dirs ({exe})\")\n",
        "+                        else:\n",
        "+                            f.rationale = [f\"policy escalation: executable outside approved dirs ({exe})\"]\n",
        "+                        if f.tags is not None:\n",
        "+                            if 'policy:denied_path' not in f.tags:\n",
        "+                                f.tags.append('policy:denied_path')\n",
        "+                            sev_tag = f'severity:{target}'\n",
        "+                            if sev_tag not in f.tags:\n",
        "+                                f.tags.append(sev_tag)\n",
        "+                # risk_subscores impact bump (optional)\n",
        "+                if f.risk_subscores:\n",
        "+                    new_imp = min(10.0, (f.risk_subscores.get('impact',0)+1.0))\n",
        "+                    if new_imp != f.risk_subscores.get('impact'):\n",
        "+                        f.risk_subscores['impact'] = new_imp\n",
        "+                        _recompute_finding_risk(f)\n",
        "+    return state\n",
        "+\n"
      ]
    },
    {
      "path": "agent/legacy/reduction.py",
      "status": "added",
      "additions": 4,
      "deletions": 0,
      "patch": "@@ -0,0 +1,4 @@\n+\"\"\"\n+Legacy reduction module - imports functions from main reduction directory for backward compatibility.\n+\"\"\"\n+from ..reduction import reduce_all\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -0,0 +1,4 @@\n",
        "+\"\"\"\n",
        "+Legacy reduction module - imports functions from main reduction directory for backward compatibility.\n",
        "+\"\"\"\n",
        "+from ..reduction import reduce_all\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "agent/legacy/rules.py",
      "status": "added",
      "additions": 4,
      "deletions": 0,
      "patch": "@@ -0,0 +1,4 @@\n+\"\"\"\n+Legacy rules module - imports functions from main rules directory for backward compatibility.\n+\"\"\"\n+from ..rules import Correlator, DEFAULT_RULES, load_rules_dir\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -0,0 +1,4 @@\n",
        "+\"\"\"\n",
        "+Legacy rules module - imports functions from main rules directory for backward compatibility.\n",
        "+\"\"\"\n",
        "+from ..rules import Correlator, DEFAULT_RULES, load_rules_dir\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "agent/pipeline.py",
      "status": "modified",
      "additions": 14,
      "deletions": 1265,
      "patch": "@@ -1,1265 +1,14 @@\n-from __future__ import annotations\n-import json, hashlib, os, uuid\n-import json as _json\n-from pathlib import Path\n-from typing import List\n-from .models import AgentState, Report, Finding, EnrichedOutput, ActionItem, ScannerResult, MultiHostCorrelation, Correlation, AgentWarning\n-from .knowledge import apply_external_knowledge\n-from .rules import Correlator, DEFAULT_RULES\n-from .reduction import reduce_all\n-from .llm_provider import get_llm_provider\n-from .data_governance import get_data_governor\n-from .baseline import BaselineStore\n-from .metrics import get_metrics_collector\n-from .canonicalize import canonicalize_enriched_output_dict\n-from .risk import compute_risk, load_persistent_weights\n-from .calibration import apply_probability\n-from .graph_analysis import annotate_and_summarize\n-from .endpoint_classification import classify as classify_host_role\n-from .executors import hash_binary, query_package_manager\n-from .integrity import sha256_file, verify_file\n-from .audit import log_stage, hash_text\n-import yaml\n-from .baseline import process_feature_vector\n-import yaml as _yaml\n-import uuid as _uuid\n-from .config import load_config\n-import concurrent.futures\n-\n-# -----------------\n-# Internal helpers (risk recomputation & error logging)\n-# -----------------\n-\n-def _recompute_finding_risk(f: Finding):\n-    \"\"\"Recompute risk fields after any risk_subscores mutation.\n-    Safe no-op if subscores absent; logs errors instead of raising.\"\"\"\n-    try:\n-        subs = getattr(f, 'risk_subscores', None)\n-        if not subs:\n-            return\n-        weights = load_persistent_weights()\n-        score, raw = compute_risk(subs, weights)\n-        f.risk_score = score\n-        f.risk_total = score\n-        subs[\"_raw_weighted_sum\"] = round(raw, 3)\n-        f.probability_actionable = apply_probability(raw)\n-    except (ValueError, TypeError) as e:  # expected computation issues\n-        try:\n-            log_stage('risk_recompute_error', error=str(e), type=type(e).__name__)\n-        except Exception:\n-            pass\n-    except Exception as e:  # unexpected\n-        try:\n-            log_stage('risk_recompute_error_unexpected', error=str(e), type=type(e).__name__)\n-        except Exception:\n-            pass\n-\n-def _log_error(stage: str, e: Exception, state: AgentState | None = None, module: str = 'pipeline', severity: str = 'warning', hint: str | None = None):\n-    if state is not None:\n-        try:\n-            state.agent_warnings.append(AgentWarning(module=module, stage=stage, error_type=type(e).__name__, message=str(e), severity=severity, hint=hint).model_dump())\n-        except Exception:\n-            pass\n-    try:\n-        log_stage(f'{stage}_error', error=str(e), type=type(e).__name__)\n-    except Exception:\n-        pass\n-\n-# Node functions (imperative; future step: convert to LangGraph graph)\n-\n-def load_report(state: AgentState, path: Path) -> AgentState:\n-    \"\"\"Securely load and parse the raw JSON report.\n-\n-    Hardening steps:\n-    1. Enforce maximum size (default 5 MB, override via AGENT_MAX_REPORT_MB env).\n-    2. Read bytes then decode strictly as UTF-8 (reject invalid sequences).\n-    3. Canonicalize newlines to '\\n' before JSON parsing to avoid platform variance.\n-    \"\"\"\n-    max_mb_env = os.environ.get('AGENT_MAX_REPORT_MB')\n-    try:\n-        max_mb = int(max_mb_env) if max_mb_env else 5\n-    except ValueError:\n-        max_mb = 5\n-    mc = get_metrics_collector()\n-    with mc.time_stage('load_report.read_bytes'):\n-        raw_bytes = Path(path).read_bytes()\n-    size_mb = len(raw_bytes) / (1024 * 1024)\n-    if size_mb > max_mb:\n-        raise ValueError(f\"Report size {size_mb:.2f} MB exceeds maximum size {max_mb} MB\")\n-    try:\n-        text = raw_bytes.decode('utf-8', errors='strict')\n-    except UnicodeDecodeError as e:\n-        raise ValueError(f\"Report is not valid UTF-8: {e}\") from e\n-    # Canonicalize newlines (CRLF, CR -> LF)\n-    if '\\r' in text:\n-        text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n-    try:\n-        with mc.time_stage('load_report.json_parse'):\n-            data = json.loads(text)\n-    except json.JSONDecodeError as e:\n-        raise ValueError(f\"Report JSON parse error: {e}\") from e\n-    state.raw_report = data\n-    # Normalize risk naming migration (base_severity_score -> risk_score) BEFORE schema validation.\n-    # The C++ core now emits base_severity_score only. Downstream Python pipeline still expects\n-    # risk_score. We inject risk_score where missing for backward compatibility and retain the\n-    # original field (mapped into Finding.base_severity_score by pydantic if present).\n-    try:\n-        results = data.get('results') if isinstance(data, dict) else None\n-        if isinstance(results, list):\n-            for sr in results:\n-                findings = sr.get('findings') if isinstance(sr, dict) else None\n-                if not isinstance(findings, list):\n-                    continue\n-                for f in findings:\n-                    if not isinstance(f, dict):\n-                        continue\n-                    # If legacy risk_score missing but new base_severity_score present, copy.\n-                    if 'risk_score' not in f and 'base_severity_score' in f:\n-                        try:\n-                            f['risk_score'] = int(f.get('base_severity_score') or 0)\n-                        except (TypeError, ValueError):\n-                            f['risk_score'] = 0\n-                    # If both present but divergent (shouldn't happen), prefer explicit risk_score and log later.\n-                    # risk_total duplication if absent\n-                    if 'risk_total' not in f and 'risk_score' in f:\n-                        f['risk_total'] = f['risk_score']\n-    except Exception as norm_e:\n-        # Non-fatal; proceed to validation which may still fail with clearer message.\n-        try:\n-            log_stage('load_report.normalization_warning', error=str(norm_e), type=type(norm_e).__name__)\n-        except Exception:\n-            pass\n-    try:\n-        with mc.time_stage('load_report.validate'):\n-            state.report = Report.model_validate(data)\n-    except Exception as e:\n-        raise ValueError(f\"Report schema validation failed: {e}\") from e\n-    return state\n-\n-\n-def augment(state: AgentState) -> AgentState:\n-    \"\"\"Derive host_id, scan_id, finding categories & basic tags without modifying core C++ schema.\n-    host_id: stable hash of hostname (and kernel if present) unless provided.\n-    scan_id: random uuid4 hex per run.\n-    category: inferred from scanner name.\n-    tags: severity, scanner, plus simple heuristics (network_port, suid, module, kernel_param).\n-    risk_subscores: placeholder computation (impact/exposure/anomaly/confidence) using existing fields only.\n-    \"\"\"\n-    if not state.report:\n-        return state\n-    meta_raw = state.raw_report.get(\"meta\", {}) if state.raw_report else {}\n-    hostname = meta_raw.get(\"hostname\", \"unknown\")\n-    kernel = meta_raw.get(\"kernel\", \"\")\n-    # Derive host_id if absent\n-    if not state.report.meta.host_id:\n-        h = hashlib.sha256()\n-        h.update(hostname.encode())\n-        h.update(b\"|\")\n-        h.update(kernel.encode())\n-        state.report.meta.host_id = h.hexdigest()[:32]\n-    # Always assign a fresh scan_id (caller can override later if desired)\n-    state.report.meta.scan_id = uuid.uuid4().hex\n-    # Category mapping table\n-    cat_map = {\n-        \"process\": \"process\",\n-        \"network\": \"network_socket\",\n-        \"kernel_params\": \"kernel_param\",\n-        \"kernel_modules\": \"kernel_module\",\n-        \"modules\": \"kernel_module\",\n-        \"world_writable\": \"filesystem\",\n-        \"suid\": \"privilege_escalation_surface\",\n-        \"ioc\": \"ioc\",\n-        \"mac\": \"mac\",\n-        \"integrity\": \"integrity\",\n-        \"rules\": \"rule_enrichment\"\n-    }\n-    # Policy multipliers for impact based on category/policy nature\n-    policy_multiplier = {\n-        \"ioc\": 2.0,\n-        \"privilege_escalation_surface\": 1.5,\n-        \"network_socket\": 1.3,\n-        \"kernel_module\": 1.2,\n-        \"kernel_param\": 1.1,\n-    }\n-    severity_base = {\"info\":1, \"low\":2, \"medium\":3, \"high\":4, \"critical\":5, \"error\":4}\n-    # Iterate findings to enrich\n-    if not state.report or not state.report.results:\n-        return state\n-    # Apply external knowledge dictionaries after base tagging\n-    # First pass host role classification prerequisites (we need basic tags/listeners etc) so classification after loop\n-    mc = get_metrics_collector()\n-    with mc.time_stage('augment.iter_findings'):\n-        for sr in state.report.results:\n-            inferred_cat = cat_map.get(sr.scanner.lower(), sr.scanner.lower())\n-            for finding in sr.findings:\n-                if not finding.category:\n-                    finding.category = inferred_cat\n-                # Base tags\n-                base_tags = {f\"scanner:{sr.scanner}\", f\"severity:{finding.severity}\"}\n-                # Heuristic tags\n-                md = finding.metadata\n-                if md.get(\"port\"): base_tags.add(\"network_port\")\n-                if md.get(\"state\") == \"LISTEN\": base_tags.add(\"listening\")\n-                if md.get(\"suid\") == \"true\": base_tags.add(\"suid\")\n-                if md.get(\"module\"): base_tags.add(\"module\")\n-                if md.get(\"sysctl_key\"): base_tags.add(\"kernel_param\")\n-                if not finding.tags:\n-                    finding.tags = list(sorted(base_tags))\n-                else:\n-                    # merge preserving existing list order\n-                    existing = set(finding.tags)\n-                    for t in sorted(base_tags):\n-                        if t not in existing:\n-                            finding.tags.append(t)\n-                # Structured risk subscores initialization\n-                if not finding.risk_subscores:\n-                    exposure = 0.0\n-                    if any(t in finding.tags for t in [\"listening\",\"suid\",\"routing\",\"nat\"]):\n-                        # exposure scoring additive, clamp later\n-                        if \"listening\" in finding.tags: exposure += 1.0\n-                        if \"suid\" in finding.tags: exposure += 1.0\n-                        if any(t.startswith(\"network_port\") for t in finding.tags): exposure += 0.5\n-                        if \"routing\" in finding.tags: exposure += 0.5\n-                        if \"nat\" in finding.tags: exposure += 0.5\n-                    cat_key = finding.category or inferred_cat or \"unknown\"\n-                    impact = float(severity_base.get(finding.severity,1)) * policy_multiplier.get(cat_key,1.0)\n-                    anomaly = 0.0  # baseline stage will add weights\n-                    confidence = 1.0  # default; heuristic rules may lower\n-                    finding.risk_subscores = {\n-                        \"impact\": round(impact,2),\n-                        \"exposure\": round(min(exposure,3.0),2),\n-                        \"anomaly\": anomaly,\n-                        \"confidence\": confidence\n-                    }\n-    # Host role classification (second pass after initial tagging so we can count listeners etc.)\n-    if state.report:\n-        role, role_signals = classify_host_role(state.report)\n-        for sr in state.report.results:\n-            for f in sr.findings:\n-                f.host_role = role\n-                if not f.host_role_rationale:\n-                    f.host_role_rationale = role_signals\n-                if f.category == 'kernel_param' and f.metadata.get('sysctl_key') == 'net.ipv4.ip_forward' and f.risk_subscores:\n-                    impact_changed = False\n-                    if role in {'lightweight_router','container_host'}:\n-                        new_imp = round(max(0.5, f.risk_subscores['impact'] * 0.6),2)\n-                        if new_imp != f.risk_subscores['impact']:\n-                            f.risk_subscores['impact'] = new_imp; impact_changed = True\n-                        note = f\"host_role {role} => ip_forward normalized (impact adjusted)\"\n-                    elif role in {'workstation','dev_workstation'}:\n-                        new_imp = round(min(10.0, f.risk_subscores['impact'] * 1.2 + 0.5),2)\n-                        if new_imp != f.risk_subscores['impact']:\n-                            f.risk_subscores['impact'] = new_imp; impact_changed = True\n-                        note = f\"host_role {role} => ip_forward unusual (impact raised)\"\n-                    else:\n-                        note = None\n-                    if note:\n-                        if f.rationale:\n-                            f.rationale.append(note)\n-                        else:\n-                            f.rationale = [note]\n-                    if impact_changed:\n-                        _recompute_finding_risk(f)\n-    # Initial risk computation for findings lacking risk_score\n-    if state.report:\n-        with mc.time_stage('augment.risk_recompute_initial'):\n-            for sr in state.report.results:\n-                for finding in sr.findings:\n-                    if finding.risk_subscores and finding.risk_score is None:\n-                        _recompute_finding_risk(finding)\n-    return state\n-\n-\n-def correlate(state: AgentState) -> AgentState:\n-    # Add external knowledge enrichment pass before correlation (ensures knowledge tags in correlations)\n-    mc = get_metrics_collector()\n-    with mc.time_stage('knowledge.enrichment'):\n-        state = apply_external_knowledge(state)\n-    all_findings: List[Finding] = []\n-    if not state.report:\n-        return state\n-    for r in state.report.results:\n-        for finding in r.findings:\n-            all_findings.append(finding)\n-    cfg = load_config()\n-    # Merge default + rule dirs (dedupe by id keeping first)\n-    from .rules import load_rules_dir, DEFAULT_RULES\n-    merged = []\n-    seen = set()\n-    for rd in (cfg.paths.rule_dirs or []):\n-        for rule in load_rules_dir(rd):\n-            rid = rule.get('id')\n-            if rid and rid in seen: continue\n-            merged.append(rule); seen.add(rid)\n-    for rule in DEFAULT_RULES:\n-        rid = rule.get('id')\n-        if rid and rid in seen: continue\n-        merged.append(rule); seen.add(rid)\n-    correlator = Correlator(merged)\n-    with mc.time_stage('correlate.apply_rules'):\n-        state.correlations = correlator.apply(all_findings)\n-    mc.incr('correlate.rules_loaded', len(merged))\n-    mc.incr('correlate.correlations', len(state.correlations))\n-    # back-reference correlation ids (simple example: attach first correlation id)\n-    corr_map = {}\n-    for c in state.correlations:\n-        for fid in c.related_finding_ids:\n-            corr_map.setdefault(fid, []).append(c.id)\n-    for finding in all_findings:\n-        finding.correlation_refs = corr_map.get(finding.id, [])\n-    return state\n-\n-def integrate_compliance(state: AgentState) -> AgentState:\n-    \"\"\"Extract compliance summary/gaps from raw report (if present) and surface in metrics for downstream summarization.\n-    Adds keys:\n-      metrics.compliance_summary.<standard> = {passed, failed, score, total_controls}\n-      metrics.compliance_gap_count\n-      metrics.compliance_gaps (first N gap dicts)\n-    \"\"\"\n-    if not state.report or not state.raw_report:\n-        return state\n-    meta = state.raw_report\n-    comp_sum = meta.get('compliance_summary') or {}\n-    gaps = meta.get('compliance_gaps') or []\n-    # Enrich gaps with standardized severity normalization & richer remediation hints if minimal\n-    if gaps:\n-        # Static mapping (could later externalize) control_id/keyword -> remediation & severity normalization\n-        remediation_map = {\n-            '2.2.4': 'Baseline and harden system services; disable or remove unused services.',\n-            '164.312(e)': 'Ensure transmission security: enforce TLS 1.2+, disable weak ciphers, encrypt PHI in transit.',\n-            '164.308(a)(1)': 'Implement risk management processes; document risk analysis and ongoing monitoring.',\n-            'ID.AM-01': 'Maintain accurate asset inventory (automated discovery + periodic reconciliation).',\n-            'PR.AC-01': 'Centralize access control; enforce MFA for privileged accounts.',\n-            'PR.DS-01': 'Encrypt sensitive data at rest with strong algorithms and manage keys securely.'\n-        }\n-        sev_order = {'info':0,'low':1,'medium':2,'moderate':2,'high':3,'critical':4}\n-        # Normalize severities and backfill remediation_hints\n-        for g in gaps:\n-            cid = str(g.get('control_id') or '')\n-            # severity normalization\n-            sev = (g.get('severity') or '').lower()\n-            if sev and sev not in sev_order:\n-                # map alternative labels\n-                if sev in {'moderate'}:\n-                    g['severity'] = 'medium'\n-            # add mapped remediation if existing is missing or too short\n-            hint = g.get('remediation_hint') or ''\n-            if len(hint.strip()) < 12:\n-                mapped = remediation_map.get(cid)\n-                if mapped:\n-                    g['remediation_hint'] = mapped\n-    # Attach into summaries.metrics (create if absent)\n-    if not state.summaries:\n-        from .models import Summaries\n-        state.summaries = Summaries(metrics={})\n-    metrics = state.summaries.metrics or {}\n-    comp_export = {}\n-    for std, vals in comp_sum.items():\n-        # filter numeric fields\n-        comp_export[std] = {k: vals.get(k) for k in ['passed','failed','score','total_controls','not_applicable'] if k in vals}\n-    if comp_export:\n-        metrics['compliance_summary'] = comp_export\n-    if gaps:\n-        metrics['compliance_gap_count'] = len(gaps)\n-        # only include first 50 to cap size\n-        metrics['compliance_gaps'] = gaps[:50]\n-    state.summaries.metrics = metrics\n-    return state\n-\n-\n-def baseline_rarity(state: AgentState, baseline_path: Path = Path(\"agent_baseline.db\")) -> AgentState:\n-    \"\"\"Update findings with rarity/anomaly score based on baseline store.\n-    New finding => anomaly=1.0, existing => anomaly decays toward 0.\n-    Exposure + impact remain unchanged; recalculates composite risk_score (simple formula for now).\n-    \"\"\"\n-    if not state.report:\n-        return state\n-    import os as _os\n-    env_path = _os.environ.get('AGENT_BASELINE_DB')\n-    if env_path:\n-        baseline_path = Path(env_path)\n-    store = BaselineStore(baseline_path)\n-    host_id = state.report.meta.host_id or \"unknown_host\"\n-    all_pairs = []\n-    all_findings: List[Finding] = []\n-    for sr in state.report.results:\n-        for f in sr.findings:\n-            all_pairs.append((sr.scanner, f))\n-            all_findings.append(f)\n-    deltas = store.update_and_diff(host_id, all_pairs)\n-    # Map back anomaly score\n-    for sr in state.report.results:\n-        for finding in sr.findings:\n-            from .baseline import hashlib_sha\n-            h = finding.identity_hash()\n-            comp = hashlib_sha(sr.scanner, h)\n-            d = deltas.get(comp)\n-            if not finding.risk_subscores:\n-                continue\n-            # Anomaly weighting: new +2, existing changed +1 else decay\n-            rationale_bits = []\n-            if d:\n-                if d[\"status\"] == \"new\":\n-                    finding.risk_subscores[\"anomaly\"] = 2.0\n-                    if \"baseline:new\" not in finding.tags:\n-                        finding.tags.append(\"baseline:new\")\n-                    finding.baseline_status = \"new\"\n-                    rationale_bits.append(\"new finding (anomaly +2)\")\n-                else:\n-                    prev = d.get(\"prev_seen_count\", 1)\n-                    # Changed vs stable heuristic: if prev_seen_count just incremented from 1->2 treat as +1 else decay\n-                    if prev <= 2:\n-                        finding.risk_subscores[\"anomaly\"] = 1.0\n-                        finding.baseline_status = \"recent\"\n-                        rationale_bits.append(\"recent finding (anomaly +1)\")\n-                    else:\n-                        finding.risk_subscores[\"anomaly\"] = round(max(0.1, 1.0 / (prev)), 2)\n-                        finding.baseline_status = \"existing\"\n-                        rationale_bits.append(f\"established finding (anomaly {finding.risk_subscores['anomaly']})\")\n-                        if prev >= 5:\n-                            # Very common => tag and downweight anomaly further contextually\n-                            if 'baseline:common' not in finding.tags:\n-                                finding.tags.append('baseline:common')\n-                            rationale_bits.append('very common baseline occurrence')\n-            # Confidence adjustment (placeholder): if only pattern-based IOC (tag contains 'ioc-pattern') lower confidence\n-            if any(t.startswith(\"ioc-pattern\") for t in finding.tags):\n-                finding.risk_subscores[\"confidence\"] = min(finding.risk_subscores.get(\"confidence\",1.0), 0.7)\n-                rationale_bits.append(\"heuristic IOC pattern (confidence down)\")\n-            # Calibrated risk using weights\n-            weights = load_persistent_weights()\n-            score, raw = compute_risk(finding.risk_subscores, weights)\n-            finding.risk_score = score\n-            finding.risk_subscores[\"_raw_weighted_sum\"] = round(raw,3)\n-            finding.probability_actionable = apply_probability(raw)\n-            # Impact & exposure rationale\n-            impact = finding.risk_subscores.get(\"impact\")\n-            exposure = finding.risk_subscores.get(\"exposure\")\n-            rationale_bits.insert(0, f\"impact={impact}\")\n-            rationale_bits.insert(1, f\"exposure={exposure}\")\n-            if not finding.rationale:\n-                finding.rationale = rationale_bits\n-            else:\n-                finding.rationale.extend(rationale_bits)\n-            finding.risk_total = finding.risk_score\n-            # Log calibration observation (raw sum) for future supervised tuning\n-            if state.report and state.report.meta and state.report.meta.scan_id:\n-                comp_hash = comp  # composite hash from earlier\n-                try:\n-                    store.log_calibration_observation(host_id, state.report.meta.scan_id, comp_hash, raw)\n-                except Exception as e:\n-                    _log_error('calibration_observe', e)\n-    return state\n-\n-\n-def process_novelty(state: AgentState, baseline_path: Path = Path(\"agent_baseline.db\"), distance_threshold: float | None = None, anomaly_boost: float = 1.5) -> AgentState:\n-    \"\"\"Assign lightweight embedding-based novelty for process findings.\n-    Uses config threshold if distance_threshold not provided.\n-    Parallelizes feature vector computation if configured (CPU-bound hashing/light transforms).\"\"\"\n-    if not state.report:\n-        return state\n-    cfg = load_config()\n-    if distance_threshold is None:\n-        # Fallback to configured threshold; if missing or None, choose conservative default 1.0\n-        dt_cfg = getattr(cfg.thresholds, 'process_novelty_distance', None)\n-        distance_threshold = float(dt_cfg) if dt_cfg is not None else 1.0\n-    env_path = os.environ.get('AGENT_BASELINE_DB')\n-    if env_path:\n-        baseline_path = Path(env_path)\n-    store = BaselineStore(baseline_path)\n-    host_id = state.report.meta.host_id or \"unknown_host\"\n-    # Collect candidate findings\n-    candidates: List[Finding] = []\n-    for sr in state.report.results:\n-        if sr.scanner.lower() != 'process':\n-            continue\n-        for f in sr.findings:\n-            candidates.append(f)\n-    if not candidates:\n-        return state\n-    # Pre-compute feature vectors (parallel if enabled)\n-    vecs: dict[str, list[float]] = {}\n-    def _build_vec(f: Finding):\n-        cmd = f.metadata.get('cmdline') or f.title or f.metadata.get('process') or ''\n-        return f.id, process_feature_vector(cmd)\n-    if cfg.performance.parallel_baseline and len(candidates) > 4:\n-        with concurrent.futures.ThreadPoolExecutor(max_workers=cfg.performance.workers) as ex:\n-            for fid, v in ex.map(_build_vec, candidates):\n-                vecs[fid] = v\n-    else:\n-        for f in candidates:\n-            fid, v = _build_vec(f)\n-            vecs[fid] = v\n-    for f in candidates:\n-        vec = vecs.get(f.id)\n-        if vec is None:\n-            continue  # no vector computed\n-        cid, dist, is_new = store.assign_process_vector(host_id, vec, distance_threshold=float(distance_threshold))\n-        if is_new or dist > float(distance_threshold):\n-            if 'process_novel' not in f.tags:\n-                f.tags.append('process_novel')\n-            if f.risk_subscores:\n-                prev = f.risk_subscores.get('anomaly', 0.0)\n-                from .risk import CAPS\n-                cap = CAPS.get('anomaly', 2.0)\n-                new_anom = round(min(prev + anomaly_boost, cap),2)\n-                if new_anom != prev:\n-                    f.risk_subscores['anomaly'] = new_anom\n-                    _recompute_finding_risk(f)\n-            rationale_note = f\"novel process cluster (cid={cid} dist={dist:.2f})\"\n-            if f.rationale:\n-                f.rationale.append(rationale_note)\n-            else:\n-                f.rationale = [rationale_note]\n-        else:\n-            if dist > float(distance_threshold) * 0.8:\n-                note = f\"near-novel process (cid={cid} dist={dist:.2f})\"\n-                if f.rationale:\n-                    f.rationale.append(note)\n-                else:\n-                    f.rationale = [note]\n-    return state\n-\n-\n-def sequence_correlation(state: AgentState) -> AgentState:\n-    \"\"\"Detect suspicious temporal sequences inside a single scan.\n-    Current heuristic pattern:\n-      1. New SUID binary (tag baseline:new + suid) appears\n-      2. net.ipv4.ip_forward kernel param enabled (value=1) in same scan after the SUID finding order.\n-    If both occur, emit synthetic correlation with tag sequence_anomaly.\n-    Ordering proxy: we use appearance order in report results since per-scanner timestamps absent.\n-    \"\"\"\n-    if not state.report:\n-        return state\n-    # Flatten findings preserving order\n-    ordered: List[Finding] = []\n-    for r in state.report.results:\n-        for f in r.findings:\n-            ordered.append(f)\n-    suid_indices = []\n-    ip_forward_indices = []\n-    for idx, f in enumerate(ordered):\n-        if 'suid' in (f.tags or []) and any(t == 'baseline:new' for t in (f.tags or [])):\n-            suid_indices.append((idx, f))\n-        if f.category == 'kernel_param' and f.metadata.get('sysctl_key') == 'net.ipv4.ip_forward':\n-            val = str(f.metadata.get('value') or f.metadata.get('desired') or f.metadata.get('current') or '')\n-            if val in {'1','true','enabled'}:\n-                ip_forward_indices.append((idx, f))\n-    if suid_indices and ip_forward_indices:\n-        # Check if any suid index precedes any ip_forward index\n-        trigger_pairs = [(s,i) for (s,_) in suid_indices for (i,_) in ip_forward_indices if s < i]\n-        if trigger_pairs:\n-            # Build correlation referencing the involved findings (limit to first few to bound size)\n-            related = []\n-            for (s_idx, s_f) in suid_indices[:3]:\n-                related.append(s_f.id)\n-            for (i_idx, i_f) in ip_forward_indices[:2]:\n-                related.append(i_f.id)\n-            # Avoid duplicate correlation creation\n-            already = any(c.related_finding_ids == related and 'sequence_anomaly' in (c.tags or []) for c in state.correlations)\n-            if not already:\n-                # Deterministic ID: sequence_anom_<n>\n-                existing = [c for c in state.correlations if 'sequence_anomaly' in (c.tags or []) and c.id.startswith('sequence_anom_')]\n-                corr_id = f'sequence_anom_{len(existing)+1}'\n-                corr = Correlation(\n-                    id=corr_id,\n-                    title='Suspicious Sequence: New SUID followed by IP forwarding enabled',\n-                    rationale='Heuristic: newly introduced SUID binary preceded enabling IP forwarding in same scan',\n-                    related_finding_ids=related,\n-                    risk_score_delta=8,\n-                    tags=['sequence_anomaly','routing','privilege_escalation_surface'],\n-                    severity='high'\n-                )\n-                state.correlations.append(corr)\n-                # Back-reference on findings\n-                for f in ordered:\n-                    if f.id in related:\n-                        if corr.id not in f.correlation_refs:\n-                            f.correlation_refs.append(corr.id)\n-    return state\n-\n-\n-def reduce(state: AgentState) -> AgentState:\n-    if not state.report:\n-        return state\n-    all_findings: List[Finding] = [f for r in state.report.results for f in r.findings]\n-    mc = get_metrics_collector()\n-    with mc.time_stage('reduce.reduce_all'):\n-        state.reductions = reduce_all(all_findings)\n-    return state\n-\n-\n-def summarize(state: AgentState) -> AgentState:\n-    client = get_llm_provider()\n-    governor = get_data_governor()\n-    cfg = load_config()\n-    threshold = cfg.thresholds.summarization_risk_sum\n-    high_med_sum = 0\n-    new_found = False\n-    all_findings = [f for r in state.report.results for f in r.findings] if state.report else []\n-    for f in all_findings:\n-        sev = f.severity.lower()\n-        if sev in {\"medium\",\"high\",\"critical\"}:\n-            # Exclude operational_error pseudo-findings from security risk aggregation\n-            if not getattr(f, 'operational_error', False):\n-                high_med_sum += (f.risk_total or f.risk_score or 0)\n-        if any(t == 'baseline:new' for t in (f.tags or [])):\n-            new_found = True\n-    skip = (high_med_sum < threshold) and (not new_found)\n-    prev = getattr(state, 'summaries', None)\n-    # Redact inputs before passing to provider (governance enforcement)\n-    red_reductions = governor.redact_for_llm(state.reductions)\n-    red_correlations = [governor.redact_for_llm(c) for c in state.correlations]\n-    red_actions = [governor.redact_for_llm(a) for a in state.actions]\n-    mc = get_metrics_collector()\n-    with mc.time_stage('summarize.llm'):\n-        state.summaries = client.summarize(red_reductions, red_correlations, red_actions, skip=skip, previous=prev, skip_reason=\"low_risk_no_change\" if skip else None, baseline_context=None)\n-    # Attach token accounting snapshot for future cost modeling\n-    try:\n-        # Token accounting model removed / deprecated; inline dict if needed in future.\n-        m = state.summaries.metrics or {}\n-        state.summaries.metrics = state.summaries.metrics or {}\n-        state.summaries.metrics['token_accounting'] = {\n-            'prompt_tokens': m.get('tokens_prompt',0),\n-            'completion_tokens': m.get('tokens_completion',0),\n-            'cached': bool(m.get('skipped')),\n-            'unit_cost_prompt': float(os.environ.get('AGENT_COST_PROMPT_PER_1K', '0') or 0)/1000.0,\n-            'unit_cost_completion': float(os.environ.get('AGENT_COST_COMPLETION_PER_1K','0') or 0)/1000.0,\n-        }\n-        state.summaries.metrics['estimated_cost'] = round(\n-            state.summaries.metrics['token_accounting']['prompt_tokens'] * state.summaries.metrics['token_accounting']['unit_cost_prompt'] +\n-            state.summaries.metrics['token_accounting']['completion_tokens'] * state.summaries.metrics['token_accounting']['unit_cost_completion'], 6\n-        )\n-    except Exception:\n-        pass\n-    # Scrub narrative fields prior to persistence/output\n-    state.summaries = governor.redact_output_narratives(state.summaries)\n-    # ATT&CK coverage computation\n-    try:\n-        mapping = _load_attack_mapping()\n-        if state.report and state.summaries:\n-            covered = {}\n-            all_tags = set()\n-            for r in state.report.results:\n-                for f in r.findings:\n-                    for t in (f.tags or []):\n-                        all_tags.add(t)\n-            for c in state.correlations:\n-                for t in (c.tags or []):\n-                    all_tags.add(t)\n-            techniques = set()\n-            tag_hits = {}\n-            for tag in all_tags:\n-                techs = mapping.get(tag)\n-                if not techs:\n-                    continue\n-                if isinstance(techs, list):\n-                    for tid in techs:\n-                        techniques.add(tid)\n-                elif isinstance(techs, str):\n-                    techniques.add(techs)\n-                tag_hits[tag] = techs\n-            state.summaries.attack_coverage = {\n-                'technique_count': len(techniques),\n-                'techniques': sorted(techniques),\n-                'tag_hits': tag_hits\n-            }\n-    except Exception as e:\n-        _log_error('attack_coverage', e, state)\n-    # Experimental causal hypotheses\n-    try:\n-        if state.summaries:\n-            state.summaries.causal_hypotheses = generate_causal_hypotheses(state)\n-    except Exception as e:\n-        _log_error('causal_hypotheses', e, state)\n-    return state\n-\n-\n-def actions(state: AgentState) -> AgentState:\n-    items: List[ActionItem] = []\n-    # Simple deterministic mapping examples\n-    mc = get_metrics_collector()\n-    with mc.time_stage('actions.build'):\n-        for c in state.correlations:\n-            if \"routing\" in c.tags:\n-                items.append(ActionItem(priority=len(items)+1, action=\"Confirm intent for routing/NAT configuration; disable if not required.\", correlation_refs=[c.id]))\n-    # Baseline noise example: if many SUID unexpected\n-    if state.reductions.suid_summary and state.reductions.suid_summary.get(\"unexpected_suid\"):\n-        items.append(ActionItem(priority=len(items)+1, action=\"Review unexpected SUID binaries; remove SUID bit if unnecessary.\", correlation_refs=[]))\n-    mc.incr('actions.count', len(items))\n-    state.actions = items\n-    return state\n-\n-\n-def build_output(state: AgentState, raw_path: Path) -> EnrichedOutput:\n-    sha = hashlib.sha256(Path(raw_path).read_bytes()).hexdigest() if raw_path.exists() else None\n-    integrity_status = None\n-    try:\n-        # If verification key provided via env, attempt signature verification\n-        vkey = os.environ.get('AGENT_VERIFY_KEY_B64')\n-        if raw_path.exists():\n-            if vkey:\n-                integrity_status = verify_file(raw_path, vkey)\n-            else:\n-                # minimal status with sha only\n-                integrity_status = {'sha256_actual': sha}\n-    except Exception as e:\n-        _log_error('integrity_verify', e, state, severity='error', hint='Check signature key / file permissions')\n-        integrity_status = {'sha256_actual': sha, 'error': 'integrity_check_failed'}\n-    flat_findings = []\n-    if state.report:\n-        for r in state.report.results:\n-            flat_findings.extend(r.findings)\n-    # Correlation graph metrics\n-    mc = get_metrics_collector()\n-    with mc.time_stage('graph.annotate'):\n-        graph_meta = annotate_and_summarize(state)\n-    out = EnrichedOutput(\n-        correlations=state.correlations,\n-        reductions=state.reductions,\n-        summaries=state.summaries,\n-        actions=state.actions,\n-        raw_reference=sha,\n-        enriched_findings=flat_findings,\n-        correlation_graph=graph_meta if graph_meta else None,\n-        followups=state.followups if state.followups else None,\n-    enrichment_results=state.enrichment_results or None,\n-    multi_host_correlation=state.multi_host_correlation or None,\n-    integrity=integrity_status\n-    )\n-    if out.enrichment_results is None:\n-        out.enrichment_results = {}\n-    if state.agent_warnings:\n-        out.enrichment_results['agent_warnings'] = state.agent_warnings\n-    # Performance metrics and baseline regression detection\n-    perf_snap = mc.snapshot()\n-    baseline_path = os.environ.get('AGENT_PERF_BASELINE_PATH', 'artifacts/perf_baseline.json')\n-    threshold_env = os.environ.get('AGENT_PERF_REGRESSION_PCT', '30')\n-    try:\n-        threshold = max(0.0, float(threshold_env))/100.0\n-    except ValueError:\n-        threshold = 0.30\n-    from .metrics import MetricsCollector\n-    base = MetricsCollector.load_baseline(baseline_path)\n-    regressions = MetricsCollector.compare_to_baseline(perf_snap, base or {}, threshold) if base else []\n-    perf_snap['baseline_regressions'] = regressions\n-    perf_snap['baseline_threshold_pct'] = threshold*100\n-    out.enrichment_results['perf'] = perf_snap\n-    # Save current snapshot as new baseline (rolling)\n-    MetricsCollector.save_baseline(baseline_path, perf_snap)\n-    # Surface summary perf metrics\n-    if out.summaries:\n-        totals = perf_snap.get('durations', {})\n-        total_time = sum(v.get('total',0) for v in totals.values())\n-        slowest = None\n-        slowest_time = -1\n-        for stage, stats in totals.items():\n-            if stats.get('total',0) > slowest_time:\n-                slowest = stage\n-                slowest_time = stats.get('total',0)\n-        metrics_map = out.summaries.metrics or {}\n-        metrics_map.update({\n-            'perf.total_ms': total_time,\n-            'perf.slowest_stage': slowest,\n-            'perf.slowest_ms': slowest_time,\n-            'perf.regression_count': len(regressions)\n-        })\n-        out.summaries.metrics = metrics_map\n-    # Populate meta.analytics (INT-OBS-001) with concise summary (avoid large arrays)\n-    try:\n-        if state.report and state.report.meta:\n-            if not state.report.meta.analytics:\n-                state.report.meta.analytics = {}\n-            # Summarize durations\n-            dur_summary = {k: { 'avg_ms': round(v.get('avg',0),2), 'total_ms': v.get('total',0), 'count': v.get('count',0)} for k,v in perf_snap.get('durations', {}).items()}\n-            state.report.meta.analytics.update({\n-                'performance': {\n-                    'durations': dur_summary,\n-                    'counters': perf_snap.get('counters', {}),\n-                    'regressions': perf_snap.get('baseline_regressions', []),\n-                    'regression_threshold_pct': perf_snap.get('baseline_threshold_pct')\n-                }\n-            })\n-    except Exception:\n-        pass\n-    # Apply deterministic canonical ordering to entire output\n-    try:\n-        out_dict = out.model_dump()\n-        canon = canonicalize_enriched_output_dict(out_dict)\n-        from .models import EnrichedOutput as _EO\n-        out = _EO(**canon)\n-    except Exception:\n-        # On failure, fall back to original out\n-        pass\n-    return out\n-\n-\n-# -----------------\n-# Optional External Corpus Integration (Hugging Face datasets)\n-# -----------------\n-\n-def _augment_with_corpus_insights(state: AgentState):\n-    \"\"\"Optionally load external cybersecurity corpora (if token & pandas available) to\n-    attach high-level corpus metrics to summaries.metrics for adaptive reasoning.\n-\n-    Controlled by env AGENT_LOAD_HF_CORPUS=1. Lightweight: only counts / sample hash.\n-    Avoids loading if already present in metrics. Silent (logs on error).\"\"\"\n-    if not os.environ.get('AGENT_LOAD_HF_CORPUS'):\n-        return state\n-    try:\n-        from . import hf_loader  # lazy import\n-    except Exception as e:  # module absent\n-        _log_error('corpus_import', e)\n-        return state\n-    try:\n-        jsonl_df = hf_loader.load_cybersec_jsonl()\n-        parquet_df = hf_loader.load_cybersec_parquet()\n-        j_rows = int(len(jsonl_df)) if jsonl_df is not None else None\n-        p_rows = int(len(parquet_df)) if parquet_df is not None else None\n-        # Minimal content fingerprint (no sensitive data): column name hash\n-        import hashlib as _hl\n-        def _col_fprint(df):\n-            if df is None: return None\n-            h = _hl.sha256()\n-            for c in sorted(df.columns):\n-                h.update(c.encode())\n-            return h.hexdigest()[:16]\n-        metrics_add = {\n-            'corpus.jsonl_rows': j_rows,\n-            'corpus.parquet_rows': p_rows,\n-            'corpus.jsonl_schema_fprint': _col_fprint(jsonl_df),\n-            'corpus.parquet_schema_fprint': _col_fprint(parquet_df)\n-        }\n-        if state.summaries:\n-            base = state.summaries.metrics or {}\n-            # Do not overwrite existing keys unless None\n-            for k,v in metrics_add.items():\n-                if k not in base or base[k] is None:\n-                    base[k] = v\n-            state.summaries.metrics = base\n-    except Exception as e:\n-        _log_error('corpus_insights', e)\n-    return state\n-\n-\n-def generate_causal_hypotheses(state: AgentState, max_hypotheses: int = 3) -> list[dict]:\n-    \"\"\"Generate speculative causal hypotheses from correlations & findings.\n-    Heuristics only (deterministic):\n-      - sequence_anomaly => privilege escalation chain.\n-      - module_propagation => lateral movement via module.\n-      - presence of metric_drift finding + routing correlation => config change root cause.\n-    Mark all as speculative with low confidence.\n-    \"\"\"\n-    hyps = []\n-    for c in state.correlations:\n-        if 'sequence_anomaly' in c.tags:\n-            hyps.append({\n-                'id': f\"hyp_{len(hyps)+1}\",\n-                'summary': 'Potential privilege escalation chain (new SUID then IP forwarding)',\n-                'rationale': [c.rationale],\n-                'confidence': 'low',\n-                'speculative': True\n-            })\n-        if 'module_propagation' in c.tags:\n-            hyps.append({\n-                'id': f\"hyp_{len(hyps)+1}\",\n-                'summary': 'Possible lateral movement via near-simultaneous kernel module deployment',\n-                'rationale': [c.rationale or 'simultaneous module emergence across hosts'],\n-                'confidence': 'low',\n-                'speculative': True\n-            })\n-    drift_present = any('metric_drift' in (f.tags or []) for r in (state.report.results if state.report else []) for f in r.findings)\n-    routing_corr = any('routing' in c.tags for c in state.correlations)\n-    if drift_present and routing_corr:\n-        hyps.append({\n-            'id': f\"hyp_{len(hyps)+1}\",\n-            'summary': 'Configuration change likely triggered routing and risk metric drift',\n-            'rationale': ['metric drift finding plus routing-related correlation(s)'],\n-            'confidence': 'low',\n-            'speculative': True\n-        })\n-    # Deduplicate by summary, cap\n-    out = []\n-    seen = set()\n-    for h in hyps:\n-        if h['summary'] in seen: continue\n-        seen.add(h['summary'])\n-        out.append(h)\n-        if len(out) >= max_hypotheses:\n-            break\n-    return out\n-\n-\n-def _load_attack_mapping(path: Path = Path('agent/attack_mapping.yaml')) -> dict:\n-    try:\n-        if not path.exists():\n-            return {}\n-        data = _yaml.safe_load(path.read_text()) or {}\n-        return data\n-    except Exception:\n-        return {}\n-\n-\n-def run_pipeline(report_path: Path) -> EnrichedOutput:\n-    state = AgentState()\n-    state = load_report(state, report_path)\n-    try:\n-        log_stage('load_report', file=str(report_path), sha256=hashlib.sha256(report_path.read_bytes()).hexdigest())\n-    except Exception as e:\n-        _log_error('load_report_log', e)\n-    state = augment(state)\n-    state = integrate_compliance(state)\n-    # Policy enforcement (denylist executable paths)\n-    try:\n-        state = apply_policy(state)\n-        log_stage('policy_enforce')\n-    except Exception as e:\n-        _log_error('policy_enforce', e)\n-    try:\n-        log_stage('augment', findings=sum(len(r.findings) for r in (state.report.results if state.report else [])))\n-    except Exception as e:\n-        _log_error('augment_log', e)\n-    state = correlate(state)\n-    try:\n-        log_stage('correlate', correlations=len(state.correlations))\n-    except Exception as e:\n-        _log_error('correlate_log', e)\n-    # Temporal sequence correlations\n-    try:\n-        state = sequence_correlation(state)\n-        log_stage('sequence_correlation')\n-    except Exception as e:\n-        _log_error('sequence_correlation', e)\n-    state = baseline_rarity(state)\n-    try:\n-        log_stage('baseline_rarity')\n-    except Exception as e:\n-        _log_error('baseline_rarity_log', e)\n-    # Embedding-based process novelty\n-    try:\n-        state = process_novelty(state)\n-        log_stage('process_novelty')\n-    except Exception as e:\n-        _log_error('process_novelty', e)\n-    # Metric drift detection: derive simple metrics and record; synthesize findings if z>|threshold|\n-    if state.report and state.report.meta and state.report.meta.host_id and state.report.meta.scan_id:\n-        host_id = state.report.meta.host_id\n-        scan_id = state.report.meta.scan_id\n-        # Derive metrics (can expand later). Examples:\n-        # 1. total findings\n-        # 2. high severity count\n-        # 3. sum risk_total of medium+ severity\n-        all_findings = [f for r in state.report.results for f in r.findings]\n-        total_findings = len(all_findings)\n-        high_count = sum(1 for f in all_findings if f.severity.lower() in {\"high\",\"critical\"})\n-        med_hi_risk_sum = sum((f.risk_total or f.risk_score or 0) for f in all_findings if f.severity.lower() in {\"medium\",\"high\",\"critical\"} and not getattr(f, 'operational_error', False))\n-        metrics = {\n-            'finding.count.total': float(total_findings),\n-            'finding.count.high': float(high_count),\n-            'risk.sum.medium_high': float(med_hi_risk_sum)\n-        }\n-        store = BaselineStore(Path(\"agent_baseline.db\"))\n-        metric_stats = store.record_metrics(host_id, scan_id, metrics, history_limit=10)\n-        # Lower initial thresholds: if history_n >=2 compute z; threshold 2.5; if no std yet use simple delta heuristic\n-        drift_threshold = 2.5\n-        drift_findings = []\n-        for mname, stats in metric_stats.items():\n-            z = stats.get('z')\n-            hist_n = stats.get('history_n',0)\n-            # Accept drift if enough history for z OR early large delta vs mean when hist>=2\n-            trigger = False\n-            if z is not None and hist_n >= 2 and abs(z) >= drift_threshold:\n-                trigger = True\n-            elif z is None and hist_n >= 2 and stats.get('mean') is not None:\n-                mean = stats['mean']; val = stats['value']\n-                # simple 100% increase heuristic\n-                if mean and (val - mean)/mean >= 1.0:\n-                    trigger = True\n-            if trigger:\n-                # Build synthetic finding\n-                fid = f\"metric:{mname}:drift\"\n-                z_for_sev = abs(z) if z is not None else 0\n-                from .risk import CAPS\n-                anomaly_cap = CAPS.get('anomaly', 2.0)\n-                raw_anom = (abs(z)/3.0) if z is not None else 1.0\n-                anomaly_val = min(anomaly_cap, raw_anom)\n-                drift = Finding(\n-                    id=fid,\n-                    title=\"Metric Drift Detected\",\n-                    severity=\"medium\" if z_for_sev < 5 else \"high\",\n-                    risk_score=0,\n-                    metadata={'metric': mname, 'z': z, 'value': stats['value'], 'mean': stats['mean'], 'std': stats['std']},\n-                    category='telemetry',\n-                    tags=['synthetic','metric_drift'],\n-                    risk_subscores={'impact': 3.0, 'exposure': 0.0, 'anomaly': anomaly_val, 'confidence': 0.9},\n-                    baseline_status='new',\n-                    metric_drift=stats\n-                )\n-                # compute risk for synthetic\n-                weights = load_persistent_weights()\n-                # risk_subscores is always set above\n-                score, raw = compute_risk(drift.risk_subscores or {}, weights)\n-                drift.risk_score = score\n-                drift.risk_total = score\n-                drift.probability_actionable = apply_probability(raw)\n-                if z is not None:\n-                    drift.rationale = [f\"metric {mname} drift z={z:.2f} value={stats['value']} mean={stats['mean']:.2f} std={stats['std']:.2f}\"]\n-                else:\n-                    drift.rationale = [f\"metric {mname} early drift value={stats['value']} mean={stats['mean']:.2f} (delta >=100%)\"]\n-                drift_findings.append(drift)\n-        if drift_findings:\n-            # Append to a synthetic scanner result so downstream logic sees them\n-            sr = ScannerResult(scanner='metric_drift', finding_count=len(drift_findings), findings=drift_findings)\n-            state.report.results.append(sr)\n-    state = reduce(state)\n-    try:\n-        log_stage('reduce', top_findings=len(state.reductions.top_findings))\n-    except Exception as e:\n-        _log_error('reduce_log', e)\n-    state = actions(state)\n-    try:\n-        log_stage('actions', actions=len(state.actions))\n-    except Exception as e:\n-        _log_error('actions_log', e)\n-    # Cross-host anomaly: module simultaneous emergence\n-    try:\n-        if state.report and state.report.results:\n-            store = BaselineStore(Path(os.environ.get('AGENT_BASELINE_DB','agent_baseline.db')))\n-            recent = store.recent_module_first_seen(within_seconds=86400)\n-            threshold = int(os.environ.get('PROPAGATION_HOST_THRESHOLD','3'))\n-            current_host = state.report.meta.host_id if state.report.meta else None  # reserved for future filtering\n-            for module, hosts in recent.items():\n-                if len(hosts) >= threshold:\n-                    corr = MultiHostCorrelation(type='module_propagation', key=module, host_ids=hosts, rationale=f\"Module '{module}' first appeared on {len(hosts)} hosts within 24h window\")\n-                    state.multi_host_correlation.append(corr)\n-                    fid = f\"multi_host_module:{module}\"\n-                    synth = Finding(\n-                        id=fid,\n-                        title=f\"Potential Propagation: module {module}\",\n-                        severity='medium',\n-                        risk_score=0,\n-                        metadata={'module': module, 'host_cluster_size': len(hosts)},\n-                        category='cross_host',\n-                        tags=['synthetic','cross_host','module_propagation'],\n-                        risk_subscores={'impact': 5.0, 'exposure': 0.0, 'anomaly': min(1.5, (__import__('agent.risk', fromlist=['CAPS']).CAPS.get('anomaly',2.0))), 'confidence': 0.8},\n-                        baseline_status='new'\n-                    )\n-                    _recompute_finding_risk(synth)\n-                    synth.rationale = [f\"Module simultaneously observed on {len(hosts)} hosts (>= {threshold})\"]\n-                    added = False\n-                    for sr in state.report.results:\n-                        if sr.scanner == 'multi_host':\n-                            sr.findings.append(synth)\n-                            sr.finding_count += 1\n-                            added = True\n-                            break\n-                    if not added:\n-                        state.report.results.append(ScannerResult(scanner='multi_host', finding_count=1, findings=[synth]))\n-    except Exception as e:\n-        _log_error('multi_host_correlation', e)\n-    # Follow-up planning/execution (deterministic gate)\n-    # Criteria: finding tagged ioc:development-tool and not allowlisted\n-    if state.report:\n-        for r in state.report.results:\n-            for f in r.findings:\n-                follow = False\n-                # Heuristic: mark certain IOC executables as development tools\n-                exe_path = f.metadata.get('exe') or ''\n-                if exe_path and any(tok in exe_path for tok in ['cpptools','python-env-tools']):\n-                    if 'ioc:development-tool' not in f.tags:\n-                        f.tags.append('ioc:development-tool')\n-                if any(t == 'ioc:development-tool' for t in (f.tags or [])) and not f.allowlist_reason:\n-                    follow = True\n-                if r.scanner.lower() == 'suid' and f.metadata.get('path'):\n-                    follow = True\n-                if follow:\n-                    plan = [\"hash_binary\", \"query_package_manager\"]\n-                    results = {}\n-                    bin_path = f.metadata.get('exe') or f.metadata.get('path')\n-                    if bin_path:\n-                        results['hash_binary'] = hash_binary(bin_path)\n-                        results['query_package_manager'] = query_package_manager(bin_path)\n-                    from .models import FollowupResult\n-                    state.followups.append(FollowupResult(finding_id=f.id, plan=plan, results=results))\n-                    try:\n-                        log_stage('followup_execute', finding_id=f.id, plan=\";\".join(plan))\n-                    except Exception as e:\n-                        _log_error('followup_execute', e)\n-    # Post follow-up aggregation / severity adjustments\n-    if state.followups and state.report:\n-        # Load trusted manifest\n-        trust_path = Path(__file__).parent / 'knowledge' / 'trusted_binaries.yaml'\n-        trusted = {}\n-        if trust_path.exists():\n-            try:\n-                trusted = yaml.safe_load(trust_path.read_text()) or {}\n-            except Exception as e:\n-                _log_error('trusted_manifest_load', e)\n-                trusted = {}\n-        trust_map = trusted.get('trusted', {})\n-        report_results = state.report.results or []\n-        # Build hash->(tool_key, downgrade) index for faster match & robustness\n-        hash_index = {}\n-        for tk, meta in trust_map.items():\n-            for hv in meta.get('sha256', []) or []:\n-                hash_index[hv] = (tk, meta.get('downgrade_severity_to'))\n-        for fu in state.followups:\n-            fobj = None\n-            for r in report_results:\n-                for f in r.findings:\n-                    if f.id == fu.finding_id:\n-                        fobj = f\n-                        break\n-                if fobj:\n-                    break\n-            state.enrichment_results.setdefault(fu.finding_id, fu.results)\n-            # Evaluate trust\n-            hdata = fu.results.get('hash_binary') or {}\n-            sha = hdata.get('sha256')\n-            if fobj and sha:\n-                # First, direct hash index lookup\n-                trust_entry = hash_index.get(sha)\n-                tool_key = None\n-                downgrade = None\n-                if trust_entry:\n-                    tool_key, downgrade = trust_entry\n-                else:\n-                    # Fallback heuristic by tool key substring present in path/title\n-                    title_lower = fobj.title.lower()\n-                    path_val = fobj.metadata.get('exe') or ''\n-                    if 'cpptools' in title_lower or 'cpptools' in path_val:\n-                        tool_key = 'cpptools'\n-                        meta = trust_map.get(tool_key) or {}\n-                        if sha in (meta.get('sha256') or []):\n-                            downgrade = meta.get('downgrade_severity_to')\n-                if tool_key and downgrade and downgrade != fobj.severity:\n-                    old = fobj.severity\n-                    fobj.severity = downgrade\n-                    if fobj.rationale:\n-                        fobj.rationale.append(f\"trusted binary hash matched ({tool_key}); severity {old}->{downgrade}\")\n-                    else:\n-                        fobj.rationale = [f\"trusted binary hash matched ({tool_key}); severity {old}->{downgrade}\"]\n-                    if 'trusted_binary' not in (fobj.tags or []):\n-                        fobj.tags.append('trusted_binary')\n-                    # Also add/update severity: tag list (not removing old to preserve provenance)\n-                    sev_tag = f\"severity:{downgrade}\"\n-                    if fobj.tags and sev_tag not in fobj.tags:\n-                        fobj.tags.append(sev_tag)\n-                    # Recompute risk if impact/exposure might depend on severity externally later\n-                    _recompute_finding_risk(fobj)\n-    state = summarize(state)\n-    # Optional external corpus insights after summaries produced\n-    state = _augment_with_corpus_insights(state)\n-    try:\n-        metrics = (state.summaries.metrics if state.summaries else {}) or {}\n-        log_stage('summarize', tokens_prompt=metrics.get('tokens_prompt'), tokens_completion=metrics.get('tokens_completion'))\n-    except Exception as e:\n-        _log_error('summarize_log', e)\n-    return build_output(state, report_path)\n-\n-\n-# -----------------\n-# Policy Layer\n-# -----------------\n-\n-APPROVED_DEFAULT = [\"/bin\",\"/usr/bin\",\"/usr/local/bin\",\"/sbin\",\"/usr/sbin\",\"/opt/trusted\"]\n-SEVERITY_ORDER = [\"info\",\"low\",\"medium\",\"high\",\"critical\"]\n-\n-def _load_policy_allowlist() -> set[str]:\n-    import yaml\n-    paths: set[str] = set()\n-    # Config-based allowlist\n-    try:\n-        cfg = load_config()\n-        for p in cfg.paths.policy_allowlist:\n-            paths.add(p)\n-    except Exception:\n-        pass\n-    # File allowlist\n-    allow_file = Path('policy_allowlist.yaml')\n-    if allow_file.exists():\n-        try:\n-            data = yaml.safe_load(allow_file.read_text()) or {}\n-            for p in (data.get('allow_executables') or []):\n-                if isinstance(p, str):\n-                    paths.add(p)\n-        except Exception as e:\n-            _log_error('policy_allowlist_load', e)\n-    # Env variable (colon separated)\n-    import os as _os\n-    env_list = _os.environ.get('AGENT_POLICY_ALLOWLIST','')\n-    for part in env_list.split(':'):\n-        part = part.strip()\n-        if part:\n-            paths.add(part)\n-    return paths\n-\n-def _approved_dirs() -> list[str]:\n-    import os as _os\n-    env_dirs = _os.environ.get('AGENT_APPROVED_DIRS')\n-    if env_dirs:\n-        return [d for d in (p.strip() for p in env_dirs.split(':')) if d]\n-    return APPROVED_DEFAULT\n-\n-def apply_policy(state: AgentState) -> AgentState:\n-    if not state.report:\n-        return state\n-    allow = _load_policy_allowlist()\n-    approved = _approved_dirs()\n-    # Resolve approved dirs to absolute canonical paths\n-    approved_real = []\n-    for d in approved:\n-        try:\n-            approved_real.append(str(Path(d).resolve()))\n-        except Exception:\n-            approved_real.append(d)\n-    for sr in state.report.results:\n-        for f in sr.findings:\n-            exe = f.metadata.get('exe') if isinstance(f.metadata, dict) else None\n-            if not exe:\n-                continue\n-            # Already allowlisted\n-            if exe in allow:\n-                continue\n-            try:\n-                exe_real = str(Path(exe).resolve())\n-            except Exception:\n-                exe_real = exe\n-            in_approved = False\n-            for d in approved_real:\n-                try:\n-                    if os.path.commonpath([exe_real, d]) == d:\n-                        in_approved = True\n-                        break\n-                except Exception:\n-                    continue\n-            if not in_approved:\n-                # Escalate severity to at least high unless already critical\n-                try:\n-                    sev_idx = SEVERITY_ORDER.index(f.severity.lower()) if f.severity else 0\n-                except ValueError:\n-                    sev_idx = 0\n-                target = 'high'\n-                if f.severity.lower() != 'critical' and f.severity.lower() != target:\n-                    # Only raise if below target\n-                    if SEVERITY_ORDER.index(target) > sev_idx:\n-                        old = f.severity\n-                        f.severity = target\n-                        f.severity_source = 'policy'\n-                        if f.rationale:\n-                            f.rationale.append(f\"policy escalation: executable outside approved dirs ({exe})\")\n-                        else:\n-                            f.rationale = [f\"policy escalation: executable outside approved dirs ({exe})\"]\n-                        if f.tags is not None:\n-                            if 'policy:denied_path' not in f.tags:\n-                                f.tags.append('policy:denied_path')\n-                            sev_tag = f'severity:{target}'\n-                            if sev_tag not in f.tags:\n-                                f.tags.append(sev_tag)\n-                # risk_subscores impact bump (optional)\n-                if f.risk_subscores:\n-                    new_imp = min(10.0, (f.risk_subscores.get('impact',0)+1.0))\n-                    if new_imp != f.risk_subscores.get('impact'):\n-                        f.risk_subscores['impact'] = new_imp\n-                        _recompute_finding_risk(f)\n-    return state\n-\n+\"\"\"\n+Pipeline module - imports functions from legacy pipeline for backward compatibility.\n+\"\"\"\n+from .legacy.pipeline import (\n+    load_report,\n+    augment,\n+    correlate,\n+    baseline_rarity,\n+    process_novelty,\n+    sequence_correlation,\n+    reduce,\n+    summarize,\n+    run_pipeline\n+)\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -1,1265 +1,14 @@\n",
        "-from __future__ import annotations\n",
        "-import json, hashlib, os, uuid\n",
        "-import json as _json\n",
        "-from pathlib import Path\n",
        "-from typing import List\n",
        "-from .models import AgentState, Report, Finding, EnrichedOutput, ActionItem, ScannerResult, MultiHostCorrelation, Correlation, AgentWarning\n",
        "-from .knowledge import apply_external_knowledge\n",
        "-from .rules import Correlator, DEFAULT_RULES\n",
        "-from .reduction import reduce_all\n",
        "-from .llm_provider import get_llm_provider\n",
        "-from .data_governance import get_data_governor\n",
        "-from .baseline import BaselineStore\n",
        "-from .metrics import get_metrics_collector\n",
        "-from .canonicalize import canonicalize_enriched_output_dict\n",
        "-from .risk import compute_risk, load_persistent_weights\n",
        "-from .calibration import apply_probability\n",
        "-from .graph_analysis import annotate_and_summarize\n",
        "-from .endpoint_classification import classify as classify_host_role\n",
        "-from .executors import hash_binary, query_package_manager\n",
        "-from .integrity import sha256_file, verify_file\n",
        "-from .audit import log_stage, hash_text\n",
        "-import yaml\n",
        "-from .baseline import process_feature_vector\n",
        "-import yaml as _yaml\n",
        "-import uuid as _uuid\n",
        "-from .config import load_config\n",
        "-import concurrent.futures\n",
        "-\n",
        "-# -----------------\n",
        "-# Internal helpers (risk recomputation & error logging)\n",
        "-# -----------------\n",
        "-\n",
        "-def _recompute_finding_risk(f: Finding):\n",
        "-    \"\"\"Recompute risk fields after any risk_subscores mutation.\n",
        "-    Safe no-op if subscores absent; logs errors instead of raising.\"\"\"\n",
        "-    try:\n",
        "-        subs = getattr(f, 'risk_subscores', None)\n",
        "-        if not subs:\n",
        "-            return\n",
        "-        weights = load_persistent_weights()\n",
        "-        score, raw = compute_risk(subs, weights)\n",
        "-        f.risk_score = score\n",
        "-        f.risk_total = score\n",
        "-        subs[\"_raw_weighted_sum\"] = round(raw, 3)\n",
        "-        f.probability_actionable = apply_probability(raw)\n",
        "-    except (ValueError, TypeError) as e:  # expected computation issues\n",
        "-        try:\n",
        "-            log_stage('risk_recompute_error', error=str(e), type=type(e).__name__)\n",
        "-        except Exception:\n",
        "-            pass\n",
        "-    except Exception as e:  # unexpected\n",
        "-        try:\n",
        "-            log_stage('risk_recompute_error_unexpected', error=str(e), type=type(e).__name__)\n",
        "-        except Exception:\n",
        "-            pass\n",
        "-\n",
        "-def _log_error(stage: str, e: Exception, state: AgentState | None = None, module: str = 'pipeline', severity: str = 'warning', hint: str | None = None):\n",
        "-    if state is not None:\n",
        "-        try:\n",
        "-            state.agent_warnings.append(AgentWarning(module=module, stage=stage, error_type=type(e).__name__, message=str(e), severity=severity, hint=hint).model_dump())\n",
        "-        except Exception:\n",
        "-            pass\n",
        "-    try:\n",
        "-        log_stage(f'{stage}_error', error=str(e), type=type(e).__name__)\n",
        "-    except Exception:\n",
        "-        pass\n",
        "-\n",
        "-# Node functions (imperative; future step: convert to LangGraph graph)\n",
        "-\n",
        "-def load_report(state: AgentState, path: Path) -> AgentState:\n",
        "-    \"\"\"Securely load and parse the raw JSON report.\n",
        "-\n",
        "-    Hardening steps:\n",
        "-    1. Enforce maximum size (default 5 MB, override via AGENT_MAX_REPORT_MB env).\n",
        "-    2. Read bytes then decode strictly as UTF-8 (reject invalid sequences).\n",
        "-    3. Canonicalize newlines to '\\n' before JSON parsing to avoid platform variance.\n",
        "-    \"\"\"\n",
        "-    max_mb_env = os.environ.get('AGENT_MAX_REPORT_MB')\n",
        "-    try:\n",
        "-        max_mb = int(max_mb_env) if max_mb_env else 5\n",
        "-    except ValueError:\n",
        "-        max_mb = 5\n",
        "-    mc = get_metrics_collector()\n",
        "-    with mc.time_stage('load_report.read_bytes'):\n",
        "-        raw_bytes = Path(path).read_bytes()\n",
        "-    size_mb = len(raw_bytes) / (1024 * 1024)\n",
        "-    if size_mb > max_mb:\n",
        "-        raise ValueError(f\"Report size {size_mb:.2f} MB exceeds maximum size {max_mb} MB\")\n",
        "-    try:\n",
        "-        text = raw_bytes.decode('utf-8', errors='strict')\n",
        "-    except UnicodeDecodeError as e:\n",
        "-        raise ValueError(f\"Report is not valid UTF-8: {e}\") from e\n",
        "-    # Canonicalize newlines (CRLF, CR -> LF)\n",
        "-    if '\\r' in text:\n",
        "-        text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
        "-    try:\n",
        "-        with mc.time_stage('load_report.json_parse'):\n",
        "-            data = json.loads(text)\n",
        "-    except json.JSONDecodeError as e:\n",
        "-        raise ValueError(f\"Report JSON parse error: {e}\") from e\n",
        "-    state.raw_report = data\n",
        "-    # Normalize risk naming migration (base_severity_score -> risk_score) BEFORE schema validation.\n",
        "-    # The C++ core now emits base_severity_score only. Downstream Python pipeline still expects\n",
        "-    # risk_score. We inject risk_score where missing for backward compatibility and retain the\n",
        "-    # original field (mapped into Finding.base_severity_score by pydantic if present).\n",
        "-    try:\n",
        "-        results = data.get('results') if isinstance(data, dict) else None\n",
        "-        if isinstance(results, list):\n",
        "-            for sr in results:\n",
        "-                findings = sr.get('findings') if isinstance(sr, dict) else None\n",
        "-                if not isinstance(findings, list):\n",
        "-                    continue\n",
        "-                for f in findings:\n",
        "-                    if not isinstance(f, dict):\n",
        "-                        continue\n",
        "-                    # If legacy risk_score missing but new base_severity_score present, copy.\n",
        "-                    if 'risk_score' not in f and 'base_severity_score' in f:\n",
        "-                        try:\n",
        "-                            f['risk_score'] = int(f.get('base_severity_score') or 0)\n",
        "-                        except (TypeError, ValueError):\n",
        "-                            f['risk_score'] = 0\n",
        "-                    # If both present but divergent (shouldn't happen), prefer explicit risk_score and log later.\n",
        "-                    # risk_total duplication if absent\n",
        "-                    if 'risk_total' not in f and 'risk_score' in f:\n",
        "-                        f['risk_total'] = f['risk_score']\n",
        "-    except Exception as norm_e:\n",
        "-        # Non-fatal; proceed to validation which may still fail with clearer message.\n",
        "-        try:\n",
        "-            log_stage('load_report.normalization_warning', error=str(norm_e), type=type(norm_e).__name__)\n",
        "-        except Exception:\n",
        "-            pass\n",
        "-    try:\n",
        "-        with mc.time_stage('load_report.validate'):\n",
        "-            state.report = Report.model_validate(data)\n",
        "-    except Exception as e:\n",
        "-        raise ValueError(f\"Report schema validation failed: {e}\") from e\n",
        "-    return state\n",
        "-\n",
        "-\n",
        "-def augment(state: AgentState) -> AgentState:\n",
        "-    \"\"\"Derive host_id, scan_id, finding categories & basic tags without modifying core C++ schema.\n",
        "-    host_id: stable hash of hostname (and kernel if present) unless provided.\n",
        "-    scan_id: random uuid4 hex per run.\n",
        "-    category: inferred from scanner name.\n",
        "-    tags: severity, scanner, plus simple heuristics (network_port, suid, module, kernel_param).\n",
        "-    risk_subscores: placeholder computation (impact/exposure/anomaly/confidence) using existing fields only.\n",
        "-    \"\"\"\n",
        "-    if not state.report:\n",
        "-        return state\n",
        "-    meta_raw = state.raw_report.get(\"meta\", {}) if state.raw_report else {}\n",
        "-    hostname = meta_raw.get(\"hostname\", \"unknown\")\n",
        "-    kernel = meta_raw.get(\"kernel\", \"\")\n",
        "-    # Derive host_id if absent\n",
        "-    if not state.report.meta.host_id:\n",
        "-        h = hashlib.sha256()\n",
        "-        h.update(hostname.encode())\n",
        "-        h.update(b\"|\")\n",
        "-        h.update(kernel.encode())\n",
        "-        state.report.meta.host_id = h.hexdigest()[:32]\n",
        "-    # Always assign a fresh scan_id (caller can override later if desired)\n",
        "-    state.report.meta.scan_id = uuid.uuid4().hex\n",
        "-    # Category mapping table\n",
        "-    cat_map = {\n",
        "-        \"process\": \"process\",\n",
        "-        \"network\": \"network_socket\",\n",
        "-        \"kernel_params\": \"kernel_param\",\n",
        "-        \"kernel_modules\": \"kernel_module\",\n",
        "-        \"modules\": \"kernel_module\",\n",
        "-        \"world_writable\": \"filesystem\",\n",
        "-        \"suid\": \"privilege_escalation_surface\",\n",
        "-        \"ioc\": \"ioc\",\n",
        "-        \"mac\": \"mac\",\n",
        "-        \"integrity\": \"integrity\",\n",
        "-        \"rules\": \"rule_enrichment\"\n",
        "-    }\n",
        "-    # Policy multipliers for impact based on category/policy nature\n",
        "-    policy_multiplier = {\n",
        "-        \"ioc\": 2.0,\n",
        "-        \"privilege_escalation_surface\": 1.5,\n",
        "-        \"network_socket\": 1.3,\n",
        "-        \"kernel_module\": 1.2,\n",
        "-        \"kernel_param\": 1.1,\n",
        "-    }\n",
        "-    severity_base = {\"info\":1, \"low\":2, \"medium\":3, \"high\":4, \"critical\":5, \"error\":4}\n",
        "-    # Iterate findings to enrich\n",
        "-    if not state.report or not state.report.results:\n",
        "-        return state\n",
        "-    # Apply external knowledge dictionaries after base tagging\n",
        "-    # First pass host role classification prerequisites (we need basic tags/listeners etc) so classification after loop\n",
        "-    mc = get_metrics_collector()\n",
        "-    with mc.time_stage('augment.iter_findings'):\n",
        "-        for sr in state.report.results:\n",
        "-            inferred_cat = cat_map.get(sr.scanner.lower(), sr.scanner.lower())\n",
        "-            for finding in sr.findings:\n",
        "-                if not finding.category:\n",
        "-                    finding.category = inferred_cat\n",
        "-                # Base tags\n",
        "-                base_tags = {f\"scanner:{sr.scanner}\", f\"severity:{finding.severity}\"}\n",
        "-                # Heuristic tags\n",
        "-                md = finding.metadata\n",
        "-                if md.get(\"port\"): base_tags.add(\"network_port\")\n",
        "-                if md.get(\"state\") == \"LISTEN\": base_tags.add(\"listening\")\n",
        "-                if md.get(\"suid\") == \"true\": base_tags.add(\"suid\")\n",
        "-                if md.get(\"module\"): base_tags.add(\"module\")\n",
        "-                if md.get(\"sysctl_key\"): base_tags.add(\"kernel_param\")\n",
        "-                if not finding.tags:\n",
        "-                    finding.tags = list(sorted(base_tags))\n",
        "-                else:\n",
        "-                    # merge preserving existing list order\n",
        "-                    existing = set(finding.tags)\n",
        "-                    for t in sorted(base_tags):\n",
        "-                        if t not in existing:\n",
        "-                            finding.tags.append(t)\n",
        "-                # Structured risk subscores initialization\n",
        "-                if not finding.risk_subscores:\n",
        "-                    exposure = 0.0\n",
        "-                    if any(t in finding.tags for t in [\"listening\",\"suid\",\"routing\",\"nat\"]):\n",
        "-                        # exposure scoring additive, clamp later\n",
        "-                        if \"listening\" in finding.tags: exposure += 1.0\n",
        "-                        if \"suid\" in finding.tags: exposure += 1.0\n",
        "-                        if any(t.startswith(\"network_port\") for t in finding.tags): exposure += 0.5\n",
        "-                        if \"routing\" in finding.tags: exposure += 0.5\n",
        "-                        if \"nat\" in finding.tags: exposure += 0.5\n",
        "-                    cat_key = finding.category or inferred_cat or \"unknown\"\n",
        "-                    impact = float(severity_base.get(finding.severity,1)) * policy_multiplier.get(cat_key,1.0)\n",
        "-                    anomaly = 0.0  # baseline stage will add weights\n",
        "-                    confidence = 1.0  # default; heuristic rules may lower\n",
        "-                    finding.risk_subscores = {\n",
        "-                        \"impact\": round(impact,2),\n",
        "-                        \"exposure\": round(min(exposure,3.0),2),\n",
        "-                        \"anomaly\": anomaly,\n",
        "-                        \"confidence\": confidence\n",
        "-                    }\n",
        "-    # Host role classification (second pass after initial tagging so we can count listeners etc.)\n",
        "-    if state.report:\n",
        "-        role, role_signals = classify_host_role(state.report)\n",
        "-        for sr in state.report.results:\n",
        "-            for f in sr.findings:\n",
        "-                f.host_role = role\n",
        "-                if not f.host_role_rationale:\n",
        "-                    f.host_role_rationale = role_signals\n",
        "-                if f.category == 'kernel_param' and f.metadata.get('sysctl_key') == 'net.ipv4.ip_forward' and f.risk_subscores:\n",
        "-                    impact_changed = False\n",
        "-                    if role in {'lightweight_router','container_host'}:\n",
        "-                        new_imp = round(max(0.5, f.risk_subscores['impact'] * 0.6),2)\n",
        "-                        if new_imp != f.risk_subscores['impact']:\n",
        "-                            f.risk_subscores['impact'] = new_imp; impact_changed = True\n",
        "-                        note = f\"host_role {role} => ip_forward normalized (impact adjusted)\"\n",
        "-                    elif role in {'workstation','dev_workstation'}:\n",
        "-                        new_imp = round(min(10.0, f.risk_subscores['impact'] * 1.2 + 0.5),2)\n",
        "-                        if new_imp != f.risk_subscores['impact']:\n",
        "-                            f.risk_subscores['impact'] = new_imp; impact_changed = True\n",
        "-                        note = f\"host_role {role} => ip_forward unusual (impact raised)\"\n",
        "-                    else:\n",
        "-                        note = None\n",
        "-                    if note:\n",
        "-                        if f.rationale:\n",
        "-                            f.rationale.append(note)\n",
        "-                        else:\n",
        "-                            f.rationale = [note]\n",
        "-                    if impact_changed:\n",
        "-                        _recompute_finding_risk(f)\n",
        "-    # Initial risk computation for findings lacking risk_score\n",
        "-    if state.report:\n",
        "-        with mc.time_stage('augment.risk_recompute_initial'):\n",
        "-            for sr in state.report.results:\n",
        "-                for finding in sr.findings:\n",
        "-                    if finding.risk_subscores and finding.risk_score is None:\n",
        "-                        _recompute_finding_risk(finding)\n",
        "-    return state\n",
        "-\n",
        "-\n",
        "-def correlate(state: AgentState) -> AgentState:\n",
        "-    # Add external knowledge enrichment pass before correlation (ensures knowledge tags in correlations)\n",
        "-    mc = get_metrics_collector()\n",
        "-    with mc.time_stage('knowledge.enrichment'):\n",
        "-        state = apply_external_knowledge(state)\n",
        "-    all_findings: List[Finding] = []\n",
        "-    if not state.report:\n",
        "-        return state\n",
        "-    for r in state.report.results:\n",
        "-        for finding in r.findings:\n",
        "-            all_findings.append(finding)\n",
        "-    cfg = load_config()\n",
        "-    # Merge default + rule dirs (dedupe by id keeping first)\n",
        "-    from .rules import load_rules_dir, DEFAULT_RULES\n",
        "-    merged = []\n",
        "-    seen = set()\n",
        "-    for rd in (cfg.paths.rule_dirs or []):\n",
        "-        for rule in load_rules_dir(rd):\n",
        "-            rid = rule.get('id')\n",
        "-            if rid and rid in seen: continue\n",
        "-            merged.append(rule); seen.add(rid)\n",
        "-    for rule in DEFAULT_RULES:\n",
        "-        rid = rule.get('id')\n",
        "-        if rid and rid in seen: continue\n",
        "-        merged.append(rule); seen.add(rid)\n",
        "-    correlator = Correlator(merged)\n",
        "-    with mc.time_stage('correlate.apply_rules'):\n",
        "-        state.correlations = correlator.apply(all_findings)\n",
        "-    mc.incr('correlate.rules_loaded', len(merged))\n",
        "-    mc.incr('correlate.correlations', len(state.correlations))\n",
        "-    # back-reference correlation ids (simple example: attach first correlation id)\n",
        "-    corr_map = {}\n",
        "-    for c in state.correlations:\n",
        "-        for fid in c.related_finding_ids:\n",
        "-            corr_map.setdefault(fid, []).append(c.id)\n",
        "-    for finding in all_findings:\n",
        "-        finding.correlation_refs = corr_map.get(finding.id, [])\n",
        "-    return state\n",
        "-\n",
        "-def integrate_compliance(state: AgentState) -> AgentState:\n",
        "-    \"\"\"Extract compliance summary/gaps from raw report (if present) and surface in metrics for downstream summarization.\n",
        "-    Adds keys:\n",
        "-      metrics.compliance_summary.<standard> = {passed, failed, score, total_controls}\n",
        "-      metrics.compliance_gap_count\n",
        "-      metrics.compliance_gaps (first N gap dicts)\n",
        "-    \"\"\"\n",
        "-    if not state.report or not state.raw_report:\n",
        "-        return state\n",
        "-    meta = state.raw_report\n",
        "-    comp_sum = meta.get('compliance_summary') or {}\n",
        "-    gaps = meta.get('compliance_gaps') or []\n",
        "-    # Enrich gaps with standardized severity normalization & richer remediation hints if minimal\n",
        "-    if gaps:\n",
        "-        # Static mapping (could later externalize) control_id/keyword -> remediation & severity normalization\n",
        "-        remediation_map = {\n",
        "-            '2.2.4': 'Baseline and harden system services; disable or remove unused services.',\n",
        "-            '164.312(e)': 'Ensure transmission security: enforce TLS 1.2+, disable weak ciphers, encrypt PHI in transit.',\n",
        "-            '164.308(a)(1)': 'Implement risk management processes; document risk analysis and ongoing monitoring.',\n",
        "-            'ID.AM-01': 'Maintain accurate asset inventory (automated discovery + periodic reconciliation).',\n",
        "-            'PR.AC-01': 'Centralize access control; enforce MFA for privileged accounts.',\n",
        "-            'PR.DS-01': 'Encrypt sensitive data at rest with strong algorithms and manage keys securely.'\n",
        "-        }\n",
        "-        sev_order = {'info':0,'low':1,'medium':2,'moderate':2,'high':3,'critical':4}\n",
        "-        # Normalize severities and backfill remediation_hints\n",
        "-        for g in gaps:\n",
        "-            cid = str(g.get('control_id') or '')\n",
        "-            # severity normalization\n",
        "-            sev = (g.get('severity') or '').lower()\n",
        "-            if sev and sev not in sev_order:\n",
        "-                # map alternative labels\n",
        "-                if sev in {'moderate'}:\n",
        "-                    g['severity'] = 'medium'\n",
        "-            # add mapped remediation if existing is missing or too short\n",
        "-            hint = g.get('remediation_hint') or ''\n",
        "-            if len(hint.strip()) < 12:\n",
        "-                mapped = remediation_map.get(cid)\n",
        "-                if mapped:\n",
        "-                    g['remediation_hint'] = mapped\n",
        "-    # Attach into summaries.metrics (create if absent)\n",
        "-    if not state.summaries:\n",
        "-        from .models import Summaries\n",
        "-        state.summaries = Summaries(metrics={})\n",
        "-    metrics = state.summaries.metrics or {}\n",
        "-    comp_export = {}\n",
        "-    for std, vals in comp_sum.items():\n",
        "-        # filter numeric fields\n",
        "-        comp_export[std] = {k: vals.get(k) for k in ['passed','failed','score','total_controls','not_applicable'] if k in vals}\n",
        "-    if comp_export:\n",
        "-        metrics['compliance_summary'] = comp_export\n",
        "-    if gaps:\n",
        "-        metrics['compliance_gap_count'] = len(gaps)\n",
        "-        # only include first 50 to cap size\n",
        "-        metrics['compliance_gaps'] = gaps[:50]\n",
        "-    state.summaries.metrics = metrics\n",
        "-    return state\n",
        "-\n",
        "-\n",
        "-def baseline_rarity(state: AgentState, baseline_path: Path = Path(\"agent_baseline.db\")) -> AgentState:\n",
        "-    \"\"\"Update findings with rarity/anomaly score based on baseline store.\n",
        "-    New finding => anomaly=1.0, existing => anomaly decays toward 0.\n",
        "-    Exposure + impact remain unchanged; recalculates composite risk_score (simple formula for now).\n",
        "-    \"\"\"\n",
        "-    if not state.report:\n",
        "-        return state\n",
        "-    import os as _os\n",
        "-    env_path = _os.environ.get('AGENT_BASELINE_DB')\n",
        "-    if env_path:\n",
        "-        baseline_path = Path(env_path)\n",
        "-    store = BaselineStore(baseline_path)\n",
        "-    host_id = state.report.meta.host_id or \"unknown_host\"\n",
        "-    all_pairs = []\n",
        "-    all_findings: List[Finding] = []\n",
        "-    for sr in state.report.results:\n",
        "-        for f in sr.findings:\n",
        "-            all_pairs.append((sr.scanner, f))\n",
        "-            all_findings.append(f)\n",
        "-    deltas = store.update_and_diff(host_id, all_pairs)\n",
        "-    # Map back anomaly score\n",
        "-    for sr in state.report.results:\n",
        "-        for finding in sr.findings:\n",
        "-            from .baseline import hashlib_sha\n",
        "-            h = finding.identity_hash()\n",
        "-            comp = hashlib_sha(sr.scanner, h)\n",
        "-            d = deltas.get(comp)\n",
        "-            if not finding.risk_subscores:\n",
        "-                continue\n",
        "-            # Anomaly weighting: new +2, existing changed +1 else decay\n",
        "-            rationale_bits = []\n",
        "-            if d:\n",
        "-                if d[\"status\"] == \"new\":\n",
        "-                    finding.risk_subscores[\"anomaly\"] = 2.0\n",
        "-                    if \"baseline:new\" not in finding.tags:\n",
        "-                        finding.tags.append(\"baseline:new\")\n",
        "-                    finding.baseline_status = \"new\"\n",
        "-                    rationale_bits.append(\"new finding (anomaly +2)\")\n",
        "-                else:\n",
        "-                    prev = d.get(\"prev_seen_count\", 1)\n",
        "-                    # Changed vs stable heuristic: if prev_seen_count just incremented from 1->2 treat as +1 else decay\n",
        "-                    if prev <= 2:\n",
        "-                        finding.risk_subscores[\"anomaly\"] = 1.0\n",
        "-                        finding.baseline_status = \"recent\"\n",
        "-                        rationale_bits.append(\"recent finding (anomaly +1)\")\n",
        "-                    else:\n",
        "-                        finding.risk_subscores[\"anomaly\"] = round(max(0.1, 1.0 / (prev)), 2)\n",
        "-                        finding.baseline_status = \"existing\"\n",
        "-                        rationale_bits.append(f\"established finding (anomaly {finding.risk_subscores['anomaly']})\")\n",
        "-                        if prev >= 5:\n",
        "-                            # Very common => tag and downweight anomaly further contextually\n",
        "-                            if 'baseline:common' not in finding.tags:\n",
        "-                                finding.tags.append('baseline:common')\n",
        "-                            rationale_bits.append('very common baseline occurrence')\n",
        "-            # Confidence adjustment (placeholder): if only pattern-based IOC (tag contains 'ioc-pattern') lower confidence\n",
        "-            if any(t.startswith(\"ioc-pattern\") for t in finding.tags):\n",
        "-                finding.risk_subscores[\"confidence\"] = min(finding.risk_subscores.get(\"confidence\",1.0), 0.7)\n",
        "-                rationale_bits.append(\"heuristic IOC pattern (confidence down)\")\n",
        "-            # Calibrated risk using weights\n",
        "-            weights = load_persistent_weights()\n",
        "-            score, raw = compute_risk(finding.risk_subscores, weights)\n",
        "-            finding.risk_score = score\n",
        "-            finding.risk_subscores[\"_raw_weighted_sum\"] = round(raw,3)\n",
        "-            finding.probability_actionable = apply_probability(raw)\n",
        "-            # Impact & exposure rationale\n",
        "-            impact = finding.risk_subscores.get(\"impact\")\n",
        "-            exposure = finding.risk_subscores.get(\"exposure\")\n",
        "-            rationale_bits.insert(0, f\"impact={impact}\")\n",
        "-            rationale_bits.insert(1, f\"exposure={exposure}\")\n",
        "-            if not finding.rationale:\n",
        "-                finding.rationale = rationale_bits\n",
        "-            else:\n",
        "-                finding.rationale.extend(rationale_bits)\n",
        "-            finding.risk_total = finding.risk_score\n",
        "-            # Log calibration observation (raw sum) for future supervised tuning\n",
        "-            if state.report and state.report.meta and state.report.meta.scan_id:\n",
        "-                comp_hash = comp  # composite hash from earlier\n",
        "-                try:\n",
        "-                    store.log_calibration_observation(host_id, state.report.meta.scan_id, comp_hash, raw)\n",
        "-                except Exception as e:\n",
        "-                    _log_error('calibration_observe', e)\n",
        "-    return state\n",
        "-\n",
        "-\n",
        "-def process_novelty(state: AgentState, baseline_path: Path = Path(\"agent_baseline.db\"), distance_threshold: float | None = None, anomaly_boost: float = 1.5) -> AgentState:\n",
        "-    \"\"\"Assign lightweight embedding-based novelty for process findings.\n",
        "-    Uses config threshold if distance_threshold not provided.\n",
        "-    Parallelizes feature vector computation if configured (CPU-bound hashing/light transforms).\"\"\"\n",
        "-    if not state.report:\n",
        "-        return state\n",
        "-    cfg = load_config()\n",
        "-    if distance_threshold is None:\n",
        "-        # Fallback to configured threshold; if missing or None, choose conservative default 1.0\n",
        "-        dt_cfg = getattr(cfg.thresholds, 'process_novelty_distance', None)\n",
        "-        distance_threshold = float(dt_cfg) if dt_cfg is not None else 1.0\n",
        "-    env_path = os.environ.get('AGENT_BASELINE_DB')\n",
        "-    if env_path:\n",
        "-        baseline_path = Path(env_path)\n",
        "-    store = BaselineStore(baseline_path)\n",
        "-    host_id = state.report.meta.host_id or \"unknown_host\"\n",
        "-    # Collect candidate findings\n",
        "-    candidates: List[Finding] = []\n",
        "-    for sr in state.report.results:\n",
        "-        if sr.scanner.lower() != 'process':\n",
        "-            continue\n",
        "-        for f in sr.findings:\n",
        "-            candidates.append(f)\n",
        "-    if not candidates:\n",
        "-        return state\n",
        "-    # Pre-compute feature vectors (parallel if enabled)\n",
        "-    vecs: dict[str, list[float]] = {}\n",
        "-    def _build_vec(f: Finding):\n",
        "-        cmd = f.metadata.get('cmdline') or f.title or f.metadata.get('process') or ''\n",
        "-        return f.id, process_feature_vector(cmd)\n",
        "-    if cfg.performance.parallel_baseline and len(candidates) > 4:\n",
        "-        with concurrent.futures.ThreadPoolExecutor(max_workers=cfg.performance.workers) as ex:\n",
        "-            for fid, v in ex.map(_build_vec, candidates):\n",
        "-                vecs[fid] = v\n",
        "-    else:\n",
        "-        for f in candidates:\n",
        "-            fid, v = _build_vec(f)\n",
        "-            vecs[fid] = v\n",
        "-    for f in candidates:\n",
        "-        vec = vecs.get(f.id)\n",
        "-        if vec is None:\n",
        "-            continue  # no vector computed\n",
        "-        cid, dist, is_new = store.assign_process_vector(host_id, vec, distance_threshold=float(distance_threshold))\n",
        "-        if is_new or dist > float(distance_threshold):\n",
        "-            if 'process_novel' not in f.tags:\n",
        "-                f.tags.append('process_novel')\n",
        "-            if f.risk_subscores:\n",
        "-                prev = f.risk_subscores.get('anomaly', 0.0)\n",
        "-                from .risk import CAPS\n",
        "-                cap = CAPS.get('anomaly', 2.0)\n",
        "-                new_anom = round(min(prev + anomaly_boost, cap),2)\n",
        "-                if new_anom != prev:\n",
        "-                    f.risk_subscores['anomaly'] = new_anom\n",
        "-                    _recompute_finding_risk(f)\n",
        "-            rationale_note = f\"novel process cluster (cid={cid} dist={dist:.2f})\"\n",
        "-            if f.rationale:\n",
        "-                f.rationale.append(rationale_note)\n",
        "-            else:\n",
        "-                f.rationale = [rationale_note]\n",
        "-        else:\n",
        "-            if dist > float(distance_threshold) * 0.8:\n",
        "-                note = f\"near-novel process (cid={cid} dist={dist:.2f})\"\n",
        "-                if f.rationale:\n",
        "-                    f.rationale.append(note)\n",
        "-                else:\n",
        "-                    f.rationale = [note]\n",
        "-    return state\n",
        "-\n",
        "-\n",
        "-def sequence_correlation(state: AgentState) -> AgentState:\n",
        "-    \"\"\"Detect suspicious temporal sequences inside a single scan.\n",
        "-    Current heuristic pattern:\n",
        "-      1. New SUID binary (tag baseline:new + suid) appears\n",
        "-      2. net.ipv4.ip_forward kernel param enabled (value=1) in same scan after the SUID finding order.\n",
        "-    If both occur, emit synthetic correlation with tag sequence_anomaly.\n",
        "-    Ordering proxy: we use appearance order in report results since per-scanner timestamps absent.\n",
        "-    \"\"\"\n",
        "-    if not state.report:\n",
        "-        return state\n",
        "-    # Flatten findings preserving order\n",
        "-    ordered: List[Finding] = []\n",
        "-    for r in state.report.results:\n",
        "-        for f in r.findings:\n",
        "-            ordered.append(f)\n",
        "-    suid_indices = []\n",
        "-    ip_forward_indices = []\n",
        "-    for idx, f in enumerate(ordered):\n",
        "-        if 'suid' in (f.tags or []) and any(t == 'baseline:new' for t in (f.tags or [])):\n",
        "-            suid_indices.append((idx, f))\n",
        "-        if f.category == 'kernel_param' and f.metadata.get('sysctl_key') == 'net.ipv4.ip_forward':\n",
        "-            val = str(f.metadata.get('value') or f.metadata.get('desired') or f.metadata.get('current') or '')\n",
        "-            if val in {'1','true','enabled'}:\n",
        "-                ip_forward_indices.append((idx, f))\n",
        "-    if suid_indices and ip_forward_indices:\n",
        "-        # Check if any suid index precedes any ip_forward index\n",
        "-        trigger_pairs = [(s,i) for (s,_) in suid_indices for (i,_) in ip_forward_indices if s < i]\n",
        "-        if trigger_pairs:\n",
        "-            # Build correlation referencing the involved findings (limit to first few to bound size)\n",
        "-            related = []\n",
        "-            for (s_idx, s_f) in suid_indices[:3]:\n",
        "-                related.append(s_f.id)\n",
        "-            for (i_idx, i_f) in ip_forward_indices[:2]:\n",
        "-                related.append(i_f.id)\n",
        "-            # Avoid duplicate correlation creation\n",
        "-            already = any(c.related_finding_ids == related and 'sequence_anomaly' in (c.tags or []) for c in state.correlations)\n",
        "-            if not already:\n",
        "-                # Deterministic ID: sequence_anom_<n>\n",
        "-                existing = [c for c in state.correlations if 'sequence_anomaly' in (c.tags or []) and c.id.startswith('sequence_anom_')]\n",
        "-                corr_id = f'sequence_anom_{len(existing)+1}'\n",
        "-                corr = Correlation(\n",
        "-                    id=corr_id,\n",
        "-                    title='Suspicious Sequence: New SUID followed by IP forwarding enabled',\n",
        "-                    rationale='Heuristic: newly introduced SUID binary preceded enabling IP forwarding in same scan',\n",
        "-                    related_finding_ids=related,\n",
        "-                    risk_score_delta=8,\n",
        "-                    tags=['sequence_anomaly','routing','privilege_escalation_surface'],\n",
        "-                    severity='high'\n",
        "-                )\n",
        "-                state.correlations.append(corr)\n",
        "-                # Back-reference on findings\n",
        "-                for f in ordered:\n",
        "-                    if f.id in related:\n",
        "-                        if corr.id not in f.correlation_refs:\n",
        "-                            f.correlation_refs.append(corr.id)\n",
        "-    return state\n",
        "-\n",
        "-\n",
        "-def reduce(state: AgentState) -> AgentState:\n",
        "-    if not state.report:\n",
        "-        return state\n",
        "-    all_findings: List[Finding] = [f for r in state.report.results for f in r.findings]\n",
        "-    mc = get_metrics_collector()\n",
        "-    with mc.time_stage('reduce.reduce_all'):\n",
        "-        state.reductions = reduce_all(all_findings)\n",
        "-    return state\n",
        "-\n",
        "-\n",
        "-def summarize(state: AgentState) -> AgentState:\n",
        "-    client = get_llm_provider()\n",
        "-    governor = get_data_governor()\n",
        "-    cfg = load_config()\n",
        "-    threshold = cfg.thresholds.summarization_risk_sum\n",
        "-    high_med_sum = 0\n",
        "-    new_found = False\n",
        "-    all_findings = [f for r in state.report.results for f in r.findings] if state.report else []\n",
        "-    for f in all_findings:\n",
        "-        sev = f.severity.lower()\n",
        "-        if sev in {\"medium\",\"high\",\"critical\"}:\n",
        "-            # Exclude operational_error pseudo-findings from security risk aggregation\n",
        "-            if not getattr(f, 'operational_error', False):\n",
        "-                high_med_sum += (f.risk_total or f.risk_score or 0)\n",
        "-        if any(t == 'baseline:new' for t in (f.tags or [])):\n",
        "-            new_found = True\n",
        "-    skip = (high_med_sum < threshold) and (not new_found)\n",
        "-    prev = getattr(state, 'summaries', None)\n",
        "-    # Redact inputs before passing to provider (governance enforcement)\n",
        "-    red_reductions = governor.redact_for_llm(state.reductions)\n",
        "-    red_correlations = [governor.redact_for_llm(c) for c in state.correlations]\n",
        "-    red_actions = [governor.redact_for_llm(a) for a in state.actions]\n",
        "-    mc = get_metrics_collector()\n",
        "-    with mc.time_stage('summarize.llm'):\n",
        "-        state.summaries = client.summarize(red_reductions, red_correlations, red_actions, skip=skip, previous=prev, skip_reason=\"low_risk_no_change\" if skip else None, baseline_context=None)\n",
        "-    # Attach token accounting snapshot for future cost modeling\n",
        "-    try:\n",
        "-        # Token accounting model removed / deprecated; inline dict if needed in future.\n",
        "-        m = state.summaries.metrics or {}\n",
        "-        state.summaries.metrics = state.summaries.metrics or {}\n",
        "-        state.summaries.metrics['token_accounting'] = {\n",
        "-            'prompt_tokens': m.get('tokens_prompt',0),\n",
        "-            'completion_tokens': m.get('tokens_completion',0),\n",
        "-            'cached': bool(m.get('skipped')),\n",
        "-            'unit_cost_prompt': float(os.environ.get('AGENT_COST_PROMPT_PER_1K', '0') or 0)/1000.0,\n",
        "-            'unit_cost_completion': float(os.environ.get('AGENT_COST_COMPLETION_PER_1K','0') or 0)/1000.0,\n",
        "-        }\n",
        "-        state.summaries.metrics['estimated_cost'] = round(\n",
        "-            state.summaries.metrics['token_accounting']['prompt_tokens'] * state.summaries.metrics['token_accounting']['unit_cost_prompt'] +\n",
        "-            state.summaries.metrics['token_accounting']['completion_tokens'] * state.summaries.metrics['token_accounting']['unit_cost_completion'], 6\n",
        "-        )\n",
        "-    except Exception:\n",
        "-        pass\n",
        "-    # Scrub narrative fields prior to persistence/output\n",
        "-    state.summaries = governor.redact_output_narratives(state.summaries)\n",
        "-    # ATT&CK coverage computation\n",
        "-    try:\n",
        "-        mapping = _load_attack_mapping()\n",
        "-        if state.report and state.summaries:\n",
        "-            covered = {}\n",
        "-            all_tags = set()\n",
        "-            for r in state.report.results:\n",
        "-                for f in r.findings:\n",
        "-                    for t in (f.tags or []):\n",
        "-                        all_tags.add(t)\n",
        "-            for c in state.correlations:\n",
        "-                for t in (c.tags or []):\n",
        "-                    all_tags.add(t)\n",
        "-            techniques = set()\n",
        "-            tag_hits = {}\n",
        "-            for tag in all_tags:\n",
        "-                techs = mapping.get(tag)\n",
        "-                if not techs:\n",
        "-                    continue\n",
        "-                if isinstance(techs, list):\n",
        "-                    for tid in techs:\n",
        "-                        techniques.add(tid)\n",
        "-                elif isinstance(techs, str):\n",
        "-                    techniques.add(techs)\n",
        "-                tag_hits[tag] = techs\n",
        "-            state.summaries.attack_coverage = {\n",
        "-                'technique_count': len(techniques),\n",
        "-                'techniques': sorted(techniques),\n",
        "-                'tag_hits': tag_hits\n",
        "-            }\n",
        "-    except Exception as e:\n",
        "-        _log_error('attack_coverage', e, state)\n",
        "-    # Experimental causal hypotheses\n",
        "-    try:\n",
        "-        if state.summaries:\n",
        "-            state.summaries.causal_hypotheses = generate_causal_hypotheses(state)\n",
        "-    except Exception as e:\n",
        "-        _log_error('causal_hypotheses', e, state)\n",
        "-    return state\n",
        "-\n",
        "-\n",
        "-def actions(state: AgentState) -> AgentState:\n",
        "-    items: List[ActionItem] = []\n",
        "-    # Simple deterministic mapping examples\n",
        "-    mc = get_metrics_collector()\n",
        "-    with mc.time_stage('actions.build'):\n",
        "-        for c in state.correlations:\n",
        "-            if \"routing\" in c.tags:\n",
        "-                items.append(ActionItem(priority=len(items)+1, action=\"Confirm intent for routing/NAT configuration; disable if not required.\", correlation_refs=[c.id]))\n",
        "-    # Baseline noise example: if many SUID unexpected\n",
        "-    if state.reductions.suid_summary and state.reductions.suid_summary.get(\"unexpected_suid\"):\n",
        "-        items.append(ActionItem(priority=len(items)+1, action=\"Review unexpected SUID binaries; remove SUID bit if unnecessary.\", correlation_refs=[]))\n",
        "-    mc.incr('actions.count', len(items))\n",
        "-    state.actions = items\n",
        "-    return state\n",
        "-\n",
        "-\n",
        "-def build_output(state: AgentState, raw_path: Path) -> EnrichedOutput:\n",
        "-    sha = hashlib.sha256(Path(raw_path).read_bytes()).hexdigest() if raw_path.exists() else None\n",
        "-    integrity_status = None\n",
        "-    try:\n",
        "-        # If verification key provided via env, attempt signature verification\n",
        "-        vkey = os.environ.get('AGENT_VERIFY_KEY_B64')\n",
        "-        if raw_path.exists():\n",
        "-            if vkey:\n",
        "-                integrity_status = verify_file(raw_path, vkey)\n",
        "-            else:\n",
        "-                # minimal status with sha only\n",
        "-                integrity_status = {'sha256_actual': sha}\n",
        "-    except Exception as e:\n",
        "-        _log_error('integrity_verify', e, state, severity='error', hint='Check signature key / file permissions')\n",
        "-        integrity_status = {'sha256_actual': sha, 'error': 'integrity_check_failed'}\n",
        "-    flat_findings = []\n",
        "-    if state.report:\n",
        "-        for r in state.report.results:\n",
        "-            flat_findings.extend(r.findings)\n",
        "-    # Correlation graph metrics\n",
        "-    mc = get_metrics_collector()\n",
        "-    with mc.time_stage('graph.annotate'):\n",
        "-        graph_meta = annotate_and_summarize(state)\n",
        "-    out = EnrichedOutput(\n",
        "-        correlations=state.correlations,\n",
        "-        reductions=state.reductions,\n",
        "-        summaries=state.summaries,\n",
        "-        actions=state.actions,\n",
        "-        raw_reference=sha,\n",
        "-        enriched_findings=flat_findings,\n",
        "-        correlation_graph=graph_meta if graph_meta else None,\n",
        "-        followups=state.followups if state.followups else None,\n",
        "-    enrichment_results=state.enrichment_results or None,\n",
        "-    multi_host_correlation=state.multi_host_correlation or None,\n",
        "-    integrity=integrity_status\n",
        "-    )\n",
        "-    if out.enrichment_results is None:\n",
        "-        out.enrichment_results = {}\n",
        "-    if state.agent_warnings:\n",
        "-        out.enrichment_results['agent_warnings'] = state.agent_warnings\n",
        "-    # Performance metrics and baseline regression detection\n",
        "-    perf_snap = mc.snapshot()\n",
        "-    baseline_path = os.environ.get('AGENT_PERF_BASELINE_PATH', 'artifacts/perf_baseline.json')\n",
        "-    threshold_env = os.environ.get('AGENT_PERF_REGRESSION_PCT', '30')\n",
        "-    try:\n",
        "-        threshold = max(0.0, float(threshold_env))/100.0\n",
        "-    except ValueError:\n",
        "-        threshold = 0.30\n",
        "-    from .metrics import MetricsCollector\n",
        "-    base = MetricsCollector.load_baseline(baseline_path)\n",
        "-    regressions = MetricsCollector.compare_to_baseline(perf_snap, base or {}, threshold) if base else []\n",
        "-    perf_snap['baseline_regressions'] = regressions\n",
        "-    perf_snap['baseline_threshold_pct'] = threshold*100\n",
        "-    out.enrichment_results['perf'] = perf_snap\n",
        "-    # Save current snapshot as new baseline (rolling)\n",
        "-    MetricsCollector.save_baseline(baseline_path, perf_snap)\n",
        "-    # Surface summary perf metrics\n",
        "-    if out.summaries:\n",
        "-        totals = perf_snap.get('durations', {})\n",
        "-        total_time = sum(v.get('total',0) for v in totals.values())\n",
        "-        slowest = None\n",
        "-        slowest_time = -1\n",
        "-        for stage, stats in totals.items():\n",
        "-            if stats.get('total',0) > slowest_time:\n",
        "-                slowest = stage\n",
        "-                slowest_time = stats.get('total',0)\n",
        "-        metrics_map = out.summaries.metrics or {}\n",
        "-        metrics_map.update({\n",
        "-            'perf.total_ms': total_time,\n",
        "-            'perf.slowest_stage': slowest,\n",
        "-            'perf.slowest_ms': slowest_time,\n",
        "-            'perf.regression_count': len(regressions)\n",
        "-        })\n",
        "-        out.summaries.metrics = metrics_map\n",
        "-    # Populate meta.analytics (INT-OBS-001) with concise summary (avoid large arrays)\n",
        "-    try:\n",
        "-        if state.report and state.report.meta:\n",
        "-            if not state.report.meta.analytics:\n",
        "-                state.report.meta.analytics = {}\n",
        "-            # Summarize durations\n",
        "-            dur_summary = {k: { 'avg_ms': round(v.get('avg',0),2), 'total_ms': v.get('total',0), 'count': v.get('count',0)} for k,v in perf_snap.get('durations', {}).items()}\n",
        "-            state.report.meta.analytics.update({\n",
        "-                'performance': {\n",
        "-                    'durations': dur_summary,\n",
        "-                    'counters': perf_snap.get('counters', {}),\n",
        "-                    'regressions': perf_snap.get('baseline_regressions', []),\n",
        "-                    'regression_threshold_pct': perf_snap.get('baseline_threshold_pct')\n",
        "-                }\n",
        "-            })\n",
        "-    except Exception:\n",
        "-        pass\n",
        "-    # Apply deterministic canonical ordering to entire output\n",
        "-    try:\n",
        "-        out_dict = out.model_dump()\n",
        "-        canon = canonicalize_enriched_output_dict(out_dict)\n",
        "-        from .models import EnrichedOutput as _EO\n",
        "-        out = _EO(**canon)\n",
        "-    except Exception:\n",
        "-        # On failure, fall back to original out\n",
        "-        pass\n",
        "-    return out\n",
        "-\n",
        "-\n",
        "-# -----------------\n",
        "-# Optional External Corpus Integration (Hugging Face datasets)\n",
        "-# -----------------\n",
        "-\n",
        "-def _augment_with_corpus_insights(state: AgentState):\n",
        "-    \"\"\"Optionally load external cybersecurity corpora (if token & pandas available) to\n",
        "-    attach high-level corpus metrics to summaries.metrics for adaptive reasoning.\n",
        "-\n",
        "-    Controlled by env AGENT_LOAD_HF_CORPUS=1. Lightweight: only counts / sample hash.\n",
        "-    Avoids loading if already present in metrics. Silent (logs on error).\"\"\"\n",
        "-    if not os.environ.get('AGENT_LOAD_HF_CORPUS'):\n",
        "-        return state\n",
        "-    try:\n",
        "-        from . import hf_loader  # lazy import\n",
        "-    except Exception as e:  # module absent\n",
        "-        _log_error('corpus_import', e)\n",
        "-        return state\n",
        "-    try:\n",
        "-        jsonl_df = hf_loader.load_cybersec_jsonl()\n",
        "-        parquet_df = hf_loader.load_cybersec_parquet()\n",
        "-        j_rows = int(len(jsonl_df)) if jsonl_df is not None else None\n",
        "-        p_rows = int(len(parquet_df)) if parquet_df is not None else None\n",
        "-        # Minimal content fingerprint (no sensitive data): column name hash\n",
        "-        import hashlib as _hl\n",
        "-        def _col_fprint(df):\n",
        "-            if df is None: return None\n",
        "-            h = _hl.sha256()\n",
        "-            for c in sorted(df.columns):\n",
        "-                h.update(c.encode())\n",
        "-            return h.hexdigest()[:16]\n",
        "-        metrics_add = {\n",
        "-            'corpus.jsonl_rows': j_rows,\n",
        "-            'corpus.parquet_rows': p_rows,\n",
        "-            'corpus.jsonl_schema_fprint': _col_fprint(jsonl_df),\n",
        "-            'corpus.parquet_schema_fprint': _col_fprint(parquet_df)\n",
        "-        }\n",
        "-        if state.summaries:\n",
        "-            base = state.summaries.metrics or {}\n",
        "-            # Do not overwrite existing keys unless None\n",
        "-            for k,v in metrics_add.items():\n",
        "-                if k not in base or base[k] is None:\n",
        "-                    base[k] = v\n",
        "-            state.summaries.metrics = base\n",
        "-    except Exception as e:\n",
        "-        _log_error('corpus_insights', e)\n",
        "-    return state\n",
        "-\n",
        "-\n",
        "-def generate_causal_hypotheses(state: AgentState, max_hypotheses: int = 3) -> list[dict]:\n",
        "-    \"\"\"Generate speculative causal hypotheses from correlations & findings.\n",
        "-    Heuristics only (deterministic):\n",
        "-      - sequence_anomaly => privilege escalation chain.\n",
        "-      - module_propagation => lateral movement via module.\n",
        "-      - presence of metric_drift finding + routing correlation => config change root cause.\n",
        "-    Mark all as speculative with low confidence.\n",
        "-    \"\"\"\n",
        "-    hyps = []\n",
        "-    for c in state.correlations:\n",
        "-        if 'sequence_anomaly' in c.tags:\n",
        "-            hyps.append({\n",
        "-                'id': f\"hyp_{len(hyps)+1}\",\n",
        "-                'summary': 'Potential privilege escalation chain (new SUID then IP forwarding)',\n",
        "-                'rationale': [c.rationale],\n",
        "-                'confidence': 'low',\n",
        "-                'speculative': True\n",
        "-            })\n",
        "-        if 'module_propagation' in c.tags:\n",
        "-            hyps.append({\n",
        "-                'id': f\"hyp_{len(hyps)+1}\",\n",
        "-                'summary': 'Possible lateral movement via near-simultaneous kernel module deployment',\n",
        "-                'rationale': [c.rationale or 'simultaneous module emergence across hosts'],\n",
        "-                'confidence': 'low',\n",
        "-                'speculative': True\n",
        "-            })\n",
        "-    drift_present = any('metric_drift' in (f.tags or []) for r in (state.report.results if state.report else []) for f in r.findings)\n",
        "-    routing_corr = any('routing' in c.tags for c in state.correlations)\n",
        "-    if drift_present and routing_corr:\n",
        "-        hyps.append({\n",
        "-            'id': f\"hyp_{len(hyps)+1}\",\n",
        "-            'summary': 'Configuration change likely triggered routing and risk metric drift',\n",
        "-            'rationale': ['metric drift finding plus routing-related correlation(s)'],\n",
        "-            'confidence': 'low',\n",
        "-            'speculative': True\n",
        "-        })\n",
        "-    # Deduplicate by summary, cap\n",
        "-    out = []\n",
        "-    seen = set()\n",
        "-    for h in hyps:\n",
        "-        if h['summary'] in seen: continue\n",
        "-        seen.add(h['summary'])\n",
        "-        out.append(h)\n",
        "-        if len(out) >= max_hypotheses:\n",
        "-            break\n",
        "-    return out\n",
        "-\n",
        "-\n",
        "-def _load_attack_mapping(path: Path = Path('agent/attack_mapping.yaml')) -> dict:\n",
        "-    try:\n",
        "-        if not path.exists():\n",
        "-            return {}\n",
        "-        data = _yaml.safe_load(path.read_text()) or {}\n",
        "-        return data\n",
        "-    except Exception:\n",
        "-        return {}\n",
        "-\n",
        "-\n",
        "-def run_pipeline(report_path: Path) -> EnrichedOutput:\n",
        "-    state = AgentState()\n",
        "-    state = load_report(state, report_path)\n",
        "-    try:\n",
        "-        log_stage('load_report', file=str(report_path), sha256=hashlib.sha256(report_path.read_bytes()).hexdigest())\n",
        "-    except Exception as e:\n",
        "-        _log_error('load_report_log', e)\n",
        "-    state = augment(state)\n",
        "-    state = integrate_compliance(state)\n",
        "-    # Policy enforcement (denylist executable paths)\n",
        "-    try:\n",
        "-        state = apply_policy(state)\n",
        "-        log_stage('policy_enforce')\n",
        "-    except Exception as e:\n",
        "-        _log_error('policy_enforce', e)\n",
        "-    try:\n",
        "-        log_stage('augment', findings=sum(len(r.findings) for r in (state.report.results if state.report else [])))\n",
        "-    except Exception as e:\n",
        "-        _log_error('augment_log', e)\n",
        "-    state = correlate(state)\n",
        "-    try:\n",
        "-        log_stage('correlate', correlations=len(state.correlations))\n",
        "-    except Exception as e:\n",
        "-        _log_error('correlate_log', e)\n",
        "-    # Temporal sequence correlations\n",
        "-    try:\n",
        "-        state = sequence_correlation(state)\n",
        "-        log_stage('sequence_correlation')\n",
        "-    except Exception as e:\n",
        "-        _log_error('sequence_correlation', e)\n",
        "-    state = baseline_rarity(state)\n",
        "-    try:\n",
        "-        log_stage('baseline_rarity')\n",
        "-    except Exception as e:\n",
        "-        _log_error('baseline_rarity_log', e)\n",
        "-    # Embedding-based process novelty\n",
        "-    try:\n",
        "-        state = process_novelty(state)\n",
        "-        log_stage('process_novelty')\n",
        "-    except Exception as e:\n",
        "-        _log_error('process_novelty', e)\n",
        "-    # Metric drift detection: derive simple metrics and record; synthesize findings if z>|threshold|\n",
        "-    if state.report and state.report.meta and state.report.meta.host_id and state.report.meta.scan_id:\n",
        "-        host_id = state.report.meta.host_id\n",
        "-        scan_id = state.report.meta.scan_id\n",
        "-        # Derive metrics (can expand later). Examples:\n",
        "-        # 1. total findings\n",
        "-        # 2. high severity count\n",
        "-        # 3. sum risk_total of medium+ severity\n",
        "-        all_findings = [f for r in state.report.results for f in r.findings]\n",
        "-        total_findings = len(all_findings)\n",
        "-        high_count = sum(1 for f in all_findings if f.severity.lower() in {\"high\",\"critical\"})\n",
        "-        med_hi_risk_sum = sum((f.risk_total or f.risk_score or 0) for f in all_findings if f.severity.lower() in {\"medium\",\"high\",\"critical\"} and not getattr(f, 'operational_error', False))\n",
        "-        metrics = {\n",
        "-            'finding.count.total': float(total_findings),\n",
        "-            'finding.count.high': float(high_count),\n",
        "-            'risk.sum.medium_high': float(med_hi_risk_sum)\n",
        "-        }\n",
        "-        store = BaselineStore(Path(\"agent_baseline.db\"))\n",
        "-        metric_stats = store.record_metrics(host_id, scan_id, metrics, history_limit=10)\n",
        "-        # Lower initial thresholds: if history_n >=2 compute z; threshold 2.5; if no std yet use simple delta heuristic\n",
        "-        drift_threshold = 2.5\n",
        "-        drift_findings = []\n",
        "-        for mname, stats in metric_stats.items():\n",
        "-            z = stats.get('z')\n",
        "-            hist_n = stats.get('history_n',0)\n",
        "-            # Accept drift if enough history for z OR early large delta vs mean when hist>=2\n",
        "-            trigger = False\n",
        "-            if z is not None and hist_n >= 2 and abs(z) >= drift_threshold:\n",
        "-                trigger = True\n",
        "-            elif z is None and hist_n >= 2 and stats.get('mean') is not None:\n",
        "-                mean = stats['mean']; val = stats['value']\n",
        "-                # simple 100% increase heuristic\n",
        "-                if mean and (val - mean)/mean >= 1.0:\n",
        "-                    trigger = True\n",
        "-            if trigger:\n",
        "-                # Build synthetic finding\n",
        "-                fid = f\"metric:{mname}:drift\"\n",
        "-                z_for_sev = abs(z) if z is not None else 0\n",
        "-                from .risk import CAPS\n",
        "-                anomaly_cap = CAPS.get('anomaly', 2.0)\n",
        "-                raw_anom = (abs(z)/3.0) if z is not None else 1.0\n",
        "-                anomaly_val = min(anomaly_cap, raw_anom)\n",
        "-                drift = Finding(\n",
        "-                    id=fid,\n",
        "-                    title=\"Metric Drift Detected\",\n",
        "-                    severity=\"medium\" if z_for_sev < 5 else \"high\",\n",
        "-                    risk_score=0,\n",
        "-                    metadata={'metric': mname, 'z': z, 'value': stats['value'], 'mean': stats['mean'], 'std': stats['std']},\n",
        "-                    category='telemetry',\n",
        "-                    tags=['synthetic','metric_drift'],\n",
        "-                    risk_subscores={'impact': 3.0, 'exposure': 0.0, 'anomaly': anomaly_val, 'confidence': 0.9},\n",
        "-                    baseline_status='new',\n",
        "-                    metric_drift=stats\n",
        "-                )\n",
        "-                # compute risk for synthetic\n",
        "-                weights = load_persistent_weights()\n",
        "-                # risk_subscores is always set above\n",
        "-                score, raw = compute_risk(drift.risk_subscores or {}, weights)\n",
        "-                drift.risk_score = score\n",
        "-                drift.risk_total = score\n",
        "-                drift.probability_actionable = apply_probability(raw)\n",
        "-                if z is not None:\n",
        "-                    drift.rationale = [f\"metric {mname} drift z={z:.2f} value={stats['value']} mean={stats['mean']:.2f} std={stats['std']:.2f}\"]\n",
        "-                else:\n",
        "-                    drift.rationale = [f\"metric {mname} early drift value={stats['value']} mean={stats['mean']:.2f} (delta >=100%)\"]\n",
        "-                drift_findings.append(drift)\n",
        "-        if drift_findings:\n",
        "-            # Append to a synthetic scanner result so downstream logic sees them\n",
        "-            sr = ScannerResult(scanner='metric_drift', finding_count=len(drift_findings), findings=drift_findings)\n",
        "-            state.report.results.append(sr)\n",
        "-    state = reduce(state)\n",
        "-    try:\n",
        "-        log_stage('reduce', top_findings=len(state.reductions.top_findings))\n",
        "-    except Exception as e:\n",
        "-        _log_error('reduce_log', e)\n",
        "-    state = actions(state)\n",
        "-    try:\n",
        "-        log_stage('actions', actions=len(state.actions))\n",
        "-    except Exception as e:\n",
        "-        _log_error('actions_log', e)\n",
        "-    # Cross-host anomaly: module simultaneous emergence\n",
        "-    try:\n",
        "-        if state.report and state.report.results:\n",
        "-            store = BaselineStore(Path(os.environ.get('AGENT_BASELINE_DB','agent_baseline.db')))\n",
        "-            recent = store.recent_module_first_seen(within_seconds=86400)\n",
        "-            threshold = int(os.environ.get('PROPAGATION_HOST_THRESHOLD','3'))\n",
        "-            current_host = state.report.meta.host_id if state.report.meta else None  # reserved for future filtering\n",
        "-            for module, hosts in recent.items():\n",
        "-                if len(hosts) >= threshold:\n",
        "-                    corr = MultiHostCorrelation(type='module_propagation', key=module, host_ids=hosts, rationale=f\"Module '{module}' first appeared on {len(hosts)} hosts within 24h window\")\n",
        "-                    state.multi_host_correlation.append(corr)\n",
        "-                    fid = f\"multi_host_module:{module}\"\n",
        "-                    synth = Finding(\n",
        "-                        id=fid,\n",
        "-                        title=f\"Potential Propagation: module {module}\",\n",
        "-                        severity='medium',\n",
        "-                        risk_score=0,\n",
        "-                        metadata={'module': module, 'host_cluster_size': len(hosts)},\n",
        "-                        category='cross_host',\n",
        "-                        tags=['synthetic','cross_host','module_propagation'],\n",
        "-                        risk_subscores={'impact': 5.0, 'exposure': 0.0, 'anomaly': min(1.5, (__import__('agent.risk', fromlist=['CAPS']).CAPS.get('anomaly',2.0))), 'confidence': 0.8},\n",
        "-                        baseline_status='new'\n",
        "-                    )\n",
        "-                    _recompute_finding_risk(synth)\n",
        "-                    synth.rationale = [f\"Module simultaneously observed on {len(hosts)} hosts (>= {threshold})\"]\n",
        "-                    added = False\n",
        "-                    for sr in state.report.results:\n",
        "-                        if sr.scanner == 'multi_host':\n",
        "-                            sr.findings.append(synth)\n",
        "-                            sr.finding_count += 1\n",
        "-                            added = True\n",
        "-                            break\n",
        "-                    if not added:\n",
        "-                        state.report.results.append(ScannerResult(scanner='multi_host', finding_count=1, findings=[synth]))\n",
        "-    except Exception as e:\n",
        "-        _log_error('multi_host_correlation', e)\n",
        "-    # Follow-up planning/execution (deterministic gate)\n",
        "-    # Criteria: finding tagged ioc:development-tool and not allowlisted\n",
        "-    if state.report:\n",
        "-        for r in state.report.results:\n",
        "-            for f in r.findings:\n",
        "-                follow = False\n",
        "-                # Heuristic: mark certain IOC executables as development tools\n",
        "-                exe_path = f.metadata.get('exe') or ''\n",
        "-                if exe_path and any(tok in exe_path for tok in ['cpptools','python-env-tools']):\n",
        "-                    if 'ioc:development-tool' not in f.tags:\n",
        "-                        f.tags.append('ioc:development-tool')\n",
        "-                if any(t == 'ioc:development-tool' for t in (f.tags or [])) and not f.allowlist_reason:\n",
        "-                    follow = True\n",
        "-                if r.scanner.lower() == 'suid' and f.metadata.get('path'):\n",
        "-                    follow = True\n",
        "-                if follow:\n",
        "-                    plan = [\"hash_binary\", \"query_package_manager\"]\n",
        "-                    results = {}\n",
        "-                    bin_path = f.metadata.get('exe') or f.metadata.get('path')\n",
        "-                    if bin_path:\n",
        "-                        results['hash_binary'] = hash_binary(bin_path)\n",
        "-                        results['query_package_manager'] = query_package_manager(bin_path)\n",
        "-                    from .models import FollowupResult\n",
        "-                    state.followups.append(FollowupResult(finding_id=f.id, plan=plan, results=results))\n",
        "-                    try:\n",
        "-                        log_stage('followup_execute', finding_id=f.id, plan=\";\".join(plan))\n",
        "-                    except Exception as e:\n",
        "-                        _log_error('followup_execute', e)\n",
        "-    # Post follow-up aggregation / severity adjustments\n",
        "-    if state.followups and state.report:\n",
        "-        # Load trusted manifest\n",
        "-        trust_path = Path(__file__).parent / 'knowledge' / 'trusted_binaries.yaml'\n",
        "-        trusted = {}\n",
        "-        if trust_path.exists():\n",
        "-            try:\n",
        "-                trusted = yaml.safe_load(trust_path.read_text()) or {}\n",
        "-            except Exception as e:\n",
        "-                _log_error('trusted_manifest_load', e)\n",
        "-                trusted = {}\n",
        "-        trust_map = trusted.get('trusted', {})\n",
        "-        report_results = state.report.results or []\n",
        "-        # Build hash->(tool_key, downgrade) index for faster match & robustness\n",
        "-        hash_index = {}\n",
        "-        for tk, meta in trust_map.items():\n",
        "-            for hv in meta.get('sha256', []) or []:\n",
        "-                hash_index[hv] = (tk, meta.get('downgrade_severity_to'))\n",
        "-        for fu in state.followups:\n",
        "-            fobj = None\n",
        "-            for r in report_results:\n",
        "-                for f in r.findings:\n",
        "-                    if f.id == fu.finding_id:\n",
        "-                        fobj = f\n",
        "-                        break\n",
        "-                if fobj:\n",
        "-                    break\n",
        "-            state.enrichment_results.setdefault(fu.finding_id, fu.results)\n",
        "-            # Evaluate trust\n",
        "-            hdata = fu.results.get('hash_binary') or {}\n",
        "-            sha = hdata.get('sha256')\n",
        "-            if fobj and sha:\n",
        "-                # First, direct hash index lookup\n",
        "-                trust_entry = hash_index.get(sha)\n",
        "-                tool_key = None\n",
        "-                downgrade = None\n",
        "-                if trust_entry:\n",
        "-                    tool_key, downgrade = trust_entry\n",
        "-                else:\n",
        "-                    # Fallback heuristic by tool key substring present in path/title\n",
        "-                    title_lower = fobj.title.lower()\n",
        "-                    path_val = fobj.metadata.get('exe') or ''\n",
        "-                    if 'cpptools' in title_lower or 'cpptools' in path_val:\n",
        "-                        tool_key = 'cpptools'\n",
        "-                        meta = trust_map.get(tool_key) or {}\n",
        "-                        if sha in (meta.get('sha256') or []):\n",
        "-                            downgrade = meta.get('downgrade_severity_to')\n",
        "-                if tool_key and downgrade and downgrade != fobj.severity:\n",
        "-                    old = fobj.severity\n",
        "-                    fobj.severity = downgrade\n",
        "-                    if fobj.rationale:\n",
        "-                        fobj.rationale.append(f\"trusted binary hash matched ({tool_key}); severity {old}->{downgrade}\")\n",
        "-                    else:\n",
        "-                        fobj.rationale = [f\"trusted binary hash matched ({tool_key}); severity {old}->{downgrade}\"]\n",
        "-                    if 'trusted_binary' not in (fobj.tags or []):\n",
        "-                        fobj.tags.append('trusted_binary')\n",
        "-                    # Also add/update severity: tag list (not removing old to preserve provenance)\n",
        "-                    sev_tag = f\"severity:{downgrade}\"\n",
        "-                    if fobj.tags and sev_tag not in fobj.tags:\n",
        "-                        fobj.tags.append(sev_tag)\n",
        "-                    # Recompute risk if impact/exposure might depend on severity externally later\n",
        "-                    _recompute_finding_risk(fobj)\n",
        "-    state = summarize(state)\n",
        "-    # Optional external corpus insights after summaries produced\n",
        "-    state = _augment_with_corpus_insights(state)\n",
        "-    try:\n",
        "-        metrics = (state.summaries.metrics if state.summaries else {}) or {}\n",
        "-        log_stage('summarize', tokens_prompt=metrics.get('tokens_prompt'), tokens_completion=metrics.get('tokens_completion'))\n",
        "-    except Exception as e:\n",
        "-        _log_error('summarize_log', e)\n",
        "-    return build_output(state, report_path)\n",
        "-\n",
        "-\n",
        "-# -----------------\n",
        "-# Policy Layer\n",
        "-# -----------------\n",
        "-\n",
        "-APPROVED_DEFAULT = [\"/bin\",\"/usr/bin\",\"/usr/local/bin\",\"/sbin\",\"/usr/sbin\",\"/opt/trusted\"]\n",
        "-SEVERITY_ORDER = [\"info\",\"low\",\"medium\",\"high\",\"critical\"]\n",
        "-\n",
        "-def _load_policy_allowlist() -> set[str]:\n",
        "-    import yaml\n",
        "-    paths: set[str] = set()\n",
        "-    # Config-based allowlist\n",
        "-    try:\n",
        "-        cfg = load_config()\n",
        "-        for p in cfg.paths.policy_allowlist:\n",
        "-            paths.add(p)\n",
        "-    except Exception:\n",
        "-        pass\n",
        "-    # File allowlist\n",
        "-    allow_file = Path('policy_allowlist.yaml')\n",
        "-    if allow_file.exists():\n",
        "-        try:\n",
        "-            data = yaml.safe_load(allow_file.read_text()) or {}\n",
        "-            for p in (data.get('allow_executables') or []):\n",
        "-                if isinstance(p, str):\n",
        "-                    paths.add(p)\n",
        "-        except Exception as e:\n",
        "-            _log_error('policy_allowlist_load', e)\n",
        "-    # Env variable (colon separated)\n",
        "-    import os as _os\n",
        "-    env_list = _os.environ.get('AGENT_POLICY_ALLOWLIST','')\n",
        "-    for part in env_list.split(':'):\n",
        "-        part = part.strip()\n",
        "-        if part:\n",
        "-            paths.add(part)\n",
        "-    return paths\n",
        "-\n",
        "-def _approved_dirs() -> list[str]:\n",
        "-    import os as _os\n",
        "-    env_dirs = _os.environ.get('AGENT_APPROVED_DIRS')\n",
        "-    if env_dirs:\n",
        "-        return [d for d in (p.strip() for p in env_dirs.split(':')) if d]\n",
        "-    return APPROVED_DEFAULT\n",
        "-\n",
        "-def apply_policy(state: AgentState) -> AgentState:\n",
        "-    if not state.report:\n",
        "-        return state\n",
        "-    allow = _load_policy_allowlist()\n",
        "-    approved = _approved_dirs()\n",
        "-    # Resolve approved dirs to absolute canonical paths\n",
        "-    approved_real = []\n",
        "-    for d in approved:\n",
        "-        try:\n",
        "-            approved_real.append(str(Path(d).resolve()))\n",
        "-        except Exception:\n",
        "-            approved_real.append(d)\n",
        "-    for sr in state.report.results:\n",
        "-        for f in sr.findings:\n",
        "-            exe = f.metadata.get('exe') if isinstance(f.metadata, dict) else None\n",
        "-            if not exe:\n",
        "-                continue\n",
        "-            # Already allowlisted\n",
        "-            if exe in allow:\n",
        "-                continue\n",
        "-            try:\n",
        "-                exe_real = str(Path(exe).resolve())\n",
        "-            except Exception:\n",
        "-                exe_real = exe\n",
        "-            in_approved = False\n",
        "-            for d in approved_real:\n",
        "-                try:\n",
        "-                    if os.path.commonpath([exe_real, d]) == d:\n",
        "-                        in_approved = True\n",
        "-                        break\n",
        "-                except Exception:\n",
        "-                    continue\n",
        "-            if not in_approved:\n",
        "-                # Escalate severity to at least high unless already critical\n",
        "-                try:\n",
        "-                    sev_idx = SEVERITY_ORDER.index(f.severity.lower()) if f.severity else 0\n",
        "-                except ValueError:\n",
        "-                    sev_idx = 0\n",
        "-                target = 'high'\n",
        "-                if f.severity.lower() != 'critical' and f.severity.lower() != target:\n",
        "-                    # Only raise if below target\n",
        "-                    if SEVERITY_ORDER.index(target) > sev_idx:\n",
        "-                        old = f.severity\n",
        "-                        f.severity = target\n",
        "-                        f.severity_source = 'policy'\n",
        "-                        if f.rationale:\n",
        "-                            f.rationale.append(f\"policy escalation: executable outside approved dirs ({exe})\")\n",
        "-                        else:\n",
        "-                            f.rationale = [f\"policy escalation: executable outside approved dirs ({exe})\"]\n",
        "-                        if f.tags is not None:\n",
        "-                            if 'policy:denied_path' not in f.tags:\n",
        "-                                f.tags.append('policy:denied_path')\n",
        "-                            sev_tag = f'severity:{target}'\n",
        "-                            if sev_tag not in f.tags:\n",
        "-                                f.tags.append(sev_tag)\n",
        "-                # risk_subscores impact bump (optional)\n",
        "-                if f.risk_subscores:\n",
        "-                    new_imp = min(10.0, (f.risk_subscores.get('impact',0)+1.0))\n",
        "-                    if new_imp != f.risk_subscores.get('impact'):\n",
        "-                        f.risk_subscores['impact'] = new_imp\n",
        "-                        _recompute_finding_risk(f)\n",
        "-    return state\n",
        "-\n",
        "+\"\"\"\n",
        "+Pipeline module - imports functions from legacy pipeline for backward compatibility.\n",
        "+\"\"\"\n",
        "+from .legacy.pipeline import (\n",
        "+    load_report,\n",
        "+    augment,\n",
        "+    correlate,\n",
        "+    baseline_rarity,\n",
        "+    process_novelty,\n",
        "+    sequence_correlation,\n",
        "+    reduce,\n",
        "+    summarize,\n",
        "+    run_pipeline\n",
        "+)\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "agent/tests/__pycache__/__init__.cpython-312.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/tests/__pycache__/test_attack_coverage.cpython-312-pytest-8.4.1.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/tests/__pycache__/test_audit.cpython-312-pytest-8.4.1.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/tests/__pycache__/test_causal_hypotheses.cpython-312-pytest-8.4.1.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/tests/__pycache__/test_compromised_dev_host_snapshot.cpython-312-pytest-8.4.1.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/tests/__pycache__/test_counterfactual.cpython-312-pytest-8.4.1.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/tests/__pycache__/test_fleet_report_schema.cpython-312-pytest-8.4.1.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/tests/__pycache__/test_input_security.cpython-312-pytest-8.4.1.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/tests/__pycache__/test_integrity.cpython-312-pytest-8.4.1.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/tests/__pycache__/test_metric_drift.cpython-312-pytest-8.4.1.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/tests/__pycache__/test_policy.cpython-312-pytest-8.4.1.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/tests/__pycache__/test_process_novelty.cpython-312-pytest-8.4.1.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/tests/__pycache__/test_redaction.cpython-312-pytest-8.4.1.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/tests/__pycache__/test_reduction_snapshot.cpython-312-pytest-8.4.1.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/tests/__pycache__/test_rule_gap_miner.cpython-312-pytest-8.4.1.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/tests/__pycache__/test_rule_gap_refinement.cpython-312-pytest-8.4.1.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/tests/__pycache__/test_rule_redundancy.cpython-312-pytest-8.4.1.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/tests/__pycache__/test_rule_suggest.cpython-312-pytest-8.4.1.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/tests/__pycache__/test_sequence_correlation.cpython-312-pytest-8.4.1.pyc",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/bin/Activate.ps1",
      "status": "added",
      "additions": 247,
      "deletions": 0,
      "patch": "@@ -0,0 +1,247 @@\n+<#\r\n+.Synopsis\r\n+Activate a Python virtual environment for the current PowerShell session.\r\n+\r\n+.Description\r\n+Pushes the python executable for a virtual environment to the front of the\r\n+$Env:PATH environment variable and sets the prompt to signify that you are\r\n+in a Python virtual environment. Makes use of the command line switches as\r\n+well as the `pyvenv.cfg` file values present in the virtual environment.\r\n+\r\n+.Parameter VenvDir\r\n+Path to the directory that contains the virtual environment to activate. The\r\n+default value for this is the parent of the directory that the Activate.ps1\r\n+script is located within.\r\n+\r\n+.Parameter Prompt\r\n+The prompt prefix to display when this virtual environment is activated. By\r\n+default, this prompt is the name of the virtual environment folder (VenvDir)\r\n+surrounded by parentheses and followed by a single space (ie. '(.venv) ').\r\n+\r\n+.Example\r\n+Activate.ps1\r\n+Activates the Python virtual environment that contains the Activate.ps1 script.\r\n+\r\n+.Example\r\n+Activate.ps1 -Verbose\r\n+Activates the Python virtual environment that contains the Activate.ps1 script,\r\n+and shows extra information about the activation as it executes.\r\n+\r\n+.Example\r\n+Activate.ps1 -VenvDir C:\\Users\\MyUser\\Common\\.venv\r\n+Activates the Python virtual environment located in the specified location.\r\n+\r\n+.Example\r\n+Activate.ps1 -Prompt \"MyPython\"\r\n+Activates the Python virtual environment that contains the Activate.ps1 script,\r\n+and prefixes the current prompt with the specified string (surrounded in\r\n+parentheses) while the virtual environment is active.\r\n+\r\n+.Notes\r\n+On Windows, it may be required to enable this Activate.ps1 script by setting the\r\n+execution policy for the user. You can do this by issuing the following PowerShell\r\n+command:\r\n+\r\n+PS C:\\> Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\r\n+\r\n+For more information on Execution Policies: \r\n+https://go.microsoft.com/fwlink/?LinkID=135170\r\n+\r\n+#>\r\n+Param(\r\n+    [Parameter(Mandatory = $false)]\r\n+    [String]\r\n+    $VenvDir,\r\n+    [Parameter(Mandatory = $false)]\r\n+    [String]\r\n+    $Prompt\r\n+)\r\n+\r\n+<# Function declarations --------------------------------------------------- #>\r\n+\r\n+<#\r\n+.Synopsis\r\n+Remove all shell session elements added by the Activate script, including the\r\n+addition of the virtual environment's Python executable from the beginning of\r\n+the PATH variable.\r\n+\r\n+.Parameter NonDestructive\r\n+If present, do not remove this function from the global namespace for the\r\n+session.\r\n+\r\n+#>\r\n+function global:deactivate ([switch]$NonDestructive) {\r\n+    # Revert to original values\r\n+\r\n+    # The prior prompt:\r\n+    if (Test-Path -Path Function:_OLD_VIRTUAL_PROMPT) {\r\n+        Copy-Item -Path Function:_OLD_VIRTUAL_PROMPT -Destination Function:prompt\r\n+        Remove-Item -Path Function:_OLD_VIRTUAL_PROMPT\r\n+    }\r\n+\r\n+    # The prior PYTHONHOME:\r\n+    if (Test-Path -Path Env:_OLD_VIRTUAL_PYTHONHOME) {\r\n+        Copy-Item -Path Env:_OLD_VIRTUAL_PYTHONHOME -Destination Env:PYTHONHOME\r\n+        Remove-Item -Path Env:_OLD_VIRTUAL_PYTHONHOME\r\n+    }\r\n+\r\n+    # The prior PATH:\r\n+    if (Test-Path -Path Env:_OLD_VIRTUAL_PATH) {\r\n+        Copy-Item -Path Env:_OLD_VIRTUAL_PATH -Destination Env:PATH\r\n+        Remove-Item -Path Env:_OLD_VIRTUAL_PATH\r\n+    }\r\n+\r\n+    # Just remove the VIRTUAL_ENV altogether:\r\n+    if (Test-Path -Path Env:VIRTUAL_ENV) {\r\n+        Remove-Item -Path env:VIRTUAL_ENV\r\n+    }\r\n+\r\n+    # Just remove VIRTUAL_ENV_PROMPT altogether.\r\n+    if (Test-Path -Path Env:VIRTUAL_ENV_PROMPT) {\r\n+        Remove-Item -Path env:VIRTUAL_ENV_PROMPT\r\n+    }\r\n+\r\n+    # Just remove the _PYTHON_VENV_PROMPT_PREFIX altogether:\r\n+    if (Get-Variable -Name \"_PYTHON_VENV_PROMPT_PREFIX\" -ErrorAction SilentlyContinue) {\r\n+        Remove-Variable -Name _PYTHON_VENV_PROMPT_PREFIX -Scope Global -Force\r\n+    }\r\n+\r\n+    # Leave deactivate function in the global namespace if requested:\r\n+    if (-not $NonDestructive) {\r\n+        Remove-Item -Path function:deactivate\r\n+    }\r\n+}\r\n+\r\n+<#\r\n+.Description\r\n+Get-PyVenvConfig parses the values from the pyvenv.cfg file located in the\r\n+given folder, and returns them in a map.\r\n+\r\n+For each line in the pyvenv.cfg file, if that line can be parsed into exactly\r\n+two strings separated by `=` (with any amount of whitespace surrounding the =)\r\n+then it is considered a `key = value` line. The left hand string is the key,\r\n+the right hand is the value.\r\n+\r\n+If the value starts with a `'` or a `\"` then the first and last character is\r\n+stripped from the value before being captured.\r\n+\r\n+.Parameter ConfigDir\r\n+Path to the directory that contains the `pyvenv.cfg` file.\r\n+#>\r\n+function Get-PyVenvConfig(\r\n+    [String]\r\n+    $ConfigDir\r\n+) {\r\n+    Write-Verbose \"Given ConfigDir=$ConfigDir, obtain values in pyvenv.cfg\"\r\n+\r\n+    # Ensure the file exists, and issue a warning if it doesn't (but still allow the function to continue).\r\n+    $pyvenvConfigPath = Join-Path -Resolve -Path $ConfigDir -ChildPath 'pyvenv.cfg' -ErrorAction Continue\r\n+\r\n+    # An empty map will be returned if no config file is found.\r\n+    $pyvenvConfig = @{ }\r\n+\r\n+    if ($pyvenvConfigPath) {\r\n+\r\n+        Write-Verbose \"File exists, parse `key = value` lines\"\r\n+        $pyvenvConfigContent = Get-Content -Path $pyvenvConfigPath\r\n+\r\n+        $pyvenvConfigContent | ForEach-Object {\r\n+            $keyval = $PSItem -split \"\\s*=\\s*\", 2\r\n+            if ($keyval[0] -and $keyval[1]) {\r\n+                $val = $keyval[1]\r\n+\r\n+                # Remove extraneous quotations around a string value.\r\n+                if (\"'\"\"\".Contains($val.Substring(0, 1))) {\r\n+                    $val = $val.Substring(1, $val.Length - 2)\r\n+                }\r\n+\r\n+                $pyvenvConfig[$keyval[0]] = $val\r\n+                Write-Verbose \"Adding Key: '$($keyval[0])'='$val'\"\r\n+            }\r\n+        }\r\n+    }\r\n+    return $pyvenvConfig\r\n+}\r\n+\r\n+\r\n+<# Begin Activate script --------------------------------------------------- #>\r\n+\r\n+# Determine the containing directory of this script\r\n+$VenvExecPath = Split-Path -Parent $MyInvocation.MyCommand.Definition\r\n+$VenvExecDir = Get-Item -Path $VenvExecPath\r\n+\r\n+Write-Verbose \"Activation script is located in path: '$VenvExecPath'\"\r\n+Write-Verbose \"VenvExecDir Fullname: '$($VenvExecDir.FullName)\"\r\n+Write-Verbose \"VenvExecDir Name: '$($VenvExecDir.Name)\"\r\n+\r\n+# Set values required in priority: CmdLine, ConfigFile, Default\r\n+# First, get the location of the virtual environment, it might not be\r\n+# VenvExecDir if specified on the command line.\r\n+if ($VenvDir) {\r\n+    Write-Verbose \"VenvDir given as parameter, using '$VenvDir' to determine values\"\r\n+}\r\n+else {\r\n+    Write-Verbose \"VenvDir not given as a parameter, using parent directory name as VenvDir.\"\r\n+    $VenvDir = $VenvExecDir.Parent.FullName.TrimEnd(\"\\\\/\")\r\n+    Write-Verbose \"VenvDir=$VenvDir\"\r\n+}\r\n+\r\n+# Next, read the `pyvenv.cfg` file to determine any required value such\r\n+# as `prompt`.\r\n+$pyvenvCfg = Get-PyVenvConfig -ConfigDir $VenvDir\r\n+\r\n+# Next, set the prompt from the command line, or the config file, or\r\n+# just use the name of the virtual environment folder.\r\n+if ($Prompt) {\r\n+    Write-Verbose \"Prompt specified as argument, using '$Prompt'\"\r\n+}\r\n+else {\r\n+    Write-Verbose \"Prompt not specified as argument to script, checking pyvenv.cfg value\"\r\n+    if ($pyvenvCfg -and $pyvenvCfg['prompt']) {\r\n+        Write-Verbose \"  Setting based on value in pyvenv.cfg='$($pyvenvCfg['prompt'])'\"\r\n+        $Prompt = $pyvenvCfg['prompt'];\r\n+    }\r\n+    else {\r\n+        Write-Verbose \"  Setting prompt based on parent's directory's name. (Is the directory name passed to venv module when creating the virtual environment)\"\r\n+        Write-Verbose \"  Got leaf-name of $VenvDir='$(Split-Path -Path $venvDir -Leaf)'\"\r\n+        $Prompt = Split-Path -Path $venvDir -Leaf\r\n+    }\r\n+}\r\n+\r\n+Write-Verbose \"Prompt = '$Prompt'\"\r\n+Write-Verbose \"VenvDir='$VenvDir'\"\r\n+\r\n+# Deactivate any currently active virtual environment, but leave the\r\n+# deactivate function in place.\r\n+deactivate -nondestructive\r\n+\r\n+# Now set the environment variable VIRTUAL_ENV, used by many tools to determine\r\n+# that there is an activated venv.\r\n+$env:VIRTUAL_ENV = $VenvDir\r\n+\r\n+if (-not $Env:VIRTUAL_ENV_DISABLE_PROMPT) {\r\n+\r\n+    Write-Verbose \"Setting prompt to '$Prompt'\"\r\n+\r\n+    # Set the prompt to include the env name\r\n+    # Make sure _OLD_VIRTUAL_PROMPT is global\r\n+    function global:_OLD_VIRTUAL_PROMPT { \"\" }\r\n+    Copy-Item -Path function:prompt -Destination function:_OLD_VIRTUAL_PROMPT\r\n+    New-Variable -Name _PYTHON_VENV_PROMPT_PREFIX -Description \"Python virtual environment prompt prefix\" -Scope Global -Option ReadOnly -Visibility Public -Value $Prompt\r\n+\r\n+    function global:prompt {\r\n+        Write-Host -NoNewline -ForegroundColor Green \"($_PYTHON_VENV_PROMPT_PREFIX) \"\r\n+        _OLD_VIRTUAL_PROMPT\r\n+    }\r\n+    $env:VIRTUAL_ENV_PROMPT = $Prompt\r\n+}\r\n+\r\n+# Clear PYTHONHOME\r\n+if (Test-Path -Path Env:PYTHONHOME) {\r\n+    Copy-Item -Path Env:PYTHONHOME -Destination Env:_OLD_VIRTUAL_PYTHONHOME\r\n+    Remove-Item -Path Env:PYTHONHOME\r\n+}\r\n+\r\n+# Add the venv to the PATH\r\n+Copy-Item -Path Env:PATH -Destination Env:_OLD_VIRTUAL_PATH\r\n+$Env:PATH = \"$VenvExecDir$([System.IO.Path]::PathSeparator)$Env:PATH\"\r",
      "patch_lines": [
        "@@ -0,0 +1,247 @@\n",
        "+<#\r\n",
        "+.Synopsis\r\n",
        "+Activate a Python virtual environment for the current PowerShell session.\r\n",
        "+\r\n",
        "+.Description\r\n",
        "+Pushes the python executable for a virtual environment to the front of the\r\n",
        "+$Env:PATH environment variable and sets the prompt to signify that you are\r\n",
        "+in a Python virtual environment. Makes use of the command line switches as\r\n",
        "+well as the `pyvenv.cfg` file values present in the virtual environment.\r\n",
        "+\r\n",
        "+.Parameter VenvDir\r\n",
        "+Path to the directory that contains the virtual environment to activate. The\r\n",
        "+default value for this is the parent of the directory that the Activate.ps1\r\n",
        "+script is located within.\r\n",
        "+\r\n",
        "+.Parameter Prompt\r\n",
        "+The prompt prefix to display when this virtual environment is activated. By\r\n",
        "+default, this prompt is the name of the virtual environment folder (VenvDir)\r\n",
        "+surrounded by parentheses and followed by a single space (ie. '(.venv) ').\r\n",
        "+\r\n",
        "+.Example\r\n",
        "+Activate.ps1\r\n",
        "+Activates the Python virtual environment that contains the Activate.ps1 script.\r\n",
        "+\r\n",
        "+.Example\r\n",
        "+Activate.ps1 -Verbose\r\n",
        "+Activates the Python virtual environment that contains the Activate.ps1 script,\r\n",
        "+and shows extra information about the activation as it executes.\r\n",
        "+\r\n",
        "+.Example\r\n",
        "+Activate.ps1 -VenvDir C:\\Users\\MyUser\\Common\\.venv\r\n",
        "+Activates the Python virtual environment located in the specified location.\r\n",
        "+\r\n",
        "+.Example\r\n",
        "+Activate.ps1 -Prompt \"MyPython\"\r\n",
        "+Activates the Python virtual environment that contains the Activate.ps1 script,\r\n",
        "+and prefixes the current prompt with the specified string (surrounded in\r\n",
        "+parentheses) while the virtual environment is active.\r\n",
        "+\r\n",
        "+.Notes\r\n",
        "+On Windows, it may be required to enable this Activate.ps1 script by setting the\r\n",
        "+execution policy for the user. You can do this by issuing the following PowerShell\r\n",
        "+command:\r\n",
        "+\r\n",
        "+PS C:\\> Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\r\n",
        "+\r\n",
        "+For more information on Execution Policies: \r\n",
        "+https://go.microsoft.com/fwlink/?LinkID=135170\r\n",
        "+\r\n",
        "+#>\r\n",
        "+Param(\r\n",
        "+    [Parameter(Mandatory = $false)]\r\n",
        "+    [String]\r\n",
        "+    $VenvDir,\r\n",
        "+    [Parameter(Mandatory = $false)]\r\n",
        "+    [String]\r\n",
        "+    $Prompt\r\n",
        "+)\r\n",
        "+\r\n",
        "+<# Function declarations --------------------------------------------------- #>\r\n",
        "+\r\n",
        "+<#\r\n",
        "+.Synopsis\r\n",
        "+Remove all shell session elements added by the Activate script, including the\r\n",
        "+addition of the virtual environment's Python executable from the beginning of\r\n",
        "+the PATH variable.\r\n",
        "+\r\n",
        "+.Parameter NonDestructive\r\n",
        "+If present, do not remove this function from the global namespace for the\r\n",
        "+session.\r\n",
        "+\r\n",
        "+#>\r\n",
        "+function global:deactivate ([switch]$NonDestructive) {\r\n",
        "+    # Revert to original values\r\n",
        "+\r\n",
        "+    # The prior prompt:\r\n",
        "+    if (Test-Path -Path Function:_OLD_VIRTUAL_PROMPT) {\r\n",
        "+        Copy-Item -Path Function:_OLD_VIRTUAL_PROMPT -Destination Function:prompt\r\n",
        "+        Remove-Item -Path Function:_OLD_VIRTUAL_PROMPT\r\n",
        "+    }\r\n",
        "+\r\n",
        "+    # The prior PYTHONHOME:\r\n",
        "+    if (Test-Path -Path Env:_OLD_VIRTUAL_PYTHONHOME) {\r\n",
        "+        Copy-Item -Path Env:_OLD_VIRTUAL_PYTHONHOME -Destination Env:PYTHONHOME\r\n",
        "+        Remove-Item -Path Env:_OLD_VIRTUAL_PYTHONHOME\r\n",
        "+    }\r\n",
        "+\r\n",
        "+    # The prior PATH:\r\n",
        "+    if (Test-Path -Path Env:_OLD_VIRTUAL_PATH) {\r\n",
        "+        Copy-Item -Path Env:_OLD_VIRTUAL_PATH -Destination Env:PATH\r\n",
        "+        Remove-Item -Path Env:_OLD_VIRTUAL_PATH\r\n",
        "+    }\r\n",
        "+\r\n",
        "+    # Just remove the VIRTUAL_ENV altogether:\r\n",
        "+    if (Test-Path -Path Env:VIRTUAL_ENV) {\r\n",
        "+        Remove-Item -Path env:VIRTUAL_ENV\r\n",
        "+    }\r\n",
        "+\r\n",
        "+    # Just remove VIRTUAL_ENV_PROMPT altogether.\r\n",
        "+    if (Test-Path -Path Env:VIRTUAL_ENV_PROMPT) {\r\n",
        "+        Remove-Item -Path env:VIRTUAL_ENV_PROMPT\r\n",
        "+    }\r\n",
        "+\r\n",
        "+    # Just remove the _PYTHON_VENV_PROMPT_PREFIX altogether:\r\n",
        "+    if (Get-Variable -Name \"_PYTHON_VENV_PROMPT_PREFIX\" -ErrorAction SilentlyContinue) {\r\n",
        "+        Remove-Variable -Name _PYTHON_VENV_PROMPT_PREFIX -Scope Global -Force\r\n",
        "+    }\r\n",
        "+\r\n",
        "+    # Leave deactivate function in the global namespace if requested:\r\n",
        "+    if (-not $NonDestructive) {\r\n",
        "+        Remove-Item -Path function:deactivate\r\n",
        "+    }\r\n",
        "+}\r\n",
        "+\r\n",
        "+<#\r\n",
        "+.Description\r\n",
        "+Get-PyVenvConfig parses the values from the pyvenv.cfg file located in the\r\n",
        "+given folder, and returns them in a map.\r\n",
        "+\r\n",
        "+For each line in the pyvenv.cfg file, if that line can be parsed into exactly\r\n",
        "+two strings separated by `=` (with any amount of whitespace surrounding the =)\r\n",
        "+then it is considered a `key = value` line. The left hand string is the key,\r\n",
        "+the right hand is the value.\r\n",
        "+\r\n",
        "+If the value starts with a `'` or a `\"` then the first and last character is\r\n",
        "+stripped from the value before being captured.\r\n",
        "+\r\n",
        "+.Parameter ConfigDir\r\n",
        "+Path to the directory that contains the `pyvenv.cfg` file.\r\n",
        "+#>\r\n",
        "+function Get-PyVenvConfig(\r\n",
        "+    [String]\r\n",
        "+    $ConfigDir\r\n",
        "+) {\r\n",
        "+    Write-Verbose \"Given ConfigDir=$ConfigDir, obtain values in pyvenv.cfg\"\r\n",
        "+\r\n",
        "+    # Ensure the file exists, and issue a warning if it doesn't (but still allow the function to continue).\r\n",
        "+    $pyvenvConfigPath = Join-Path -Resolve -Path $ConfigDir -ChildPath 'pyvenv.cfg' -ErrorAction Continue\r\n",
        "+\r\n",
        "+    # An empty map will be returned if no config file is found.\r\n",
        "+    $pyvenvConfig = @{ }\r\n",
        "+\r\n",
        "+    if ($pyvenvConfigPath) {\r\n",
        "+\r\n",
        "+        Write-Verbose \"File exists, parse `key = value` lines\"\r\n",
        "+        $pyvenvConfigContent = Get-Content -Path $pyvenvConfigPath\r\n",
        "+\r\n",
        "+        $pyvenvConfigContent | ForEach-Object {\r\n",
        "+            $keyval = $PSItem -split \"\\s*=\\s*\", 2\r\n",
        "+            if ($keyval[0] -and $keyval[1]) {\r\n",
        "+                $val = $keyval[1]\r\n",
        "+\r\n",
        "+                # Remove extraneous quotations around a string value.\r\n",
        "+                if (\"'\"\"\".Contains($val.Substring(0, 1))) {\r\n",
        "+                    $val = $val.Substring(1, $val.Length - 2)\r\n",
        "+                }\r\n",
        "+\r\n",
        "+                $pyvenvConfig[$keyval[0]] = $val\r\n",
        "+                Write-Verbose \"Adding Key: '$($keyval[0])'='$val'\"\r\n",
        "+            }\r\n",
        "+        }\r\n",
        "+    }\r\n",
        "+    return $pyvenvConfig\r\n",
        "+}\r\n",
        "+\r\n",
        "+\r\n",
        "+<# Begin Activate script --------------------------------------------------- #>\r\n",
        "+\r\n",
        "+# Determine the containing directory of this script\r\n",
        "+$VenvExecPath = Split-Path -Parent $MyInvocation.MyCommand.Definition\r\n",
        "+$VenvExecDir = Get-Item -Path $VenvExecPath\r\n",
        "+\r\n",
        "+Write-Verbose \"Activation script is located in path: '$VenvExecPath'\"\r\n",
        "+Write-Verbose \"VenvExecDir Fullname: '$($VenvExecDir.FullName)\"\r\n",
        "+Write-Verbose \"VenvExecDir Name: '$($VenvExecDir.Name)\"\r\n",
        "+\r\n",
        "+# Set values required in priority: CmdLine, ConfigFile, Default\r\n",
        "+# First, get the location of the virtual environment, it might not be\r\n",
        "+# VenvExecDir if specified on the command line.\r\n",
        "+if ($VenvDir) {\r\n",
        "+    Write-Verbose \"VenvDir given as parameter, using '$VenvDir' to determine values\"\r\n",
        "+}\r\n",
        "+else {\r\n",
        "+    Write-Verbose \"VenvDir not given as a parameter, using parent directory name as VenvDir.\"\r\n",
        "+    $VenvDir = $VenvExecDir.Parent.FullName.TrimEnd(\"\\\\/\")\r\n",
        "+    Write-Verbose \"VenvDir=$VenvDir\"\r\n",
        "+}\r\n",
        "+\r\n",
        "+# Next, read the `pyvenv.cfg` file to determine any required value such\r\n",
        "+# as `prompt`.\r\n",
        "+$pyvenvCfg = Get-PyVenvConfig -ConfigDir $VenvDir\r\n",
        "+\r\n",
        "+# Next, set the prompt from the command line, or the config file, or\r\n",
        "+# just use the name of the virtual environment folder.\r\n",
        "+if ($Prompt) {\r\n",
        "+    Write-Verbose \"Prompt specified as argument, using '$Prompt'\"\r\n",
        "+}\r\n",
        "+else {\r\n",
        "+    Write-Verbose \"Prompt not specified as argument to script, checking pyvenv.cfg value\"\r\n",
        "+    if ($pyvenvCfg -and $pyvenvCfg['prompt']) {\r\n",
        "+        Write-Verbose \"  Setting based on value in pyvenv.cfg='$($pyvenvCfg['prompt'])'\"\r\n",
        "+        $Prompt = $pyvenvCfg['prompt'];\r\n",
        "+    }\r\n",
        "+    else {\r\n",
        "+        Write-Verbose \"  Setting prompt based on parent's directory's name. (Is the directory name passed to venv module when creating the virtual environment)\"\r\n",
        "+        Write-Verbose \"  Got leaf-name of $VenvDir='$(Split-Path -Path $venvDir -Leaf)'\"\r\n",
        "+        $Prompt = Split-Path -Path $venvDir -Leaf\r\n",
        "+    }\r\n",
        "+}\r\n",
        "+\r\n",
        "+Write-Verbose \"Prompt = '$Prompt'\"\r\n",
        "+Write-Verbose \"VenvDir='$VenvDir'\"\r\n",
        "+\r\n",
        "+# Deactivate any currently active virtual environment, but leave the\r\n",
        "+# deactivate function in place.\r\n",
        "+deactivate -nondestructive\r\n",
        "+\r\n",
        "+# Now set the environment variable VIRTUAL_ENV, used by many tools to determine\r\n",
        "+# that there is an activated venv.\r\n",
        "+$env:VIRTUAL_ENV = $VenvDir\r\n",
        "+\r\n",
        "+if (-not $Env:VIRTUAL_ENV_DISABLE_PROMPT) {\r\n",
        "+\r\n",
        "+    Write-Verbose \"Setting prompt to '$Prompt'\"\r\n",
        "+\r\n",
        "+    # Set the prompt to include the env name\r\n",
        "+    # Make sure _OLD_VIRTUAL_PROMPT is global\r\n",
        "+    function global:_OLD_VIRTUAL_PROMPT { \"\" }\r\n",
        "+    Copy-Item -Path function:prompt -Destination function:_OLD_VIRTUAL_PROMPT\r\n",
        "+    New-Variable -Name _PYTHON_VENV_PROMPT_PREFIX -Description \"Python virtual environment prompt prefix\" -Scope Global -Option ReadOnly -Visibility Public -Value $Prompt\r\n",
        "+\r\n",
        "+    function global:prompt {\r\n",
        "+        Write-Host -NoNewline -ForegroundColor Green \"($_PYTHON_VENV_PROMPT_PREFIX) \"\r\n",
        "+        _OLD_VIRTUAL_PROMPT\r\n",
        "+    }\r\n",
        "+    $env:VIRTUAL_ENV_PROMPT = $Prompt\r\n",
        "+}\r\n",
        "+\r\n",
        "+# Clear PYTHONHOME\r\n",
        "+if (Test-Path -Path Env:PYTHONHOME) {\r\n",
        "+    Copy-Item -Path Env:PYTHONHOME -Destination Env:_OLD_VIRTUAL_PYTHONHOME\r\n",
        "+    Remove-Item -Path Env:PYTHONHOME\r\n",
        "+}\r\n",
        "+\r\n",
        "+# Add the venv to the PATH\r\n",
        "+Copy-Item -Path Env:PATH -Destination Env:_OLD_VIRTUAL_PATH\r\n",
        "+$Env:PATH = \"$VenvExecDir$([System.IO.Path]::PathSeparator)$Env:PATH\"\r\n"
      ]
    },
    {
      "path": "agent/venv/bin/activate",
      "status": "added",
      "additions": 70,
      "deletions": 0,
      "patch": "@@ -0,0 +1,70 @@\n+# This file must be used with \"source bin/activate\" *from bash*\n+# You cannot run it directly\n+\n+deactivate () {\n+    # reset old environment variables\n+    if [ -n \"${_OLD_VIRTUAL_PATH:-}\" ] ; then\n+        PATH=\"${_OLD_VIRTUAL_PATH:-}\"\n+        export PATH\n+        unset _OLD_VIRTUAL_PATH\n+    fi\n+    if [ -n \"${_OLD_VIRTUAL_PYTHONHOME:-}\" ] ; then\n+        PYTHONHOME=\"${_OLD_VIRTUAL_PYTHONHOME:-}\"\n+        export PYTHONHOME\n+        unset _OLD_VIRTUAL_PYTHONHOME\n+    fi\n+\n+    # Call hash to forget past commands. Without forgetting\n+    # past commands the $PATH changes we made may not be respected\n+    hash -r 2> /dev/null\n+\n+    if [ -n \"${_OLD_VIRTUAL_PS1:-}\" ] ; then\n+        PS1=\"${_OLD_VIRTUAL_PS1:-}\"\n+        export PS1\n+        unset _OLD_VIRTUAL_PS1\n+    fi\n+\n+    unset VIRTUAL_ENV\n+    unset VIRTUAL_ENV_PROMPT\n+    if [ ! \"${1:-}\" = \"nondestructive\" ] ; then\n+    # Self destruct!\n+        unset -f deactivate\n+    fi\n+}\n+\n+# unset irrelevant variables\n+deactivate nondestructive\n+\n+# on Windows, a path can contain colons and backslashes and has to be converted:\n+if [ \"${OSTYPE:-}\" = \"cygwin\" ] || [ \"${OSTYPE:-}\" = \"msys\" ] ; then\n+    # transform D:\\path\\to\\venv to /d/path/to/venv on MSYS\n+    # and to /cygdrive/d/path/to/venv on Cygwin\n+    export VIRTUAL_ENV=$(cygpath /home/joseph-mazzini/sys-scan-graph/agent/venv)\n+else\n+    # use the path as-is\n+    export VIRTUAL_ENV=/home/joseph-mazzini/sys-scan-graph/agent/venv\n+fi\n+\n+_OLD_VIRTUAL_PATH=\"$PATH\"\n+PATH=\"$VIRTUAL_ENV/\"bin\":$PATH\"\n+export PATH\n+\n+# unset PYTHONHOME if set\n+# this will fail if PYTHONHOME is set to the empty string (which is bad anyway)\n+# could use `if (set -u; : $PYTHONHOME) ;` in bash\n+if [ -n \"${PYTHONHOME:-}\" ] ; then\n+    _OLD_VIRTUAL_PYTHONHOME=\"${PYTHONHOME:-}\"\n+    unset PYTHONHOME\n+fi\n+\n+if [ -z \"${VIRTUAL_ENV_DISABLE_PROMPT:-}\" ] ; then\n+    _OLD_VIRTUAL_PS1=\"${PS1:-}\"\n+    PS1='(venv) '\"${PS1:-}\"\n+    export PS1\n+    VIRTUAL_ENV_PROMPT='(venv) '\n+    export VIRTUAL_ENV_PROMPT\n+fi\n+\n+# Call hash to forget past commands. Without forgetting\n+# past commands the $PATH changes we made may not be respected\n+hash -r 2> /dev/null",
      "patch_lines": [
        "@@ -0,0 +1,70 @@\n",
        "+# This file must be used with \"source bin/activate\" *from bash*\n",
        "+# You cannot run it directly\n",
        "+\n",
        "+deactivate () {\n",
        "+    # reset old environment variables\n",
        "+    if [ -n \"${_OLD_VIRTUAL_PATH:-}\" ] ; then\n",
        "+        PATH=\"${_OLD_VIRTUAL_PATH:-}\"\n",
        "+        export PATH\n",
        "+        unset _OLD_VIRTUAL_PATH\n",
        "+    fi\n",
        "+    if [ -n \"${_OLD_VIRTUAL_PYTHONHOME:-}\" ] ; then\n",
        "+        PYTHONHOME=\"${_OLD_VIRTUAL_PYTHONHOME:-}\"\n",
        "+        export PYTHONHOME\n",
        "+        unset _OLD_VIRTUAL_PYTHONHOME\n",
        "+    fi\n",
        "+\n",
        "+    # Call hash to forget past commands. Without forgetting\n",
        "+    # past commands the $PATH changes we made may not be respected\n",
        "+    hash -r 2> /dev/null\n",
        "+\n",
        "+    if [ -n \"${_OLD_VIRTUAL_PS1:-}\" ] ; then\n",
        "+        PS1=\"${_OLD_VIRTUAL_PS1:-}\"\n",
        "+        export PS1\n",
        "+        unset _OLD_VIRTUAL_PS1\n",
        "+    fi\n",
        "+\n",
        "+    unset VIRTUAL_ENV\n",
        "+    unset VIRTUAL_ENV_PROMPT\n",
        "+    if [ ! \"${1:-}\" = \"nondestructive\" ] ; then\n",
        "+    # Self destruct!\n",
        "+        unset -f deactivate\n",
        "+    fi\n",
        "+}\n",
        "+\n",
        "+# unset irrelevant variables\n",
        "+deactivate nondestructive\n",
        "+\n",
        "+# on Windows, a path can contain colons and backslashes and has to be converted:\n",
        "+if [ \"${OSTYPE:-}\" = \"cygwin\" ] || [ \"${OSTYPE:-}\" = \"msys\" ] ; then\n",
        "+    # transform D:\\path\\to\\venv to /d/path/to/venv on MSYS\n",
        "+    # and to /cygdrive/d/path/to/venv on Cygwin\n",
        "+    export VIRTUAL_ENV=$(cygpath /home/joseph-mazzini/sys-scan-graph/agent/venv)\n",
        "+else\n",
        "+    # use the path as-is\n",
        "+    export VIRTUAL_ENV=/home/joseph-mazzini/sys-scan-graph/agent/venv\n",
        "+fi\n",
        "+\n",
        "+_OLD_VIRTUAL_PATH=\"$PATH\"\n",
        "+PATH=\"$VIRTUAL_ENV/\"bin\":$PATH\"\n",
        "+export PATH\n",
        "+\n",
        "+# unset PYTHONHOME if set\n",
        "+# this will fail if PYTHONHOME is set to the empty string (which is bad anyway)\n",
        "+# could use `if (set -u; : $PYTHONHOME) ;` in bash\n",
        "+if [ -n \"${PYTHONHOME:-}\" ] ; then\n",
        "+    _OLD_VIRTUAL_PYTHONHOME=\"${PYTHONHOME:-}\"\n",
        "+    unset PYTHONHOME\n",
        "+fi\n",
        "+\n",
        "+if [ -z \"${VIRTUAL_ENV_DISABLE_PROMPT:-}\" ] ; then\n",
        "+    _OLD_VIRTUAL_PS1=\"${PS1:-}\"\n",
        "+    PS1='(venv) '\"${PS1:-}\"\n",
        "+    export PS1\n",
        "+    VIRTUAL_ENV_PROMPT='(venv) '\n",
        "+    export VIRTUAL_ENV_PROMPT\n",
        "+fi\n",
        "+\n",
        "+# Call hash to forget past commands. Without forgetting\n",
        "+# past commands the $PATH changes we made may not be respected\n",
        "+hash -r 2> /dev/null\n"
      ]
    },
    {
      "path": "agent/venv/bin/activate.csh",
      "status": "added",
      "additions": 27,
      "deletions": 0,
      "patch": "@@ -0,0 +1,27 @@\n+# This file must be used with \"source bin/activate.csh\" *from csh*.\n+# You cannot run it directly.\n+\n+# Created by Davide Di Blasi <davidedb@gmail.com>.\n+# Ported to Python 3.3 venv by Andrew Svetlov <andrew.svetlov@gmail.com>\n+\n+alias deactivate 'test $?_OLD_VIRTUAL_PATH != 0 && setenv PATH \"$_OLD_VIRTUAL_PATH\" && unset _OLD_VIRTUAL_PATH; rehash; test $?_OLD_VIRTUAL_PROMPT != 0 && set prompt=\"$_OLD_VIRTUAL_PROMPT\" && unset _OLD_VIRTUAL_PROMPT; unsetenv VIRTUAL_ENV; unsetenv VIRTUAL_ENV_PROMPT; test \"\\!:*\" != \"nondestructive\" && unalias deactivate'\n+\n+# Unset irrelevant variables.\n+deactivate nondestructive\n+\n+setenv VIRTUAL_ENV /home/joseph-mazzini/sys-scan-graph/agent/venv\n+\n+set _OLD_VIRTUAL_PATH=\"$PATH\"\n+setenv PATH \"$VIRTUAL_ENV/\"bin\":$PATH\"\n+\n+\n+set _OLD_VIRTUAL_PROMPT=\"$prompt\"\n+\n+if (! \"$?VIRTUAL_ENV_DISABLE_PROMPT\") then\n+    set prompt = '(venv) '\"$prompt\"\n+    setenv VIRTUAL_ENV_PROMPT '(venv) '\n+endif\n+\n+alias pydoc python -m pydoc\n+\n+rehash",
      "patch_lines": [
        "@@ -0,0 +1,27 @@\n",
        "+# This file must be used with \"source bin/activate.csh\" *from csh*.\n",
        "+# You cannot run it directly.\n",
        "+\n",
        "+# Created by Davide Di Blasi <davidedb@gmail.com>.\n",
        "+# Ported to Python 3.3 venv by Andrew Svetlov <andrew.svetlov@gmail.com>\n",
        "+\n",
        "+alias deactivate 'test $?_OLD_VIRTUAL_PATH != 0 && setenv PATH \"$_OLD_VIRTUAL_PATH\" && unset _OLD_VIRTUAL_PATH; rehash; test $?_OLD_VIRTUAL_PROMPT != 0 && set prompt=\"$_OLD_VIRTUAL_PROMPT\" && unset _OLD_VIRTUAL_PROMPT; unsetenv VIRTUAL_ENV; unsetenv VIRTUAL_ENV_PROMPT; test \"\\!:*\" != \"nondestructive\" && unalias deactivate'\n",
        "+\n",
        "+# Unset irrelevant variables.\n",
        "+deactivate nondestructive\n",
        "+\n",
        "+setenv VIRTUAL_ENV /home/joseph-mazzini/sys-scan-graph/agent/venv\n",
        "+\n",
        "+set _OLD_VIRTUAL_PATH=\"$PATH\"\n",
        "+setenv PATH \"$VIRTUAL_ENV/\"bin\":$PATH\"\n",
        "+\n",
        "+\n",
        "+set _OLD_VIRTUAL_PROMPT=\"$prompt\"\n",
        "+\n",
        "+if (! \"$?VIRTUAL_ENV_DISABLE_PROMPT\") then\n",
        "+    set prompt = '(venv) '\"$prompt\"\n",
        "+    setenv VIRTUAL_ENV_PROMPT '(venv) '\n",
        "+endif\n",
        "+\n",
        "+alias pydoc python -m pydoc\n",
        "+\n",
        "+rehash\n"
      ]
    },
    {
      "path": "agent/venv/bin/activate.fish",
      "status": "added",
      "additions": 69,
      "deletions": 0,
      "patch": "@@ -0,0 +1,69 @@\n+# This file must be used with \"source <venv>/bin/activate.fish\" *from fish*\n+# (https://fishshell.com/). You cannot run it directly.\n+\n+function deactivate  -d \"Exit virtual environment and return to normal shell environment\"\n+    # reset old environment variables\n+    if test -n \"$_OLD_VIRTUAL_PATH\"\n+        set -gx PATH $_OLD_VIRTUAL_PATH\n+        set -e _OLD_VIRTUAL_PATH\n+    end\n+    if test -n \"$_OLD_VIRTUAL_PYTHONHOME\"\n+        set -gx PYTHONHOME $_OLD_VIRTUAL_PYTHONHOME\n+        set -e _OLD_VIRTUAL_PYTHONHOME\n+    end\n+\n+    if test -n \"$_OLD_FISH_PROMPT_OVERRIDE\"\n+        set -e _OLD_FISH_PROMPT_OVERRIDE\n+        # prevents error when using nested fish instances (Issue #93858)\n+        if functions -q _old_fish_prompt\n+            functions -e fish_prompt\n+            functions -c _old_fish_prompt fish_prompt\n+            functions -e _old_fish_prompt\n+        end\n+    end\n+\n+    set -e VIRTUAL_ENV\n+    set -e VIRTUAL_ENV_PROMPT\n+    if test \"$argv[1]\" != \"nondestructive\"\n+        # Self-destruct!\n+        functions -e deactivate\n+    end\n+end\n+\n+# Unset irrelevant variables.\n+deactivate nondestructive\n+\n+set -gx VIRTUAL_ENV /home/joseph-mazzini/sys-scan-graph/agent/venv\n+\n+set -gx _OLD_VIRTUAL_PATH $PATH\n+set -gx PATH \"$VIRTUAL_ENV/\"bin $PATH\n+\n+# Unset PYTHONHOME if set.\n+if set -q PYTHONHOME\n+    set -gx _OLD_VIRTUAL_PYTHONHOME $PYTHONHOME\n+    set -e PYTHONHOME\n+end\n+\n+if test -z \"$VIRTUAL_ENV_DISABLE_PROMPT\"\n+    # fish uses a function instead of an env var to generate the prompt.\n+\n+    # Save the current fish_prompt function as the function _old_fish_prompt.\n+    functions -c fish_prompt _old_fish_prompt\n+\n+    # With the original prompt function renamed, we can override with our own.\n+    function fish_prompt\n+        # Save the return status of the last command.\n+        set -l old_status $status\n+\n+        # Output the venv prompt; color taken from the blue of the Python logo.\n+        printf \"%s%s%s\" (set_color 4B8BBE) '(venv) ' (set_color normal)\n+\n+        # Restore the return status of the previous command.\n+        echo \"exit $old_status\" | .\n+        # Output the original/\"old\" prompt.\n+        _old_fish_prompt\n+    end\n+\n+    set -gx _OLD_FISH_PROMPT_OVERRIDE \"$VIRTUAL_ENV\"\n+    set -gx VIRTUAL_ENV_PROMPT '(venv) '\n+end",
      "patch_lines": [
        "@@ -0,0 +1,69 @@\n",
        "+# This file must be used with \"source <venv>/bin/activate.fish\" *from fish*\n",
        "+# (https://fishshell.com/). You cannot run it directly.\n",
        "+\n",
        "+function deactivate  -d \"Exit virtual environment and return to normal shell environment\"\n",
        "+    # reset old environment variables\n",
        "+    if test -n \"$_OLD_VIRTUAL_PATH\"\n",
        "+        set -gx PATH $_OLD_VIRTUAL_PATH\n",
        "+        set -e _OLD_VIRTUAL_PATH\n",
        "+    end\n",
        "+    if test -n \"$_OLD_VIRTUAL_PYTHONHOME\"\n",
        "+        set -gx PYTHONHOME $_OLD_VIRTUAL_PYTHONHOME\n",
        "+        set -e _OLD_VIRTUAL_PYTHONHOME\n",
        "+    end\n",
        "+\n",
        "+    if test -n \"$_OLD_FISH_PROMPT_OVERRIDE\"\n",
        "+        set -e _OLD_FISH_PROMPT_OVERRIDE\n",
        "+        # prevents error when using nested fish instances (Issue #93858)\n",
        "+        if functions -q _old_fish_prompt\n",
        "+            functions -e fish_prompt\n",
        "+            functions -c _old_fish_prompt fish_prompt\n",
        "+            functions -e _old_fish_prompt\n",
        "+        end\n",
        "+    end\n",
        "+\n",
        "+    set -e VIRTUAL_ENV\n",
        "+    set -e VIRTUAL_ENV_PROMPT\n",
        "+    if test \"$argv[1]\" != \"nondestructive\"\n",
        "+        # Self-destruct!\n",
        "+        functions -e deactivate\n",
        "+    end\n",
        "+end\n",
        "+\n",
        "+# Unset irrelevant variables.\n",
        "+deactivate nondestructive\n",
        "+\n",
        "+set -gx VIRTUAL_ENV /home/joseph-mazzini/sys-scan-graph/agent/venv\n",
        "+\n",
        "+set -gx _OLD_VIRTUAL_PATH $PATH\n",
        "+set -gx PATH \"$VIRTUAL_ENV/\"bin $PATH\n",
        "+\n",
        "+# Unset PYTHONHOME if set.\n",
        "+if set -q PYTHONHOME\n",
        "+    set -gx _OLD_VIRTUAL_PYTHONHOME $PYTHONHOME\n",
        "+    set -e PYTHONHOME\n",
        "+end\n",
        "+\n",
        "+if test -z \"$VIRTUAL_ENV_DISABLE_PROMPT\"\n",
        "+    # fish uses a function instead of an env var to generate the prompt.\n",
        "+\n",
        "+    # Save the current fish_prompt function as the function _old_fish_prompt.\n",
        "+    functions -c fish_prompt _old_fish_prompt\n",
        "+\n",
        "+    # With the original prompt function renamed, we can override with our own.\n",
        "+    function fish_prompt\n",
        "+        # Save the return status of the last command.\n",
        "+        set -l old_status $status\n",
        "+\n",
        "+        # Output the venv prompt; color taken from the blue of the Python logo.\n",
        "+        printf \"%s%s%s\" (set_color 4B8BBE) '(venv) ' (set_color normal)\n",
        "+\n",
        "+        # Restore the return status of the previous command.\n",
        "+        echo \"exit $old_status\" | .\n",
        "+        # Output the original/\"old\" prompt.\n",
        "+        _old_fish_prompt\n",
        "+    end\n",
        "+\n",
        "+    set -gx _OLD_FISH_PROMPT_OVERRIDE \"$VIRTUAL_ENV\"\n",
        "+    set -gx VIRTUAL_ENV_PROMPT '(venv) '\n",
        "+end\n"
      ]
    },
    {
      "path": "agent/venv/bin/f2py",
      "status": "added",
      "additions": 8,
      "deletions": 0,
      "patch": "@@ -0,0 +1,8 @@\n+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n+# -*- coding: utf-8 -*-\n+import re\n+import sys\n+from numpy.f2py.f2py2e import main\n+if __name__ == '__main__':\n+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n+    sys.exit(main())",
      "patch_lines": [
        "@@ -0,0 +1,8 @@\n",
        "+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n",
        "+# -*- coding: utf-8 -*-\n",
        "+import re\n",
        "+import sys\n",
        "+from numpy.f2py.f2py2e import main\n",
        "+if __name__ == '__main__':\n",
        "+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n",
        "+    sys.exit(main())\n"
      ]
    },
    {
      "path": "agent/venv/bin/httpx",
      "status": "added",
      "additions": 8,
      "deletions": 0,
      "patch": "@@ -0,0 +1,8 @@\n+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n+# -*- coding: utf-8 -*-\n+import re\n+import sys\n+from httpx import main\n+if __name__ == '__main__':\n+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n+    sys.exit(main())",
      "patch_lines": [
        "@@ -0,0 +1,8 @@\n",
        "+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n",
        "+# -*- coding: utf-8 -*-\n",
        "+import re\n",
        "+import sys\n",
        "+from httpx import main\n",
        "+if __name__ == '__main__':\n",
        "+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n",
        "+    sys.exit(main())\n"
      ]
    },
    {
      "path": "agent/venv/bin/jsondiff",
      "status": "added",
      "additions": 41,
      "deletions": 0,
      "patch": "@@ -0,0 +1,41 @@\n+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n+# -*- coding: utf-8 -*-\n+\n+from __future__ import print_function\n+\n+import sys\n+import json\n+import jsonpatch\n+import argparse\n+\n+\n+parser = argparse.ArgumentParser(description='Diff two JSON files')\n+parser.add_argument('FILE1', type=argparse.FileType('r'))\n+parser.add_argument('FILE2', type=argparse.FileType('r'))\n+parser.add_argument('--indent', type=int, default=None,\n+                    help='Indent output by n spaces')\n+parser.add_argument('-u', '--preserve-unicode', action='store_true',\n+                    help='Output Unicode character as-is without using Code Point')\n+parser.add_argument('-v', '--version', action='version',\n+                    version='%(prog)s ' + jsonpatch.__version__)\n+\n+\n+def main():\n+    try:\n+        diff_files()\n+    except KeyboardInterrupt:\n+        sys.exit(1)\n+\n+\n+def diff_files():\n+    \"\"\" Diffs two JSON files and prints a patch \"\"\"\n+    args = parser.parse_args()\n+    doc1 = json.load(args.FILE1)\n+    doc2 = json.load(args.FILE2)\n+    patch = jsonpatch.make_patch(doc1, doc2)\n+    if patch.patch:\n+        print(json.dumps(patch.patch, indent=args.indent, ensure_ascii=not(args.preserve_unicode)))\n+        sys.exit(1)\n+\n+if __name__ == \"__main__\":\n+    main()",
      "patch_lines": [
        "@@ -0,0 +1,41 @@\n",
        "+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n",
        "+# -*- coding: utf-8 -*-\n",
        "+\n",
        "+from __future__ import print_function\n",
        "+\n",
        "+import sys\n",
        "+import json\n",
        "+import jsonpatch\n",
        "+import argparse\n",
        "+\n",
        "+\n",
        "+parser = argparse.ArgumentParser(description='Diff two JSON files')\n",
        "+parser.add_argument('FILE1', type=argparse.FileType('r'))\n",
        "+parser.add_argument('FILE2', type=argparse.FileType('r'))\n",
        "+parser.add_argument('--indent', type=int, default=None,\n",
        "+                    help='Indent output by n spaces')\n",
        "+parser.add_argument('-u', '--preserve-unicode', action='store_true',\n",
        "+                    help='Output Unicode character as-is without using Code Point')\n",
        "+parser.add_argument('-v', '--version', action='version',\n",
        "+                    version='%(prog)s ' + jsonpatch.__version__)\n",
        "+\n",
        "+\n",
        "+def main():\n",
        "+    try:\n",
        "+        diff_files()\n",
        "+    except KeyboardInterrupt:\n",
        "+        sys.exit(1)\n",
        "+\n",
        "+\n",
        "+def diff_files():\n",
        "+    \"\"\" Diffs two JSON files and prints a patch \"\"\"\n",
        "+    args = parser.parse_args()\n",
        "+    doc1 = json.load(args.FILE1)\n",
        "+    doc2 = json.load(args.FILE2)\n",
        "+    patch = jsonpatch.make_patch(doc1, doc2)\n",
        "+    if patch.patch:\n",
        "+        print(json.dumps(patch.patch, indent=args.indent, ensure_ascii=not(args.preserve_unicode)))\n",
        "+        sys.exit(1)\n",
        "+\n",
        "+if __name__ == \"__main__\":\n",
        "+    main()\n"
      ]
    },
    {
      "path": "agent/venv/bin/jsonpatch",
      "status": "added",
      "additions": 107,
      "deletions": 0,
      "patch": "@@ -0,0 +1,107 @@\n+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n+# -*- coding: utf-8 -*-\n+\n+import sys\n+import os.path\n+import json\n+import jsonpatch\n+import tempfile\n+import argparse\n+\n+\n+parser = argparse.ArgumentParser(\n+    description='Apply a JSON patch on a JSON file')\n+parser.add_argument('ORIGINAL', type=argparse.FileType('r'),\n+                    help='Original file')\n+parser.add_argument('PATCH', type=argparse.FileType('r'),\n+                    nargs='?', default=sys.stdin,\n+                    help='Patch file (read from stdin if omitted)')\n+parser.add_argument('--indent', type=int, default=None,\n+                    help='Indent output by n spaces')\n+parser.add_argument('-b', '--backup', action='store_true',\n+                    help='Back up ORIGINAL if modifying in-place')\n+parser.add_argument('-i', '--in-place', action='store_true',\n+                    help='Modify ORIGINAL in-place instead of to stdout')\n+parser.add_argument('-v', '--version', action='version',\n+                    version='%(prog)s ' + jsonpatch.__version__)\n+parser.add_argument('-u', '--preserve-unicode', action='store_true',\n+                    help='Output Unicode character as-is without using Code Point')\n+\n+def main():\n+    try:\n+        patch_files()\n+    except KeyboardInterrupt:\n+        sys.exit(1)\n+\n+\n+def patch_files():\n+    \"\"\" Diffs two JSON files and prints a patch \"\"\"\n+    args = parser.parse_args()\n+    doc = json.load(args.ORIGINAL)\n+    patch = json.load(args.PATCH)\n+    result = jsonpatch.apply_patch(doc, patch)\n+\n+    if args.in_place:\n+        dirname = os.path.abspath(os.path.dirname(args.ORIGINAL.name))\n+\n+        try:\n+            # Attempt to replace the file atomically.  We do this by\n+            # creating a temporary file in the same directory as the\n+            # original file so we can atomically move the new file over\n+            # the original later.  (This is done in the same directory\n+\t    # because atomic renames do not work across mount points.)\n+\n+            fd, pathname = tempfile.mkstemp(dir=dirname)\n+            fp = os.fdopen(fd, 'w')\n+            atomic = True\n+\n+        except OSError:\n+            # We failed to create the temporary file for an atomic\n+            # replace, so fall back to non-atomic mode by backing up\n+            # the original (if desired) and writing a new file.\n+\n+            if args.backup:\n+                os.rename(args.ORIGINAL.name, args.ORIGINAL.name + '.orig')\n+            fp = open(args.ORIGINAL.name, 'w')\n+            atomic = False\n+\n+    else:\n+        # Since we're not replacing the original file in-place, write\n+        # the modified JSON to stdout instead.\n+\n+        fp = sys.stdout\n+\n+    # By this point we have some sort of file object we can write the \n+    # modified JSON to.\n+    \n+    json.dump(result, fp, indent=args.indent, ensure_ascii=not(args.preserve_unicode))\n+    fp.write('\\n')\n+\n+    if args.in_place:\n+        # Close the new file.  If we aren't replacing atomically, this\n+        # is our last step, since everything else is already in place.\n+\n+        fp.close()\n+\n+        if atomic:\n+            try:\n+                # Complete the atomic replace by linking the original\n+                # to a backup (if desired), fixing up the permissions\n+                # on the temporary file, and moving it into place.\n+\n+                if args.backup:\n+                    os.link(args.ORIGINAL.name, args.ORIGINAL.name + '.orig')\n+                os.chmod(pathname, os.stat(args.ORIGINAL.name).st_mode)\n+                os.rename(pathname, args.ORIGINAL.name)\n+\n+            except OSError:\n+                # In the event we could not actually do the atomic\n+                # replace, unlink the original to move it out of the\n+                # way and finally move the temporary file into place.\n+                \n+                os.unlink(args.ORIGINAL.name)\n+                os.rename(pathname, args.ORIGINAL.name)\n+\n+\n+if __name__ == \"__main__\":\n+    main()",
      "patch_lines": [
        "@@ -0,0 +1,107 @@\n",
        "+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n",
        "+# -*- coding: utf-8 -*-\n",
        "+\n",
        "+import sys\n",
        "+import os.path\n",
        "+import json\n",
        "+import jsonpatch\n",
        "+import tempfile\n",
        "+import argparse\n",
        "+\n",
        "+\n",
        "+parser = argparse.ArgumentParser(\n",
        "+    description='Apply a JSON patch on a JSON file')\n",
        "+parser.add_argument('ORIGINAL', type=argparse.FileType('r'),\n",
        "+                    help='Original file')\n",
        "+parser.add_argument('PATCH', type=argparse.FileType('r'),\n",
        "+                    nargs='?', default=sys.stdin,\n",
        "+                    help='Patch file (read from stdin if omitted)')\n",
        "+parser.add_argument('--indent', type=int, default=None,\n",
        "+                    help='Indent output by n spaces')\n",
        "+parser.add_argument('-b', '--backup', action='store_true',\n",
        "+                    help='Back up ORIGINAL if modifying in-place')\n",
        "+parser.add_argument('-i', '--in-place', action='store_true',\n",
        "+                    help='Modify ORIGINAL in-place instead of to stdout')\n",
        "+parser.add_argument('-v', '--version', action='version',\n",
        "+                    version='%(prog)s ' + jsonpatch.__version__)\n",
        "+parser.add_argument('-u', '--preserve-unicode', action='store_true',\n",
        "+                    help='Output Unicode character as-is without using Code Point')\n",
        "+\n",
        "+def main():\n",
        "+    try:\n",
        "+        patch_files()\n",
        "+    except KeyboardInterrupt:\n",
        "+        sys.exit(1)\n",
        "+\n",
        "+\n",
        "+def patch_files():\n",
        "+    \"\"\" Diffs two JSON files and prints a patch \"\"\"\n",
        "+    args = parser.parse_args()\n",
        "+    doc = json.load(args.ORIGINAL)\n",
        "+    patch = json.load(args.PATCH)\n",
        "+    result = jsonpatch.apply_patch(doc, patch)\n",
        "+\n",
        "+    if args.in_place:\n",
        "+        dirname = os.path.abspath(os.path.dirname(args.ORIGINAL.name))\n",
        "+\n",
        "+        try:\n",
        "+            # Attempt to replace the file atomically.  We do this by\n",
        "+            # creating a temporary file in the same directory as the\n",
        "+            # original file so we can atomically move the new file over\n",
        "+            # the original later.  (This is done in the same directory\n",
        "+\t    # because atomic renames do not work across mount points.)\n",
        "+\n",
        "+            fd, pathname = tempfile.mkstemp(dir=dirname)\n",
        "+            fp = os.fdopen(fd, 'w')\n",
        "+            atomic = True\n",
        "+\n",
        "+        except OSError:\n",
        "+            # We failed to create the temporary file for an atomic\n",
        "+            # replace, so fall back to non-atomic mode by backing up\n",
        "+            # the original (if desired) and writing a new file.\n",
        "+\n",
        "+            if args.backup:\n",
        "+                os.rename(args.ORIGINAL.name, args.ORIGINAL.name + '.orig')\n",
        "+            fp = open(args.ORIGINAL.name, 'w')\n",
        "+            atomic = False\n",
        "+\n",
        "+    else:\n",
        "+        # Since we're not replacing the original file in-place, write\n",
        "+        # the modified JSON to stdout instead.\n",
        "+\n",
        "+        fp = sys.stdout\n",
        "+\n",
        "+    # By this point we have some sort of file object we can write the \n",
        "+    # modified JSON to.\n",
        "+    \n",
        "+    json.dump(result, fp, indent=args.indent, ensure_ascii=not(args.preserve_unicode))\n",
        "+    fp.write('\\n')\n",
        "+\n",
        "+    if args.in_place:\n",
        "+        # Close the new file.  If we aren't replacing atomically, this\n",
        "+        # is our last step, since everything else is already in place.\n",
        "+\n",
        "+        fp.close()\n",
        "+\n",
        "+        if atomic:\n",
        "+            try:\n",
        "+                # Complete the atomic replace by linking the original\n",
        "+                # to a backup (if desired), fixing up the permissions\n",
        "+                # on the temporary file, and moving it into place.\n",
        "+\n",
        "+                if args.backup:\n",
        "+                    os.link(args.ORIGINAL.name, args.ORIGINAL.name + '.orig')\n",
        "+                os.chmod(pathname, os.stat(args.ORIGINAL.name).st_mode)\n",
        "+                os.rename(pathname, args.ORIGINAL.name)\n",
        "+\n",
        "+            except OSError:\n",
        "+                # In the event we could not actually do the atomic\n",
        "+                # replace, unlink the original to move it out of the\n",
        "+                # way and finally move the temporary file into place.\n",
        "+                \n",
        "+                os.unlink(args.ORIGINAL.name)\n",
        "+                os.rename(pathname, args.ORIGINAL.name)\n",
        "+\n",
        "+\n",
        "+if __name__ == \"__main__\":\n",
        "+    main()\n"
      ]
    },
    {
      "path": "agent/venv/bin/jsonpointer",
      "status": "added",
      "additions": 67,
      "deletions": 0,
      "patch": "@@ -0,0 +1,67 @@\n+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n+# -*- coding: utf-8 -*-\n+\n+\n+import argparse\n+import json\n+import sys\n+\n+import jsonpointer\n+\n+parser = argparse.ArgumentParser(\n+    description='Resolve a JSON pointer on JSON files')\n+\n+# Accept pointer as argument or as file\n+ptr_group = parser.add_mutually_exclusive_group(required=True)\n+\n+ptr_group.add_argument('-f', '--pointer-file', type=argparse.FileType('r'),\n+                       nargs='?',\n+                       help='File containing a JSON pointer expression')\n+\n+ptr_group.add_argument('POINTER', type=str, nargs='?',\n+                       help='A JSON pointer expression')\n+\n+parser.add_argument('FILE', type=argparse.FileType('r'), nargs='+',\n+                    help='Files for which the pointer should be resolved')\n+parser.add_argument('--indent', type=int, default=None,\n+                    help='Indent output by n spaces')\n+parser.add_argument('-v', '--version', action='version',\n+                    version='%(prog)s ' + jsonpointer.__version__)\n+\n+\n+def main():\n+    try:\n+        resolve_files()\n+    except KeyboardInterrupt:\n+        sys.exit(1)\n+\n+\n+def parse_pointer(args):\n+    if args.POINTER:\n+        ptr = args.POINTER\n+    elif args.pointer_file:\n+        ptr = args.pointer_file.read().strip()\n+    else:\n+        parser.print_usage()\n+        sys.exit(1)\n+\n+    return ptr\n+\n+\n+def resolve_files():\n+    \"\"\" Resolve a JSON pointer on JSON files \"\"\"\n+    args = parser.parse_args()\n+\n+    ptr = parse_pointer(args)\n+\n+    for f in args.FILE:\n+        doc = json.load(f)\n+        try:\n+            result = jsonpointer.resolve_pointer(doc, ptr)\n+            print(json.dumps(result, indent=args.indent))\n+        except jsonpointer.JsonPointerException as e:\n+            print('Could not resolve pointer: %s' % str(e), file=sys.stderr)\n+\n+\n+if __name__ == \"__main__\":\n+    main()",
      "patch_lines": [
        "@@ -0,0 +1,67 @@\n",
        "+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n",
        "+# -*- coding: utf-8 -*-\n",
        "+\n",
        "+\n",
        "+import argparse\n",
        "+import json\n",
        "+import sys\n",
        "+\n",
        "+import jsonpointer\n",
        "+\n",
        "+parser = argparse.ArgumentParser(\n",
        "+    description='Resolve a JSON pointer on JSON files')\n",
        "+\n",
        "+# Accept pointer as argument or as file\n",
        "+ptr_group = parser.add_mutually_exclusive_group(required=True)\n",
        "+\n",
        "+ptr_group.add_argument('-f', '--pointer-file', type=argparse.FileType('r'),\n",
        "+                       nargs='?',\n",
        "+                       help='File containing a JSON pointer expression')\n",
        "+\n",
        "+ptr_group.add_argument('POINTER', type=str, nargs='?',\n",
        "+                       help='A JSON pointer expression')\n",
        "+\n",
        "+parser.add_argument('FILE', type=argparse.FileType('r'), nargs='+',\n",
        "+                    help='Files for which the pointer should be resolved')\n",
        "+parser.add_argument('--indent', type=int, default=None,\n",
        "+                    help='Indent output by n spaces')\n",
        "+parser.add_argument('-v', '--version', action='version',\n",
        "+                    version='%(prog)s ' + jsonpointer.__version__)\n",
        "+\n",
        "+\n",
        "+def main():\n",
        "+    try:\n",
        "+        resolve_files()\n",
        "+    except KeyboardInterrupt:\n",
        "+        sys.exit(1)\n",
        "+\n",
        "+\n",
        "+def parse_pointer(args):\n",
        "+    if args.POINTER:\n",
        "+        ptr = args.POINTER\n",
        "+    elif args.pointer_file:\n",
        "+        ptr = args.pointer_file.read().strip()\n",
        "+    else:\n",
        "+        parser.print_usage()\n",
        "+        sys.exit(1)\n",
        "+\n",
        "+    return ptr\n",
        "+\n",
        "+\n",
        "+def resolve_files():\n",
        "+    \"\"\" Resolve a JSON pointer on JSON files \"\"\"\n",
        "+    args = parser.parse_args()\n",
        "+\n",
        "+    ptr = parse_pointer(args)\n",
        "+\n",
        "+    for f in args.FILE:\n",
        "+        doc = json.load(f)\n",
        "+        try:\n",
        "+            result = jsonpointer.resolve_pointer(doc, ptr)\n",
        "+            print(json.dumps(result, indent=args.indent))\n",
        "+        except jsonpointer.JsonPointerException as e:\n",
        "+            print('Could not resolve pointer: %s' % str(e), file=sys.stderr)\n",
        "+\n",
        "+\n",
        "+if __name__ == \"__main__\":\n",
        "+    main()\n"
      ]
    },
    {
      "path": "agent/venv/bin/jsonschema",
      "status": "added",
      "additions": 8,
      "deletions": 0,
      "patch": "@@ -0,0 +1,8 @@\n+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n+# -*- coding: utf-8 -*-\n+import re\n+import sys\n+from jsonschema.cli import main\n+if __name__ == '__main__':\n+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n+    sys.exit(main())",
      "patch_lines": [
        "@@ -0,0 +1,8 @@\n",
        "+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n",
        "+# -*- coding: utf-8 -*-\n",
        "+import re\n",
        "+import sys\n",
        "+from jsonschema.cli import main\n",
        "+if __name__ == '__main__':\n",
        "+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n",
        "+    sys.exit(main())\n"
      ]
    },
    {
      "path": "agent/venv/bin/langchain-server",
      "status": "added",
      "additions": 8,
      "deletions": 0,
      "patch": "@@ -0,0 +1,8 @@\n+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n+# -*- coding: utf-8 -*-\n+import re\n+import sys\n+from langchain.server import main\n+if __name__ == '__main__':\n+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n+    sys.exit(main())",
      "patch_lines": [
        "@@ -0,0 +1,8 @@\n",
        "+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n",
        "+# -*- coding: utf-8 -*-\n",
        "+import re\n",
        "+import sys\n",
        "+from langchain.server import main\n",
        "+if __name__ == '__main__':\n",
        "+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n",
        "+    sys.exit(main())\n"
      ]
    },
    {
      "path": "agent/venv/bin/langsmith",
      "status": "added",
      "additions": 8,
      "deletions": 0,
      "patch": "@@ -0,0 +1,8 @@\n+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n+# -*- coding: utf-8 -*-\n+import re\n+import sys\n+from langsmith.cli.main import main\n+if __name__ == '__main__':\n+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n+    sys.exit(main())",
      "patch_lines": [
        "@@ -0,0 +1,8 @@\n",
        "+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n",
        "+# -*- coding: utf-8 -*-\n",
        "+import re\n",
        "+import sys\n",
        "+from langsmith.cli.main import main\n",
        "+if __name__ == '__main__':\n",
        "+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n",
        "+    sys.exit(main())\n"
      ]
    },
    {
      "path": "agent/venv/bin/markdown-it",
      "status": "added",
      "additions": 8,
      "deletions": 0,
      "patch": "@@ -0,0 +1,8 @@\n+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n+# -*- coding: utf-8 -*-\n+import re\n+import sys\n+from markdown_it.cli.parse import main\n+if __name__ == '__main__':\n+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n+    sys.exit(main())",
      "patch_lines": [
        "@@ -0,0 +1,8 @@\n",
        "+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n",
        "+# -*- coding: utf-8 -*-\n",
        "+import re\n",
        "+import sys\n",
        "+from markdown_it.cli.parse import main\n",
        "+if __name__ == '__main__':\n",
        "+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n",
        "+    sys.exit(main())\n"
      ]
    },
    {
      "path": "agent/venv/bin/normalizer",
      "status": "added",
      "additions": 8,
      "deletions": 0,
      "patch": "@@ -0,0 +1,8 @@\n+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n+# -*- coding: utf-8 -*-\n+import re\n+import sys\n+from charset_normalizer.cli import cli_detect\n+if __name__ == '__main__':\n+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n+    sys.exit(cli_detect())",
      "patch_lines": [
        "@@ -0,0 +1,8 @@\n",
        "+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n",
        "+# -*- coding: utf-8 -*-\n",
        "+import re\n",
        "+import sys\n",
        "+from charset_normalizer.cli import cli_detect\n",
        "+if __name__ == '__main__':\n",
        "+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n",
        "+    sys.exit(cli_detect())\n"
      ]
    },
    {
      "path": "agent/venv/bin/pip",
      "status": "added",
      "additions": 8,
      "deletions": 0,
      "patch": "@@ -0,0 +1,8 @@\n+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n+# -*- coding: utf-8 -*-\n+import re\n+import sys\n+from pip._internal.cli.main import main\n+if __name__ == '__main__':\n+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n+    sys.exit(main())",
      "patch_lines": [
        "@@ -0,0 +1,8 @@\n",
        "+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n",
        "+# -*- coding: utf-8 -*-\n",
        "+import re\n",
        "+import sys\n",
        "+from pip._internal.cli.main import main\n",
        "+if __name__ == '__main__':\n",
        "+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n",
        "+    sys.exit(main())\n"
      ]
    },
    {
      "path": "agent/venv/bin/pip3",
      "status": "added",
      "additions": 8,
      "deletions": 0,
      "patch": "@@ -0,0 +1,8 @@\n+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n+# -*- coding: utf-8 -*-\n+import re\n+import sys\n+from pip._internal.cli.main import main\n+if __name__ == '__main__':\n+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n+    sys.exit(main())",
      "patch_lines": [
        "@@ -0,0 +1,8 @@\n",
        "+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n",
        "+# -*- coding: utf-8 -*-\n",
        "+import re\n",
        "+import sys\n",
        "+from pip._internal.cli.main import main\n",
        "+if __name__ == '__main__':\n",
        "+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n",
        "+    sys.exit(main())\n"
      ]
    },
    {
      "path": "agent/venv/bin/pip3.12",
      "status": "added",
      "additions": 8,
      "deletions": 0,
      "patch": "@@ -0,0 +1,8 @@\n+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n+# -*- coding: utf-8 -*-\n+import re\n+import sys\n+from pip._internal.cli.main import main\n+if __name__ == '__main__':\n+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n+    sys.exit(main())",
      "patch_lines": [
        "@@ -0,0 +1,8 @@\n",
        "+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n",
        "+# -*- coding: utf-8 -*-\n",
        "+import re\n",
        "+import sys\n",
        "+from pip._internal.cli.main import main\n",
        "+if __name__ == '__main__':\n",
        "+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n",
        "+    sys.exit(main())\n"
      ]
    },
    {
      "path": "agent/venv/bin/py.test",
      "status": "added",
      "additions": 8,
      "deletions": 0,
      "patch": "@@ -0,0 +1,8 @@\n+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n+# -*- coding: utf-8 -*-\n+import re\n+import sys\n+from pytest import console_main\n+if __name__ == '__main__':\n+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n+    sys.exit(console_main())",
      "patch_lines": [
        "@@ -0,0 +1,8 @@\n",
        "+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n",
        "+# -*- coding: utf-8 -*-\n",
        "+import re\n",
        "+import sys\n",
        "+from pytest import console_main\n",
        "+if __name__ == '__main__':\n",
        "+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n",
        "+    sys.exit(console_main())\n"
      ]
    },
    {
      "path": "agent/venv/bin/pygmentize",
      "status": "added",
      "additions": 8,
      "deletions": 0,
      "patch": "@@ -0,0 +1,8 @@\n+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n+# -*- coding: utf-8 -*-\n+import re\n+import sys\n+from pygments.cmdline import main\n+if __name__ == '__main__':\n+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n+    sys.exit(main())",
      "patch_lines": [
        "@@ -0,0 +1,8 @@\n",
        "+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n",
        "+# -*- coding: utf-8 -*-\n",
        "+import re\n",
        "+import sys\n",
        "+from pygments.cmdline import main\n",
        "+if __name__ == '__main__':\n",
        "+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n",
        "+    sys.exit(main())\n"
      ]
    },
    {
      "path": "agent/venv/bin/pytest",
      "status": "added",
      "additions": 8,
      "deletions": 0,
      "patch": "@@ -0,0 +1,8 @@\n+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n+# -*- coding: utf-8 -*-\n+import re\n+import sys\n+from pytest import console_main\n+if __name__ == '__main__':\n+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n+    sys.exit(console_main())",
      "patch_lines": [
        "@@ -0,0 +1,8 @@\n",
        "+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n",
        "+# -*- coding: utf-8 -*-\n",
        "+import re\n",
        "+import sys\n",
        "+from pytest import console_main\n",
        "+if __name__ == '__main__':\n",
        "+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n",
        "+    sys.exit(console_main())\n"
      ]
    },
    {
      "path": "agent/venv/bin/python",
      "status": "added",
      "additions": 1,
      "deletions": 0,
      "patch": "@@ -0,0 +1 @@\n+python3\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -0,0 +1 @@\n",
        "+python3\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "agent/venv/bin/python3",
      "status": "added",
      "additions": 1,
      "deletions": 0,
      "patch": "@@ -0,0 +1 @@\n+/usr/bin/python3\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -0,0 +1 @@\n",
        "+/usr/bin/python3\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "agent/venv/bin/python3.12",
      "status": "added",
      "additions": 1,
      "deletions": 0,
      "patch": "@@ -0,0 +1 @@\n+python3\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -0,0 +1 @@\n",
        "+python3\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "agent/venv/bin/typer",
      "status": "added",
      "additions": 8,
      "deletions": 0,
      "patch": "@@ -0,0 +1,8 @@\n+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n+# -*- coding: utf-8 -*-\n+import re\n+import sys\n+from typer.cli import main\n+if __name__ == '__main__':\n+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n+    sys.exit(main())",
      "patch_lines": [
        "@@ -0,0 +1,8 @@\n",
        "+#!/home/joseph-mazzini/sys-scan-graph/agent/venv/bin/python3\n",
        "+# -*- coding: utf-8 -*-\n",
        "+import re\n",
        "+import sys\n",
        "+from typer.cli import main\n",
        "+if __name__ == '__main__':\n",
        "+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n",
        "+    sys.exit(main())\n"
      ]
    },
    {
      "path": "agent/venv/include/site/python3.12/greenlet/greenlet.h",
      "status": "added",
      "additions": 164,
      "deletions": 0,
      "patch": "@@ -0,0 +1,164 @@\n+/* -*- indent-tabs-mode: nil; tab-width: 4; -*- */\n+\n+/* Greenlet object interface */\n+\n+#ifndef Py_GREENLETOBJECT_H\n+#define Py_GREENLETOBJECT_H\n+\n+\n+#include <Python.h>\n+\n+#ifdef __cplusplus\n+extern \"C\" {\n+#endif\n+\n+/* This is deprecated and undocumented. It does not change. */\n+#define GREENLET_VERSION \"1.0.0\"\n+\n+#ifndef GREENLET_MODULE\n+#define implementation_ptr_t void*\n+#endif\n+\n+typedef struct _greenlet {\n+    PyObject_HEAD\n+    PyObject* weakreflist;\n+    PyObject* dict;\n+    implementation_ptr_t pimpl;\n+} PyGreenlet;\n+\n+#define PyGreenlet_Check(op) (op && PyObject_TypeCheck(op, &PyGreenlet_Type))\n+\n+\n+/* C API functions */\n+\n+/* Total number of symbols that are exported */\n+#define PyGreenlet_API_pointers 12\n+\n+#define PyGreenlet_Type_NUM 0\n+#define PyExc_GreenletError_NUM 1\n+#define PyExc_GreenletExit_NUM 2\n+\n+#define PyGreenlet_New_NUM 3\n+#define PyGreenlet_GetCurrent_NUM 4\n+#define PyGreenlet_Throw_NUM 5\n+#define PyGreenlet_Switch_NUM 6\n+#define PyGreenlet_SetParent_NUM 7\n+\n+#define PyGreenlet_MAIN_NUM 8\n+#define PyGreenlet_STARTED_NUM 9\n+#define PyGreenlet_ACTIVE_NUM 10\n+#define PyGreenlet_GET_PARENT_NUM 11\n+\n+#ifndef GREENLET_MODULE\n+/* This section is used by modules that uses the greenlet C API */\n+static void** _PyGreenlet_API = NULL;\n+\n+#    define PyGreenlet_Type \\\n+        (*(PyTypeObject*)_PyGreenlet_API[PyGreenlet_Type_NUM])\n+\n+#    define PyExc_GreenletError \\\n+        ((PyObject*)_PyGreenlet_API[PyExc_GreenletError_NUM])\n+\n+#    define PyExc_GreenletExit \\\n+        ((PyObject*)_PyGreenlet_API[PyExc_GreenletExit_NUM])\n+\n+/*\n+ * PyGreenlet_New(PyObject *args)\n+ *\n+ * greenlet.greenlet(run, parent=None)\n+ */\n+#    define PyGreenlet_New                                        \\\n+        (*(PyGreenlet * (*)(PyObject * run, PyGreenlet * parent)) \\\n+             _PyGreenlet_API[PyGreenlet_New_NUM])\n+\n+/*\n+ * PyGreenlet_GetCurrent(void)\n+ *\n+ * greenlet.getcurrent()\n+ */\n+#    define PyGreenlet_GetCurrent \\\n+        (*(PyGreenlet * (*)(void)) _PyGreenlet_API[PyGreenlet_GetCurrent_NUM])\n+\n+/*\n+ * PyGreenlet_Throw(\n+ *         PyGreenlet *greenlet,\n+ *         PyObject *typ,\n+ *         PyObject *val,\n+ *         PyObject *tb)\n+ *\n+ * g.throw(...)\n+ */\n+#    define PyGreenlet_Throw                 \\\n+        (*(PyObject * (*)(PyGreenlet * self, \\\n+                          PyObject * typ,    \\\n+                          PyObject * val,    \\\n+                          PyObject * tb))    \\\n+             _PyGreenlet_API[PyGreenlet_Throw_NUM])\n+\n+/*\n+ * PyGreenlet_Switch(PyGreenlet *greenlet, PyObject *args)\n+ *\n+ * g.switch(*args, **kwargs)\n+ */\n+#    define PyGreenlet_Switch                                              \\\n+        (*(PyObject *                                                      \\\n+           (*)(PyGreenlet * greenlet, PyObject * args, PyObject * kwargs)) \\\n+             _PyGreenlet_API[PyGreenlet_Switch_NUM])\n+\n+/*\n+ * PyGreenlet_SetParent(PyObject *greenlet, PyObject *new_parent)\n+ *\n+ * g.parent = new_parent\n+ */\n+#    define PyGreenlet_SetParent                                 \\\n+        (*(int (*)(PyGreenlet * greenlet, PyGreenlet * nparent)) \\\n+             _PyGreenlet_API[PyGreenlet_SetParent_NUM])\n+\n+/*\n+ * PyGreenlet_GetParent(PyObject* greenlet)\n+ *\n+ * return greenlet.parent;\n+ *\n+ * This could return NULL even if there is no exception active.\n+ * If it does not return NULL, you are responsible for decrementing the\n+ * reference count.\n+ */\n+#     define PyGreenlet_GetParent                                    \\\n+    (*(PyGreenlet* (*)(PyGreenlet*))                                 \\\n+     _PyGreenlet_API[PyGreenlet_GET_PARENT_NUM])\n+\n+/*\n+ * deprecated, undocumented alias.\n+ */\n+#     define PyGreenlet_GET_PARENT PyGreenlet_GetParent\n+\n+#     define PyGreenlet_MAIN                                         \\\n+    (*(int (*)(PyGreenlet*))                                         \\\n+     _PyGreenlet_API[PyGreenlet_MAIN_NUM])\n+\n+#     define PyGreenlet_STARTED                                      \\\n+    (*(int (*)(PyGreenlet*))                                         \\\n+     _PyGreenlet_API[PyGreenlet_STARTED_NUM])\n+\n+#     define PyGreenlet_ACTIVE                                       \\\n+    (*(int (*)(PyGreenlet*))                                         \\\n+     _PyGreenlet_API[PyGreenlet_ACTIVE_NUM])\n+\n+\n+\n+\n+/* Macro that imports greenlet and initializes C API */\n+/* NOTE: This has actually moved to ``greenlet._greenlet._C_API``, but we\n+   keep the older definition to be sure older code that might have a copy of\n+   the header still works. */\n+#    define PyGreenlet_Import()                                               \\\n+        {                                                                     \\\n+            _PyGreenlet_API = (void**)PyCapsule_Import(\"greenlet._C_API\", 0); \\\n+        }\n+\n+#endif /* GREENLET_MODULE */\n+\n+#ifdef __cplusplus\n+}\n+#endif\n+#endif /* !Py_GREENLETOBJECT_H */",
      "patch_lines": [
        "@@ -0,0 +1,164 @@\n",
        "+/* -*- indent-tabs-mode: nil; tab-width: 4; -*- */\n",
        "+\n",
        "+/* Greenlet object interface */\n",
        "+\n",
        "+#ifndef Py_GREENLETOBJECT_H\n",
        "+#define Py_GREENLETOBJECT_H\n",
        "+\n",
        "+\n",
        "+#include <Python.h>\n",
        "+\n",
        "+#ifdef __cplusplus\n",
        "+extern \"C\" {\n",
        "+#endif\n",
        "+\n",
        "+/* This is deprecated and undocumented. It does not change. */\n",
        "+#define GREENLET_VERSION \"1.0.0\"\n",
        "+\n",
        "+#ifndef GREENLET_MODULE\n",
        "+#define implementation_ptr_t void*\n",
        "+#endif\n",
        "+\n",
        "+typedef struct _greenlet {\n",
        "+    PyObject_HEAD\n",
        "+    PyObject* weakreflist;\n",
        "+    PyObject* dict;\n",
        "+    implementation_ptr_t pimpl;\n",
        "+} PyGreenlet;\n",
        "+\n",
        "+#define PyGreenlet_Check(op) (op && PyObject_TypeCheck(op, &PyGreenlet_Type))\n",
        "+\n",
        "+\n",
        "+/* C API functions */\n",
        "+\n",
        "+/* Total number of symbols that are exported */\n",
        "+#define PyGreenlet_API_pointers 12\n",
        "+\n",
        "+#define PyGreenlet_Type_NUM 0\n",
        "+#define PyExc_GreenletError_NUM 1\n",
        "+#define PyExc_GreenletExit_NUM 2\n",
        "+\n",
        "+#define PyGreenlet_New_NUM 3\n",
        "+#define PyGreenlet_GetCurrent_NUM 4\n",
        "+#define PyGreenlet_Throw_NUM 5\n",
        "+#define PyGreenlet_Switch_NUM 6\n",
        "+#define PyGreenlet_SetParent_NUM 7\n",
        "+\n",
        "+#define PyGreenlet_MAIN_NUM 8\n",
        "+#define PyGreenlet_STARTED_NUM 9\n",
        "+#define PyGreenlet_ACTIVE_NUM 10\n",
        "+#define PyGreenlet_GET_PARENT_NUM 11\n",
        "+\n",
        "+#ifndef GREENLET_MODULE\n",
        "+/* This section is used by modules that uses the greenlet C API */\n",
        "+static void** _PyGreenlet_API = NULL;\n",
        "+\n",
        "+#    define PyGreenlet_Type \\\n",
        "+        (*(PyTypeObject*)_PyGreenlet_API[PyGreenlet_Type_NUM])\n",
        "+\n",
        "+#    define PyExc_GreenletError \\\n",
        "+        ((PyObject*)_PyGreenlet_API[PyExc_GreenletError_NUM])\n",
        "+\n",
        "+#    define PyExc_GreenletExit \\\n",
        "+        ((PyObject*)_PyGreenlet_API[PyExc_GreenletExit_NUM])\n",
        "+\n",
        "+/*\n",
        "+ * PyGreenlet_New(PyObject *args)\n",
        "+ *\n",
        "+ * greenlet.greenlet(run, parent=None)\n",
        "+ */\n",
        "+#    define PyGreenlet_New                                        \\\n",
        "+        (*(PyGreenlet * (*)(PyObject * run, PyGreenlet * parent)) \\\n",
        "+             _PyGreenlet_API[PyGreenlet_New_NUM])\n",
        "+\n",
        "+/*\n",
        "+ * PyGreenlet_GetCurrent(void)\n",
        "+ *\n",
        "+ * greenlet.getcurrent()\n",
        "+ */\n",
        "+#    define PyGreenlet_GetCurrent \\\n",
        "+        (*(PyGreenlet * (*)(void)) _PyGreenlet_API[PyGreenlet_GetCurrent_NUM])\n",
        "+\n",
        "+/*\n",
        "+ * PyGreenlet_Throw(\n",
        "+ *         PyGreenlet *greenlet,\n",
        "+ *         PyObject *typ,\n",
        "+ *         PyObject *val,\n",
        "+ *         PyObject *tb)\n",
        "+ *\n",
        "+ * g.throw(...)\n",
        "+ */\n",
        "+#    define PyGreenlet_Throw                 \\\n",
        "+        (*(PyObject * (*)(PyGreenlet * self, \\\n",
        "+                          PyObject * typ,    \\\n",
        "+                          PyObject * val,    \\\n",
        "+                          PyObject * tb))    \\\n",
        "+             _PyGreenlet_API[PyGreenlet_Throw_NUM])\n",
        "+\n",
        "+/*\n",
        "+ * PyGreenlet_Switch(PyGreenlet *greenlet, PyObject *args)\n",
        "+ *\n",
        "+ * g.switch(*args, **kwargs)\n",
        "+ */\n",
        "+#    define PyGreenlet_Switch                                              \\\n",
        "+        (*(PyObject *                                                      \\\n",
        "+           (*)(PyGreenlet * greenlet, PyObject * args, PyObject * kwargs)) \\\n",
        "+             _PyGreenlet_API[PyGreenlet_Switch_NUM])\n",
        "+\n",
        "+/*\n",
        "+ * PyGreenlet_SetParent(PyObject *greenlet, PyObject *new_parent)\n",
        "+ *\n",
        "+ * g.parent = new_parent\n",
        "+ */\n",
        "+#    define PyGreenlet_SetParent                                 \\\n",
        "+        (*(int (*)(PyGreenlet * greenlet, PyGreenlet * nparent)) \\\n",
        "+             _PyGreenlet_API[PyGreenlet_SetParent_NUM])\n",
        "+\n",
        "+/*\n",
        "+ * PyGreenlet_GetParent(PyObject* greenlet)\n",
        "+ *\n",
        "+ * return greenlet.parent;\n",
        "+ *\n",
        "+ * This could return NULL even if there is no exception active.\n",
        "+ * If it does not return NULL, you are responsible for decrementing the\n",
        "+ * reference count.\n",
        "+ */\n",
        "+#     define PyGreenlet_GetParent                                    \\\n",
        "+    (*(PyGreenlet* (*)(PyGreenlet*))                                 \\\n",
        "+     _PyGreenlet_API[PyGreenlet_GET_PARENT_NUM])\n",
        "+\n",
        "+/*\n",
        "+ * deprecated, undocumented alias.\n",
        "+ */\n",
        "+#     define PyGreenlet_GET_PARENT PyGreenlet_GetParent\n",
        "+\n",
        "+#     define PyGreenlet_MAIN                                         \\\n",
        "+    (*(int (*)(PyGreenlet*))                                         \\\n",
        "+     _PyGreenlet_API[PyGreenlet_MAIN_NUM])\n",
        "+\n",
        "+#     define PyGreenlet_STARTED                                      \\\n",
        "+    (*(int (*)(PyGreenlet*))                                         \\\n",
        "+     _PyGreenlet_API[PyGreenlet_STARTED_NUM])\n",
        "+\n",
        "+#     define PyGreenlet_ACTIVE                                       \\\n",
        "+    (*(int (*)(PyGreenlet*))                                         \\\n",
        "+     _PyGreenlet_API[PyGreenlet_ACTIVE_NUM])\n",
        "+\n",
        "+\n",
        "+\n",
        "+\n",
        "+/* Macro that imports greenlet and initializes C API */\n",
        "+/* NOTE: This has actually moved to ``greenlet._greenlet._C_API``, but we\n",
        "+   keep the older definition to be sure older code that might have a copy of\n",
        "+   the header still works. */\n",
        "+#    define PyGreenlet_Import()                                               \\\n",
        "+        {                                                                     \\\n",
        "+            _PyGreenlet_API = (void**)PyCapsule_Import(\"greenlet._C_API\", 0); \\\n",
        "+        }\n",
        "+\n",
        "+#endif /* GREENLET_MODULE */\n",
        "+\n",
        "+#ifdef __cplusplus\n",
        "+}\n",
        "+#endif\n",
        "+#endif /* !Py_GREENLETOBJECT_H */\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/PyNaCl-1.5.0.dist-info/INSTALLER",
      "status": "added",
      "additions": 1,
      "deletions": 0,
      "patch": "@@ -0,0 +1 @@\n+pip",
      "patch_lines": [
        "@@ -0,0 +1 @@\n",
        "+pip\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/PyNaCl-1.5.0.dist-info/LICENSE",
      "status": "added",
      "additions": 174,
      "deletions": 0,
      "patch": "@@ -0,0 +1,174 @@\n+                              Apache License\n+                        Version 2.0, January 2004\n+                     http://www.apache.org/licenses/\n+\n+TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n+\n+1. Definitions.\n+\n+   \"License\" shall mean the terms and conditions for use, reproduction,\n+   and distribution as defined by Sections 1 through 9 of this document.\n+\n+   \"Licensor\" shall mean the copyright owner or entity authorized by\n+   the copyright owner that is granting the License.\n+\n+   \"Legal Entity\" shall mean the union of the acting entity and all\n+   other entities that control, are controlled by, or are under common\n+   control with that entity. For the purposes of this definition,\n+   \"control\" means (i) the power, direct or indirect, to cause the\n+   direction or management of such entity, whether by contract or\n+   otherwise, or (ii) ownership of fifty percent (50%) or more of the\n+   outstanding shares, or (iii) beneficial ownership of such entity.\n+\n+   \"You\" (or \"Your\") shall mean an individual or Legal Entity\n+   exercising permissions granted by this License.\n+\n+   \"Source\" form shall mean the preferred form for making modifications,\n+   including but not limited to software source code, documentation\n+   source, and configuration files.\n+\n+   \"Object\" form shall mean any form resulting from mechanical\n+   transformation or translation of a Source form, including but\n+   not limited to compiled object code, generated documentation,\n+   and conversions to other media types.\n+\n+   \"Work\" shall mean the work of authorship, whether in Source or\n+   Object form, made available under the License, as indicated by a\n+   copyright notice that is included in or attached to the work\n+   (an example is provided in the Appendix below).\n+\n+   \"Derivative Works\" shall mean any work, whether in Source or Object\n+   form, that is based on (or derived from) the Work and for which the\n+   editorial revisions, annotations, elaborations, or other modifications\n+   represent, as a whole, an original work of authorship. For the purposes\n+   of this License, Derivative Works shall not include works that remain\n+   separable from, or merely link (or bind by name) to the interfaces of,\n+   the Work and Derivative Works thereof.\n+\n+   \"Contribution\" shall mean any work of authorship, including\n+   the original version of the Work and any modifications or additions\n+   to that Work or Derivative Works thereof, that is intentionally\n+   submitted to Licensor for inclusion in the Work by the copyright owner\n+   or by an individual or Legal Entity authorized to submit on behalf of\n+   the copyright owner. For the purposes of this definition, \"submitted\"\n+   means any form of electronic, verbal, or written communication sent\n+   to the Licensor or its representatives, including but not limited to\n+   communication on electronic mailing lists, source code control systems,\n+   and issue tracking systems that are managed by, or on behalf of, the\n+   Licensor for the purpose of discussing and improving the Work, but\n+   excluding communication that is conspicuously marked or otherwise\n+   designated in writing by the copyright owner as \"Not a Contribution.\"\n+\n+   \"Contributor\" shall mean Licensor and any individual or Legal Entity\n+   on behalf of whom a Contribution has been received by Licensor and\n+   subsequently incorporated within the Work.\n+\n+2. Grant of Copyright License. Subject to the terms and conditions of\n+   this License, each Contributor hereby grants to You a perpetual,\n+   worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n+   copyright license to reproduce, prepare Derivative Works of,\n+   publicly display, publicly perform, sublicense, and distribute the\n+   Work and such Derivative Works in Source or Object form.\n+\n+3. Grant of Patent License. Subject to the terms and conditions of\n+   this License, each Contributor hereby grants to You a perpetual,\n+   worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n+   (except as stated in this section) patent license to make, have made,\n+   use, offer to sell, sell, import, and otherwise transfer the Work,\n+   where such license applies only to those patent claims licensable\n+   by such Contributor that are necessarily infringed by their\n+   Contribution(s) alone or by combination of their Contribution(s)\n+   with the Work to which such Contribution(s) was submitted. If You\n+   institute patent litigation against any entity (including a\n+   cross-claim or counterclaim in a lawsuit) alleging that the Work\n+   or a Contribution incorporated within the Work constitutes direct\n+   or contributory patent infringement, then any patent licenses\n+   granted to You under this License for that Work shall terminate\n+   as of the date such litigation is filed.\n+\n+4. Redistribution. You may reproduce and distribute copies of the\n+   Work or Derivative Works thereof in any medium, with or without\n+   modifications, and in Source or Object form, provided that You\n+   meet the following conditions:\n+\n+   (a) You must give any other recipients of the Work or\n+       Derivative Works a copy of this License; and\n+\n+   (b) You must cause any modified files to carry prominent notices\n+       stating that You changed the files; and\n+\n+   (c) You must retain, in the Source form of any Derivative Works\n+       that You distribute, all copyright, patent, trademark, and\n+       attribution notices from the Source form of the Work,\n+       excluding those notices that do not pertain to any part of\n+       the Derivative Works; and\n+\n+   (d) If the Work includes a \"NOTICE\" text file as part of its\n+       distribution, then any Derivative Works that You distribute must\n+       include a readable copy of the attribution notices contained\n+       within such NOTICE file, excluding those notices that do not\n+       pertain to any part of the Derivative Works, in at least one\n+       of the following places: within a NOTICE text file distributed\n+       as part of the Derivative Works; within the Source form or\n+       documentation, if provided along with the Derivative Works; or,\n+       within a display generated by the Derivative Works, if and\n+       wherever such third-party notices normally appear. The contents\n+       of the NOTICE file are for informational purposes only and\n+       do not modify the License. You may add Your own attribution\n+       notices within Derivative Works that You distribute, alongside\n+       or as an addendum to the NOTICE text from the Work, provided\n+       that such additional attribution notices cannot be construed\n+       as modifying the License.\n+\n+   You may add Your own copyright statement to Your modifications and\n+   may provide additional or different license terms and conditions\n+   for use, reproduction, or distribution of Your modifications, or\n+   for any such Derivative Works as a whole, provided Your use,\n+   reproduction, and distribution of the Work otherwise complies with\n+   the conditions stated in this License.\n+\n+5. Submission of Contributions. Unless You explicitly state otherwise,\n+   any Contribution intentionally submitted for inclusion in the Work\n+   by You to the Licensor shall be under the terms and conditions of\n+   this License, without any additional terms or conditions.\n+   Notwithstanding the above, nothing herein shall supersede or modify\n+   the terms of any separate license agreement you may have executed\n+   with Licensor regarding such Contributions.\n+\n+6. Trademarks. This License does not grant permission to use the trade\n+   names, trademarks, service marks, or product names of the Licensor,\n+   except as required for reasonable and customary use in describing the\n+   origin of the Work and reproducing the content of the NOTICE file.\n+\n+7. Disclaimer of Warranty. Unless required by applicable law or\n+   agreed to in writing, Licensor provides the Work (and each\n+   Contributor provides its Contributions) on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n+   implied, including, without limitation, any warranties or conditions\n+   of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n+   PARTICULAR PURPOSE. You are solely responsible for determining the\n+   appropriateness of using or redistributing the Work and assume any\n+   risks associated with Your exercise of permissions under this License.\n+\n+8. Limitation of Liability. In no event and under no legal theory,\n+   whether in tort (including negligence), contract, or otherwise,\n+   unless required by applicable law (such as deliberate and grossly\n+   negligent acts) or agreed to in writing, shall any Contributor be\n+   liable to You for damages, including any direct, indirect, special,\n+   incidental, or consequential damages of any character arising as a\n+   result of this License or out of the use or inability to use the\n+   Work (including but not limited to damages for loss of goodwill,\n+   work stoppage, computer failure or malfunction, or any and all\n+   other commercial damages or losses), even if such Contributor\n+   has been advised of the possibility of such damages.\n+\n+9. Accepting Warranty or Additional Liability. While redistributing\n+   the Work or Derivative Works thereof, You may choose to offer,\n+   and charge a fee for, acceptance of support, warranty, indemnity,\n+   or other liability obligations and/or rights consistent with this\n+   License. However, in accepting such obligations, You may act only\n+   on Your own behalf and on Your sole responsibility, not on behalf\n+   of any other Contributor, and only if You agree to indemnify,\n+   defend, and hold each Contributor harmless for any liability\n+   incurred by, or claims asserted against, such Contributor by reason\n+   of your accepting any such warranty or additional liability.",
      "patch_lines": [
        "@@ -0,0 +1,174 @@\n",
        "+                              Apache License\n",
        "+                        Version 2.0, January 2004\n",
        "+                     http://www.apache.org/licenses/\n",
        "+\n",
        "+TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n",
        "+\n",
        "+1. Definitions.\n",
        "+\n",
        "+   \"License\" shall mean the terms and conditions for use, reproduction,\n",
        "+   and distribution as defined by Sections 1 through 9 of this document.\n",
        "+\n",
        "+   \"Licensor\" shall mean the copyright owner or entity authorized by\n",
        "+   the copyright owner that is granting the License.\n",
        "+\n",
        "+   \"Legal Entity\" shall mean the union of the acting entity and all\n",
        "+   other entities that control, are controlled by, or are under common\n",
        "+   control with that entity. For the purposes of this definition,\n",
        "+   \"control\" means (i) the power, direct or indirect, to cause the\n",
        "+   direction or management of such entity, whether by contract or\n",
        "+   otherwise, or (ii) ownership of fifty percent (50%) or more of the\n",
        "+   outstanding shares, or (iii) beneficial ownership of such entity.\n",
        "+\n",
        "+   \"You\" (or \"Your\") shall mean an individual or Legal Entity\n",
        "+   exercising permissions granted by this License.\n",
        "+\n",
        "+   \"Source\" form shall mean the preferred form for making modifications,\n",
        "+   including but not limited to software source code, documentation\n",
        "+   source, and configuration files.\n",
        "+\n",
        "+   \"Object\" form shall mean any form resulting from mechanical\n",
        "+   transformation or translation of a Source form, including but\n",
        "+   not limited to compiled object code, generated documentation,\n",
        "+   and conversions to other media types.\n",
        "+\n",
        "+   \"Work\" shall mean the work of authorship, whether in Source or\n",
        "+   Object form, made available under the License, as indicated by a\n",
        "+   copyright notice that is included in or attached to the work\n",
        "+   (an example is provided in the Appendix below).\n",
        "+\n",
        "+   \"Derivative Works\" shall mean any work, whether in Source or Object\n",
        "+   form, that is based on (or derived from) the Work and for which the\n",
        "+   editorial revisions, annotations, elaborations, or other modifications\n",
        "+   represent, as a whole, an original work of authorship. For the purposes\n",
        "+   of this License, Derivative Works shall not include works that remain\n",
        "+   separable from, or merely link (or bind by name) to the interfaces of,\n",
        "+   the Work and Derivative Works thereof.\n",
        "+\n",
        "+   \"Contribution\" shall mean any work of authorship, including\n",
        "+   the original version of the Work and any modifications or additions\n",
        "+   to that Work or Derivative Works thereof, that is intentionally\n",
        "+   submitted to Licensor for inclusion in the Work by the copyright owner\n",
        "+   or by an individual or Legal Entity authorized to submit on behalf of\n",
        "+   the copyright owner. For the purposes of this definition, \"submitted\"\n",
        "+   means any form of electronic, verbal, or written communication sent\n",
        "+   to the Licensor or its representatives, including but not limited to\n",
        "+   communication on electronic mailing lists, source code control systems,\n",
        "+   and issue tracking systems that are managed by, or on behalf of, the\n",
        "+   Licensor for the purpose of discussing and improving the Work, but\n",
        "+   excluding communication that is conspicuously marked or otherwise\n",
        "+   designated in writing by the copyright owner as \"Not a Contribution.\"\n",
        "+\n",
        "+   \"Contributor\" shall mean Licensor and any individual or Legal Entity\n",
        "+   on behalf of whom a Contribution has been received by Licensor and\n",
        "+   subsequently incorporated within the Work.\n",
        "+\n",
        "+2. Grant of Copyright License. Subject to the terms and conditions of\n",
        "+   this License, each Contributor hereby grants to You a perpetual,\n",
        "+   worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
        "+   copyright license to reproduce, prepare Derivative Works of,\n",
        "+   publicly display, publicly perform, sublicense, and distribute the\n",
        "+   Work and such Derivative Works in Source or Object form.\n",
        "+\n",
        "+3. Grant of Patent License. Subject to the terms and conditions of\n",
        "+   this License, each Contributor hereby grants to You a perpetual,\n",
        "+   worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
        "+   (except as stated in this section) patent license to make, have made,\n",
        "+   use, offer to sell, sell, import, and otherwise transfer the Work,\n",
        "+   where such license applies only to those patent claims licensable\n",
        "+   by such Contributor that are necessarily infringed by their\n",
        "+   Contribution(s) alone or by combination of their Contribution(s)\n",
        "+   with the Work to which such Contribution(s) was submitted. If You\n",
        "+   institute patent litigation against any entity (including a\n",
        "+   cross-claim or counterclaim in a lawsuit) alleging that the Work\n",
        "+   or a Contribution incorporated within the Work constitutes direct\n",
        "+   or contributory patent infringement, then any patent licenses\n",
        "+   granted to You under this License for that Work shall terminate\n",
        "+   as of the date such litigation is filed.\n",
        "+\n",
        "+4. Redistribution. You may reproduce and distribute copies of the\n",
        "+   Work or Derivative Works thereof in any medium, with or without\n",
        "+   modifications, and in Source or Object form, provided that You\n",
        "+   meet the following conditions:\n",
        "+\n",
        "+   (a) You must give any other recipients of the Work or\n",
        "+       Derivative Works a copy of this License; and\n",
        "+\n",
        "+   (b) You must cause any modified files to carry prominent notices\n",
        "+       stating that You changed the files; and\n",
        "+\n",
        "+   (c) You must retain, in the Source form of any Derivative Works\n",
        "+       that You distribute, all copyright, patent, trademark, and\n",
        "+       attribution notices from the Source form of the Work,\n",
        "+       excluding those notices that do not pertain to any part of\n",
        "+       the Derivative Works; and\n",
        "+\n",
        "+   (d) If the Work includes a \"NOTICE\" text file as part of its\n",
        "+       distribution, then any Derivative Works that You distribute must\n",
        "+       include a readable copy of the attribution notices contained\n",
        "+       within such NOTICE file, excluding those notices that do not\n",
        "+       pertain to any part of the Derivative Works, in at least one\n",
        "+       of the following places: within a NOTICE text file distributed\n",
        "+       as part of the Derivative Works; within the Source form or\n",
        "+       documentation, if provided along with the Derivative Works; or,\n",
        "+       within a display generated by the Derivative Works, if and\n",
        "+       wherever such third-party notices normally appear. The contents\n",
        "+       of the NOTICE file are for informational purposes only and\n",
        "+       do not modify the License. You may add Your own attribution\n",
        "+       notices within Derivative Works that You distribute, alongside\n",
        "+       or as an addendum to the NOTICE text from the Work, provided\n",
        "+       that such additional attribution notices cannot be construed\n",
        "+       as modifying the License.\n",
        "+\n",
        "+   You may add Your own copyright statement to Your modifications and\n",
        "+   may provide additional or different license terms and conditions\n",
        "+   for use, reproduction, or distribution of Your modifications, or\n",
        "+   for any such Derivative Works as a whole, provided Your use,\n",
        "+   reproduction, and distribution of the Work otherwise complies with\n",
        "+   the conditions stated in this License.\n",
        "+\n",
        "+5. Submission of Contributions. Unless You explicitly state otherwise,\n",
        "+   any Contribution intentionally submitted for inclusion in the Work\n",
        "+   by You to the Licensor shall be under the terms and conditions of\n",
        "+   this License, without any additional terms or conditions.\n",
        "+   Notwithstanding the above, nothing herein shall supersede or modify\n",
        "+   the terms of any separate license agreement you may have executed\n",
        "+   with Licensor regarding such Contributions.\n",
        "+\n",
        "+6. Trademarks. This License does not grant permission to use the trade\n",
        "+   names, trademarks, service marks, or product names of the Licensor,\n",
        "+   except as required for reasonable and customary use in describing the\n",
        "+   origin of the Work and reproducing the content of the NOTICE file.\n",
        "+\n",
        "+7. Disclaimer of Warranty. Unless required by applicable law or\n",
        "+   agreed to in writing, Licensor provides the Work (and each\n",
        "+   Contributor provides its Contributions) on an \"AS IS\" BASIS,\n",
        "+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
        "+   implied, including, without limitation, any warranties or conditions\n",
        "+   of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n",
        "+   PARTICULAR PURPOSE. You are solely responsible for determining the\n",
        "+   appropriateness of using or redistributing the Work and assume any\n",
        "+   risks associated with Your exercise of permissions under this License.\n",
        "+\n",
        "+8. Limitation of Liability. In no event and under no legal theory,\n",
        "+   whether in tort (including negligence), contract, or otherwise,\n",
        "+   unless required by applicable law (such as deliberate and grossly\n",
        "+   negligent acts) or agreed to in writing, shall any Contributor be\n",
        "+   liable to You for damages, including any direct, indirect, special,\n",
        "+   incidental, or consequential damages of any character arising as a\n",
        "+   result of this License or out of the use or inability to use the\n",
        "+   Work (including but not limited to damages for loss of goodwill,\n",
        "+   work stoppage, computer failure or malfunction, or any and all\n",
        "+   other commercial damages or losses), even if such Contributor\n",
        "+   has been advised of the possibility of such damages.\n",
        "+\n",
        "+9. Accepting Warranty or Additional Liability. While redistributing\n",
        "+   the Work or Derivative Works thereof, You may choose to offer,\n",
        "+   and charge a fee for, acceptance of support, warranty, indemnity,\n",
        "+   or other liability obligations and/or rights consistent with this\n",
        "+   License. However, in accepting such obligations, You may act only\n",
        "+   on Your own behalf and on Your sole responsibility, not on behalf\n",
        "+   of any other Contributor, and only if You agree to indemnify,\n",
        "+   defend, and hold each Contributor harmless for any liability\n",
        "+   incurred by, or claims asserted against, such Contributor by reason\n",
        "+   of your accepting any such warranty or additional liability.\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/PyNaCl-1.5.0.dist-info/METADATA",
      "status": "added",
      "additions": 245,
      "deletions": 0,
      "patch": "@@ -0,0 +1,245 @@\n+Metadata-Version: 2.1\n+Name: PyNaCl\n+Version: 1.5.0\n+Summary: Python binding to the Networking and Cryptography (NaCl) library\n+Home-page: https://github.com/pyca/pynacl/\n+Author: The PyNaCl developers\n+Author-email: cryptography-dev@python.org\n+License: Apache License 2.0\n+Platform: UNKNOWN\n+Classifier: Programming Language :: Python :: Implementation :: CPython\n+Classifier: Programming Language :: Python :: Implementation :: PyPy\n+Classifier: Programming Language :: Python :: 3\n+Classifier: Programming Language :: Python :: 3.6\n+Classifier: Programming Language :: Python :: 3.7\n+Classifier: Programming Language :: Python :: 3.8\n+Classifier: Programming Language :: Python :: 3.9\n+Classifier: Programming Language :: Python :: 3.10\n+Requires-Python: >=3.6\n+Requires-Dist: cffi (>=1.4.1)\n+Provides-Extra: docs\n+Requires-Dist: sphinx (>=1.6.5) ; extra == 'docs'\n+Requires-Dist: sphinx-rtd-theme ; extra == 'docs'\n+Provides-Extra: tests\n+Requires-Dist: pytest (!=3.3.0,>=3.2.1) ; extra == 'tests'\n+Requires-Dist: hypothesis (>=3.27.0) ; extra == 'tests'\n+\n+===============================================\n+PyNaCl: Python binding to the libsodium library\n+===============================================\n+\n+.. image:: https://img.shields.io/pypi/v/pynacl.svg\n+    :target: https://pypi.org/project/PyNaCl/\n+    :alt: Latest Version\n+\n+.. image:: https://codecov.io/github/pyca/pynacl/coverage.svg?branch=main\n+    :target: https://codecov.io/github/pyca/pynacl?branch=main\n+\n+.. image:: https://img.shields.io/pypi/pyversions/pynacl.svg\n+    :target: https://pypi.org/project/PyNaCl/\n+    :alt: Compatible Python Versions\n+\n+PyNaCl is a Python binding to `libsodium`_, which is a fork of the\n+`Networking and Cryptography library`_. These libraries have a stated goal of\n+improving usability, security and speed. It supports Python 3.6+ as well as\n+PyPy 3.\n+\n+.. _libsodium: https://github.com/jedisct1/libsodium\n+.. _Networking and Cryptography library: https://nacl.cr.yp.to/\n+\n+Features\n+--------\n+\n+* Digital signatures\n+* Secret-key encryption\n+* Public-key encryption\n+* Hashing and message authentication\n+* Password based key derivation and password hashing\n+\n+`Changelog`_\n+------------\n+\n+.. _Changelog: https://pynacl.readthedocs.io/en/stable/changelog/\n+\n+Installation\n+============\n+\n+Binary wheel install\n+--------------------\n+\n+PyNaCl ships as a binary wheel on macOS, Windows and Linux ``manylinux1`` [#many]_ ,\n+so all dependencies are included. Make sure you have an up-to-date pip\n+and run:\n+\n+.. code-block:: console\n+\n+    $ pip install pynacl\n+\n+Faster wheel build\n+------------------\n+\n+You can define the environment variable ``LIBSODIUM_MAKE_ARGS`` to pass arguments to ``make``\n+and enable `parallelization`_:\n+\n+.. code-block:: console\n+\n+    $ LIBSODIUM_MAKE_ARGS=-j4 pip install pynacl\n+\n+Linux source build\n+------------------\n+\n+PyNaCl relies on `libsodium`_, a portable C library. A copy is bundled\n+with PyNaCl so to install you can run:\n+\n+.. code-block:: console\n+\n+    $ pip install pynacl\n+\n+If you'd prefer to use the version of ``libsodium`` provided by your\n+distribution, you can disable the bundled copy during install by running:\n+\n+.. code-block:: console\n+\n+    $ SODIUM_INSTALL=system pip install pynacl\n+\n+.. warning:: Usage of the legacy ``easy_install`` command provided by setuptools\n+   is generally discouraged, and is completely unsupported in PyNaCl's case.\n+\n+.. _parallelization: https://www.gnu.org/software/make/manual/html_node/Parallel.html\n+\n+.. _libsodium: https://github.com/jedisct1/libsodium\n+\n+.. [#many] `manylinux1 wheels <https://www.python.org/dev/peps/pep-0513/>`_\n+    are built on a baseline linux environment based on Centos 5.11\n+    and should work on most x86 and x86_64 glibc based linux environments.\n+\n+Changelog\n+=========\n+\n+1.5.0 (2022-01-07)\n+------------------\n+\n+* **BACKWARDS INCOMPATIBLE:** Removed support for Python 2.7 and Python 3.5.\n+* **BACKWARDS INCOMPATIBLE:** We no longer distribute ``manylinux1``\n+  wheels.\n+* Added ``manylinux2014``, ``manylinux_2_24``, ``musllinux``, and macOS\n+  ``universal2`` wheels (the latter supports macOS ``arm64``).\n+* Update ``libsodium`` to 1.0.18-stable (July 25, 2021 release).\n+* Add inline type hints.\n+\n+1.4.0 (2020-05-25)\n+------------------\n+\n+* Update ``libsodium`` to 1.0.18.\n+* **BACKWARDS INCOMPATIBLE:** We no longer distribute 32-bit ``manylinux1``\n+  wheels. Continuing to produce them was a maintenance burden.\n+* Added support for Python 3.8, and removed support for Python 3.4.\n+* Add low level bindings for extracting the seed and the public key\n+  from crypto_sign_ed25519 secret key\n+* Add low level bindings for deterministic random generation.\n+* Add ``wheel`` and ``setuptools`` setup_requirements in ``setup.py`` (#485)\n+* Fix checks on very slow builders (#481, #495)\n+* Add low-level bindings to ed25519 arithmetic functions\n+* Update low-level blake2b state implementation\n+* Fix wrong short-input behavior of SealedBox.decrypt() (#517)\n+* Raise CryptPrefixError exception instead of InvalidkeyError when trying\n+  to check a password against a verifier stored in a unknown format (#519)\n+* Add support for minimal builds of libsodium. Trying to call functions\n+  not available in a minimal build will raise an UnavailableError\n+  exception. To compile a minimal build of the bundled libsodium, set\n+  the SODIUM_INSTALL_MINIMAL environment variable to any non-empty\n+  string (e.g. ``SODIUM_INSTALL_MINIMAL=1``) for setup.\n+\n+1.3.0 2018-09-26\n+----------------\n+\n+* Added support for Python 3.7.\n+* Update ``libsodium`` to 1.0.16.\n+* Run and test all code examples in PyNaCl docs through sphinx's\n+  doctest builder.\n+* Add low-level bindings for chacha20-poly1305 AEAD constructions.\n+* Add low-level bindings for the chacha20-poly1305 secretstream constructions.\n+* Add low-level bindings for ed25519ph pre-hashed signing construction.\n+* Add low-level bindings for constant-time increment and addition\n+  on fixed-precision big integers represented as little-endian\n+  byte sequences.\n+* Add low-level bindings for the ISO/IEC 7816-4 compatible padding API.\n+* Add low-level bindings for libsodium's crypto_kx... key exchange\n+  construction.\n+* Set hypothesis deadline to None in tests/test_pwhash.py to avoid\n+  incorrect test failures on slower processor architectures.  GitHub\n+  issue #370\n+\n+1.2.1 - 2017-12-04\n+------------------\n+\n+* Update hypothesis minimum allowed version.\n+* Infrastructure: add proper configuration for readthedocs builder\n+  runtime environment.\n+\n+1.2.0 - 2017-11-01\n+------------------\n+\n+* Update ``libsodium`` to 1.0.15.\n+* Infrastructure: add jenkins support for automatic build of\n+  ``manylinux1`` binary wheels\n+* Added support for ``SealedBox`` construction.\n+* Added support for ``argon2i`` and ``argon2id`` password hashing constructs\n+  and restructured high-level password hashing implementation to expose\n+  the same interface for all hashers.\n+* Added support for 128 bit ``siphashx24`` variant of ``siphash24``.\n+* Added support for ``from_seed`` APIs for X25519 keypair generation.\n+* Dropped support for Python 3.3.\n+\n+1.1.2 - 2017-03-31\n+------------------\n+\n+* reorder link time library search path when using bundled\n+  libsodium\n+\n+1.1.1 - 2017-03-15\n+------------------\n+\n+* Fixed a circular import bug in ``nacl.utils``.\n+\n+1.1.0 - 2017-03-14\n+------------------\n+\n+* Dropped support for Python 2.6.\n+* Added ``shared_key()`` method on ``Box``.\n+* You can now pass ``None`` to ``nonce`` when encrypting with ``Box`` or\n+  ``SecretBox`` and it will automatically generate a random nonce.\n+* Added support for ``siphash24``.\n+* Added support for ``blake2b``.\n+* Added support for ``scrypt``.\n+* Update ``libsodium`` to 1.0.11.\n+* Default to the bundled ``libsodium`` when compiling.\n+* All raised exceptions are defined mixing-in\n+  ``nacl.exceptions.CryptoError``\n+\n+1.0.1 - 2016-01-24\n+------------------\n+\n+* Fix an issue with absolute paths that prevented the creation of wheels.\n+\n+1.0 - 2016-01-23\n+----------------\n+\n+* PyNaCl has been ported to use the new APIs available in cffi 1.0+.\n+  Due to this change we no longer support PyPy releases older than 2.6.\n+* Python 3.2 support has been dropped.\n+* Functions to convert between Ed25519 and Curve25519 keys have been added.\n+\n+0.3.0 - 2015-03-04\n+------------------\n+\n+* The low-level API (`nacl.c.*`) has been changed to match the\n+  upstream NaCl C/C++ conventions (as well as those of other NaCl bindings).\n+  The order of arguments and return values has changed significantly. To\n+  avoid silent failures, `nacl.c` has been removed, and replaced with\n+  `nacl.bindings` (with the new argument ordering). If you have code which\n+  calls these functions (e.g. `nacl.c.crypto_box_keypair()`), you must review\n+  the new docstrings and update your code/imports to match the new\n+  conventions.\n+\n+",
      "patch_lines": [
        "@@ -0,0 +1,245 @@\n",
        "+Metadata-Version: 2.1\n",
        "+Name: PyNaCl\n",
        "+Version: 1.5.0\n",
        "+Summary: Python binding to the Networking and Cryptography (NaCl) library\n",
        "+Home-page: https://github.com/pyca/pynacl/\n",
        "+Author: The PyNaCl developers\n",
        "+Author-email: cryptography-dev@python.org\n",
        "+License: Apache License 2.0\n",
        "+Platform: UNKNOWN\n",
        "+Classifier: Programming Language :: Python :: Implementation :: CPython\n",
        "+Classifier: Programming Language :: Python :: Implementation :: PyPy\n",
        "+Classifier: Programming Language :: Python :: 3\n",
        "+Classifier: Programming Language :: Python :: 3.6\n",
        "+Classifier: Programming Language :: Python :: 3.7\n",
        "+Classifier: Programming Language :: Python :: 3.8\n",
        "+Classifier: Programming Language :: Python :: 3.9\n",
        "+Classifier: Programming Language :: Python :: 3.10\n",
        "+Requires-Python: >=3.6\n",
        "+Requires-Dist: cffi (>=1.4.1)\n",
        "+Provides-Extra: docs\n",
        "+Requires-Dist: sphinx (>=1.6.5) ; extra == 'docs'\n",
        "+Requires-Dist: sphinx-rtd-theme ; extra == 'docs'\n",
        "+Provides-Extra: tests\n",
        "+Requires-Dist: pytest (!=3.3.0,>=3.2.1) ; extra == 'tests'\n",
        "+Requires-Dist: hypothesis (>=3.27.0) ; extra == 'tests'\n",
        "+\n",
        "+===============================================\n",
        "+PyNaCl: Python binding to the libsodium library\n",
        "+===============================================\n",
        "+\n",
        "+.. image:: https://img.shields.io/pypi/v/pynacl.svg\n",
        "+    :target: https://pypi.org/project/PyNaCl/\n",
        "+    :alt: Latest Version\n",
        "+\n",
        "+.. image:: https://codecov.io/github/pyca/pynacl/coverage.svg?branch=main\n",
        "+    :target: https://codecov.io/github/pyca/pynacl?branch=main\n",
        "+\n",
        "+.. image:: https://img.shields.io/pypi/pyversions/pynacl.svg\n",
        "+    :target: https://pypi.org/project/PyNaCl/\n",
        "+    :alt: Compatible Python Versions\n",
        "+\n",
        "+PyNaCl is a Python binding to `libsodium`_, which is a fork of the\n",
        "+`Networking and Cryptography library`_. These libraries have a stated goal of\n",
        "+improving usability, security and speed. It supports Python 3.6+ as well as\n",
        "+PyPy 3.\n",
        "+\n",
        "+.. _libsodium: https://github.com/jedisct1/libsodium\n",
        "+.. _Networking and Cryptography library: https://nacl.cr.yp.to/\n",
        "+\n",
        "+Features\n",
        "+--------\n",
        "+\n",
        "+* Digital signatures\n",
        "+* Secret-key encryption\n",
        "+* Public-key encryption\n",
        "+* Hashing and message authentication\n",
        "+* Password based key derivation and password hashing\n",
        "+\n",
        "+`Changelog`_\n",
        "+------------\n",
        "+\n",
        "+.. _Changelog: https://pynacl.readthedocs.io/en/stable/changelog/\n",
        "+\n",
        "+Installation\n",
        "+============\n",
        "+\n",
        "+Binary wheel install\n",
        "+--------------------\n",
        "+\n",
        "+PyNaCl ships as a binary wheel on macOS, Windows and Linux ``manylinux1`` [#many]_ ,\n",
        "+so all dependencies are included. Make sure you have an up-to-date pip\n",
        "+and run:\n",
        "+\n",
        "+.. code-block:: console\n",
        "+\n",
        "+    $ pip install pynacl\n",
        "+\n",
        "+Faster wheel build\n",
        "+------------------\n",
        "+\n",
        "+You can define the environment variable ``LIBSODIUM_MAKE_ARGS`` to pass arguments to ``make``\n",
        "+and enable `parallelization`_:\n",
        "+\n",
        "+.. code-block:: console\n",
        "+\n",
        "+    $ LIBSODIUM_MAKE_ARGS=-j4 pip install pynacl\n",
        "+\n",
        "+Linux source build\n",
        "+------------------\n",
        "+\n",
        "+PyNaCl relies on `libsodium`_, a portable C library. A copy is bundled\n",
        "+with PyNaCl so to install you can run:\n",
        "+\n",
        "+.. code-block:: console\n",
        "+\n",
        "+    $ pip install pynacl\n",
        "+\n",
        "+If you'd prefer to use the version of ``libsodium`` provided by your\n",
        "+distribution, you can disable the bundled copy during install by running:\n",
        "+\n",
        "+.. code-block:: console\n",
        "+\n",
        "+    $ SODIUM_INSTALL=system pip install pynacl\n",
        "+\n",
        "+.. warning:: Usage of the legacy ``easy_install`` command provided by setuptools\n",
        "+   is generally discouraged, and is completely unsupported in PyNaCl's case.\n",
        "+\n",
        "+.. _parallelization: https://www.gnu.org/software/make/manual/html_node/Parallel.html\n",
        "+\n",
        "+.. _libsodium: https://github.com/jedisct1/libsodium\n",
        "+\n",
        "+.. [#many] `manylinux1 wheels <https://www.python.org/dev/peps/pep-0513/>`_\n",
        "+    are built on a baseline linux environment based on Centos 5.11\n",
        "+    and should work on most x86 and x86_64 glibc based linux environments.\n",
        "+\n",
        "+Changelog\n",
        "+=========\n",
        "+\n",
        "+1.5.0 (2022-01-07)\n",
        "+------------------\n",
        "+\n",
        "+* **BACKWARDS INCOMPATIBLE:** Removed support for Python 2.7 and Python 3.5.\n",
        "+* **BACKWARDS INCOMPATIBLE:** We no longer distribute ``manylinux1``\n",
        "+  wheels.\n",
        "+* Added ``manylinux2014``, ``manylinux_2_24``, ``musllinux``, and macOS\n",
        "+  ``universal2`` wheels (the latter supports macOS ``arm64``).\n",
        "+* Update ``libsodium`` to 1.0.18-stable (July 25, 2021 release).\n",
        "+* Add inline type hints.\n",
        "+\n",
        "+1.4.0 (2020-05-25)\n",
        "+------------------\n",
        "+\n",
        "+* Update ``libsodium`` to 1.0.18.\n",
        "+* **BACKWARDS INCOMPATIBLE:** We no longer distribute 32-bit ``manylinux1``\n",
        "+  wheels. Continuing to produce them was a maintenance burden.\n",
        "+* Added support for Python 3.8, and removed support for Python 3.4.\n",
        "+* Add low level bindings for extracting the seed and the public key\n",
        "+  from crypto_sign_ed25519 secret key\n",
        "+* Add low level bindings for deterministic random generation.\n",
        "+* Add ``wheel`` and ``setuptools`` setup_requirements in ``setup.py`` (#485)\n",
        "+* Fix checks on very slow builders (#481, #495)\n",
        "+* Add low-level bindings to ed25519 arithmetic functions\n",
        "+* Update low-level blake2b state implementation\n",
        "+* Fix wrong short-input behavior of SealedBox.decrypt() (#517)\n",
        "+* Raise CryptPrefixError exception instead of InvalidkeyError when trying\n",
        "+  to check a password against a verifier stored in a unknown format (#519)\n",
        "+* Add support for minimal builds of libsodium. Trying to call functions\n",
        "+  not available in a minimal build will raise an UnavailableError\n",
        "+  exception. To compile a minimal build of the bundled libsodium, set\n",
        "+  the SODIUM_INSTALL_MINIMAL environment variable to any non-empty\n",
        "+  string (e.g. ``SODIUM_INSTALL_MINIMAL=1``) for setup.\n",
        "+\n",
        "+1.3.0 2018-09-26\n",
        "+----------------\n",
        "+\n",
        "+* Added support for Python 3.7.\n",
        "+* Update ``libsodium`` to 1.0.16.\n",
        "+* Run and test all code examples in PyNaCl docs through sphinx's\n",
        "+  doctest builder.\n",
        "+* Add low-level bindings for chacha20-poly1305 AEAD constructions.\n",
        "+* Add low-level bindings for the chacha20-poly1305 secretstream constructions.\n",
        "+* Add low-level bindings for ed25519ph pre-hashed signing construction.\n",
        "+* Add low-level bindings for constant-time increment and addition\n",
        "+  on fixed-precision big integers represented as little-endian\n",
        "+  byte sequences.\n",
        "+* Add low-level bindings for the ISO/IEC 7816-4 compatible padding API.\n",
        "+* Add low-level bindings for libsodium's crypto_kx... key exchange\n",
        "+  construction.\n",
        "+* Set hypothesis deadline to None in tests/test_pwhash.py to avoid\n",
        "+  incorrect test failures on slower processor architectures.  GitHub\n",
        "+  issue #370\n",
        "+\n",
        "+1.2.1 - 2017-12-04\n",
        "+------------------\n",
        "+\n",
        "+* Update hypothesis minimum allowed version.\n",
        "+* Infrastructure: add proper configuration for readthedocs builder\n",
        "+  runtime environment.\n",
        "+\n",
        "+1.2.0 - 2017-11-01\n",
        "+------------------\n",
        "+\n",
        "+* Update ``libsodium`` to 1.0.15.\n",
        "+* Infrastructure: add jenkins support for automatic build of\n",
        "+  ``manylinux1`` binary wheels\n",
        "+* Added support for ``SealedBox`` construction.\n",
        "+* Added support for ``argon2i`` and ``argon2id`` password hashing constructs\n",
        "+  and restructured high-level password hashing implementation to expose\n",
        "+  the same interface for all hashers.\n",
        "+* Added support for 128 bit ``siphashx24`` variant of ``siphash24``.\n",
        "+* Added support for ``from_seed`` APIs for X25519 keypair generation.\n",
        "+* Dropped support for Python 3.3.\n",
        "+\n",
        "+1.1.2 - 2017-03-31\n",
        "+------------------\n",
        "+\n",
        "+* reorder link time library search path when using bundled\n",
        "+  libsodium\n",
        "+\n",
        "+1.1.1 - 2017-03-15\n",
        "+------------------\n",
        "+\n",
        "+* Fixed a circular import bug in ``nacl.utils``.\n",
        "+\n",
        "+1.1.0 - 2017-03-14\n",
        "+------------------\n",
        "+\n",
        "+* Dropped support for Python 2.6.\n",
        "+* Added ``shared_key()`` method on ``Box``.\n",
        "+* You can now pass ``None`` to ``nonce`` when encrypting with ``Box`` or\n",
        "+  ``SecretBox`` and it will automatically generate a random nonce.\n",
        "+* Added support for ``siphash24``.\n",
        "+* Added support for ``blake2b``.\n",
        "+* Added support for ``scrypt``.\n",
        "+* Update ``libsodium`` to 1.0.11.\n",
        "+* Default to the bundled ``libsodium`` when compiling.\n",
        "+* All raised exceptions are defined mixing-in\n",
        "+  ``nacl.exceptions.CryptoError``\n",
        "+\n",
        "+1.0.1 - 2016-01-24\n",
        "+------------------\n",
        "+\n",
        "+* Fix an issue with absolute paths that prevented the creation of wheels.\n",
        "+\n",
        "+1.0 - 2016-01-23\n",
        "+----------------\n",
        "+\n",
        "+* PyNaCl has been ported to use the new APIs available in cffi 1.0+.\n",
        "+  Due to this change we no longer support PyPy releases older than 2.6.\n",
        "+* Python 3.2 support has been dropped.\n",
        "+* Functions to convert between Ed25519 and Curve25519 keys have been added.\n",
        "+\n",
        "+0.3.0 - 2015-03-04\n",
        "+------------------\n",
        "+\n",
        "+* The low-level API (`nacl.c.*`) has been changed to match the\n",
        "+  upstream NaCl C/C++ conventions (as well as those of other NaCl bindings).\n",
        "+  The order of arguments and return values has changed significantly. To\n",
        "+  avoid silent failures, `nacl.c` has been removed, and replaced with\n",
        "+  `nacl.bindings` (with the new argument ordering). If you have code which\n",
        "+  calls these functions (e.g. `nacl.c.crypto_box_keypair()`), you must review\n",
        "+  the new docstrings and update your code/imports to match the new\n",
        "+  conventions.\n",
        "+\n",
        "+\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/PyNaCl-1.5.0.dist-info/RECORD",
      "status": "added",
      "additions": 69,
      "deletions": 0,
      "patch": "@@ -0,0 +1,69 @@\n+PyNaCl-1.5.0.dist-info/INSTALLER,sha256=zuuue4knoyJ-UwPPXg8fezS7VCrXJQrAP7zeNuwvFQg,4\r\n+PyNaCl-1.5.0.dist-info/LICENSE,sha256=0xdK1j5yHUydzLitQyCEiZLTFDabxGMZcgtYAskVP-k,9694\r\n+PyNaCl-1.5.0.dist-info/METADATA,sha256=OJaXCiHgNRywLY9cj3X2euddUPZ4dnyyqAQMU01X4j0,8634\r\n+PyNaCl-1.5.0.dist-info/RECORD,,\r\n+PyNaCl-1.5.0.dist-info/REQUESTED,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0\r\n+PyNaCl-1.5.0.dist-info/WHEEL,sha256=TIQeZFe3DwXBO5UGlCH1aKpf5Cx6FJLbIUqd-Sq2juI,185\r\n+PyNaCl-1.5.0.dist-info/top_level.txt,sha256=wfdEOI_G2RIzmzsMyhpqP17HUh6Jcqi99to9aHLEslo,13\r\n+nacl/__init__.py,sha256=0IUunzBT8_Jn0DUdHacBExOYeAEMggo8slkfjo7O0XM,1116\r\n+nacl/__pycache__/__init__.cpython-312.pyc,,\r\n+nacl/__pycache__/encoding.cpython-312.pyc,,\r\n+nacl/__pycache__/exceptions.cpython-312.pyc,,\r\n+nacl/__pycache__/hash.cpython-312.pyc,,\r\n+nacl/__pycache__/hashlib.cpython-312.pyc,,\r\n+nacl/__pycache__/public.cpython-312.pyc,,\r\n+nacl/__pycache__/secret.cpython-312.pyc,,\r\n+nacl/__pycache__/signing.cpython-312.pyc,,\r\n+nacl/__pycache__/utils.cpython-312.pyc,,\r\n+nacl/_sodium.abi3.so,sha256=uJ6RwSnbb9wO4esR0bVUqrfFHtBOGm34IQIdmaE1fGY,2740136\r\n+nacl/bindings/__init__.py,sha256=BDlStrds2EuUS4swOL4pnf92PWVS_CHRCptX3KhEX-s,16997\r\n+nacl/bindings/__pycache__/__init__.cpython-312.pyc,,\r\n+nacl/bindings/__pycache__/crypto_aead.cpython-312.pyc,,\r\n+nacl/bindings/__pycache__/crypto_box.cpython-312.pyc,,\r\n+nacl/bindings/__pycache__/crypto_core.cpython-312.pyc,,\r\n+nacl/bindings/__pycache__/crypto_generichash.cpython-312.pyc,,\r\n+nacl/bindings/__pycache__/crypto_hash.cpython-312.pyc,,\r\n+nacl/bindings/__pycache__/crypto_kx.cpython-312.pyc,,\r\n+nacl/bindings/__pycache__/crypto_pwhash.cpython-312.pyc,,\r\n+nacl/bindings/__pycache__/crypto_scalarmult.cpython-312.pyc,,\r\n+nacl/bindings/__pycache__/crypto_secretbox.cpython-312.pyc,,\r\n+nacl/bindings/__pycache__/crypto_secretstream.cpython-312.pyc,,\r\n+nacl/bindings/__pycache__/crypto_shorthash.cpython-312.pyc,,\r\n+nacl/bindings/__pycache__/crypto_sign.cpython-312.pyc,,\r\n+nacl/bindings/__pycache__/randombytes.cpython-312.pyc,,\r\n+nacl/bindings/__pycache__/sodium_core.cpython-312.pyc,,\r\n+nacl/bindings/__pycache__/utils.cpython-312.pyc,,\r\n+nacl/bindings/crypto_aead.py,sha256=BIw1k_JCfr5ylZk0RF5rCFIM1fhfLkEa-aiWkrfffNE,15597\r\n+nacl/bindings/crypto_box.py,sha256=Ox0NG2t4MsGhBAa7Kgah4o0gc99ULMsqkdX56ofOouY,10139\r\n+nacl/bindings/crypto_core.py,sha256=6u9G3y7H-QrawO785UkFFFtwDoCkeHE63GOUl9p5-eA,13736\r\n+nacl/bindings/crypto_generichash.py,sha256=9mX0DGIIzicr-uXrqFM1nU4tirasbixDwbcdfV7W1fc,8852\r\n+nacl/bindings/crypto_hash.py,sha256=Rg1rsEwE3azhsQT-dNVPA4NB9VogJAKn1EfxYt0pPe0,2175\r\n+nacl/bindings/crypto_kx.py,sha256=oZNVlNgROpHOa1XQ_uZe0tqIkdfuApeJlRnwR23_74k,6723\r\n+nacl/bindings/crypto_pwhash.py,sha256=laVDo4xFUuGyEjtZAU510AklBF6ablBy7Z3HN1WDYjY,18848\r\n+nacl/bindings/crypto_scalarmult.py,sha256=_DX-mst2uCnzjo6fP5HRTnhv1BC95B9gmJc3L_or16g,8244\r\n+nacl/bindings/crypto_secretbox.py,sha256=KgZ1VvkCJDlQ85jtfe9c02VofPvuEgZEhWni-aX3MsM,2914\r\n+nacl/bindings/crypto_secretstream.py,sha256=G0FgZS01qA5RzWzm5Bdms8Yy_lvgdZDoUYYBActPmvQ,11165\r\n+nacl/bindings/crypto_shorthash.py,sha256=PQU7djHTLDGdVs-w_TsivjFHHp5EK5k2Yh6p-6z0T60,2603\r\n+nacl/bindings/crypto_sign.py,sha256=53j2im9E4F79qT_2U8IfCAc3lzg0VMwEjvAPEUccVDg,10342\r\n+nacl/bindings/randombytes.py,sha256=uBK3W4WcjgnjZdWanrX0fjYZpr9KHbBgNMl9rui-Ojc,1563\r\n+nacl/bindings/sodium_core.py,sha256=9Y9CX--sq-TaPaQRPRpx8SWDSS9PJOja_Cqb-yqyJNQ,1039\r\n+nacl/bindings/utils.py,sha256=KDwQnadXeNMbqEA1SmpNyCVo5k8MiUQa07QM66VzfXM,4298\r\n+nacl/encoding.py,sha256=qTAPc2MXSkdh4cqDVY0ra6kHyViHMCmEo_re7cgGk5w,2915\r\n+nacl/exceptions.py,sha256=GZH32aJtZgqCO4uz0LRsev8z0WyvAYuV3YVqT9AAQq4,2451\r\n+nacl/hash.py,sha256=EYBOe6UVc9SUQINEmyuRSa1QGRSvdwdrBzTL1tdFLU8,6392\r\n+nacl/hashlib.py,sha256=L5Fv75St8AMPvb-GhA4YqX5p1mC_Sb4HhC1NxNQMpJA,4400\r\n+nacl/public.py,sha256=RVGCWQRjIJOmW-8sNrVLtsDjMMGx30i6UyfViGCnQNA,14792\r\n+nacl/pwhash/__init__.py,sha256=XSDXd7wQHNLEHl0mkHfVb5lFQsp6ygHkhen718h0BSM,2675\r\n+nacl/pwhash/__pycache__/__init__.cpython-312.pyc,,\r\n+nacl/pwhash/__pycache__/_argon2.cpython-312.pyc,,\r\n+nacl/pwhash/__pycache__/argon2i.cpython-312.pyc,,\r\n+nacl/pwhash/__pycache__/argon2id.cpython-312.pyc,,\r\n+nacl/pwhash/__pycache__/scrypt.cpython-312.pyc,,\r\n+nacl/pwhash/_argon2.py,sha256=jL1ChR9biwYh3RSuc-LJ2-W4DlVLHpir-XHGX8cpeJQ,1779\r\n+nacl/pwhash/argon2i.py,sha256=IIvIuO9siKUu5-Wpz0SGiltLQv7Du_mi9BUE8INRK_4,4405\r\n+nacl/pwhash/argon2id.py,sha256=H22i8O4j9Ws4L3JsXl9TRcJzDcyaVumhQRPzINAgJWM,4433\r\n+nacl/pwhash/scrypt.py,sha256=fMr3Qht1a1EY8aebNNntfLRjinIPXtKYKKrrBhY5LDc,6986\r\n+nacl/py.typed,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0\r\n+nacl/secret.py,sha256=kauBNuP-0rb3TjU2EMBMu5Vnmzjnscp1bRqMspy5LzU,12108\r\n+nacl/signing.py,sha256=kbTEUyHLUMaNLv1nCjxzGxCs82Qs5w8gxE_CnEwPuIU,8337\r\n+nacl/utils.py,sha256=gmlTD1x9ZNwzHd8LpALH1CHud-Htv8ejRb3y7TyS9f0,2341\r",
      "patch_lines": [
        "@@ -0,0 +1,69 @@\n",
        "+PyNaCl-1.5.0.dist-info/INSTALLER,sha256=zuuue4knoyJ-UwPPXg8fezS7VCrXJQrAP7zeNuwvFQg,4\r\n",
        "+PyNaCl-1.5.0.dist-info/LICENSE,sha256=0xdK1j5yHUydzLitQyCEiZLTFDabxGMZcgtYAskVP-k,9694\r\n",
        "+PyNaCl-1.5.0.dist-info/METADATA,sha256=OJaXCiHgNRywLY9cj3X2euddUPZ4dnyyqAQMU01X4j0,8634\r\n",
        "+PyNaCl-1.5.0.dist-info/RECORD,,\r\n",
        "+PyNaCl-1.5.0.dist-info/REQUESTED,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0\r\n",
        "+PyNaCl-1.5.0.dist-info/WHEEL,sha256=TIQeZFe3DwXBO5UGlCH1aKpf5Cx6FJLbIUqd-Sq2juI,185\r\n",
        "+PyNaCl-1.5.0.dist-info/top_level.txt,sha256=wfdEOI_G2RIzmzsMyhpqP17HUh6Jcqi99to9aHLEslo,13\r\n",
        "+nacl/__init__.py,sha256=0IUunzBT8_Jn0DUdHacBExOYeAEMggo8slkfjo7O0XM,1116\r\n",
        "+nacl/__pycache__/__init__.cpython-312.pyc,,\r\n",
        "+nacl/__pycache__/encoding.cpython-312.pyc,,\r\n",
        "+nacl/__pycache__/exceptions.cpython-312.pyc,,\r\n",
        "+nacl/__pycache__/hash.cpython-312.pyc,,\r\n",
        "+nacl/__pycache__/hashlib.cpython-312.pyc,,\r\n",
        "+nacl/__pycache__/public.cpython-312.pyc,,\r\n",
        "+nacl/__pycache__/secret.cpython-312.pyc,,\r\n",
        "+nacl/__pycache__/signing.cpython-312.pyc,,\r\n",
        "+nacl/__pycache__/utils.cpython-312.pyc,,\r\n",
        "+nacl/_sodium.abi3.so,sha256=uJ6RwSnbb9wO4esR0bVUqrfFHtBOGm34IQIdmaE1fGY,2740136\r\n",
        "+nacl/bindings/__init__.py,sha256=BDlStrds2EuUS4swOL4pnf92PWVS_CHRCptX3KhEX-s,16997\r\n",
        "+nacl/bindings/__pycache__/__init__.cpython-312.pyc,,\r\n",
        "+nacl/bindings/__pycache__/crypto_aead.cpython-312.pyc,,\r\n",
        "+nacl/bindings/__pycache__/crypto_box.cpython-312.pyc,,\r\n",
        "+nacl/bindings/__pycache__/crypto_core.cpython-312.pyc,,\r\n",
        "+nacl/bindings/__pycache__/crypto_generichash.cpython-312.pyc,,\r\n",
        "+nacl/bindings/__pycache__/crypto_hash.cpython-312.pyc,,\r\n",
        "+nacl/bindings/__pycache__/crypto_kx.cpython-312.pyc,,\r\n",
        "+nacl/bindings/__pycache__/crypto_pwhash.cpython-312.pyc,,\r\n",
        "+nacl/bindings/__pycache__/crypto_scalarmult.cpython-312.pyc,,\r\n",
        "+nacl/bindings/__pycache__/crypto_secretbox.cpython-312.pyc,,\r\n",
        "+nacl/bindings/__pycache__/crypto_secretstream.cpython-312.pyc,,\r\n",
        "+nacl/bindings/__pycache__/crypto_shorthash.cpython-312.pyc,,\r\n",
        "+nacl/bindings/__pycache__/crypto_sign.cpython-312.pyc,,\r\n",
        "+nacl/bindings/__pycache__/randombytes.cpython-312.pyc,,\r\n",
        "+nacl/bindings/__pycache__/sodium_core.cpython-312.pyc,,\r\n",
        "+nacl/bindings/__pycache__/utils.cpython-312.pyc,,\r\n",
        "+nacl/bindings/crypto_aead.py,sha256=BIw1k_JCfr5ylZk0RF5rCFIM1fhfLkEa-aiWkrfffNE,15597\r\n",
        "+nacl/bindings/crypto_box.py,sha256=Ox0NG2t4MsGhBAa7Kgah4o0gc99ULMsqkdX56ofOouY,10139\r\n",
        "+nacl/bindings/crypto_core.py,sha256=6u9G3y7H-QrawO785UkFFFtwDoCkeHE63GOUl9p5-eA,13736\r\n",
        "+nacl/bindings/crypto_generichash.py,sha256=9mX0DGIIzicr-uXrqFM1nU4tirasbixDwbcdfV7W1fc,8852\r\n",
        "+nacl/bindings/crypto_hash.py,sha256=Rg1rsEwE3azhsQT-dNVPA4NB9VogJAKn1EfxYt0pPe0,2175\r\n",
        "+nacl/bindings/crypto_kx.py,sha256=oZNVlNgROpHOa1XQ_uZe0tqIkdfuApeJlRnwR23_74k,6723\r\n",
        "+nacl/bindings/crypto_pwhash.py,sha256=laVDo4xFUuGyEjtZAU510AklBF6ablBy7Z3HN1WDYjY,18848\r\n",
        "+nacl/bindings/crypto_scalarmult.py,sha256=_DX-mst2uCnzjo6fP5HRTnhv1BC95B9gmJc3L_or16g,8244\r\n",
        "+nacl/bindings/crypto_secretbox.py,sha256=KgZ1VvkCJDlQ85jtfe9c02VofPvuEgZEhWni-aX3MsM,2914\r\n",
        "+nacl/bindings/crypto_secretstream.py,sha256=G0FgZS01qA5RzWzm5Bdms8Yy_lvgdZDoUYYBActPmvQ,11165\r\n",
        "+nacl/bindings/crypto_shorthash.py,sha256=PQU7djHTLDGdVs-w_TsivjFHHp5EK5k2Yh6p-6z0T60,2603\r\n",
        "+nacl/bindings/crypto_sign.py,sha256=53j2im9E4F79qT_2U8IfCAc3lzg0VMwEjvAPEUccVDg,10342\r\n",
        "+nacl/bindings/randombytes.py,sha256=uBK3W4WcjgnjZdWanrX0fjYZpr9KHbBgNMl9rui-Ojc,1563\r\n",
        "+nacl/bindings/sodium_core.py,sha256=9Y9CX--sq-TaPaQRPRpx8SWDSS9PJOja_Cqb-yqyJNQ,1039\r\n",
        "+nacl/bindings/utils.py,sha256=KDwQnadXeNMbqEA1SmpNyCVo5k8MiUQa07QM66VzfXM,4298\r\n",
        "+nacl/encoding.py,sha256=qTAPc2MXSkdh4cqDVY0ra6kHyViHMCmEo_re7cgGk5w,2915\r\n",
        "+nacl/exceptions.py,sha256=GZH32aJtZgqCO4uz0LRsev8z0WyvAYuV3YVqT9AAQq4,2451\r\n",
        "+nacl/hash.py,sha256=EYBOe6UVc9SUQINEmyuRSa1QGRSvdwdrBzTL1tdFLU8,6392\r\n",
        "+nacl/hashlib.py,sha256=L5Fv75St8AMPvb-GhA4YqX5p1mC_Sb4HhC1NxNQMpJA,4400\r\n",
        "+nacl/public.py,sha256=RVGCWQRjIJOmW-8sNrVLtsDjMMGx30i6UyfViGCnQNA,14792\r\n",
        "+nacl/pwhash/__init__.py,sha256=XSDXd7wQHNLEHl0mkHfVb5lFQsp6ygHkhen718h0BSM,2675\r\n",
        "+nacl/pwhash/__pycache__/__init__.cpython-312.pyc,,\r\n",
        "+nacl/pwhash/__pycache__/_argon2.cpython-312.pyc,,\r\n",
        "+nacl/pwhash/__pycache__/argon2i.cpython-312.pyc,,\r\n",
        "+nacl/pwhash/__pycache__/argon2id.cpython-312.pyc,,\r\n",
        "+nacl/pwhash/__pycache__/scrypt.cpython-312.pyc,,\r\n",
        "+nacl/pwhash/_argon2.py,sha256=jL1ChR9biwYh3RSuc-LJ2-W4DlVLHpir-XHGX8cpeJQ,1779\r\n",
        "+nacl/pwhash/argon2i.py,sha256=IIvIuO9siKUu5-Wpz0SGiltLQv7Du_mi9BUE8INRK_4,4405\r\n",
        "+nacl/pwhash/argon2id.py,sha256=H22i8O4j9Ws4L3JsXl9TRcJzDcyaVumhQRPzINAgJWM,4433\r\n",
        "+nacl/pwhash/scrypt.py,sha256=fMr3Qht1a1EY8aebNNntfLRjinIPXtKYKKrrBhY5LDc,6986\r\n",
        "+nacl/py.typed,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0\r\n",
        "+nacl/secret.py,sha256=kauBNuP-0rb3TjU2EMBMu5Vnmzjnscp1bRqMspy5LzU,12108\r\n",
        "+nacl/signing.py,sha256=kbTEUyHLUMaNLv1nCjxzGxCs82Qs5w8gxE_CnEwPuIU,8337\r\n",
        "+nacl/utils.py,sha256=gmlTD1x9ZNwzHd8LpALH1CHud-Htv8ejRb3y7TyS9f0,2341\r\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/PyNaCl-1.5.0.dist-info/REQUESTED",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/PyNaCl-1.5.0.dist-info/WHEEL",
      "status": "added",
      "additions": 7,
      "deletions": 0,
      "patch": "@@ -0,0 +1,7 @@\n+Wheel-Version: 1.0\n+Generator: bdist_wheel (0.37.1)\n+Root-Is-Purelib: false\n+Tag: cp36-abi3-manylinux_2_17_x86_64\n+Tag: cp36-abi3-manylinux2014_x86_64\n+Tag: cp36-abi3-manylinux_2_24_x86_64\n+",
      "patch_lines": [
        "@@ -0,0 +1,7 @@\n",
        "+Wheel-Version: 1.0\n",
        "+Generator: bdist_wheel (0.37.1)\n",
        "+Root-Is-Purelib: false\n",
        "+Tag: cp36-abi3-manylinux_2_17_x86_64\n",
        "+Tag: cp36-abi3-manylinux2014_x86_64\n",
        "+Tag: cp36-abi3-manylinux_2_24_x86_64\n",
        "+\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/PyNaCl-1.5.0.dist-info/top_level.txt",
      "status": "added",
      "additions": 2,
      "deletions": 0,
      "patch": "@@ -0,0 +1,2 @@\n+_sodium\n+nacl",
      "patch_lines": [
        "@@ -0,0 +1,2 @@\n",
        "+_sodium\n",
        "+nacl\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/PyYAML-6.0.2.dist-info/INSTALLER",
      "status": "added",
      "additions": 1,
      "deletions": 0,
      "patch": "@@ -0,0 +1 @@\n+pip",
      "patch_lines": [
        "@@ -0,0 +1 @@\n",
        "+pip\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/PyYAML-6.0.2.dist-info/LICENSE",
      "status": "added",
      "additions": 20,
      "deletions": 0,
      "patch": "@@ -0,0 +1,20 @@\n+Copyright (c) 2017-2021 Ingy d\u00f6t Net\n+Copyright (c) 2006-2016 Kirill Simonov\n+\n+Permission is hereby granted, free of charge, to any person obtaining a copy of\n+this software and associated documentation files (the \"Software\"), to deal in\n+the Software without restriction, including without limitation the rights to\n+use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n+of the Software, and to permit persons to whom the Software is furnished to do\n+so, subject to the following conditions:\n+\n+The above copyright notice and this permission notice shall be included in all\n+copies or substantial portions of the Software.\n+\n+THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n+SOFTWARE.",
      "patch_lines": [
        "@@ -0,0 +1,20 @@\n",
        "+Copyright (c) 2017-2021 Ingy d\u00f6t Net\n",
        "+Copyright (c) 2006-2016 Kirill Simonov\n",
        "+\n",
        "+Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
        "+this software and associated documentation files (the \"Software\"), to deal in\n",
        "+the Software without restriction, including without limitation the rights to\n",
        "+use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n",
        "+of the Software, and to permit persons to whom the Software is furnished to do\n",
        "+so, subject to the following conditions:\n",
        "+\n",
        "+The above copyright notice and this permission notice shall be included in all\n",
        "+copies or substantial portions of the Software.\n",
        "+\n",
        "+THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "+SOFTWARE.\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/PyYAML-6.0.2.dist-info/METADATA",
      "status": "added",
      "additions": 46,
      "deletions": 0,
      "patch": "@@ -0,0 +1,46 @@\n+Metadata-Version: 2.1\n+Name: PyYAML\n+Version: 6.0.2\n+Summary: YAML parser and emitter for Python\n+Home-page: https://pyyaml.org/\n+Download-URL: https://pypi.org/project/PyYAML/\n+Author: Kirill Simonov\n+Author-email: xi@resolvent.net\n+License: MIT\n+Project-URL: Bug Tracker, https://github.com/yaml/pyyaml/issues\n+Project-URL: CI, https://github.com/yaml/pyyaml/actions\n+Project-URL: Documentation, https://pyyaml.org/wiki/PyYAMLDocumentation\n+Project-URL: Mailing lists, http://lists.sourceforge.net/lists/listinfo/yaml-core\n+Project-URL: Source Code, https://github.com/yaml/pyyaml\n+Platform: Any\n+Classifier: Development Status :: 5 - Production/Stable\n+Classifier: Intended Audience :: Developers\n+Classifier: License :: OSI Approved :: MIT License\n+Classifier: Operating System :: OS Independent\n+Classifier: Programming Language :: Cython\n+Classifier: Programming Language :: Python\n+Classifier: Programming Language :: Python :: 3\n+Classifier: Programming Language :: Python :: 3.8\n+Classifier: Programming Language :: Python :: 3.9\n+Classifier: Programming Language :: Python :: 3.10\n+Classifier: Programming Language :: Python :: 3.11\n+Classifier: Programming Language :: Python :: 3.12\n+Classifier: Programming Language :: Python :: 3.13\n+Classifier: Programming Language :: Python :: Implementation :: CPython\n+Classifier: Programming Language :: Python :: Implementation :: PyPy\n+Classifier: Topic :: Software Development :: Libraries :: Python Modules\n+Classifier: Topic :: Text Processing :: Markup\n+Requires-Python: >=3.8\n+License-File: LICENSE\n+\n+YAML is a data serialization format designed for human readability\n+and interaction with scripting languages.  PyYAML is a YAML parser\n+and emitter for Python.\n+\n+PyYAML features a complete YAML 1.1 parser, Unicode support, pickle\n+support, capable extension API, and sensible error messages.  PyYAML\n+supports standard YAML tags and provides Python-specific tags that\n+allow to represent an arbitrary Python object.\n+\n+PyYAML is applicable for a broad range of tasks from complex\n+configuration files to object serialization and persistence.",
      "patch_lines": [
        "@@ -0,0 +1,46 @@\n",
        "+Metadata-Version: 2.1\n",
        "+Name: PyYAML\n",
        "+Version: 6.0.2\n",
        "+Summary: YAML parser and emitter for Python\n",
        "+Home-page: https://pyyaml.org/\n",
        "+Download-URL: https://pypi.org/project/PyYAML/\n",
        "+Author: Kirill Simonov\n",
        "+Author-email: xi@resolvent.net\n",
        "+License: MIT\n",
        "+Project-URL: Bug Tracker, https://github.com/yaml/pyyaml/issues\n",
        "+Project-URL: CI, https://github.com/yaml/pyyaml/actions\n",
        "+Project-URL: Documentation, https://pyyaml.org/wiki/PyYAMLDocumentation\n",
        "+Project-URL: Mailing lists, http://lists.sourceforge.net/lists/listinfo/yaml-core\n",
        "+Project-URL: Source Code, https://github.com/yaml/pyyaml\n",
        "+Platform: Any\n",
        "+Classifier: Development Status :: 5 - Production/Stable\n",
        "+Classifier: Intended Audience :: Developers\n",
        "+Classifier: License :: OSI Approved :: MIT License\n",
        "+Classifier: Operating System :: OS Independent\n",
        "+Classifier: Programming Language :: Cython\n",
        "+Classifier: Programming Language :: Python\n",
        "+Classifier: Programming Language :: Python :: 3\n",
        "+Classifier: Programming Language :: Python :: 3.8\n",
        "+Classifier: Programming Language :: Python :: 3.9\n",
        "+Classifier: Programming Language :: Python :: 3.10\n",
        "+Classifier: Programming Language :: Python :: 3.11\n",
        "+Classifier: Programming Language :: Python :: 3.12\n",
        "+Classifier: Programming Language :: Python :: 3.13\n",
        "+Classifier: Programming Language :: Python :: Implementation :: CPython\n",
        "+Classifier: Programming Language :: Python :: Implementation :: PyPy\n",
        "+Classifier: Topic :: Software Development :: Libraries :: Python Modules\n",
        "+Classifier: Topic :: Text Processing :: Markup\n",
        "+Requires-Python: >=3.8\n",
        "+License-File: LICENSE\n",
        "+\n",
        "+YAML is a data serialization format designed for human readability\n",
        "+and interaction with scripting languages.  PyYAML is a YAML parser\n",
        "+and emitter for Python.\n",
        "+\n",
        "+PyYAML features a complete YAML 1.1 parser, Unicode support, pickle\n",
        "+support, capable extension API, and sensible error messages.  PyYAML\n",
        "+supports standard YAML tags and provides Python-specific tags that\n",
        "+allow to represent an arbitrary Python object.\n",
        "+\n",
        "+PyYAML is applicable for a broad range of tasks from complex\n",
        "+configuration files to object serialization and persistence.\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/PyYAML-6.0.2.dist-info/RECORD",
      "status": "added",
      "additions": 44,
      "deletions": 0,
      "patch": "@@ -0,0 +1,44 @@\n+PyYAML-6.0.2.dist-info/INSTALLER,sha256=zuuue4knoyJ-UwPPXg8fezS7VCrXJQrAP7zeNuwvFQg,4\r\n+PyYAML-6.0.2.dist-info/LICENSE,sha256=jTko-dxEkP1jVwfLiOsmvXZBAqcoKVQwfT5RZ6V36KQ,1101\r\n+PyYAML-6.0.2.dist-info/METADATA,sha256=9-odFB5seu4pGPcEv7E8iyxNF51_uKnaNGjLAhz2lto,2060\r\n+PyYAML-6.0.2.dist-info/RECORD,,\r\n+PyYAML-6.0.2.dist-info/REQUESTED,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0\r\n+PyYAML-6.0.2.dist-info/WHEEL,sha256=1pP4yhrbipRtdbm4Rbg3aoTjzc7pDhpHKO0CEY24CNM,152\r\n+PyYAML-6.0.2.dist-info/top_level.txt,sha256=rpj0IVMTisAjh_1vG3Ccf9v5jpCQwAz6cD1IVU5ZdhQ,11\r\n+_yaml/__init__.py,sha256=04Ae_5osxahpJHa3XBZUAf4wi6XX32gR8D6X6p64GEA,1402\r\n+_yaml/__pycache__/__init__.cpython-312.pyc,,\r\n+yaml/__init__.py,sha256=N35S01HMesFTe0aRRMWkPj0Pa8IEbHpE9FK7cr5Bdtw,12311\r\n+yaml/__pycache__/__init__.cpython-312.pyc,,\r\n+yaml/__pycache__/composer.cpython-312.pyc,,\r\n+yaml/__pycache__/constructor.cpython-312.pyc,,\r\n+yaml/__pycache__/cyaml.cpython-312.pyc,,\r\n+yaml/__pycache__/dumper.cpython-312.pyc,,\r\n+yaml/__pycache__/emitter.cpython-312.pyc,,\r\n+yaml/__pycache__/error.cpython-312.pyc,,\r\n+yaml/__pycache__/events.cpython-312.pyc,,\r\n+yaml/__pycache__/loader.cpython-312.pyc,,\r\n+yaml/__pycache__/nodes.cpython-312.pyc,,\r\n+yaml/__pycache__/parser.cpython-312.pyc,,\r\n+yaml/__pycache__/reader.cpython-312.pyc,,\r\n+yaml/__pycache__/representer.cpython-312.pyc,,\r\n+yaml/__pycache__/resolver.cpython-312.pyc,,\r\n+yaml/__pycache__/scanner.cpython-312.pyc,,\r\n+yaml/__pycache__/serializer.cpython-312.pyc,,\r\n+yaml/__pycache__/tokens.cpython-312.pyc,,\r\n+yaml/_yaml.cpython-312-x86_64-linux-gnu.so,sha256=PJFgxnc0f5Dyde6WKmBm6fZWapawmWl7aBRruXjRA80,2481784\r\n+yaml/composer.py,sha256=_Ko30Wr6eDWUeUpauUGT3Lcg9QPBnOPVlTnIMRGJ9FM,4883\r\n+yaml/constructor.py,sha256=kNgkfaeLUkwQYY_Q6Ff1Tz2XVw_pG1xVE9Ak7z-viLA,28639\r\n+yaml/cyaml.py,sha256=6ZrAG9fAYvdVe2FK_w0hmXoG7ZYsoYUwapG8CiC72H0,3851\r\n+yaml/dumper.py,sha256=PLctZlYwZLp7XmeUdwRuv4nYOZ2UBnDIUy8-lKfLF-o,2837\r\n+yaml/emitter.py,sha256=jghtaU7eFwg31bG0B7RZea_29Adi9CKmXq_QjgQpCkQ,43006\r\n+yaml/error.py,sha256=Ah9z-toHJUbE9j-M8YpxgSRM5CgLCcwVzJgLLRF2Fxo,2533\r\n+yaml/events.py,sha256=50_TksgQiE4up-lKo_V-nBy-tAIxkIPQxY5qDhKCeHw,2445\r\n+yaml/loader.py,sha256=UVa-zIqmkFSCIYq_PgSGm4NSJttHY2Rf_zQ4_b1fHN0,2061\r\n+yaml/nodes.py,sha256=gPKNj8pKCdh2d4gr3gIYINnPOaOxGhJAUiYhGRnPE84,1440\r\n+yaml/parser.py,sha256=ilWp5vvgoHFGzvOZDItFoGjD6D42nhlZrZyjAwa0oJo,25495\r\n+yaml/reader.py,sha256=0dmzirOiDG4Xo41RnuQS7K9rkY3xjHiVasfDMNTqCNw,6794\r\n+yaml/representer.py,sha256=IuWP-cAW9sHKEnS0gCqSa894k1Bg4cgTxaDwIcbRQ-Y,14190\r\n+yaml/resolver.py,sha256=9L-VYfm4mWHxUD1Vg4X7rjDRK_7VZd6b92wzq7Y2IKY,9004\r\n+yaml/scanner.py,sha256=YEM3iLZSaQwXcQRg2l2R4MdT0zGP2F9eHkKGKnHyWQY,51279\r\n+yaml/serializer.py,sha256=ChuFgmhU01hj4xgI8GaKv6vfM2Bujwa9i7d2FAHj7cA,4165\r\n+yaml/tokens.py,sha256=lTQIzSVw8Mg9wv459-TjiOQe6wVziqaRlqX2_89rp54,2573\r",
      "patch_lines": [
        "@@ -0,0 +1,44 @@\n",
        "+PyYAML-6.0.2.dist-info/INSTALLER,sha256=zuuue4knoyJ-UwPPXg8fezS7VCrXJQrAP7zeNuwvFQg,4\r\n",
        "+PyYAML-6.0.2.dist-info/LICENSE,sha256=jTko-dxEkP1jVwfLiOsmvXZBAqcoKVQwfT5RZ6V36KQ,1101\r\n",
        "+PyYAML-6.0.2.dist-info/METADATA,sha256=9-odFB5seu4pGPcEv7E8iyxNF51_uKnaNGjLAhz2lto,2060\r\n",
        "+PyYAML-6.0.2.dist-info/RECORD,,\r\n",
        "+PyYAML-6.0.2.dist-info/REQUESTED,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0\r\n",
        "+PyYAML-6.0.2.dist-info/WHEEL,sha256=1pP4yhrbipRtdbm4Rbg3aoTjzc7pDhpHKO0CEY24CNM,152\r\n",
        "+PyYAML-6.0.2.dist-info/top_level.txt,sha256=rpj0IVMTisAjh_1vG3Ccf9v5jpCQwAz6cD1IVU5ZdhQ,11\r\n",
        "+_yaml/__init__.py,sha256=04Ae_5osxahpJHa3XBZUAf4wi6XX32gR8D6X6p64GEA,1402\r\n",
        "+_yaml/__pycache__/__init__.cpython-312.pyc,,\r\n",
        "+yaml/__init__.py,sha256=N35S01HMesFTe0aRRMWkPj0Pa8IEbHpE9FK7cr5Bdtw,12311\r\n",
        "+yaml/__pycache__/__init__.cpython-312.pyc,,\r\n",
        "+yaml/__pycache__/composer.cpython-312.pyc,,\r\n",
        "+yaml/__pycache__/constructor.cpython-312.pyc,,\r\n",
        "+yaml/__pycache__/cyaml.cpython-312.pyc,,\r\n",
        "+yaml/__pycache__/dumper.cpython-312.pyc,,\r\n",
        "+yaml/__pycache__/emitter.cpython-312.pyc,,\r\n",
        "+yaml/__pycache__/error.cpython-312.pyc,,\r\n",
        "+yaml/__pycache__/events.cpython-312.pyc,,\r\n",
        "+yaml/__pycache__/loader.cpython-312.pyc,,\r\n",
        "+yaml/__pycache__/nodes.cpython-312.pyc,,\r\n",
        "+yaml/__pycache__/parser.cpython-312.pyc,,\r\n",
        "+yaml/__pycache__/reader.cpython-312.pyc,,\r\n",
        "+yaml/__pycache__/representer.cpython-312.pyc,,\r\n",
        "+yaml/__pycache__/resolver.cpython-312.pyc,,\r\n",
        "+yaml/__pycache__/scanner.cpython-312.pyc,,\r\n",
        "+yaml/__pycache__/serializer.cpython-312.pyc,,\r\n",
        "+yaml/__pycache__/tokens.cpython-312.pyc,,\r\n",
        "+yaml/_yaml.cpython-312-x86_64-linux-gnu.so,sha256=PJFgxnc0f5Dyde6WKmBm6fZWapawmWl7aBRruXjRA80,2481784\r\n",
        "+yaml/composer.py,sha256=_Ko30Wr6eDWUeUpauUGT3Lcg9QPBnOPVlTnIMRGJ9FM,4883\r\n",
        "+yaml/constructor.py,sha256=kNgkfaeLUkwQYY_Q6Ff1Tz2XVw_pG1xVE9Ak7z-viLA,28639\r\n",
        "+yaml/cyaml.py,sha256=6ZrAG9fAYvdVe2FK_w0hmXoG7ZYsoYUwapG8CiC72H0,3851\r\n",
        "+yaml/dumper.py,sha256=PLctZlYwZLp7XmeUdwRuv4nYOZ2UBnDIUy8-lKfLF-o,2837\r\n",
        "+yaml/emitter.py,sha256=jghtaU7eFwg31bG0B7RZea_29Adi9CKmXq_QjgQpCkQ,43006\r\n",
        "+yaml/error.py,sha256=Ah9z-toHJUbE9j-M8YpxgSRM5CgLCcwVzJgLLRF2Fxo,2533\r\n",
        "+yaml/events.py,sha256=50_TksgQiE4up-lKo_V-nBy-tAIxkIPQxY5qDhKCeHw,2445\r\n",
        "+yaml/loader.py,sha256=UVa-zIqmkFSCIYq_PgSGm4NSJttHY2Rf_zQ4_b1fHN0,2061\r\n",
        "+yaml/nodes.py,sha256=gPKNj8pKCdh2d4gr3gIYINnPOaOxGhJAUiYhGRnPE84,1440\r\n",
        "+yaml/parser.py,sha256=ilWp5vvgoHFGzvOZDItFoGjD6D42nhlZrZyjAwa0oJo,25495\r\n",
        "+yaml/reader.py,sha256=0dmzirOiDG4Xo41RnuQS7K9rkY3xjHiVasfDMNTqCNw,6794\r\n",
        "+yaml/representer.py,sha256=IuWP-cAW9sHKEnS0gCqSa894k1Bg4cgTxaDwIcbRQ-Y,14190\r\n",
        "+yaml/resolver.py,sha256=9L-VYfm4mWHxUD1Vg4X7rjDRK_7VZd6b92wzq7Y2IKY,9004\r\n",
        "+yaml/scanner.py,sha256=YEM3iLZSaQwXcQRg2l2R4MdT0zGP2F9eHkKGKnHyWQY,51279\r\n",
        "+yaml/serializer.py,sha256=ChuFgmhU01hj4xgI8GaKv6vfM2Bujwa9i7d2FAHj7cA,4165\r\n",
        "+yaml/tokens.py,sha256=lTQIzSVw8Mg9wv459-TjiOQe6wVziqaRlqX2_89rp54,2573\r\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/PyYAML-6.0.2.dist-info/REQUESTED",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/PyYAML-6.0.2.dist-info/WHEEL",
      "status": "added",
      "additions": 6,
      "deletions": 0,
      "patch": "@@ -0,0 +1,6 @@\n+Wheel-Version: 1.0\n+Generator: bdist_wheel (0.44.0)\n+Root-Is-Purelib: false\n+Tag: cp312-cp312-manylinux_2_17_x86_64\n+Tag: cp312-cp312-manylinux2014_x86_64\n+",
      "patch_lines": [
        "@@ -0,0 +1,6 @@\n",
        "+Wheel-Version: 1.0\n",
        "+Generator: bdist_wheel (0.44.0)\n",
        "+Root-Is-Purelib: false\n",
        "+Tag: cp312-cp312-manylinux_2_17_x86_64\n",
        "+Tag: cp312-cp312-manylinux2014_x86_64\n",
        "+\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/PyYAML-6.0.2.dist-info/top_level.txt",
      "status": "added",
      "additions": 2,
      "deletions": 0,
      "patch": "@@ -0,0 +1,2 @@\n+_yaml\n+yaml",
      "patch_lines": [
        "@@ -0,0 +1,2 @@\n",
        "+_yaml\n",
        "+yaml\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/__init__.py",
      "status": "added",
      "additions": 13,
      "deletions": 0,
      "patch": "@@ -0,0 +1,13 @@\n+from __future__ import annotations\n+\n+\n+__all__ = [\"__version__\", \"version_tuple\"]\n+\n+try:\n+    from ._version import version as __version__\n+    from ._version import version_tuple\n+except ImportError:  # pragma: no cover\n+    # broken installation, we don't even try\n+    # unknown only works because we do poor mans version compare\n+    __version__ = \"unknown\"\n+    version_tuple = (0, 0, \"unknown\")",
      "patch_lines": [
        "@@ -0,0 +1,13 @@\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+\n",
        "+__all__ = [\"__version__\", \"version_tuple\"]\n",
        "+\n",
        "+try:\n",
        "+    from ._version import version as __version__\n",
        "+    from ._version import version_tuple\n",
        "+except ImportError:  # pragma: no cover\n",
        "+    # broken installation, we don't even try\n",
        "+    # unknown only works because we do poor mans version compare\n",
        "+    __version__ = \"unknown\"\n",
        "+    version_tuple = (0, 0, \"unknown\")\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/_argcomplete.py",
      "status": "added",
      "additions": 117,
      "deletions": 0,
      "patch": "@@ -0,0 +1,117 @@\n+\"\"\"Allow bash-completion for argparse with argcomplete if installed.\n+\n+Needs argcomplete>=0.5.6 for python 3.2/3.3 (older versions fail\n+to find the magic string, so _ARGCOMPLETE env. var is never set, and\n+this does not need special code).\n+\n+Function try_argcomplete(parser) should be called directly before\n+the call to ArgumentParser.parse_args().\n+\n+The filescompleter is what you normally would use on the positional\n+arguments specification, in order to get \"dirname/\" after \"dirn<TAB>\"\n+instead of the default \"dirname \":\n+\n+   optparser.add_argument(Config._file_or_dir, nargs='*').completer=filescompleter\n+\n+Other, application specific, completers should go in the file\n+doing the add_argument calls as they need to be specified as .completer\n+attributes as well. (If argcomplete is not installed, the function the\n+attribute points to will not be used).\n+\n+SPEEDUP\n+=======\n+\n+The generic argcomplete script for bash-completion\n+(/etc/bash_completion.d/python-argcomplete.sh)\n+uses a python program to determine startup script generated by pip.\n+You can speed up completion somewhat by changing this script to include\n+  # PYTHON_ARGCOMPLETE_OK\n+so the python-argcomplete-check-easy-install-script does not\n+need to be called to find the entry point of the code and see if that is\n+marked  with PYTHON_ARGCOMPLETE_OK.\n+\n+INSTALL/DEBUGGING\n+=================\n+\n+To include this support in another application that has setup.py generated\n+scripts:\n+\n+- Add the line:\n+    # PYTHON_ARGCOMPLETE_OK\n+  near the top of the main python entry point.\n+\n+- Include in the file calling parse_args():\n+    from _argcomplete import try_argcomplete, filescompleter\n+  Call try_argcomplete just before parse_args(), and optionally add\n+  filescompleter to the positional arguments' add_argument().\n+\n+If things do not work right away:\n+\n+- Switch on argcomplete debugging with (also helpful when doing custom\n+  completers):\n+    export _ARC_DEBUG=1\n+\n+- Run:\n+    python-argcomplete-check-easy-install-script $(which appname)\n+    echo $?\n+  will echo 0 if the magic line has been found, 1 if not.\n+\n+- Sometimes it helps to find early on errors using:\n+    _ARGCOMPLETE=1 _ARC_DEBUG=1 appname\n+  which should throw a KeyError: 'COMPLINE' (which is properly set by the\n+  global argcomplete script).\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+from glob import glob\n+import os\n+import sys\n+from typing import Any\n+\n+\n+class FastFilesCompleter:\n+    \"\"\"Fast file completer class.\"\"\"\n+\n+    def __init__(self, directories: bool = True) -> None:\n+        self.directories = directories\n+\n+    def __call__(self, prefix: str, **kwargs: Any) -> list[str]:\n+        # Only called on non option completions.\n+        if os.sep in prefix[1:]:\n+            prefix_dir = len(os.path.dirname(prefix) + os.sep)\n+        else:\n+            prefix_dir = 0\n+        completion = []\n+        globbed = []\n+        if \"*\" not in prefix and \"?\" not in prefix:\n+            # We are on unix, otherwise no bash.\n+            if not prefix or prefix[-1] == os.sep:\n+                globbed.extend(glob(prefix + \".*\"))\n+            prefix += \"*\"\n+        globbed.extend(glob(prefix))\n+        for x in sorted(globbed):\n+            if os.path.isdir(x):\n+                x += \"/\"\n+            # Append stripping the prefix (like bash, not like compgen).\n+            completion.append(x[prefix_dir:])\n+        return completion\n+\n+\n+if os.environ.get(\"_ARGCOMPLETE\"):\n+    try:\n+        import argcomplete.completers\n+    except ImportError:\n+        sys.exit(-1)\n+    filescompleter: FastFilesCompleter | None = FastFilesCompleter()\n+\n+    def try_argcomplete(parser: argparse.ArgumentParser) -> None:\n+        argcomplete.autocomplete(parser, always_complete_options=False)\n+\n+else:\n+\n+    def try_argcomplete(parser: argparse.ArgumentParser) -> None:\n+        pass\n+\n+    filescompleter = None",
      "patch_lines": [
        "@@ -0,0 +1,117 @@\n",
        "+\"\"\"Allow bash-completion for argparse with argcomplete if installed.\n",
        "+\n",
        "+Needs argcomplete>=0.5.6 for python 3.2/3.3 (older versions fail\n",
        "+to find the magic string, so _ARGCOMPLETE env. var is never set, and\n",
        "+this does not need special code).\n",
        "+\n",
        "+Function try_argcomplete(parser) should be called directly before\n",
        "+the call to ArgumentParser.parse_args().\n",
        "+\n",
        "+The filescompleter is what you normally would use on the positional\n",
        "+arguments specification, in order to get \"dirname/\" after \"dirn<TAB>\"\n",
        "+instead of the default \"dirname \":\n",
        "+\n",
        "+   optparser.add_argument(Config._file_or_dir, nargs='*').completer=filescompleter\n",
        "+\n",
        "+Other, application specific, completers should go in the file\n",
        "+doing the add_argument calls as they need to be specified as .completer\n",
        "+attributes as well. (If argcomplete is not installed, the function the\n",
        "+attribute points to will not be used).\n",
        "+\n",
        "+SPEEDUP\n",
        "+=======\n",
        "+\n",
        "+The generic argcomplete script for bash-completion\n",
        "+(/etc/bash_completion.d/python-argcomplete.sh)\n",
        "+uses a python program to determine startup script generated by pip.\n",
        "+You can speed up completion somewhat by changing this script to include\n",
        "+  # PYTHON_ARGCOMPLETE_OK\n",
        "+so the python-argcomplete-check-easy-install-script does not\n",
        "+need to be called to find the entry point of the code and see if that is\n",
        "+marked  with PYTHON_ARGCOMPLETE_OK.\n",
        "+\n",
        "+INSTALL/DEBUGGING\n",
        "+=================\n",
        "+\n",
        "+To include this support in another application that has setup.py generated\n",
        "+scripts:\n",
        "+\n",
        "+- Add the line:\n",
        "+    # PYTHON_ARGCOMPLETE_OK\n",
        "+  near the top of the main python entry point.\n",
        "+\n",
        "+- Include in the file calling parse_args():\n",
        "+    from _argcomplete import try_argcomplete, filescompleter\n",
        "+  Call try_argcomplete just before parse_args(), and optionally add\n",
        "+  filescompleter to the positional arguments' add_argument().\n",
        "+\n",
        "+If things do not work right away:\n",
        "+\n",
        "+- Switch on argcomplete debugging with (also helpful when doing custom\n",
        "+  completers):\n",
        "+    export _ARC_DEBUG=1\n",
        "+\n",
        "+- Run:\n",
        "+    python-argcomplete-check-easy-install-script $(which appname)\n",
        "+    echo $?\n",
        "+  will echo 0 if the magic line has been found, 1 if not.\n",
        "+\n",
        "+- Sometimes it helps to find early on errors using:\n",
        "+    _ARGCOMPLETE=1 _ARC_DEBUG=1 appname\n",
        "+  which should throw a KeyError: 'COMPLINE' (which is properly set by the\n",
        "+  global argcomplete script).\n",
        "+\"\"\"\n",
        "+\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+import argparse\n",
        "+from glob import glob\n",
        "+import os\n",
        "+import sys\n",
        "+from typing import Any\n",
        "+\n",
        "+\n",
        "+class FastFilesCompleter:\n",
        "+    \"\"\"Fast file completer class.\"\"\"\n",
        "+\n",
        "+    def __init__(self, directories: bool = True) -> None:\n",
        "+        self.directories = directories\n",
        "+\n",
        "+    def __call__(self, prefix: str, **kwargs: Any) -> list[str]:\n",
        "+        # Only called on non option completions.\n",
        "+        if os.sep in prefix[1:]:\n",
        "+            prefix_dir = len(os.path.dirname(prefix) + os.sep)\n",
        "+        else:\n",
        "+            prefix_dir = 0\n",
        "+        completion = []\n",
        "+        globbed = []\n",
        "+        if \"*\" not in prefix and \"?\" not in prefix:\n",
        "+            # We are on unix, otherwise no bash.\n",
        "+            if not prefix or prefix[-1] == os.sep:\n",
        "+                globbed.extend(glob(prefix + \".*\"))\n",
        "+            prefix += \"*\"\n",
        "+        globbed.extend(glob(prefix))\n",
        "+        for x in sorted(globbed):\n",
        "+            if os.path.isdir(x):\n",
        "+                x += \"/\"\n",
        "+            # Append stripping the prefix (like bash, not like compgen).\n",
        "+            completion.append(x[prefix_dir:])\n",
        "+        return completion\n",
        "+\n",
        "+\n",
        "+if os.environ.get(\"_ARGCOMPLETE\"):\n",
        "+    try:\n",
        "+        import argcomplete.completers\n",
        "+    except ImportError:\n",
        "+        sys.exit(-1)\n",
        "+    filescompleter: FastFilesCompleter | None = FastFilesCompleter()\n",
        "+\n",
        "+    def try_argcomplete(parser: argparse.ArgumentParser) -> None:\n",
        "+        argcomplete.autocomplete(parser, always_complete_options=False)\n",
        "+\n",
        "+else:\n",
        "+\n",
        "+    def try_argcomplete(parser: argparse.ArgumentParser) -> None:\n",
        "+        pass\n",
        "+\n",
        "+    filescompleter = None\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/_code/__init__.py",
      "status": "added",
      "additions": 26,
      "deletions": 0,
      "patch": "@@ -0,0 +1,26 @@\n+\"\"\"Python inspection/code generation API.\"\"\"\n+\n+from __future__ import annotations\n+\n+from .code import Code\n+from .code import ExceptionInfo\n+from .code import filter_traceback\n+from .code import Frame\n+from .code import getfslineno\n+from .code import Traceback\n+from .code import TracebackEntry\n+from .source import getrawcode\n+from .source import Source\n+\n+\n+__all__ = [\n+    \"Code\",\n+    \"ExceptionInfo\",\n+    \"Frame\",\n+    \"Source\",\n+    \"Traceback\",\n+    \"TracebackEntry\",\n+    \"filter_traceback\",\n+    \"getfslineno\",\n+    \"getrawcode\",\n+]",
      "patch_lines": [
        "@@ -0,0 +1,26 @@\n",
        "+\"\"\"Python inspection/code generation API.\"\"\"\n",
        "+\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+from .code import Code\n",
        "+from .code import ExceptionInfo\n",
        "+from .code import filter_traceback\n",
        "+from .code import Frame\n",
        "+from .code import getfslineno\n",
        "+from .code import Traceback\n",
        "+from .code import TracebackEntry\n",
        "+from .source import getrawcode\n",
        "+from .source import Source\n",
        "+\n",
        "+\n",
        "+__all__ = [\n",
        "+    \"Code\",\n",
        "+    \"ExceptionInfo\",\n",
        "+    \"Frame\",\n",
        "+    \"Source\",\n",
        "+    \"Traceback\",\n",
        "+    \"TracebackEntry\",\n",
        "+    \"filter_traceback\",\n",
        "+    \"getfslineno\",\n",
        "+    \"getrawcode\",\n",
        "+]\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/_code/code.py",
      "status": "added",
      "additions": 1567,
      "deletions": 0,
      "patch": "@@ -0,0 +1,1567 @@\n+# mypy: allow-untyped-defs\n+from __future__ import annotations\n+\n+import ast\n+from collections.abc import Callable\n+from collections.abc import Iterable\n+from collections.abc import Mapping\n+from collections.abc import Sequence\n+import dataclasses\n+import inspect\n+from inspect import CO_VARARGS\n+from inspect import CO_VARKEYWORDS\n+from io import StringIO\n+import os\n+from pathlib import Path\n+import re\n+import sys\n+from traceback import extract_tb\n+from traceback import format_exception\n+from traceback import format_exception_only\n+from traceback import FrameSummary\n+from types import CodeType\n+from types import FrameType\n+from types import TracebackType\n+from typing import Any\n+from typing import ClassVar\n+from typing import Final\n+from typing import final\n+from typing import Generic\n+from typing import Literal\n+from typing import overload\n+from typing import SupportsIndex\n+from typing import TYPE_CHECKING\n+from typing import TypeVar\n+from typing import Union\n+\n+import pluggy\n+\n+import _pytest\n+from _pytest._code.source import findsource\n+from _pytest._code.source import getrawcode\n+from _pytest._code.source import getstatementrange_ast\n+from _pytest._code.source import Source\n+from _pytest._io import TerminalWriter\n+from _pytest._io.saferepr import safeformat\n+from _pytest._io.saferepr import saferepr\n+from _pytest.compat import get_real_func\n+from _pytest.deprecated import check_ispytest\n+from _pytest.pathlib import absolutepath\n+from _pytest.pathlib import bestrelpath\n+\n+\n+if sys.version_info < (3, 11):\n+    from exceptiongroup import BaseExceptionGroup\n+\n+TracebackStyle = Literal[\"long\", \"short\", \"line\", \"no\", \"native\", \"value\", \"auto\"]\n+\n+EXCEPTION_OR_MORE = Union[type[BaseException], tuple[type[BaseException], ...]]\n+\n+\n+class Code:\n+    \"\"\"Wrapper around Python code objects.\"\"\"\n+\n+    __slots__ = (\"raw\",)\n+\n+    def __init__(self, obj: CodeType) -> None:\n+        self.raw = obj\n+\n+    @classmethod\n+    def from_function(cls, obj: object) -> Code:\n+        return cls(getrawcode(obj))\n+\n+    def __eq__(self, other):\n+        return self.raw == other.raw\n+\n+    # Ignore type because of https://github.com/python/mypy/issues/4266.\n+    __hash__ = None  # type: ignore\n+\n+    @property\n+    def firstlineno(self) -> int:\n+        return self.raw.co_firstlineno - 1\n+\n+    @property\n+    def name(self) -> str:\n+        return self.raw.co_name\n+\n+    @property\n+    def path(self) -> Path | str:\n+        \"\"\"Return a path object pointing to source code, or an ``str`` in\n+        case of ``OSError`` / non-existing file.\"\"\"\n+        if not self.raw.co_filename:\n+            return \"\"\n+        try:\n+            p = absolutepath(self.raw.co_filename)\n+            # maybe don't try this checking\n+            if not p.exists():\n+                raise OSError(\"path check failed.\")\n+            return p\n+        except OSError:\n+            # XXX maybe try harder like the weird logic\n+            # in the standard lib [linecache.updatecache] does?\n+            return self.raw.co_filename\n+\n+    @property\n+    def fullsource(self) -> Source | None:\n+        \"\"\"Return a _pytest._code.Source object for the full source file of the code.\"\"\"\n+        full, _ = findsource(self.raw)\n+        return full\n+\n+    def source(self) -> Source:\n+        \"\"\"Return a _pytest._code.Source object for the code object's source only.\"\"\"\n+        # return source only for that part of code\n+        return Source(self.raw)\n+\n+    def getargs(self, var: bool = False) -> tuple[str, ...]:\n+        \"\"\"Return a tuple with the argument names for the code object.\n+\n+        If 'var' is set True also return the names of the variable and\n+        keyword arguments when present.\n+        \"\"\"\n+        # Handy shortcut for getting args.\n+        raw = self.raw\n+        argcount = raw.co_argcount\n+        if var:\n+            argcount += raw.co_flags & CO_VARARGS\n+            argcount += raw.co_flags & CO_VARKEYWORDS\n+        return raw.co_varnames[:argcount]\n+\n+\n+class Frame:\n+    \"\"\"Wrapper around a Python frame holding f_locals and f_globals\n+    in which expressions can be evaluated.\"\"\"\n+\n+    __slots__ = (\"raw\",)\n+\n+    def __init__(self, frame: FrameType) -> None:\n+        self.raw = frame\n+\n+    @property\n+    def lineno(self) -> int:\n+        return self.raw.f_lineno - 1\n+\n+    @property\n+    def f_globals(self) -> dict[str, Any]:\n+        return self.raw.f_globals\n+\n+    @property\n+    def f_locals(self) -> dict[str, Any]:\n+        return self.raw.f_locals\n+\n+    @property\n+    def code(self) -> Code:\n+        return Code(self.raw.f_code)\n+\n+    @property\n+    def statement(self) -> Source:\n+        \"\"\"Statement this frame is at.\"\"\"\n+        if self.code.fullsource is None:\n+            return Source(\"\")\n+        return self.code.fullsource.getstatement(self.lineno)\n+\n+    def eval(self, code, **vars):\n+        \"\"\"Evaluate 'code' in the frame.\n+\n+        'vars' are optional additional local variables.\n+\n+        Returns the result of the evaluation.\n+        \"\"\"\n+        f_locals = self.f_locals.copy()\n+        f_locals.update(vars)\n+        return eval(code, self.f_globals, f_locals)\n+\n+    def repr(self, object: object) -> str:\n+        \"\"\"Return a 'safe' (non-recursive, one-line) string repr for 'object'.\"\"\"\n+        return saferepr(object)\n+\n+    def getargs(self, var: bool = False):\n+        \"\"\"Return a list of tuples (name, value) for all arguments.\n+\n+        If 'var' is set True, also include the variable and keyword arguments\n+        when present.\n+        \"\"\"\n+        retval = []\n+        for arg in self.code.getargs(var):\n+            try:\n+                retval.append((arg, self.f_locals[arg]))\n+            except KeyError:\n+                pass  # this can occur when using Psyco\n+        return retval\n+\n+\n+class TracebackEntry:\n+    \"\"\"A single entry in a Traceback.\"\"\"\n+\n+    __slots__ = (\"_rawentry\", \"_repr_style\")\n+\n+    def __init__(\n+        self,\n+        rawentry: TracebackType,\n+        repr_style: Literal[\"short\", \"long\"] | None = None,\n+    ) -> None:\n+        self._rawentry: Final = rawentry\n+        self._repr_style: Final = repr_style\n+\n+    def with_repr_style(\n+        self, repr_style: Literal[\"short\", \"long\"] | None\n+    ) -> TracebackEntry:\n+        return TracebackEntry(self._rawentry, repr_style)\n+\n+    @property\n+    def lineno(self) -> int:\n+        return self._rawentry.tb_lineno - 1\n+\n+    def get_python_framesummary(self) -> FrameSummary:\n+        # Python's built-in traceback module implements all the nitty gritty\n+        # details to get column numbers of out frames.\n+        stack_summary = extract_tb(self._rawentry, limit=1)\n+        return stack_summary[0]\n+\n+    # Column and end line numbers introduced in python 3.11\n+    if sys.version_info < (3, 11):\n+\n+        @property\n+        def end_lineno_relative(self) -> int | None:\n+            return None\n+\n+        @property\n+        def colno(self) -> int | None:\n+            return None\n+\n+        @property\n+        def end_colno(self) -> int | None:\n+            return None\n+    else:\n+\n+        @property\n+        def end_lineno_relative(self) -> int | None:\n+            frame_summary = self.get_python_framesummary()\n+            if frame_summary.end_lineno is None:  # pragma: no cover\n+                return None\n+            return frame_summary.end_lineno - 1 - self.frame.code.firstlineno\n+\n+        @property\n+        def colno(self) -> int | None:\n+            \"\"\"Starting byte offset of the expression in the traceback entry.\"\"\"\n+            return self.get_python_framesummary().colno\n+\n+        @property\n+        def end_colno(self) -> int | None:\n+            \"\"\"Ending byte offset of the expression in the traceback entry.\"\"\"\n+            return self.get_python_framesummary().end_colno\n+\n+    @property\n+    def frame(self) -> Frame:\n+        return Frame(self._rawentry.tb_frame)\n+\n+    @property\n+    def relline(self) -> int:\n+        return self.lineno - self.frame.code.firstlineno\n+\n+    def __repr__(self) -> str:\n+        return f\"<TracebackEntry {self.frame.code.path}:{self.lineno + 1}>\"\n+\n+    @property\n+    def statement(self) -> Source:\n+        \"\"\"_pytest._code.Source object for the current statement.\"\"\"\n+        source = self.frame.code.fullsource\n+        assert source is not None\n+        return source.getstatement(self.lineno)\n+\n+    @property\n+    def path(self) -> Path | str:\n+        \"\"\"Path to the source code.\"\"\"\n+        return self.frame.code.path\n+\n+    @property\n+    def locals(self) -> dict[str, Any]:\n+        \"\"\"Locals of underlying frame.\"\"\"\n+        return self.frame.f_locals\n+\n+    def getfirstlinesource(self) -> int:\n+        return self.frame.code.firstlineno\n+\n+    def getsource(\n+        self, astcache: dict[str | Path, ast.AST] | None = None\n+    ) -> Source | None:\n+        \"\"\"Return failing source code.\"\"\"\n+        # we use the passed in astcache to not reparse asttrees\n+        # within exception info printing\n+        source = self.frame.code.fullsource\n+        if source is None:\n+            return None\n+        key = astnode = None\n+        if astcache is not None:\n+            key = self.frame.code.path\n+            if key is not None:\n+                astnode = astcache.get(key, None)\n+        start = self.getfirstlinesource()\n+        try:\n+            astnode, _, end = getstatementrange_ast(\n+                self.lineno, source, astnode=astnode\n+            )\n+        except SyntaxError:\n+            end = self.lineno + 1\n+        else:\n+            if key is not None and astcache is not None:\n+                astcache[key] = astnode\n+        return source[start:end]\n+\n+    source = property(getsource)\n+\n+    def ishidden(self, excinfo: ExceptionInfo[BaseException] | None) -> bool:\n+        \"\"\"Return True if the current frame has a var __tracebackhide__\n+        resolving to True.\n+\n+        If __tracebackhide__ is a callable, it gets called with the\n+        ExceptionInfo instance and can decide whether to hide the traceback.\n+\n+        Mostly for internal use.\n+        \"\"\"\n+        tbh: bool | Callable[[ExceptionInfo[BaseException] | None], bool] = False\n+        for maybe_ns_dct in (self.frame.f_locals, self.frame.f_globals):\n+            # in normal cases, f_locals and f_globals are dictionaries\n+            # however via `exec(...)` / `eval(...)` they can be other types\n+            # (even incorrect types!).\n+            # as such, we suppress all exceptions while accessing __tracebackhide__\n+            try:\n+                tbh = maybe_ns_dct[\"__tracebackhide__\"]\n+            except Exception:\n+                pass\n+            else:\n+                break\n+        if tbh and callable(tbh):\n+            return tbh(excinfo)\n+        return tbh\n+\n+    def __str__(self) -> str:\n+        name = self.frame.code.name\n+        try:\n+            line = str(self.statement).lstrip()\n+        except KeyboardInterrupt:\n+            raise\n+        except BaseException:\n+            line = \"???\"\n+        # This output does not quite match Python's repr for traceback entries,\n+        # but changing it to do so would break certain plugins.  See\n+        # https://github.com/pytest-dev/pytest/pull/7535/ for details.\n+        return f\"  File '{self.path}':{self.lineno + 1} in {name}\\n  {line}\\n\"\n+\n+    @property\n+    def name(self) -> str:\n+        \"\"\"co_name of underlying code.\"\"\"\n+        return self.frame.code.raw.co_name\n+\n+\n+class Traceback(list[TracebackEntry]):\n+    \"\"\"Traceback objects encapsulate and offer higher level access to Traceback entries.\"\"\"\n+\n+    def __init__(\n+        self,\n+        tb: TracebackType | Iterable[TracebackEntry],\n+    ) -> None:\n+        \"\"\"Initialize from given python traceback object and ExceptionInfo.\"\"\"\n+        if isinstance(tb, TracebackType):\n+\n+            def f(cur: TracebackType) -> Iterable[TracebackEntry]:\n+                cur_: TracebackType | None = cur\n+                while cur_ is not None:\n+                    yield TracebackEntry(cur_)\n+                    cur_ = cur_.tb_next\n+\n+            super().__init__(f(tb))\n+        else:\n+            super().__init__(tb)\n+\n+    def cut(\n+        self,\n+        path: os.PathLike[str] | str | None = None,\n+        lineno: int | None = None,\n+        firstlineno: int | None = None,\n+        excludepath: os.PathLike[str] | None = None,\n+    ) -> Traceback:\n+        \"\"\"Return a Traceback instance wrapping part of this Traceback.\n+\n+        By providing any combination of path, lineno and firstlineno, the\n+        first frame to start the to-be-returned traceback is determined.\n+\n+        This allows cutting the first part of a Traceback instance e.g.\n+        for formatting reasons (removing some uninteresting bits that deal\n+        with handling of the exception/traceback).\n+        \"\"\"\n+        path_ = None if path is None else os.fspath(path)\n+        excludepath_ = None if excludepath is None else os.fspath(excludepath)\n+        for x in self:\n+            code = x.frame.code\n+            codepath = code.path\n+            if path is not None and str(codepath) != path_:\n+                continue\n+            if (\n+                excludepath is not None\n+                and isinstance(codepath, Path)\n+                and excludepath_ in (str(p) for p in codepath.parents)  # type: ignore[operator]\n+            ):\n+                continue\n+            if lineno is not None and x.lineno != lineno:\n+                continue\n+            if firstlineno is not None and x.frame.code.firstlineno != firstlineno:\n+                continue\n+            return Traceback(x._rawentry)\n+        return self\n+\n+    @overload\n+    def __getitem__(self, key: SupportsIndex) -> TracebackEntry: ...\n+\n+    @overload\n+    def __getitem__(self, key: slice) -> Traceback: ...\n+\n+    def __getitem__(self, key: SupportsIndex | slice) -> TracebackEntry | Traceback:\n+        if isinstance(key, slice):\n+            return self.__class__(super().__getitem__(key))\n+        else:\n+            return super().__getitem__(key)\n+\n+    def filter(\n+        self,\n+        excinfo_or_fn: ExceptionInfo[BaseException] | Callable[[TracebackEntry], bool],\n+        /,\n+    ) -> Traceback:\n+        \"\"\"Return a Traceback instance with certain items removed.\n+\n+        If the filter is an `ExceptionInfo`, removes all the ``TracebackEntry``s\n+        which are hidden (see ishidden() above).\n+\n+        Otherwise, the filter is a function that gets a single argument, a\n+        ``TracebackEntry`` instance, and should return True when the item should\n+        be added to the ``Traceback``, False when not.\n+        \"\"\"\n+        if isinstance(excinfo_or_fn, ExceptionInfo):\n+            fn = lambda x: not x.ishidden(excinfo_or_fn)  # noqa: E731\n+        else:\n+            fn = excinfo_or_fn\n+        return Traceback(filter(fn, self))\n+\n+    def recursionindex(self) -> int | None:\n+        \"\"\"Return the index of the frame/TracebackEntry where recursion originates if\n+        appropriate, None if no recursion occurred.\"\"\"\n+        cache: dict[tuple[Any, int, int], list[dict[str, Any]]] = {}\n+        for i, entry in enumerate(self):\n+            # id for the code.raw is needed to work around\n+            # the strange metaprogramming in the decorator lib from pypi\n+            # which generates code objects that have hash/value equality\n+            # XXX needs a test\n+            key = entry.frame.code.path, id(entry.frame.code.raw), entry.lineno\n+            values = cache.setdefault(key, [])\n+            # Since Python 3.13 f_locals is a proxy, freeze it.\n+            loc = dict(entry.frame.f_locals)\n+            if values:\n+                for otherloc in values:\n+                    if otherloc == loc:\n+                        return i\n+            values.append(loc)\n+        return None\n+\n+\n+def stringify_exception(\n+    exc: BaseException, include_subexception_msg: bool = True\n+) -> str:\n+    try:\n+        notes = getattr(exc, \"__notes__\", [])\n+    except KeyError:\n+        # Workaround for https://github.com/python/cpython/issues/98778 on\n+        # Python <= 3.9, and some 3.10 and 3.11 patch versions.\n+        HTTPError = getattr(sys.modules.get(\"urllib.error\", None), \"HTTPError\", ())\n+        if sys.version_info < (3, 12) and isinstance(exc, HTTPError):\n+            notes = []\n+        else:  # pragma: no cover\n+            # exception not related to above bug, reraise\n+            raise\n+    if not include_subexception_msg and isinstance(exc, BaseExceptionGroup):\n+        message = exc.message\n+    else:\n+        message = str(exc)\n+\n+    return \"\\n\".join(\n+        [\n+            message,\n+            *notes,\n+        ]\n+    )\n+\n+\n+E = TypeVar(\"E\", bound=BaseException, covariant=True)\n+\n+\n+@final\n+@dataclasses.dataclass\n+class ExceptionInfo(Generic[E]):\n+    \"\"\"Wraps sys.exc_info() objects and offers help for navigating the traceback.\"\"\"\n+\n+    _assert_start_repr: ClassVar = \"AssertionError('assert \"\n+\n+    _excinfo: tuple[type[E], E, TracebackType] | None\n+    _striptext: str\n+    _traceback: Traceback | None\n+\n+    def __init__(\n+        self,\n+        excinfo: tuple[type[E], E, TracebackType] | None,\n+        striptext: str = \"\",\n+        traceback: Traceback | None = None,\n+        *,\n+        _ispytest: bool = False,\n+    ) -> None:\n+        check_ispytest(_ispytest)\n+        self._excinfo = excinfo\n+        self._striptext = striptext\n+        self._traceback = traceback\n+\n+    @classmethod\n+    def from_exception(\n+        cls,\n+        # Ignoring error: \"Cannot use a covariant type variable as a parameter\".\n+        # This is OK to ignore because this class is (conceptually) readonly.\n+        # See https://github.com/python/mypy/issues/7049.\n+        exception: E,  # type: ignore[misc]\n+        exprinfo: str | None = None,\n+    ) -> ExceptionInfo[E]:\n+        \"\"\"Return an ExceptionInfo for an existing exception.\n+\n+        The exception must have a non-``None`` ``__traceback__`` attribute,\n+        otherwise this function fails with an assertion error. This means that\n+        the exception must have been raised, or added a traceback with the\n+        :py:meth:`~BaseException.with_traceback()` method.\n+\n+        :param exprinfo:\n+            A text string helping to determine if we should strip\n+            ``AssertionError`` from the output. Defaults to the exception\n+            message/``__str__()``.\n+\n+        .. versionadded:: 7.4\n+        \"\"\"\n+        assert exception.__traceback__, (\n+            \"Exceptions passed to ExcInfo.from_exception(...)\"\n+            \" must have a non-None __traceback__.\"\n+        )\n+        exc_info = (type(exception), exception, exception.__traceback__)\n+        return cls.from_exc_info(exc_info, exprinfo)\n+\n+    @classmethod\n+    def from_exc_info(\n+        cls,\n+        exc_info: tuple[type[E], E, TracebackType],\n+        exprinfo: str | None = None,\n+    ) -> ExceptionInfo[E]:\n+        \"\"\"Like :func:`from_exception`, but using old-style exc_info tuple.\"\"\"\n+        _striptext = \"\"\n+        if exprinfo is None and isinstance(exc_info[1], AssertionError):\n+            exprinfo = getattr(exc_info[1], \"msg\", None)\n+            if exprinfo is None:\n+                exprinfo = saferepr(exc_info[1])\n+            if exprinfo and exprinfo.startswith(cls._assert_start_repr):\n+                _striptext = \"AssertionError: \"\n+\n+        return cls(exc_info, _striptext, _ispytest=True)\n+\n+    @classmethod\n+    def from_current(cls, exprinfo: str | None = None) -> ExceptionInfo[BaseException]:\n+        \"\"\"Return an ExceptionInfo matching the current traceback.\n+\n+        .. warning::\n+\n+            Experimental API\n+\n+        :param exprinfo:\n+            A text string helping to determine if we should strip\n+            ``AssertionError`` from the output. Defaults to the exception\n+            message/``__str__()``.\n+        \"\"\"\n+        tup = sys.exc_info()\n+        assert tup[0] is not None, \"no current exception\"\n+        assert tup[1] is not None, \"no current exception\"\n+        assert tup[2] is not None, \"no current exception\"\n+        exc_info = (tup[0], tup[1], tup[2])\n+        return ExceptionInfo.from_exc_info(exc_info, exprinfo)\n+\n+    @classmethod\n+    def for_later(cls) -> ExceptionInfo[E]:\n+        \"\"\"Return an unfilled ExceptionInfo.\"\"\"\n+        return cls(None, _ispytest=True)\n+\n+    def fill_unfilled(self, exc_info: tuple[type[E], E, TracebackType]) -> None:\n+        \"\"\"Fill an unfilled ExceptionInfo created with ``for_later()``.\"\"\"\n+        assert self._excinfo is None, \"ExceptionInfo was already filled\"\n+        self._excinfo = exc_info\n+\n+    @property\n+    def type(self) -> type[E]:\n+        \"\"\"The exception class.\"\"\"\n+        assert self._excinfo is not None, (\n+            \".type can only be used after the context manager exits\"\n+        )\n+        return self._excinfo[0]\n+\n+    @property\n+    def value(self) -> E:\n+        \"\"\"The exception value.\"\"\"\n+        assert self._excinfo is not None, (\n+            \".value can only be used after the context manager exits\"\n+        )\n+        return self._excinfo[1]\n+\n+    @property\n+    def tb(self) -> TracebackType:\n+        \"\"\"The exception raw traceback.\"\"\"\n+        assert self._excinfo is not None, (\n+            \".tb can only be used after the context manager exits\"\n+        )\n+        return self._excinfo[2]\n+\n+    @property\n+    def typename(self) -> str:\n+        \"\"\"The type name of the exception.\"\"\"\n+        assert self._excinfo is not None, (\n+            \".typename can only be used after the context manager exits\"\n+        )\n+        return self.type.__name__\n+\n+    @property\n+    def traceback(self) -> Traceback:\n+        \"\"\"The traceback.\"\"\"\n+        if self._traceback is None:\n+            self._traceback = Traceback(self.tb)\n+        return self._traceback\n+\n+    @traceback.setter\n+    def traceback(self, value: Traceback) -> None:\n+        self._traceback = value\n+\n+    def __repr__(self) -> str:\n+        if self._excinfo is None:\n+            return \"<ExceptionInfo for raises contextmanager>\"\n+        return f\"<{self.__class__.__name__} {saferepr(self._excinfo[1])} tblen={len(self.traceback)}>\"\n+\n+    def exconly(self, tryshort: bool = False) -> str:\n+        \"\"\"Return the exception as a string.\n+\n+        When 'tryshort' resolves to True, and the exception is an\n+        AssertionError, only the actual exception part of the exception\n+        representation is returned (so 'AssertionError: ' is removed from\n+        the beginning).\n+        \"\"\"\n+\n+        def _get_single_subexc(\n+            eg: BaseExceptionGroup[BaseException],\n+        ) -> BaseException | None:\n+            if len(eg.exceptions) != 1:\n+                return None\n+            if isinstance(e := eg.exceptions[0], BaseExceptionGroup):\n+                return _get_single_subexc(e)\n+            return e\n+\n+        if (\n+            tryshort\n+            and isinstance(self.value, BaseExceptionGroup)\n+            and (subexc := _get_single_subexc(self.value)) is not None\n+        ):\n+            return f\"{subexc!r} [single exception in {type(self.value).__name__}]\"\n+\n+        lines = format_exception_only(self.type, self.value)\n+        text = \"\".join(lines)\n+        text = text.rstrip()\n+        if tryshort:\n+            if text.startswith(self._striptext):\n+                text = text[len(self._striptext) :]\n+        return text\n+\n+    def errisinstance(self, exc: EXCEPTION_OR_MORE) -> bool:\n+        \"\"\"Return True if the exception is an instance of exc.\n+\n+        Consider using ``isinstance(excinfo.value, exc)`` instead.\n+        \"\"\"\n+        return isinstance(self.value, exc)\n+\n+    def _getreprcrash(self) -> ReprFileLocation | None:\n+        # Find last non-hidden traceback entry that led to the exception of the\n+        # traceback, or None if all hidden.\n+        for i in range(-1, -len(self.traceback) - 1, -1):\n+            entry = self.traceback[i]\n+            if not entry.ishidden(self):\n+                path, lineno = entry.frame.code.raw.co_filename, entry.lineno\n+                exconly = self.exconly(tryshort=True)\n+                return ReprFileLocation(path, lineno + 1, exconly)\n+        return None\n+\n+    def getrepr(\n+        self,\n+        showlocals: bool = False,\n+        style: TracebackStyle = \"long\",\n+        abspath: bool = False,\n+        tbfilter: bool | Callable[[ExceptionInfo[BaseException]], Traceback] = True,\n+        funcargs: bool = False,\n+        truncate_locals: bool = True,\n+        truncate_args: bool = True,\n+        chain: bool = True,\n+    ) -> ReprExceptionInfo | ExceptionChainRepr:\n+        \"\"\"Return str()able representation of this exception info.\n+\n+        :param bool showlocals:\n+            Show locals per traceback entry.\n+            Ignored if ``style==\"native\"``.\n+\n+        :param str style:\n+            long|short|line|no|native|value traceback style.\n+\n+        :param bool abspath:\n+            If paths should be changed to absolute or left unchanged.\n+\n+        :param tbfilter:\n+            A filter for traceback entries.\n+\n+            * If false, don't hide any entries.\n+            * If true, hide internal entries and entries that contain a local\n+              variable ``__tracebackhide__ = True``.\n+            * If a callable, delegates the filtering to the callable.\n+\n+            Ignored if ``style`` is ``\"native\"``.\n+\n+        :param bool funcargs:\n+            Show fixtures (\"funcargs\" for legacy purposes) per traceback entry.\n+\n+        :param bool truncate_locals:\n+            With ``showlocals==True``, make sure locals can be safely represented as strings.\n+\n+        :param bool truncate_args:\n+            With ``showargs==True``, make sure args can be safely represented as strings.\n+\n+        :param bool chain:\n+            If chained exceptions in Python 3 should be shown.\n+\n+        .. versionchanged:: 3.9\n+\n+            Added the ``chain`` parameter.\n+        \"\"\"\n+        if style == \"native\":\n+            return ReprExceptionInfo(\n+                reprtraceback=ReprTracebackNative(\n+                    format_exception(\n+                        self.type,\n+                        self.value,\n+                        self.traceback[0]._rawentry if self.traceback else None,\n+                    )\n+                ),\n+                reprcrash=self._getreprcrash(),\n+            )\n+\n+        fmt = FormattedExcinfo(\n+            showlocals=showlocals,\n+            style=style,\n+            abspath=abspath,\n+            tbfilter=tbfilter,\n+            funcargs=funcargs,\n+            truncate_locals=truncate_locals,\n+            truncate_args=truncate_args,\n+            chain=chain,\n+        )\n+        return fmt.repr_excinfo(self)\n+\n+    def match(self, regexp: str | re.Pattern[str]) -> Literal[True]:\n+        \"\"\"Check whether the regular expression `regexp` matches the string\n+        representation of the exception using :func:`python:re.search`.\n+\n+        If it matches `True` is returned, otherwise an `AssertionError` is raised.\n+        \"\"\"\n+        __tracebackhide__ = True\n+        value = stringify_exception(self.value)\n+        msg = f\"Regex pattern did not match.\\n Regex: {regexp!r}\\n Input: {value!r}\"\n+        if regexp == value:\n+            msg += \"\\n Did you mean to `re.escape()` the regex?\"\n+        assert re.search(regexp, value), msg\n+        # Return True to allow for \"assert excinfo.match()\".\n+        return True\n+\n+    def _group_contains(\n+        self,\n+        exc_group: BaseExceptionGroup[BaseException],\n+        expected_exception: EXCEPTION_OR_MORE,\n+        match: str | re.Pattern[str] | None,\n+        target_depth: int | None = None,\n+        current_depth: int = 1,\n+    ) -> bool:\n+        \"\"\"Return `True` if a `BaseExceptionGroup` contains a matching exception.\"\"\"\n+        if (target_depth is not None) and (current_depth > target_depth):\n+            # already descended past the target depth\n+            return False\n+        for exc in exc_group.exceptions:\n+            if isinstance(exc, BaseExceptionGroup):\n+                if self._group_contains(\n+                    exc, expected_exception, match, target_depth, current_depth + 1\n+                ):\n+                    return True\n+            if (target_depth is not None) and (current_depth != target_depth):\n+                # not at the target depth, no match\n+                continue\n+            if not isinstance(exc, expected_exception):\n+                continue\n+            if match is not None:\n+                value = stringify_exception(exc)\n+                if not re.search(match, value):\n+                    continue\n+            return True\n+        return False\n+\n+    def group_contains(\n+        self,\n+        expected_exception: EXCEPTION_OR_MORE,\n+        *,\n+        match: str | re.Pattern[str] | None = None,\n+        depth: int | None = None,\n+    ) -> bool:\n+        \"\"\"Check whether a captured exception group contains a matching exception.\n+\n+        :param Type[BaseException] | Tuple[Type[BaseException]] expected_exception:\n+            The expected exception type, or a tuple if one of multiple possible\n+            exception types are expected.\n+\n+        :param str | re.Pattern[str] | None match:\n+            If specified, a string containing a regular expression,\n+            or a regular expression object, that is tested against the string\n+            representation of the exception and its `PEP-678 <https://peps.python.org/pep-0678/>` `__notes__`\n+            using :func:`re.search`.\n+\n+            To match a literal string that may contain :ref:`special characters\n+            <re-syntax>`, the pattern can first be escaped with :func:`re.escape`.\n+\n+        :param Optional[int] depth:\n+            If `None`, will search for a matching exception at any nesting depth.\n+            If >= 1, will only match an exception if it's at the specified depth (depth = 1 being\n+            the exceptions contained within the topmost exception group).\n+\n+        .. versionadded:: 8.0\n+\n+        .. warning::\n+           This helper makes it easy to check for the presence of specific exceptions,\n+           but it is very bad for checking that the group does *not* contain\n+           *any other exceptions*.\n+           You should instead consider using :class:`pytest.RaisesGroup`\n+\n+        \"\"\"\n+        msg = \"Captured exception is not an instance of `BaseExceptionGroup`\"\n+        assert isinstance(self.value, BaseExceptionGroup), msg\n+        msg = \"`depth` must be >= 1 if specified\"\n+        assert (depth is None) or (depth >= 1), msg\n+        return self._group_contains(self.value, expected_exception, match, depth)\n+\n+\n+if TYPE_CHECKING:\n+    from typing_extensions import TypeAlias\n+\n+    # Type alias for the `tbfilter` setting:\n+    # bool: If True, it should be filtered using Traceback.filter()\n+    # callable: A callable that takes an ExceptionInfo and returns the filtered traceback.\n+    TracebackFilter: TypeAlias = Union[\n+        bool, Callable[[ExceptionInfo[BaseException]], Traceback]\n+    ]\n+\n+\n+@dataclasses.dataclass\n+class FormattedExcinfo:\n+    \"\"\"Presenting information about failing Functions and Generators.\"\"\"\n+\n+    # for traceback entries\n+    flow_marker: ClassVar = \">\"\n+    fail_marker: ClassVar = \"E\"\n+\n+    showlocals: bool = False\n+    style: TracebackStyle = \"long\"\n+    abspath: bool = True\n+    tbfilter: TracebackFilter = True\n+    funcargs: bool = False\n+    truncate_locals: bool = True\n+    truncate_args: bool = True\n+    chain: bool = True\n+    astcache: dict[str | Path, ast.AST] = dataclasses.field(\n+        default_factory=dict, init=False, repr=False\n+    )\n+\n+    def _getindent(self, source: Source) -> int:\n+        # Figure out indent for the given source.\n+        try:\n+            s = str(source.getstatement(len(source) - 1))\n+        except KeyboardInterrupt:\n+            raise\n+        except BaseException:\n+            try:\n+                s = str(source[-1])\n+            except KeyboardInterrupt:\n+                raise\n+            except BaseException:\n+                return 0\n+        return 4 + (len(s) - len(s.lstrip()))\n+\n+    def _getentrysource(self, entry: TracebackEntry) -> Source | None:\n+        source = entry.getsource(self.astcache)\n+        if source is not None:\n+            source = source.deindent()\n+        return source\n+\n+    def repr_args(self, entry: TracebackEntry) -> ReprFuncArgs | None:\n+        if self.funcargs:\n+            args = []\n+            for argname, argvalue in entry.frame.getargs(var=True):\n+                if self.truncate_args:\n+                    str_repr = saferepr(argvalue)\n+                else:\n+                    str_repr = saferepr(argvalue, maxsize=None)\n+                args.append((argname, str_repr))\n+            return ReprFuncArgs(args)\n+        return None\n+\n+    def get_source(\n+        self,\n+        source: Source | None,\n+        line_index: int = -1,\n+        excinfo: ExceptionInfo[BaseException] | None = None,\n+        short: bool = False,\n+        end_line_index: int | None = None,\n+        colno: int | None = None,\n+        end_colno: int | None = None,\n+    ) -> list[str]:\n+        \"\"\"Return formatted and marked up source lines.\"\"\"\n+        lines = []\n+        if source is not None and line_index < 0:\n+            line_index += len(source)\n+        if source is None or line_index >= len(source.lines) or line_index < 0:\n+            # `line_index` could still be outside `range(len(source.lines))` if\n+            # we're processing AST with pathological position attributes.\n+            source = Source(\"???\")\n+            line_index = 0\n+        space_prefix = \"    \"\n+        if short:\n+            lines.append(space_prefix + source.lines[line_index].strip())\n+            lines.extend(\n+                self.get_highlight_arrows_for_line(\n+                    raw_line=source.raw_lines[line_index],\n+                    line=source.lines[line_index].strip(),\n+                    lineno=line_index,\n+                    end_lineno=end_line_index,\n+                    colno=colno,\n+                    end_colno=end_colno,\n+                )\n+            )\n+        else:\n+            for line in source.lines[:line_index]:\n+                lines.append(space_prefix + line)\n+            lines.append(self.flow_marker + \"   \" + source.lines[line_index])\n+            lines.extend(\n+                self.get_highlight_arrows_for_line(\n+                    raw_line=source.raw_lines[line_index],\n+                    line=source.lines[line_index],\n+                    lineno=line_index,\n+                    end_lineno=end_line_index,\n+                    colno=colno,\n+                    end_colno=end_colno,\n+                )\n+            )\n+            for line in source.lines[line_index + 1 :]:\n+                lines.append(space_prefix + line)\n+        if excinfo is not None:\n+            indent = 4 if short else self._getindent(source)\n+            lines.extend(self.get_exconly(excinfo, indent=indent, markall=True))\n+        return lines\n+\n+    def get_highlight_arrows_for_line(\n+        self,\n+        line: str,\n+        raw_line: str,\n+        lineno: int | None,\n+        end_lineno: int | None,\n+        colno: int | None,\n+        end_colno: int | None,\n+    ) -> list[str]:\n+        \"\"\"Return characters highlighting a source line.\n+\n+        Example with colno and end_colno pointing to the bar expression:\n+                   \"foo() + bar()\"\n+        returns    \"        ^^^^^\"\n+        \"\"\"\n+        if lineno != end_lineno:\n+            # Don't handle expressions that span multiple lines.\n+            return []\n+        if colno is None or end_colno is None:\n+            # Can't do anything without column information.\n+            return []\n+\n+        num_stripped_chars = len(raw_line) - len(line)\n+\n+        start_char_offset = _byte_offset_to_character_offset(raw_line, colno)\n+        end_char_offset = _byte_offset_to_character_offset(raw_line, end_colno)\n+        num_carets = end_char_offset - start_char_offset\n+        # If the highlight would span the whole line, it is redundant, don't\n+        # show it.\n+        if num_carets >= len(line.strip()):\n+            return []\n+\n+        highlights = \"    \"\n+        highlights += \" \" * (start_char_offset - num_stripped_chars + 1)\n+        highlights += \"^\" * num_carets\n+        return [highlights]\n+\n+    def get_exconly(\n+        self,\n+        excinfo: ExceptionInfo[BaseException],\n+        indent: int = 4,\n+        markall: bool = False,\n+    ) -> list[str]:\n+        lines = []\n+        indentstr = \" \" * indent\n+        # Get the real exception information out.\n+        exlines = excinfo.exconly(tryshort=True).split(\"\\n\")\n+        failindent = self.fail_marker + indentstr[1:]\n+        for line in exlines:\n+            lines.append(failindent + line)\n+            if not markall:\n+                failindent = indentstr\n+        return lines\n+\n+    def repr_locals(self, locals: Mapping[str, object]) -> ReprLocals | None:\n+        if self.showlocals:\n+            lines = []\n+            keys = [loc for loc in locals if loc[0] != \"@\"]\n+            keys.sort()\n+            for name in keys:\n+                value = locals[name]\n+                if name == \"__builtins__\":\n+                    lines.append(\"__builtins__ = <builtins>\")\n+                else:\n+                    # This formatting could all be handled by the\n+                    # _repr() function, which is only reprlib.Repr in\n+                    # disguise, so is very configurable.\n+                    if self.truncate_locals:\n+                        str_repr = saferepr(value)\n+                    else:\n+                        str_repr = safeformat(value)\n+                    # if len(str_repr) < 70 or not isinstance(value, (list, tuple, dict)):\n+                    lines.append(f\"{name:<10} = {str_repr}\")\n+                    # else:\n+                    #    self._line(\"%-10s =\\\\\" % (name,))\n+                    #    # XXX\n+                    #    pprint.pprint(value, stream=self.excinfowriter)\n+            return ReprLocals(lines)\n+        return None\n+\n+    def repr_traceback_entry(\n+        self,\n+        entry: TracebackEntry | None,\n+        excinfo: ExceptionInfo[BaseException] | None = None,\n+    ) -> ReprEntry:\n+        lines: list[str] = []\n+        style = (\n+            entry._repr_style\n+            if entry is not None and entry._repr_style is not None\n+            else self.style\n+        )\n+        if style in (\"short\", \"long\") and entry is not None:\n+            source = self._getentrysource(entry)\n+            if source is None:\n+                source = Source(\"???\")\n+                line_index = 0\n+                end_line_index, colno, end_colno = None, None, None\n+            else:\n+                line_index = entry.relline\n+                end_line_index = entry.end_lineno_relative\n+                colno = entry.colno\n+                end_colno = entry.end_colno\n+            short = style == \"short\"\n+            reprargs = self.repr_args(entry) if not short else None\n+            s = self.get_source(\n+                source=source,\n+                line_index=line_index,\n+                excinfo=excinfo,\n+                short=short,\n+                end_line_index=end_line_index,\n+                colno=colno,\n+                end_colno=end_colno,\n+            )\n+            lines.extend(s)\n+            if short:\n+                message = f\"in {entry.name}\"\n+            else:\n+                message = (excinfo and excinfo.typename) or \"\"\n+            entry_path = entry.path\n+            path = self._makepath(entry_path)\n+            reprfileloc = ReprFileLocation(path, entry.lineno + 1, message)\n+            localsrepr = self.repr_locals(entry.locals)\n+            return ReprEntry(lines, reprargs, localsrepr, reprfileloc, style)\n+        elif style == \"value\":\n+            if excinfo:\n+                lines.extend(str(excinfo.value).split(\"\\n\"))\n+            return ReprEntry(lines, None, None, None, style)\n+        else:\n+            if excinfo:\n+                lines.extend(self.get_exconly(excinfo, indent=4))\n+            return ReprEntry(lines, None, None, None, style)\n+\n+    def _makepath(self, path: Path | str) -> str:\n+        if not self.abspath and isinstance(path, Path):\n+            try:\n+                np = bestrelpath(Path.cwd(), path)\n+            except OSError:\n+                return str(path)\n+            if len(np) < len(str(path)):\n+                return np\n+        return str(path)\n+\n+    def repr_traceback(self, excinfo: ExceptionInfo[BaseException]) -> ReprTraceback:\n+        traceback = filter_excinfo_traceback(self.tbfilter, excinfo)\n+\n+        if isinstance(excinfo.value, RecursionError):\n+            traceback, extraline = self._truncate_recursive_traceback(traceback)\n+        else:\n+            extraline = None\n+\n+        if not traceback:\n+            if extraline is None:\n+                extraline = \"All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.\"\n+            entries = [self.repr_traceback_entry(None, excinfo)]\n+            return ReprTraceback(entries, extraline, style=self.style)\n+\n+        last = traceback[-1]\n+        if self.style == \"value\":\n+            entries = [self.repr_traceback_entry(last, excinfo)]\n+            return ReprTraceback(entries, None, style=self.style)\n+\n+        entries = [\n+            self.repr_traceback_entry(entry, excinfo if last == entry else None)\n+            for entry in traceback\n+        ]\n+        return ReprTraceback(entries, extraline, style=self.style)\n+\n+    def _truncate_recursive_traceback(\n+        self, traceback: Traceback\n+    ) -> tuple[Traceback, str | None]:\n+        \"\"\"Truncate the given recursive traceback trying to find the starting\n+        point of the recursion.\n+\n+        The detection is done by going through each traceback entry and\n+        finding the point in which the locals of the frame are equal to the\n+        locals of a previous frame (see ``recursionindex()``).\n+\n+        Handle the situation where the recursion process might raise an\n+        exception (for example comparing numpy arrays using equality raises a\n+        TypeError), in which case we do our best to warn the user of the\n+        error and show a limited traceback.\n+        \"\"\"\n+        try:\n+            recursionindex = traceback.recursionindex()\n+        except Exception as e:\n+            max_frames = 10\n+            extraline: str | None = (\n+                \"!!! Recursion error detected, but an error occurred locating the origin of recursion.\\n\"\n+                \"  The following exception happened when comparing locals in the stack frame:\\n\"\n+                f\"    {type(e).__name__}: {e!s}\\n\"\n+                f\"  Displaying first and last {max_frames} stack frames out of {len(traceback)}.\"\n+            )\n+            # Type ignored because adding two instances of a List subtype\n+            # currently incorrectly has type List instead of the subtype.\n+            traceback = traceback[:max_frames] + traceback[-max_frames:]  # type: ignore\n+        else:\n+            if recursionindex is not None:\n+                extraline = \"!!! Recursion detected (same locals & position)\"\n+                traceback = traceback[: recursionindex + 1]\n+            else:\n+                extraline = None\n+\n+        return traceback, extraline\n+\n+    def repr_excinfo(self, excinfo: ExceptionInfo[BaseException]) -> ExceptionChainRepr:\n+        repr_chain: list[tuple[ReprTraceback, ReprFileLocation | None, str | None]] = []\n+        e: BaseException | None = excinfo.value\n+        excinfo_: ExceptionInfo[BaseException] | None = excinfo\n+        descr = None\n+        seen: set[int] = set()\n+        while e is not None and id(e) not in seen:\n+            seen.add(id(e))\n+\n+            if excinfo_:\n+                # Fall back to native traceback as a temporary workaround until\n+                # full support for exception groups added to ExceptionInfo.\n+                # See https://github.com/pytest-dev/pytest/issues/9159\n+                reprtraceback: ReprTraceback | ReprTracebackNative\n+                if isinstance(e, BaseExceptionGroup):\n+                    # don't filter any sub-exceptions since they shouldn't have any internal frames\n+                    traceback = filter_excinfo_traceback(self.tbfilter, excinfo)\n+                    reprtraceback = ReprTracebackNative(\n+                        format_exception(\n+                            type(excinfo.value),\n+                            excinfo.value,\n+                            traceback[0]._rawentry,\n+                        )\n+                    )\n+                else:\n+                    reprtraceback = self.repr_traceback(excinfo_)\n+                reprcrash = excinfo_._getreprcrash()\n+            else:\n+                # Fallback to native repr if the exception doesn't have a traceback:\n+                # ExceptionInfo objects require a full traceback to work.\n+                reprtraceback = ReprTracebackNative(format_exception(type(e), e, None))\n+                reprcrash = None\n+            repr_chain += [(reprtraceback, reprcrash, descr)]\n+\n+            if e.__cause__ is not None and self.chain:\n+                e = e.__cause__\n+                excinfo_ = ExceptionInfo.from_exception(e) if e.__traceback__ else None\n+                descr = \"The above exception was the direct cause of the following exception:\"\n+            elif (\n+                e.__context__ is not None and not e.__suppress_context__ and self.chain\n+            ):\n+                e = e.__context__\n+                excinfo_ = ExceptionInfo.from_exception(e) if e.__traceback__ else None\n+                descr = \"During handling of the above exception, another exception occurred:\"\n+            else:\n+                e = None\n+        repr_chain.reverse()\n+        return ExceptionChainRepr(repr_chain)\n+\n+\n+@dataclasses.dataclass(eq=False)\n+class TerminalRepr:\n+    def __str__(self) -> str:\n+        # FYI this is called from pytest-xdist's serialization of exception\n+        # information.\n+        io = StringIO()\n+        tw = TerminalWriter(file=io)\n+        self.toterminal(tw)\n+        return io.getvalue().strip()\n+\n+    def __repr__(self) -> str:\n+        return f\"<{self.__class__} instance at {id(self):0x}>\"\n+\n+    def toterminal(self, tw: TerminalWriter) -> None:\n+        raise NotImplementedError()\n+\n+\n+# This class is abstract -- only subclasses are instantiated.\n+@dataclasses.dataclass(eq=False)\n+class ExceptionRepr(TerminalRepr):\n+    # Provided by subclasses.\n+    reprtraceback: ReprTraceback\n+    reprcrash: ReprFileLocation | None\n+    sections: list[tuple[str, str, str]] = dataclasses.field(\n+        init=False, default_factory=list\n+    )\n+\n+    def addsection(self, name: str, content: str, sep: str = \"-\") -> None:\n+        self.sections.append((name, content, sep))\n+\n+    def toterminal(self, tw: TerminalWriter) -> None:\n+        for name, content, sep in self.sections:\n+            tw.sep(sep, name)\n+            tw.line(content)\n+\n+\n+@dataclasses.dataclass(eq=False)\n+class ExceptionChainRepr(ExceptionRepr):\n+    chain: Sequence[tuple[ReprTraceback, ReprFileLocation | None, str | None]]\n+\n+    def __init__(\n+        self,\n+        chain: Sequence[tuple[ReprTraceback, ReprFileLocation | None, str | None]],\n+    ) -> None:\n+        # reprcrash and reprtraceback of the outermost (the newest) exception\n+        # in the chain.\n+        super().__init__(\n+            reprtraceback=chain[-1][0],\n+            reprcrash=chain[-1][1],\n+        )\n+        self.chain = chain\n+\n+    def toterminal(self, tw: TerminalWriter) -> None:\n+        for element in self.chain:\n+            element[0].toterminal(tw)\n+            if element[2] is not None:\n+                tw.line(\"\")\n+                tw.line(element[2], yellow=True)\n+        super().toterminal(tw)\n+\n+\n+@dataclasses.dataclass(eq=False)\n+class ReprExceptionInfo(ExceptionRepr):\n+    reprtraceback: ReprTraceback\n+    reprcrash: ReprFileLocation | None\n+\n+    def toterminal(self, tw: TerminalWriter) -> None:\n+        self.reprtraceback.toterminal(tw)\n+        super().toterminal(tw)\n+\n+\n+@dataclasses.dataclass(eq=False)\n+class ReprTraceback(TerminalRepr):\n+    reprentries: Sequence[ReprEntry | ReprEntryNative]\n+    extraline: str | None\n+    style: TracebackStyle\n+\n+    entrysep: ClassVar = \"_ \"\n+\n+    def toterminal(self, tw: TerminalWriter) -> None:\n+        # The entries might have different styles.\n+        for i, entry in enumerate(self.reprentries):\n+            if entry.style == \"long\":\n+                tw.line(\"\")\n+            entry.toterminal(tw)\n+            if i < len(self.reprentries) - 1:\n+                next_entry = self.reprentries[i + 1]\n+                if entry.style == \"long\" or (\n+                    entry.style == \"short\" and next_entry.style == \"long\"\n+                ):\n+                    tw.sep(self.entrysep)\n+\n+        if self.extraline:\n+            tw.line(self.extraline)\n+\n+\n+class ReprTracebackNative(ReprTraceback):\n+    def __init__(self, tblines: Sequence[str]) -> None:\n+        self.reprentries = [ReprEntryNative(tblines)]\n+        self.extraline = None\n+        self.style = \"native\"\n+\n+\n+@dataclasses.dataclass(eq=False)\n+class ReprEntryNative(TerminalRepr):\n+    lines: Sequence[str]\n+\n+    style: ClassVar[TracebackStyle] = \"native\"\n+\n+    def toterminal(self, tw: TerminalWriter) -> None:\n+        tw.write(\"\".join(self.lines))\n+\n+\n+@dataclasses.dataclass(eq=False)\n+class ReprEntry(TerminalRepr):\n+    lines: Sequence[str]\n+    reprfuncargs: ReprFuncArgs | None\n+    reprlocals: ReprLocals | None\n+    reprfileloc: ReprFileLocation | None\n+    style: TracebackStyle\n+\n+    def _write_entry_lines(self, tw: TerminalWriter) -> None:\n+        \"\"\"Write the source code portions of a list of traceback entries with syntax highlighting.\n+\n+        Usually entries are lines like these:\n+\n+            \"     x = 1\"\n+            \">    assert x == 2\"\n+            \"E    assert 1 == 2\"\n+\n+        This function takes care of rendering the \"source\" portions of it (the lines without\n+        the \"E\" prefix) using syntax highlighting, taking care to not highlighting the \">\"\n+        character, as doing so might break line continuations.\n+        \"\"\"\n+        if not self.lines:\n+            return\n+\n+        if self.style == \"value\":\n+            # Using tw.write instead of tw.line for testing purposes due to TWMock implementation;\n+            # lines written with TWMock.line and TWMock._write_source cannot be distinguished\n+            # from each other, whereas lines written with TWMock.write are marked with TWMock.WRITE\n+            for line in self.lines:\n+                tw.write(line)\n+                tw.write(\"\\n\")\n+            return\n+\n+        # separate indents and source lines that are not failures: we want to\n+        # highlight the code but not the indentation, which may contain markers\n+        # such as \">   assert 0\"\n+        fail_marker = f\"{FormattedExcinfo.fail_marker}   \"\n+        indent_size = len(fail_marker)\n+        indents: list[str] = []\n+        source_lines: list[str] = []\n+        failure_lines: list[str] = []\n+        for index, line in enumerate(self.lines):\n+            is_failure_line = line.startswith(fail_marker)\n+            if is_failure_line:\n+                # from this point on all lines are considered part of the failure\n+                failure_lines.extend(self.lines[index:])\n+                break\n+            else:\n+                indents.append(line[:indent_size])\n+                source_lines.append(line[indent_size:])\n+\n+        tw._write_source(source_lines, indents)\n+\n+        # failure lines are always completely red and bold\n+        for line in failure_lines:\n+            tw.line(line, bold=True, red=True)\n+\n+    def toterminal(self, tw: TerminalWriter) -> None:\n+        if self.style == \"short\":\n+            if self.reprfileloc:\n+                self.reprfileloc.toterminal(tw)\n+            self._write_entry_lines(tw)\n+            if self.reprlocals:\n+                self.reprlocals.toterminal(tw, indent=\" \" * 8)\n+            return\n+\n+        if self.reprfuncargs:\n+            self.reprfuncargs.toterminal(tw)\n+\n+        self._write_entry_lines(tw)\n+\n+        if self.reprlocals:\n+            tw.line(\"\")\n+            self.reprlocals.toterminal(tw)\n+        if self.reprfileloc:\n+            if self.lines:\n+                tw.line(\"\")\n+            self.reprfileloc.toterminal(tw)\n+\n+    def __str__(self) -> str:\n+        return \"{}\\n{}\\n{}\".format(\n+            \"\\n\".join(self.lines), self.reprlocals, self.reprfileloc\n+        )\n+\n+\n+@dataclasses.dataclass(eq=False)\n+class ReprFileLocation(TerminalRepr):\n+    path: str\n+    lineno: int\n+    message: str\n+\n+    def __post_init__(self) -> None:\n+        self.path = str(self.path)\n+\n+    def toterminal(self, tw: TerminalWriter) -> None:\n+        # Filename and lineno output for each entry, using an output format\n+        # that most editors understand.\n+        msg = self.message\n+        i = msg.find(\"\\n\")\n+        if i != -1:\n+            msg = msg[:i]\n+        tw.write(self.path, bold=True, red=True)\n+        tw.line(f\":{self.lineno}: {msg}\")\n+\n+\n+@dataclasses.dataclass(eq=False)\n+class ReprLocals(TerminalRepr):\n+    lines: Sequence[str]\n+\n+    def toterminal(self, tw: TerminalWriter, indent=\"\") -> None:\n+        for line in self.lines:\n+            tw.line(indent + line)\n+\n+\n+@dataclasses.dataclass(eq=False)\n+class ReprFuncArgs(TerminalRepr):\n+    args: Sequence[tuple[str, object]]\n+\n+    def toterminal(self, tw: TerminalWriter) -> None:\n+        if self.args:\n+            linesofar = \"\"\n+            for name, value in self.args:\n+                ns = f\"{name} = {value}\"\n+                if len(ns) + len(linesofar) + 2 > tw.fullwidth:\n+                    if linesofar:\n+                        tw.line(linesofar)\n+                    linesofar = ns\n+                else:\n+                    if linesofar:\n+                        linesofar += \", \" + ns\n+                    else:\n+                        linesofar = ns\n+            if linesofar:\n+                tw.line(linesofar)\n+            tw.line(\"\")\n+\n+\n+def getfslineno(obj: object) -> tuple[str | Path, int]:\n+    \"\"\"Return source location (path, lineno) for the given object.\n+\n+    If the source cannot be determined return (\"\", -1).\n+\n+    The line number is 0-based.\n+    \"\"\"\n+    # xxx let decorators etc specify a sane ordering\n+    # NOTE: this used to be done in _pytest.compat.getfslineno, initially added\n+    #       in 6ec13a2b9.  It (\"place_as\") appears to be something very custom.\n+    obj = get_real_func(obj)\n+    if hasattr(obj, \"place_as\"):\n+        obj = obj.place_as\n+\n+    try:\n+        code = Code.from_function(obj)\n+    except TypeError:\n+        try:\n+            fn = inspect.getsourcefile(obj) or inspect.getfile(obj)  # type: ignore[arg-type]\n+        except TypeError:\n+            return \"\", -1\n+\n+        fspath = (fn and absolutepath(fn)) or \"\"\n+        lineno = -1\n+        if fspath:\n+            try:\n+                _, lineno = findsource(obj)\n+            except OSError:\n+                pass\n+        return fspath, lineno\n+\n+    return code.path, code.firstlineno\n+\n+\n+def _byte_offset_to_character_offset(str, offset):\n+    \"\"\"Converts a byte based offset in a string to a code-point.\"\"\"\n+    as_utf8 = str.encode(\"utf-8\")\n+    return len(as_utf8[:offset].decode(\"utf-8\", errors=\"replace\"))\n+\n+\n+# Relative paths that we use to filter traceback entries from appearing to the user;\n+# see filter_traceback.\n+# note: if we need to add more paths than what we have now we should probably use a list\n+# for better maintenance.\n+\n+_PLUGGY_DIR = Path(pluggy.__file__.rstrip(\"oc\"))\n+# pluggy is either a package or a single module depending on the version\n+if _PLUGGY_DIR.name == \"__init__.py\":\n+    _PLUGGY_DIR = _PLUGGY_DIR.parent\n+_PYTEST_DIR = Path(_pytest.__file__).parent\n+\n+\n+def filter_traceback(entry: TracebackEntry) -> bool:\n+    \"\"\"Return True if a TracebackEntry instance should be included in tracebacks.\n+\n+    We hide traceback entries of:\n+\n+    * dynamically generated code (no code to show up for it);\n+    * internal traceback from pytest or its internal libraries, py and pluggy.\n+    \"\"\"\n+    # entry.path might sometimes return a str object when the entry\n+    # points to dynamically generated code.\n+    # See https://bitbucket.org/pytest-dev/py/issues/71.\n+    raw_filename = entry.frame.code.raw.co_filename\n+    is_generated = \"<\" in raw_filename and \">\" in raw_filename\n+    if is_generated:\n+        return False\n+\n+    # entry.path might point to a non-existing file, in which case it will\n+    # also return a str object. See #1133.\n+    p = Path(entry.path)\n+\n+    parents = p.parents\n+    if _PLUGGY_DIR in parents:\n+        return False\n+    if _PYTEST_DIR in parents:\n+        return False\n+\n+    return True\n+\n+\n+def filter_excinfo_traceback(\n+    tbfilter: TracebackFilter, excinfo: ExceptionInfo[BaseException]\n+) -> Traceback:\n+    \"\"\"Filter the exception traceback in ``excinfo`` according to ``tbfilter``.\"\"\"\n+    if callable(tbfilter):\n+        return tbfilter(excinfo)\n+    elif tbfilter:\n+        return excinfo.traceback.filter(excinfo)\n+    else:\n+        return excinfo.traceback",
      "patch_lines": [
        "@@ -0,0 +1,1567 @@\n",
        "+# mypy: allow-untyped-defs\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+import ast\n",
        "+from collections.abc import Callable\n",
        "+from collections.abc import Iterable\n",
        "+from collections.abc import Mapping\n",
        "+from collections.abc import Sequence\n",
        "+import dataclasses\n",
        "+import inspect\n",
        "+from inspect import CO_VARARGS\n",
        "+from inspect import CO_VARKEYWORDS\n",
        "+from io import StringIO\n",
        "+import os\n",
        "+from pathlib import Path\n",
        "+import re\n",
        "+import sys\n",
        "+from traceback import extract_tb\n",
        "+from traceback import format_exception\n",
        "+from traceback import format_exception_only\n",
        "+from traceback import FrameSummary\n",
        "+from types import CodeType\n",
        "+from types import FrameType\n",
        "+from types import TracebackType\n",
        "+from typing import Any\n",
        "+from typing import ClassVar\n",
        "+from typing import Final\n",
        "+from typing import final\n",
        "+from typing import Generic\n",
        "+from typing import Literal\n",
        "+from typing import overload\n",
        "+from typing import SupportsIndex\n",
        "+from typing import TYPE_CHECKING\n",
        "+from typing import TypeVar\n",
        "+from typing import Union\n",
        "+\n",
        "+import pluggy\n",
        "+\n",
        "+import _pytest\n",
        "+from _pytest._code.source import findsource\n",
        "+from _pytest._code.source import getrawcode\n",
        "+from _pytest._code.source import getstatementrange_ast\n",
        "+from _pytest._code.source import Source\n",
        "+from _pytest._io import TerminalWriter\n",
        "+from _pytest._io.saferepr import safeformat\n",
        "+from _pytest._io.saferepr import saferepr\n",
        "+from _pytest.compat import get_real_func\n",
        "+from _pytest.deprecated import check_ispytest\n",
        "+from _pytest.pathlib import absolutepath\n",
        "+from _pytest.pathlib import bestrelpath\n",
        "+\n",
        "+\n",
        "+if sys.version_info < (3, 11):\n",
        "+    from exceptiongroup import BaseExceptionGroup\n",
        "+\n",
        "+TracebackStyle = Literal[\"long\", \"short\", \"line\", \"no\", \"native\", \"value\", \"auto\"]\n",
        "+\n",
        "+EXCEPTION_OR_MORE = Union[type[BaseException], tuple[type[BaseException], ...]]\n",
        "+\n",
        "+\n",
        "+class Code:\n",
        "+    \"\"\"Wrapper around Python code objects.\"\"\"\n",
        "+\n",
        "+    __slots__ = (\"raw\",)\n",
        "+\n",
        "+    def __init__(self, obj: CodeType) -> None:\n",
        "+        self.raw = obj\n",
        "+\n",
        "+    @classmethod\n",
        "+    def from_function(cls, obj: object) -> Code:\n",
        "+        return cls(getrawcode(obj))\n",
        "+\n",
        "+    def __eq__(self, other):\n",
        "+        return self.raw == other.raw\n",
        "+\n",
        "+    # Ignore type because of https://github.com/python/mypy/issues/4266.\n",
        "+    __hash__ = None  # type: ignore\n",
        "+\n",
        "+    @property\n",
        "+    def firstlineno(self) -> int:\n",
        "+        return self.raw.co_firstlineno - 1\n",
        "+\n",
        "+    @property\n",
        "+    def name(self) -> str:\n",
        "+        return self.raw.co_name\n",
        "+\n",
        "+    @property\n",
        "+    def path(self) -> Path | str:\n",
        "+        \"\"\"Return a path object pointing to source code, or an ``str`` in\n",
        "+        case of ``OSError`` / non-existing file.\"\"\"\n",
        "+        if not self.raw.co_filename:\n",
        "+            return \"\"\n",
        "+        try:\n",
        "+            p = absolutepath(self.raw.co_filename)\n",
        "+            # maybe don't try this checking\n",
        "+            if not p.exists():\n",
        "+                raise OSError(\"path check failed.\")\n",
        "+            return p\n",
        "+        except OSError:\n",
        "+            # XXX maybe try harder like the weird logic\n",
        "+            # in the standard lib [linecache.updatecache] does?\n",
        "+            return self.raw.co_filename\n",
        "+\n",
        "+    @property\n",
        "+    def fullsource(self) -> Source | None:\n",
        "+        \"\"\"Return a _pytest._code.Source object for the full source file of the code.\"\"\"\n",
        "+        full, _ = findsource(self.raw)\n",
        "+        return full\n",
        "+\n",
        "+    def source(self) -> Source:\n",
        "+        \"\"\"Return a _pytest._code.Source object for the code object's source only.\"\"\"\n",
        "+        # return source only for that part of code\n",
        "+        return Source(self.raw)\n",
        "+\n",
        "+    def getargs(self, var: bool = False) -> tuple[str, ...]:\n",
        "+        \"\"\"Return a tuple with the argument names for the code object.\n",
        "+\n",
        "+        If 'var' is set True also return the names of the variable and\n",
        "+        keyword arguments when present.\n",
        "+        \"\"\"\n",
        "+        # Handy shortcut for getting args.\n",
        "+        raw = self.raw\n",
        "+        argcount = raw.co_argcount\n",
        "+        if var:\n",
        "+            argcount += raw.co_flags & CO_VARARGS\n",
        "+            argcount += raw.co_flags & CO_VARKEYWORDS\n",
        "+        return raw.co_varnames[:argcount]\n",
        "+\n",
        "+\n",
        "+class Frame:\n",
        "+    \"\"\"Wrapper around a Python frame holding f_locals and f_globals\n",
        "+    in which expressions can be evaluated.\"\"\"\n",
        "+\n",
        "+    __slots__ = (\"raw\",)\n",
        "+\n",
        "+    def __init__(self, frame: FrameType) -> None:\n",
        "+        self.raw = frame\n",
        "+\n",
        "+    @property\n",
        "+    def lineno(self) -> int:\n",
        "+        return self.raw.f_lineno - 1\n",
        "+\n",
        "+    @property\n",
        "+    def f_globals(self) -> dict[str, Any]:\n",
        "+        return self.raw.f_globals\n",
        "+\n",
        "+    @property\n",
        "+    def f_locals(self) -> dict[str, Any]:\n",
        "+        return self.raw.f_locals\n",
        "+\n",
        "+    @property\n",
        "+    def code(self) -> Code:\n",
        "+        return Code(self.raw.f_code)\n",
        "+\n",
        "+    @property\n",
        "+    def statement(self) -> Source:\n",
        "+        \"\"\"Statement this frame is at.\"\"\"\n",
        "+        if self.code.fullsource is None:\n",
        "+            return Source(\"\")\n",
        "+        return self.code.fullsource.getstatement(self.lineno)\n",
        "+\n",
        "+    def eval(self, code, **vars):\n",
        "+        \"\"\"Evaluate 'code' in the frame.\n",
        "+\n",
        "+        'vars' are optional additional local variables.\n",
        "+\n",
        "+        Returns the result of the evaluation.\n",
        "+        \"\"\"\n",
        "+        f_locals = self.f_locals.copy()\n",
        "+        f_locals.update(vars)\n",
        "+        return eval(code, self.f_globals, f_locals)\n",
        "+\n",
        "+    def repr(self, object: object) -> str:\n",
        "+        \"\"\"Return a 'safe' (non-recursive, one-line) string repr for 'object'.\"\"\"\n",
        "+        return saferepr(object)\n",
        "+\n",
        "+    def getargs(self, var: bool = False):\n",
        "+        \"\"\"Return a list of tuples (name, value) for all arguments.\n",
        "+\n",
        "+        If 'var' is set True, also include the variable and keyword arguments\n",
        "+        when present.\n",
        "+        \"\"\"\n",
        "+        retval = []\n",
        "+        for arg in self.code.getargs(var):\n",
        "+            try:\n",
        "+                retval.append((arg, self.f_locals[arg]))\n",
        "+            except KeyError:\n",
        "+                pass  # this can occur when using Psyco\n",
        "+        return retval\n",
        "+\n",
        "+\n",
        "+class TracebackEntry:\n",
        "+    \"\"\"A single entry in a Traceback.\"\"\"\n",
        "+\n",
        "+    __slots__ = (\"_rawentry\", \"_repr_style\")\n",
        "+\n",
        "+    def __init__(\n",
        "+        self,\n",
        "+        rawentry: TracebackType,\n",
        "+        repr_style: Literal[\"short\", \"long\"] | None = None,\n",
        "+    ) -> None:\n",
        "+        self._rawentry: Final = rawentry\n",
        "+        self._repr_style: Final = repr_style\n",
        "+\n",
        "+    def with_repr_style(\n",
        "+        self, repr_style: Literal[\"short\", \"long\"] | None\n",
        "+    ) -> TracebackEntry:\n",
        "+        return TracebackEntry(self._rawentry, repr_style)\n",
        "+\n",
        "+    @property\n",
        "+    def lineno(self) -> int:\n",
        "+        return self._rawentry.tb_lineno - 1\n",
        "+\n",
        "+    def get_python_framesummary(self) -> FrameSummary:\n",
        "+        # Python's built-in traceback module implements all the nitty gritty\n",
        "+        # details to get column numbers of out frames.\n",
        "+        stack_summary = extract_tb(self._rawentry, limit=1)\n",
        "+        return stack_summary[0]\n",
        "+\n",
        "+    # Column and end line numbers introduced in python 3.11\n",
        "+    if sys.version_info < (3, 11):\n",
        "+\n",
        "+        @property\n",
        "+        def end_lineno_relative(self) -> int | None:\n",
        "+            return None\n",
        "+\n",
        "+        @property\n",
        "+        def colno(self) -> int | None:\n",
        "+            return None\n",
        "+\n",
        "+        @property\n",
        "+        def end_colno(self) -> int | None:\n",
        "+            return None\n",
        "+    else:\n",
        "+\n",
        "+        @property\n",
        "+        def end_lineno_relative(self) -> int | None:\n",
        "+            frame_summary = self.get_python_framesummary()\n",
        "+            if frame_summary.end_lineno is None:  # pragma: no cover\n",
        "+                return None\n",
        "+            return frame_summary.end_lineno - 1 - self.frame.code.firstlineno\n",
        "+\n",
        "+        @property\n",
        "+        def colno(self) -> int | None:\n",
        "+            \"\"\"Starting byte offset of the expression in the traceback entry.\"\"\"\n",
        "+            return self.get_python_framesummary().colno\n",
        "+\n",
        "+        @property\n",
        "+        def end_colno(self) -> int | None:\n",
        "+            \"\"\"Ending byte offset of the expression in the traceback entry.\"\"\"\n",
        "+            return self.get_python_framesummary().end_colno\n",
        "+\n",
        "+    @property\n",
        "+    def frame(self) -> Frame:\n",
        "+        return Frame(self._rawentry.tb_frame)\n",
        "+\n",
        "+    @property\n",
        "+    def relline(self) -> int:\n",
        "+        return self.lineno - self.frame.code.firstlineno\n",
        "+\n",
        "+    def __repr__(self) -> str:\n",
        "+        return f\"<TracebackEntry {self.frame.code.path}:{self.lineno + 1}>\"\n",
        "+\n",
        "+    @property\n",
        "+    def statement(self) -> Source:\n",
        "+        \"\"\"_pytest._code.Source object for the current statement.\"\"\"\n",
        "+        source = self.frame.code.fullsource\n",
        "+        assert source is not None\n",
        "+        return source.getstatement(self.lineno)\n",
        "+\n",
        "+    @property\n",
        "+    def path(self) -> Path | str:\n",
        "+        \"\"\"Path to the source code.\"\"\"\n",
        "+        return self.frame.code.path\n",
        "+\n",
        "+    @property\n",
        "+    def locals(self) -> dict[str, Any]:\n",
        "+        \"\"\"Locals of underlying frame.\"\"\"\n",
        "+        return self.frame.f_locals\n",
        "+\n",
        "+    def getfirstlinesource(self) -> int:\n",
        "+        return self.frame.code.firstlineno\n",
        "+\n",
        "+    def getsource(\n",
        "+        self, astcache: dict[str | Path, ast.AST] | None = None\n",
        "+    ) -> Source | None:\n",
        "+        \"\"\"Return failing source code.\"\"\"\n",
        "+        # we use the passed in astcache to not reparse asttrees\n",
        "+        # within exception info printing\n",
        "+        source = self.frame.code.fullsource\n",
        "+        if source is None:\n",
        "+            return None\n",
        "+        key = astnode = None\n",
        "+        if astcache is not None:\n",
        "+            key = self.frame.code.path\n",
        "+            if key is not None:\n",
        "+                astnode = astcache.get(key, None)\n",
        "+        start = self.getfirstlinesource()\n",
        "+        try:\n",
        "+            astnode, _, end = getstatementrange_ast(\n",
        "+                self.lineno, source, astnode=astnode\n",
        "+            )\n",
        "+        except SyntaxError:\n",
        "+            end = self.lineno + 1\n",
        "+        else:\n",
        "+            if key is not None and astcache is not None:\n",
        "+                astcache[key] = astnode\n",
        "+        return source[start:end]\n",
        "+\n",
        "+    source = property(getsource)\n",
        "+\n",
        "+    def ishidden(self, excinfo: ExceptionInfo[BaseException] | None) -> bool:\n",
        "+        \"\"\"Return True if the current frame has a var __tracebackhide__\n",
        "+        resolving to True.\n",
        "+\n",
        "+        If __tracebackhide__ is a callable, it gets called with the\n",
        "+        ExceptionInfo instance and can decide whether to hide the traceback.\n",
        "+\n",
        "+        Mostly for internal use.\n",
        "+        \"\"\"\n",
        "+        tbh: bool | Callable[[ExceptionInfo[BaseException] | None], bool] = False\n",
        "+        for maybe_ns_dct in (self.frame.f_locals, self.frame.f_globals):\n",
        "+            # in normal cases, f_locals and f_globals are dictionaries\n",
        "+            # however via `exec(...)` / `eval(...)` they can be other types\n",
        "+            # (even incorrect types!).\n",
        "+            # as such, we suppress all exceptions while accessing __tracebackhide__\n",
        "+            try:\n",
        "+                tbh = maybe_ns_dct[\"__tracebackhide__\"]\n",
        "+            except Exception:\n",
        "+                pass\n",
        "+            else:\n",
        "+                break\n",
        "+        if tbh and callable(tbh):\n",
        "+            return tbh(excinfo)\n",
        "+        return tbh\n",
        "+\n",
        "+    def __str__(self) -> str:\n",
        "+        name = self.frame.code.name\n",
        "+        try:\n",
        "+            line = str(self.statement).lstrip()\n",
        "+        except KeyboardInterrupt:\n",
        "+            raise\n",
        "+        except BaseException:\n",
        "+            line = \"???\"\n",
        "+        # This output does not quite match Python's repr for traceback entries,\n",
        "+        # but changing it to do so would break certain plugins.  See\n",
        "+        # https://github.com/pytest-dev/pytest/pull/7535/ for details.\n",
        "+        return f\"  File '{self.path}':{self.lineno + 1} in {name}\\n  {line}\\n\"\n",
        "+\n",
        "+    @property\n",
        "+    def name(self) -> str:\n",
        "+        \"\"\"co_name of underlying code.\"\"\"\n",
        "+        return self.frame.code.raw.co_name\n",
        "+\n",
        "+\n",
        "+class Traceback(list[TracebackEntry]):\n",
        "+    \"\"\"Traceback objects encapsulate and offer higher level access to Traceback entries.\"\"\"\n",
        "+\n",
        "+    def __init__(\n",
        "+        self,\n",
        "+        tb: TracebackType | Iterable[TracebackEntry],\n",
        "+    ) -> None:\n",
        "+        \"\"\"Initialize from given python traceback object and ExceptionInfo.\"\"\"\n",
        "+        if isinstance(tb, TracebackType):\n",
        "+\n",
        "+            def f(cur: TracebackType) -> Iterable[TracebackEntry]:\n",
        "+                cur_: TracebackType | None = cur\n",
        "+                while cur_ is not None:\n",
        "+                    yield TracebackEntry(cur_)\n",
        "+                    cur_ = cur_.tb_next\n",
        "+\n",
        "+            super().__init__(f(tb))\n",
        "+        else:\n",
        "+            super().__init__(tb)\n",
        "+\n",
        "+    def cut(\n",
        "+        self,\n",
        "+        path: os.PathLike[str] | str | None = None,\n",
        "+        lineno: int | None = None,\n",
        "+        firstlineno: int | None = None,\n",
        "+        excludepath: os.PathLike[str] | None = None,\n",
        "+    ) -> Traceback:\n",
        "+        \"\"\"Return a Traceback instance wrapping part of this Traceback.\n",
        "+\n",
        "+        By providing any combination of path, lineno and firstlineno, the\n",
        "+        first frame to start the to-be-returned traceback is determined.\n",
        "+\n",
        "+        This allows cutting the first part of a Traceback instance e.g.\n",
        "+        for formatting reasons (removing some uninteresting bits that deal\n",
        "+        with handling of the exception/traceback).\n",
        "+        \"\"\"\n",
        "+        path_ = None if path is None else os.fspath(path)\n",
        "+        excludepath_ = None if excludepath is None else os.fspath(excludepath)\n",
        "+        for x in self:\n",
        "+            code = x.frame.code\n",
        "+            codepath = code.path\n",
        "+            if path is not None and str(codepath) != path_:\n",
        "+                continue\n",
        "+            if (\n",
        "+                excludepath is not None\n",
        "+                and isinstance(codepath, Path)\n",
        "+                and excludepath_ in (str(p) for p in codepath.parents)  # type: ignore[operator]\n",
        "+            ):\n",
        "+                continue\n",
        "+            if lineno is not None and x.lineno != lineno:\n",
        "+                continue\n",
        "+            if firstlineno is not None and x.frame.code.firstlineno != firstlineno:\n",
        "+                continue\n",
        "+            return Traceback(x._rawentry)\n",
        "+        return self\n",
        "+\n",
        "+    @overload\n",
        "+    def __getitem__(self, key: SupportsIndex) -> TracebackEntry: ...\n",
        "+\n",
        "+    @overload\n",
        "+    def __getitem__(self, key: slice) -> Traceback: ...\n",
        "+\n",
        "+    def __getitem__(self, key: SupportsIndex | slice) -> TracebackEntry | Traceback:\n",
        "+        if isinstance(key, slice):\n",
        "+            return self.__class__(super().__getitem__(key))\n",
        "+        else:\n",
        "+            return super().__getitem__(key)\n",
        "+\n",
        "+    def filter(\n",
        "+        self,\n",
        "+        excinfo_or_fn: ExceptionInfo[BaseException] | Callable[[TracebackEntry], bool],\n",
        "+        /,\n",
        "+    ) -> Traceback:\n",
        "+        \"\"\"Return a Traceback instance with certain items removed.\n",
        "+\n",
        "+        If the filter is an `ExceptionInfo`, removes all the ``TracebackEntry``s\n",
        "+        which are hidden (see ishidden() above).\n",
        "+\n",
        "+        Otherwise, the filter is a function that gets a single argument, a\n",
        "+        ``TracebackEntry`` instance, and should return True when the item should\n",
        "+        be added to the ``Traceback``, False when not.\n",
        "+        \"\"\"\n",
        "+        if isinstance(excinfo_or_fn, ExceptionInfo):\n",
        "+            fn = lambda x: not x.ishidden(excinfo_or_fn)  # noqa: E731\n",
        "+        else:\n",
        "+            fn = excinfo_or_fn\n",
        "+        return Traceback(filter(fn, self))\n",
        "+\n",
        "+    def recursionindex(self) -> int | None:\n",
        "+        \"\"\"Return the index of the frame/TracebackEntry where recursion originates if\n",
        "+        appropriate, None if no recursion occurred.\"\"\"\n",
        "+        cache: dict[tuple[Any, int, int], list[dict[str, Any]]] = {}\n",
        "+        for i, entry in enumerate(self):\n",
        "+            # id for the code.raw is needed to work around\n",
        "+            # the strange metaprogramming in the decorator lib from pypi\n",
        "+            # which generates code objects that have hash/value equality\n",
        "+            # XXX needs a test\n",
        "+            key = entry.frame.code.path, id(entry.frame.code.raw), entry.lineno\n",
        "+            values = cache.setdefault(key, [])\n",
        "+            # Since Python 3.13 f_locals is a proxy, freeze it.\n",
        "+            loc = dict(entry.frame.f_locals)\n",
        "+            if values:\n",
        "+                for otherloc in values:\n",
        "+                    if otherloc == loc:\n",
        "+                        return i\n",
        "+            values.append(loc)\n",
        "+        return None\n",
        "+\n",
        "+\n",
        "+def stringify_exception(\n",
        "+    exc: BaseException, include_subexception_msg: bool = True\n",
        "+) -> str:\n",
        "+    try:\n",
        "+        notes = getattr(exc, \"__notes__\", [])\n",
        "+    except KeyError:\n",
        "+        # Workaround for https://github.com/python/cpython/issues/98778 on\n",
        "+        # Python <= 3.9, and some 3.10 and 3.11 patch versions.\n",
        "+        HTTPError = getattr(sys.modules.get(\"urllib.error\", None), \"HTTPError\", ())\n",
        "+        if sys.version_info < (3, 12) and isinstance(exc, HTTPError):\n",
        "+            notes = []\n",
        "+        else:  # pragma: no cover\n",
        "+            # exception not related to above bug, reraise\n",
        "+            raise\n",
        "+    if not include_subexception_msg and isinstance(exc, BaseExceptionGroup):\n",
        "+        message = exc.message\n",
        "+    else:\n",
        "+        message = str(exc)\n",
        "+\n",
        "+    return \"\\n\".join(\n",
        "+        [\n",
        "+            message,\n",
        "+            *notes,\n",
        "+        ]\n",
        "+    )\n",
        "+\n",
        "+\n",
        "+E = TypeVar(\"E\", bound=BaseException, covariant=True)\n",
        "+\n",
        "+\n",
        "+@final\n",
        "+@dataclasses.dataclass\n",
        "+class ExceptionInfo(Generic[E]):\n",
        "+    \"\"\"Wraps sys.exc_info() objects and offers help for navigating the traceback.\"\"\"\n",
        "+\n",
        "+    _assert_start_repr: ClassVar = \"AssertionError('assert \"\n",
        "+\n",
        "+    _excinfo: tuple[type[E], E, TracebackType] | None\n",
        "+    _striptext: str\n",
        "+    _traceback: Traceback | None\n",
        "+\n",
        "+    def __init__(\n",
        "+        self,\n",
        "+        excinfo: tuple[type[E], E, TracebackType] | None,\n",
        "+        striptext: str = \"\",\n",
        "+        traceback: Traceback | None = None,\n",
        "+        *,\n",
        "+        _ispytest: bool = False,\n",
        "+    ) -> None:\n",
        "+        check_ispytest(_ispytest)\n",
        "+        self._excinfo = excinfo\n",
        "+        self._striptext = striptext\n",
        "+        self._traceback = traceback\n",
        "+\n",
        "+    @classmethod\n",
        "+    def from_exception(\n",
        "+        cls,\n",
        "+        # Ignoring error: \"Cannot use a covariant type variable as a parameter\".\n",
        "+        # This is OK to ignore because this class is (conceptually) readonly.\n",
        "+        # See https://github.com/python/mypy/issues/7049.\n",
        "+        exception: E,  # type: ignore[misc]\n",
        "+        exprinfo: str | None = None,\n",
        "+    ) -> ExceptionInfo[E]:\n",
        "+        \"\"\"Return an ExceptionInfo for an existing exception.\n",
        "+\n",
        "+        The exception must have a non-``None`` ``__traceback__`` attribute,\n",
        "+        otherwise this function fails with an assertion error. This means that\n",
        "+        the exception must have been raised, or added a traceback with the\n",
        "+        :py:meth:`~BaseException.with_traceback()` method.\n",
        "+\n",
        "+        :param exprinfo:\n",
        "+            A text string helping to determine if we should strip\n",
        "+            ``AssertionError`` from the output. Defaults to the exception\n",
        "+            message/``__str__()``.\n",
        "+\n",
        "+        .. versionadded:: 7.4\n",
        "+        \"\"\"\n",
        "+        assert exception.__traceback__, (\n",
        "+            \"Exceptions passed to ExcInfo.from_exception(...)\"\n",
        "+            \" must have a non-None __traceback__.\"\n",
        "+        )\n",
        "+        exc_info = (type(exception), exception, exception.__traceback__)\n",
        "+        return cls.from_exc_info(exc_info, exprinfo)\n",
        "+\n",
        "+    @classmethod\n",
        "+    def from_exc_info(\n",
        "+        cls,\n",
        "+        exc_info: tuple[type[E], E, TracebackType],\n",
        "+        exprinfo: str | None = None,\n",
        "+    ) -> ExceptionInfo[E]:\n",
        "+        \"\"\"Like :func:`from_exception`, but using old-style exc_info tuple.\"\"\"\n",
        "+        _striptext = \"\"\n",
        "+        if exprinfo is None and isinstance(exc_info[1], AssertionError):\n",
        "+            exprinfo = getattr(exc_info[1], \"msg\", None)\n",
        "+            if exprinfo is None:\n",
        "+                exprinfo = saferepr(exc_info[1])\n",
        "+            if exprinfo and exprinfo.startswith(cls._assert_start_repr):\n",
        "+                _striptext = \"AssertionError: \"\n",
        "+\n",
        "+        return cls(exc_info, _striptext, _ispytest=True)\n",
        "+\n",
        "+    @classmethod\n",
        "+    def from_current(cls, exprinfo: str | None = None) -> ExceptionInfo[BaseException]:\n",
        "+        \"\"\"Return an ExceptionInfo matching the current traceback.\n",
        "+\n",
        "+        .. warning::\n",
        "+\n",
        "+            Experimental API\n",
        "+\n",
        "+        :param exprinfo:\n",
        "+            A text string helping to determine if we should strip\n",
        "+            ``AssertionError`` from the output. Defaults to the exception\n",
        "+            message/``__str__()``.\n",
        "+        \"\"\"\n",
        "+        tup = sys.exc_info()\n",
        "+        assert tup[0] is not None, \"no current exception\"\n",
        "+        assert tup[1] is not None, \"no current exception\"\n",
        "+        assert tup[2] is not None, \"no current exception\"\n",
        "+        exc_info = (tup[0], tup[1], tup[2])\n",
        "+        return ExceptionInfo.from_exc_info(exc_info, exprinfo)\n",
        "+\n",
        "+    @classmethod\n",
        "+    def for_later(cls) -> ExceptionInfo[E]:\n",
        "+        \"\"\"Return an unfilled ExceptionInfo.\"\"\"\n",
        "+        return cls(None, _ispytest=True)\n",
        "+\n",
        "+    def fill_unfilled(self, exc_info: tuple[type[E], E, TracebackType]) -> None:\n",
        "+        \"\"\"Fill an unfilled ExceptionInfo created with ``for_later()``.\"\"\"\n",
        "+        assert self._excinfo is None, \"ExceptionInfo was already filled\"\n",
        "+        self._excinfo = exc_info\n",
        "+\n",
        "+    @property\n",
        "+    def type(self) -> type[E]:\n",
        "+        \"\"\"The exception class.\"\"\"\n",
        "+        assert self._excinfo is not None, (\n",
        "+            \".type can only be used after the context manager exits\"\n",
        "+        )\n",
        "+        return self._excinfo[0]\n",
        "+\n",
        "+    @property\n",
        "+    def value(self) -> E:\n",
        "+        \"\"\"The exception value.\"\"\"\n",
        "+        assert self._excinfo is not None, (\n",
        "+            \".value can only be used after the context manager exits\"\n",
        "+        )\n",
        "+        return self._excinfo[1]\n",
        "+\n",
        "+    @property\n",
        "+    def tb(self) -> TracebackType:\n",
        "+        \"\"\"The exception raw traceback.\"\"\"\n",
        "+        assert self._excinfo is not None, (\n",
        "+            \".tb can only be used after the context manager exits\"\n",
        "+        )\n",
        "+        return self._excinfo[2]\n",
        "+\n",
        "+    @property\n",
        "+    def typename(self) -> str:\n",
        "+        \"\"\"The type name of the exception.\"\"\"\n",
        "+        assert self._excinfo is not None, (\n",
        "+            \".typename can only be used after the context manager exits\"\n",
        "+        )\n",
        "+        return self.type.__name__\n",
        "+\n",
        "+    @property\n",
        "+    def traceback(self) -> Traceback:\n",
        "+        \"\"\"The traceback.\"\"\"\n",
        "+        if self._traceback is None:\n",
        "+            self._traceback = Traceback(self.tb)\n",
        "+        return self._traceback\n",
        "+\n",
        "+    @traceback.setter\n",
        "+    def traceback(self, value: Traceback) -> None:\n",
        "+        self._traceback = value\n",
        "+\n",
        "+    def __repr__(self) -> str:\n",
        "+        if self._excinfo is None:\n",
        "+            return \"<ExceptionInfo for raises contextmanager>\"\n",
        "+        return f\"<{self.__class__.__name__} {saferepr(self._excinfo[1])} tblen={len(self.traceback)}>\"\n",
        "+\n",
        "+    def exconly(self, tryshort: bool = False) -> str:\n",
        "+        \"\"\"Return the exception as a string.\n",
        "+\n",
        "+        When 'tryshort' resolves to True, and the exception is an\n",
        "+        AssertionError, only the actual exception part of the exception\n",
        "+        representation is returned (so 'AssertionError: ' is removed from\n",
        "+        the beginning).\n",
        "+        \"\"\"\n",
        "+\n",
        "+        def _get_single_subexc(\n",
        "+            eg: BaseExceptionGroup[BaseException],\n",
        "+        ) -> BaseException | None:\n",
        "+            if len(eg.exceptions) != 1:\n",
        "+                return None\n",
        "+            if isinstance(e := eg.exceptions[0], BaseExceptionGroup):\n",
        "+                return _get_single_subexc(e)\n",
        "+            return e\n",
        "+\n",
        "+        if (\n",
        "+            tryshort\n",
        "+            and isinstance(self.value, BaseExceptionGroup)\n",
        "+            and (subexc := _get_single_subexc(self.value)) is not None\n",
        "+        ):\n",
        "+            return f\"{subexc!r} [single exception in {type(self.value).__name__}]\"\n",
        "+\n",
        "+        lines = format_exception_only(self.type, self.value)\n",
        "+        text = \"\".join(lines)\n",
        "+        text = text.rstrip()\n",
        "+        if tryshort:\n",
        "+            if text.startswith(self._striptext):\n",
        "+                text = text[len(self._striptext) :]\n",
        "+        return text\n",
        "+\n",
        "+    def errisinstance(self, exc: EXCEPTION_OR_MORE) -> bool:\n",
        "+        \"\"\"Return True if the exception is an instance of exc.\n",
        "+\n",
        "+        Consider using ``isinstance(excinfo.value, exc)`` instead.\n",
        "+        \"\"\"\n",
        "+        return isinstance(self.value, exc)\n",
        "+\n",
        "+    def _getreprcrash(self) -> ReprFileLocation | None:\n",
        "+        # Find last non-hidden traceback entry that led to the exception of the\n",
        "+        # traceback, or None if all hidden.\n",
        "+        for i in range(-1, -len(self.traceback) - 1, -1):\n",
        "+            entry = self.traceback[i]\n",
        "+            if not entry.ishidden(self):\n",
        "+                path, lineno = entry.frame.code.raw.co_filename, entry.lineno\n",
        "+                exconly = self.exconly(tryshort=True)\n",
        "+                return ReprFileLocation(path, lineno + 1, exconly)\n",
        "+        return None\n",
        "+\n",
        "+    def getrepr(\n",
        "+        self,\n",
        "+        showlocals: bool = False,\n",
        "+        style: TracebackStyle = \"long\",\n",
        "+        abspath: bool = False,\n",
        "+        tbfilter: bool | Callable[[ExceptionInfo[BaseException]], Traceback] = True,\n",
        "+        funcargs: bool = False,\n",
        "+        truncate_locals: bool = True,\n",
        "+        truncate_args: bool = True,\n",
        "+        chain: bool = True,\n",
        "+    ) -> ReprExceptionInfo | ExceptionChainRepr:\n",
        "+        \"\"\"Return str()able representation of this exception info.\n",
        "+\n",
        "+        :param bool showlocals:\n",
        "+            Show locals per traceback entry.\n",
        "+            Ignored if ``style==\"native\"``.\n",
        "+\n",
        "+        :param str style:\n",
        "+            long|short|line|no|native|value traceback style.\n",
        "+\n",
        "+        :param bool abspath:\n",
        "+            If paths should be changed to absolute or left unchanged.\n",
        "+\n",
        "+        :param tbfilter:\n",
        "+            A filter for traceback entries.\n",
        "+\n",
        "+            * If false, don't hide any entries.\n",
        "+            * If true, hide internal entries and entries that contain a local\n",
        "+              variable ``__tracebackhide__ = True``.\n",
        "+            * If a callable, delegates the filtering to the callable.\n",
        "+\n",
        "+            Ignored if ``style`` is ``\"native\"``.\n",
        "+\n",
        "+        :param bool funcargs:\n",
        "+            Show fixtures (\"funcargs\" for legacy purposes) per traceback entry.\n",
        "+\n",
        "+        :param bool truncate_locals:\n",
        "+            With ``showlocals==True``, make sure locals can be safely represented as strings.\n",
        "+\n",
        "+        :param bool truncate_args:\n",
        "+            With ``showargs==True``, make sure args can be safely represented as strings.\n",
        "+\n",
        "+        :param bool chain:\n",
        "+            If chained exceptions in Python 3 should be shown.\n",
        "+\n",
        "+        .. versionchanged:: 3.9\n",
        "+\n",
        "+            Added the ``chain`` parameter.\n",
        "+        \"\"\"\n",
        "+        if style == \"native\":\n",
        "+            return ReprExceptionInfo(\n",
        "+                reprtraceback=ReprTracebackNative(\n",
        "+                    format_exception(\n",
        "+                        self.type,\n",
        "+                        self.value,\n",
        "+                        self.traceback[0]._rawentry if self.traceback else None,\n",
        "+                    )\n",
        "+                ),\n",
        "+                reprcrash=self._getreprcrash(),\n",
        "+            )\n",
        "+\n",
        "+        fmt = FormattedExcinfo(\n",
        "+            showlocals=showlocals,\n",
        "+            style=style,\n",
        "+            abspath=abspath,\n",
        "+            tbfilter=tbfilter,\n",
        "+            funcargs=funcargs,\n",
        "+            truncate_locals=truncate_locals,\n",
        "+            truncate_args=truncate_args,\n",
        "+            chain=chain,\n",
        "+        )\n",
        "+        return fmt.repr_excinfo(self)\n",
        "+\n",
        "+    def match(self, regexp: str | re.Pattern[str]) -> Literal[True]:\n",
        "+        \"\"\"Check whether the regular expression `regexp` matches the string\n",
        "+        representation of the exception using :func:`python:re.search`.\n",
        "+\n",
        "+        If it matches `True` is returned, otherwise an `AssertionError` is raised.\n",
        "+        \"\"\"\n",
        "+        __tracebackhide__ = True\n",
        "+        value = stringify_exception(self.value)\n",
        "+        msg = f\"Regex pattern did not match.\\n Regex: {regexp!r}\\n Input: {value!r}\"\n",
        "+        if regexp == value:\n",
        "+            msg += \"\\n Did you mean to `re.escape()` the regex?\"\n",
        "+        assert re.search(regexp, value), msg\n",
        "+        # Return True to allow for \"assert excinfo.match()\".\n",
        "+        return True\n",
        "+\n",
        "+    def _group_contains(\n",
        "+        self,\n",
        "+        exc_group: BaseExceptionGroup[BaseException],\n",
        "+        expected_exception: EXCEPTION_OR_MORE,\n",
        "+        match: str | re.Pattern[str] | None,\n",
        "+        target_depth: int | None = None,\n",
        "+        current_depth: int = 1,\n",
        "+    ) -> bool:\n",
        "+        \"\"\"Return `True` if a `BaseExceptionGroup` contains a matching exception.\"\"\"\n",
        "+        if (target_depth is not None) and (current_depth > target_depth):\n",
        "+            # already descended past the target depth\n",
        "+            return False\n",
        "+        for exc in exc_group.exceptions:\n",
        "+            if isinstance(exc, BaseExceptionGroup):\n",
        "+                if self._group_contains(\n",
        "+                    exc, expected_exception, match, target_depth, current_depth + 1\n",
        "+                ):\n",
        "+                    return True\n",
        "+            if (target_depth is not None) and (current_depth != target_depth):\n",
        "+                # not at the target depth, no match\n",
        "+                continue\n",
        "+            if not isinstance(exc, expected_exception):\n",
        "+                continue\n",
        "+            if match is not None:\n",
        "+                value = stringify_exception(exc)\n",
        "+                if not re.search(match, value):\n",
        "+                    continue\n",
        "+            return True\n",
        "+        return False\n",
        "+\n",
        "+    def group_contains(\n",
        "+        self,\n",
        "+        expected_exception: EXCEPTION_OR_MORE,\n",
        "+        *,\n",
        "+        match: str | re.Pattern[str] | None = None,\n",
        "+        depth: int | None = None,\n",
        "+    ) -> bool:\n",
        "+        \"\"\"Check whether a captured exception group contains a matching exception.\n",
        "+\n",
        "+        :param Type[BaseException] | Tuple[Type[BaseException]] expected_exception:\n",
        "+            The expected exception type, or a tuple if one of multiple possible\n",
        "+            exception types are expected.\n",
        "+\n",
        "+        :param str | re.Pattern[str] | None match:\n",
        "+            If specified, a string containing a regular expression,\n",
        "+            or a regular expression object, that is tested against the string\n",
        "+            representation of the exception and its `PEP-678 <https://peps.python.org/pep-0678/>` `__notes__`\n",
        "+            using :func:`re.search`.\n",
        "+\n",
        "+            To match a literal string that may contain :ref:`special characters\n",
        "+            <re-syntax>`, the pattern can first be escaped with :func:`re.escape`.\n",
        "+\n",
        "+        :param Optional[int] depth:\n",
        "+            If `None`, will search for a matching exception at any nesting depth.\n",
        "+            If >= 1, will only match an exception if it's at the specified depth (depth = 1 being\n",
        "+            the exceptions contained within the topmost exception group).\n",
        "+\n",
        "+        .. versionadded:: 8.0\n",
        "+\n",
        "+        .. warning::\n",
        "+           This helper makes it easy to check for the presence of specific exceptions,\n",
        "+           but it is very bad for checking that the group does *not* contain\n",
        "+           *any other exceptions*.\n",
        "+           You should instead consider using :class:`pytest.RaisesGroup`\n",
        "+\n",
        "+        \"\"\"\n",
        "+        msg = \"Captured exception is not an instance of `BaseExceptionGroup`\"\n",
        "+        assert isinstance(self.value, BaseExceptionGroup), msg\n",
        "+        msg = \"`depth` must be >= 1 if specified\"\n",
        "+        assert (depth is None) or (depth >= 1), msg\n",
        "+        return self._group_contains(self.value, expected_exception, match, depth)\n",
        "+\n",
        "+\n",
        "+if TYPE_CHECKING:\n",
        "+    from typing_extensions import TypeAlias\n",
        "+\n",
        "+    # Type alias for the `tbfilter` setting:\n",
        "+    # bool: If True, it should be filtered using Traceback.filter()\n",
        "+    # callable: A callable that takes an ExceptionInfo and returns the filtered traceback.\n",
        "+    TracebackFilter: TypeAlias = Union[\n",
        "+        bool, Callable[[ExceptionInfo[BaseException]], Traceback]\n",
        "+    ]\n",
        "+\n",
        "+\n",
        "+@dataclasses.dataclass\n",
        "+class FormattedExcinfo:\n",
        "+    \"\"\"Presenting information about failing Functions and Generators.\"\"\"\n",
        "+\n",
        "+    # for traceback entries\n",
        "+    flow_marker: ClassVar = \">\"\n",
        "+    fail_marker: ClassVar = \"E\"\n",
        "+\n",
        "+    showlocals: bool = False\n",
        "+    style: TracebackStyle = \"long\"\n",
        "+    abspath: bool = True\n",
        "+    tbfilter: TracebackFilter = True\n",
        "+    funcargs: bool = False\n",
        "+    truncate_locals: bool = True\n",
        "+    truncate_args: bool = True\n",
        "+    chain: bool = True\n",
        "+    astcache: dict[str | Path, ast.AST] = dataclasses.field(\n",
        "+        default_factory=dict, init=False, repr=False\n",
        "+    )\n",
        "+\n",
        "+    def _getindent(self, source: Source) -> int:\n",
        "+        # Figure out indent for the given source.\n",
        "+        try:\n",
        "+            s = str(source.getstatement(len(source) - 1))\n",
        "+        except KeyboardInterrupt:\n",
        "+            raise\n",
        "+        except BaseException:\n",
        "+            try:\n",
        "+                s = str(source[-1])\n",
        "+            except KeyboardInterrupt:\n",
        "+                raise\n",
        "+            except BaseException:\n",
        "+                return 0\n",
        "+        return 4 + (len(s) - len(s.lstrip()))\n",
        "+\n",
        "+    def _getentrysource(self, entry: TracebackEntry) -> Source | None:\n",
        "+        source = entry.getsource(self.astcache)\n",
        "+        if source is not None:\n",
        "+            source = source.deindent()\n",
        "+        return source\n",
        "+\n",
        "+    def repr_args(self, entry: TracebackEntry) -> ReprFuncArgs | None:\n",
        "+        if self.funcargs:\n",
        "+            args = []\n",
        "+            for argname, argvalue in entry.frame.getargs(var=True):\n",
        "+                if self.truncate_args:\n",
        "+                    str_repr = saferepr(argvalue)\n",
        "+                else:\n",
        "+                    str_repr = saferepr(argvalue, maxsize=None)\n",
        "+                args.append((argname, str_repr))\n",
        "+            return ReprFuncArgs(args)\n",
        "+        return None\n",
        "+\n",
        "+    def get_source(\n",
        "+        self,\n",
        "+        source: Source | None,\n",
        "+        line_index: int = -1,\n",
        "+        excinfo: ExceptionInfo[BaseException] | None = None,\n",
        "+        short: bool = False,\n",
        "+        end_line_index: int | None = None,\n",
        "+        colno: int | None = None,\n",
        "+        end_colno: int | None = None,\n",
        "+    ) -> list[str]:\n",
        "+        \"\"\"Return formatted and marked up source lines.\"\"\"\n",
        "+        lines = []\n",
        "+        if source is not None and line_index < 0:\n",
        "+            line_index += len(source)\n",
        "+        if source is None or line_index >= len(source.lines) or line_index < 0:\n",
        "+            # `line_index` could still be outside `range(len(source.lines))` if\n",
        "+            # we're processing AST with pathological position attributes.\n",
        "+            source = Source(\"???\")\n",
        "+            line_index = 0\n",
        "+        space_prefix = \"    \"\n",
        "+        if short:\n",
        "+            lines.append(space_prefix + source.lines[line_index].strip())\n",
        "+            lines.extend(\n",
        "+                self.get_highlight_arrows_for_line(\n",
        "+                    raw_line=source.raw_lines[line_index],\n",
        "+                    line=source.lines[line_index].strip(),\n",
        "+                    lineno=line_index,\n",
        "+                    end_lineno=end_line_index,\n",
        "+                    colno=colno,\n",
        "+                    end_colno=end_colno,\n",
        "+                )\n",
        "+            )\n",
        "+        else:\n",
        "+            for line in source.lines[:line_index]:\n",
        "+                lines.append(space_prefix + line)\n",
        "+            lines.append(self.flow_marker + \"   \" + source.lines[line_index])\n",
        "+            lines.extend(\n",
        "+                self.get_highlight_arrows_for_line(\n",
        "+                    raw_line=source.raw_lines[line_index],\n",
        "+                    line=source.lines[line_index],\n",
        "+                    lineno=line_index,\n",
        "+                    end_lineno=end_line_index,\n",
        "+                    colno=colno,\n",
        "+                    end_colno=end_colno,\n",
        "+                )\n",
        "+            )\n",
        "+            for line in source.lines[line_index + 1 :]:\n",
        "+                lines.append(space_prefix + line)\n",
        "+        if excinfo is not None:\n",
        "+            indent = 4 if short else self._getindent(source)\n",
        "+            lines.extend(self.get_exconly(excinfo, indent=indent, markall=True))\n",
        "+        return lines\n",
        "+\n",
        "+    def get_highlight_arrows_for_line(\n",
        "+        self,\n",
        "+        line: str,\n",
        "+        raw_line: str,\n",
        "+        lineno: int | None,\n",
        "+        end_lineno: int | None,\n",
        "+        colno: int | None,\n",
        "+        end_colno: int | None,\n",
        "+    ) -> list[str]:\n",
        "+        \"\"\"Return characters highlighting a source line.\n",
        "+\n",
        "+        Example with colno and end_colno pointing to the bar expression:\n",
        "+                   \"foo() + bar()\"\n",
        "+        returns    \"        ^^^^^\"\n",
        "+        \"\"\"\n",
        "+        if lineno != end_lineno:\n",
        "+            # Don't handle expressions that span multiple lines.\n",
        "+            return []\n",
        "+        if colno is None or end_colno is None:\n",
        "+            # Can't do anything without column information.\n",
        "+            return []\n",
        "+\n",
        "+        num_stripped_chars = len(raw_line) - len(line)\n",
        "+\n",
        "+        start_char_offset = _byte_offset_to_character_offset(raw_line, colno)\n",
        "+        end_char_offset = _byte_offset_to_character_offset(raw_line, end_colno)\n",
        "+        num_carets = end_char_offset - start_char_offset\n",
        "+        # If the highlight would span the whole line, it is redundant, don't\n",
        "+        # show it.\n",
        "+        if num_carets >= len(line.strip()):\n",
        "+            return []\n",
        "+\n",
        "+        highlights = \"    \"\n",
        "+        highlights += \" \" * (start_char_offset - num_stripped_chars + 1)\n",
        "+        highlights += \"^\" * num_carets\n",
        "+        return [highlights]\n",
        "+\n",
        "+    def get_exconly(\n",
        "+        self,\n",
        "+        excinfo: ExceptionInfo[BaseException],\n",
        "+        indent: int = 4,\n",
        "+        markall: bool = False,\n",
        "+    ) -> list[str]:\n",
        "+        lines = []\n",
        "+        indentstr = \" \" * indent\n",
        "+        # Get the real exception information out.\n",
        "+        exlines = excinfo.exconly(tryshort=True).split(\"\\n\")\n",
        "+        failindent = self.fail_marker + indentstr[1:]\n",
        "+        for line in exlines:\n",
        "+            lines.append(failindent + line)\n",
        "+            if not markall:\n",
        "+                failindent = indentstr\n",
        "+        return lines\n",
        "+\n",
        "+    def repr_locals(self, locals: Mapping[str, object]) -> ReprLocals | None:\n",
        "+        if self.showlocals:\n",
        "+            lines = []\n",
        "+            keys = [loc for loc in locals if loc[0] != \"@\"]\n",
        "+            keys.sort()\n",
        "+            for name in keys:\n",
        "+                value = locals[name]\n",
        "+                if name == \"__builtins__\":\n",
        "+                    lines.append(\"__builtins__ = <builtins>\")\n",
        "+                else:\n",
        "+                    # This formatting could all be handled by the\n",
        "+                    # _repr() function, which is only reprlib.Repr in\n",
        "+                    # disguise, so is very configurable.\n",
        "+                    if self.truncate_locals:\n",
        "+                        str_repr = saferepr(value)\n",
        "+                    else:\n",
        "+                        str_repr = safeformat(value)\n",
        "+                    # if len(str_repr) < 70 or not isinstance(value, (list, tuple, dict)):\n",
        "+                    lines.append(f\"{name:<10} = {str_repr}\")\n",
        "+                    # else:\n",
        "+                    #    self._line(\"%-10s =\\\\\" % (name,))\n",
        "+                    #    # XXX\n",
        "+                    #    pprint.pprint(value, stream=self.excinfowriter)\n",
        "+            return ReprLocals(lines)\n",
        "+        return None\n",
        "+\n",
        "+    def repr_traceback_entry(\n",
        "+        self,\n",
        "+        entry: TracebackEntry | None,\n",
        "+        excinfo: ExceptionInfo[BaseException] | None = None,\n",
        "+    ) -> ReprEntry:\n",
        "+        lines: list[str] = []\n",
        "+        style = (\n",
        "+            entry._repr_style\n",
        "+            if entry is not None and entry._repr_style is not None\n",
        "+            else self.style\n",
        "+        )\n",
        "+        if style in (\"short\", \"long\") and entry is not None:\n",
        "+            source = self._getentrysource(entry)\n",
        "+            if source is None:\n",
        "+                source = Source(\"???\")\n",
        "+                line_index = 0\n",
        "+                end_line_index, colno, end_colno = None, None, None\n",
        "+            else:\n",
        "+                line_index = entry.relline\n",
        "+                end_line_index = entry.end_lineno_relative\n",
        "+                colno = entry.colno\n",
        "+                end_colno = entry.end_colno\n",
        "+            short = style == \"short\"\n",
        "+            reprargs = self.repr_args(entry) if not short else None\n",
        "+            s = self.get_source(\n",
        "+                source=source,\n",
        "+                line_index=line_index,\n",
        "+                excinfo=excinfo,\n",
        "+                short=short,\n",
        "+                end_line_index=end_line_index,\n",
        "+                colno=colno,\n",
        "+                end_colno=end_colno,\n",
        "+            )\n",
        "+            lines.extend(s)\n",
        "+            if short:\n",
        "+                message = f\"in {entry.name}\"\n",
        "+            else:\n",
        "+                message = (excinfo and excinfo.typename) or \"\"\n",
        "+            entry_path = entry.path\n",
        "+            path = self._makepath(entry_path)\n",
        "+            reprfileloc = ReprFileLocation(path, entry.lineno + 1, message)\n",
        "+            localsrepr = self.repr_locals(entry.locals)\n",
        "+            return ReprEntry(lines, reprargs, localsrepr, reprfileloc, style)\n",
        "+        elif style == \"value\":\n",
        "+            if excinfo:\n",
        "+                lines.extend(str(excinfo.value).split(\"\\n\"))\n",
        "+            return ReprEntry(lines, None, None, None, style)\n",
        "+        else:\n",
        "+            if excinfo:\n",
        "+                lines.extend(self.get_exconly(excinfo, indent=4))\n",
        "+            return ReprEntry(lines, None, None, None, style)\n",
        "+\n",
        "+    def _makepath(self, path: Path | str) -> str:\n",
        "+        if not self.abspath and isinstance(path, Path):\n",
        "+            try:\n",
        "+                np = bestrelpath(Path.cwd(), path)\n",
        "+            except OSError:\n",
        "+                return str(path)\n",
        "+            if len(np) < len(str(path)):\n",
        "+                return np\n",
        "+        return str(path)\n",
        "+\n",
        "+    def repr_traceback(self, excinfo: ExceptionInfo[BaseException]) -> ReprTraceback:\n",
        "+        traceback = filter_excinfo_traceback(self.tbfilter, excinfo)\n",
        "+\n",
        "+        if isinstance(excinfo.value, RecursionError):\n",
        "+            traceback, extraline = self._truncate_recursive_traceback(traceback)\n",
        "+        else:\n",
        "+            extraline = None\n",
        "+\n",
        "+        if not traceback:\n",
        "+            if extraline is None:\n",
        "+                extraline = \"All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.\"\n",
        "+            entries = [self.repr_traceback_entry(None, excinfo)]\n",
        "+            return ReprTraceback(entries, extraline, style=self.style)\n",
        "+\n",
        "+        last = traceback[-1]\n",
        "+        if self.style == \"value\":\n",
        "+            entries = [self.repr_traceback_entry(last, excinfo)]\n",
        "+            return ReprTraceback(entries, None, style=self.style)\n",
        "+\n",
        "+        entries = [\n",
        "+            self.repr_traceback_entry(entry, excinfo if last == entry else None)\n",
        "+            for entry in traceback\n",
        "+        ]\n",
        "+        return ReprTraceback(entries, extraline, style=self.style)\n",
        "+\n",
        "+    def _truncate_recursive_traceback(\n",
        "+        self, traceback: Traceback\n",
        "+    ) -> tuple[Traceback, str | None]:\n",
        "+        \"\"\"Truncate the given recursive traceback trying to find the starting\n",
        "+        point of the recursion.\n",
        "+\n",
        "+        The detection is done by going through each traceback entry and\n",
        "+        finding the point in which the locals of the frame are equal to the\n",
        "+        locals of a previous frame (see ``recursionindex()``).\n",
        "+\n",
        "+        Handle the situation where the recursion process might raise an\n",
        "+        exception (for example comparing numpy arrays using equality raises a\n",
        "+        TypeError), in which case we do our best to warn the user of the\n",
        "+        error and show a limited traceback.\n",
        "+        \"\"\"\n",
        "+        try:\n",
        "+            recursionindex = traceback.recursionindex()\n",
        "+        except Exception as e:\n",
        "+            max_frames = 10\n",
        "+            extraline: str | None = (\n",
        "+                \"!!! Recursion error detected, but an error occurred locating the origin of recursion.\\n\"\n",
        "+                \"  The following exception happened when comparing locals in the stack frame:\\n\"\n",
        "+                f\"    {type(e).__name__}: {e!s}\\n\"\n",
        "+                f\"  Displaying first and last {max_frames} stack frames out of {len(traceback)}.\"\n",
        "+            )\n",
        "+            # Type ignored because adding two instances of a List subtype\n",
        "+            # currently incorrectly has type List instead of the subtype.\n",
        "+            traceback = traceback[:max_frames] + traceback[-max_frames:]  # type: ignore\n",
        "+        else:\n",
        "+            if recursionindex is not None:\n",
        "+                extraline = \"!!! Recursion detected (same locals & position)\"\n",
        "+                traceback = traceback[: recursionindex + 1]\n",
        "+            else:\n",
        "+                extraline = None\n",
        "+\n",
        "+        return traceback, extraline\n",
        "+\n",
        "+    def repr_excinfo(self, excinfo: ExceptionInfo[BaseException]) -> ExceptionChainRepr:\n",
        "+        repr_chain: list[tuple[ReprTraceback, ReprFileLocation | None, str | None]] = []\n",
        "+        e: BaseException | None = excinfo.value\n",
        "+        excinfo_: ExceptionInfo[BaseException] | None = excinfo\n",
        "+        descr = None\n",
        "+        seen: set[int] = set()\n",
        "+        while e is not None and id(e) not in seen:\n",
        "+            seen.add(id(e))\n",
        "+\n",
        "+            if excinfo_:\n",
        "+                # Fall back to native traceback as a temporary workaround until\n",
        "+                # full support for exception groups added to ExceptionInfo.\n",
        "+                # See https://github.com/pytest-dev/pytest/issues/9159\n",
        "+                reprtraceback: ReprTraceback | ReprTracebackNative\n",
        "+                if isinstance(e, BaseExceptionGroup):\n",
        "+                    # don't filter any sub-exceptions since they shouldn't have any internal frames\n",
        "+                    traceback = filter_excinfo_traceback(self.tbfilter, excinfo)\n",
        "+                    reprtraceback = ReprTracebackNative(\n",
        "+                        format_exception(\n",
        "+                            type(excinfo.value),\n",
        "+                            excinfo.value,\n",
        "+                            traceback[0]._rawentry,\n",
        "+                        )\n",
        "+                    )\n",
        "+                else:\n",
        "+                    reprtraceback = self.repr_traceback(excinfo_)\n",
        "+                reprcrash = excinfo_._getreprcrash()\n",
        "+            else:\n",
        "+                # Fallback to native repr if the exception doesn't have a traceback:\n",
        "+                # ExceptionInfo objects require a full traceback to work.\n",
        "+                reprtraceback = ReprTracebackNative(format_exception(type(e), e, None))\n",
        "+                reprcrash = None\n",
        "+            repr_chain += [(reprtraceback, reprcrash, descr)]\n",
        "+\n",
        "+            if e.__cause__ is not None and self.chain:\n",
        "+                e = e.__cause__\n",
        "+                excinfo_ = ExceptionInfo.from_exception(e) if e.__traceback__ else None\n",
        "+                descr = \"The above exception was the direct cause of the following exception:\"\n",
        "+            elif (\n",
        "+                e.__context__ is not None and not e.__suppress_context__ and self.chain\n",
        "+            ):\n",
        "+                e = e.__context__\n",
        "+                excinfo_ = ExceptionInfo.from_exception(e) if e.__traceback__ else None\n",
        "+                descr = \"During handling of the above exception, another exception occurred:\"\n",
        "+            else:\n",
        "+                e = None\n",
        "+        repr_chain.reverse()\n",
        "+        return ExceptionChainRepr(repr_chain)\n",
        "+\n",
        "+\n",
        "+@dataclasses.dataclass(eq=False)\n",
        "+class TerminalRepr:\n",
        "+    def __str__(self) -> str:\n",
        "+        # FYI this is called from pytest-xdist's serialization of exception\n",
        "+        # information.\n",
        "+        io = StringIO()\n",
        "+        tw = TerminalWriter(file=io)\n",
        "+        self.toterminal(tw)\n",
        "+        return io.getvalue().strip()\n",
        "+\n",
        "+    def __repr__(self) -> str:\n",
        "+        return f\"<{self.__class__} instance at {id(self):0x}>\"\n",
        "+\n",
        "+    def toterminal(self, tw: TerminalWriter) -> None:\n",
        "+        raise NotImplementedError()\n",
        "+\n",
        "+\n",
        "+# This class is abstract -- only subclasses are instantiated.\n",
        "+@dataclasses.dataclass(eq=False)\n",
        "+class ExceptionRepr(TerminalRepr):\n",
        "+    # Provided by subclasses.\n",
        "+    reprtraceback: ReprTraceback\n",
        "+    reprcrash: ReprFileLocation | None\n",
        "+    sections: list[tuple[str, str, str]] = dataclasses.field(\n",
        "+        init=False, default_factory=list\n",
        "+    )\n",
        "+\n",
        "+    def addsection(self, name: str, content: str, sep: str = \"-\") -> None:\n",
        "+        self.sections.append((name, content, sep))\n",
        "+\n",
        "+    def toterminal(self, tw: TerminalWriter) -> None:\n",
        "+        for name, content, sep in self.sections:\n",
        "+            tw.sep(sep, name)\n",
        "+            tw.line(content)\n",
        "+\n",
        "+\n",
        "+@dataclasses.dataclass(eq=False)\n",
        "+class ExceptionChainRepr(ExceptionRepr):\n",
        "+    chain: Sequence[tuple[ReprTraceback, ReprFileLocation | None, str | None]]\n",
        "+\n",
        "+    def __init__(\n",
        "+        self,\n",
        "+        chain: Sequence[tuple[ReprTraceback, ReprFileLocation | None, str | None]],\n",
        "+    ) -> None:\n",
        "+        # reprcrash and reprtraceback of the outermost (the newest) exception\n",
        "+        # in the chain.\n",
        "+        super().__init__(\n",
        "+            reprtraceback=chain[-1][0],\n",
        "+            reprcrash=chain[-1][1],\n",
        "+        )\n",
        "+        self.chain = chain\n",
        "+\n",
        "+    def toterminal(self, tw: TerminalWriter) -> None:\n",
        "+        for element in self.chain:\n",
        "+            element[0].toterminal(tw)\n",
        "+            if element[2] is not None:\n",
        "+                tw.line(\"\")\n",
        "+                tw.line(element[2], yellow=True)\n",
        "+        super().toterminal(tw)\n",
        "+\n",
        "+\n",
        "+@dataclasses.dataclass(eq=False)\n",
        "+class ReprExceptionInfo(ExceptionRepr):\n",
        "+    reprtraceback: ReprTraceback\n",
        "+    reprcrash: ReprFileLocation | None\n",
        "+\n",
        "+    def toterminal(self, tw: TerminalWriter) -> None:\n",
        "+        self.reprtraceback.toterminal(tw)\n",
        "+        super().toterminal(tw)\n",
        "+\n",
        "+\n",
        "+@dataclasses.dataclass(eq=False)\n",
        "+class ReprTraceback(TerminalRepr):\n",
        "+    reprentries: Sequence[ReprEntry | ReprEntryNative]\n",
        "+    extraline: str | None\n",
        "+    style: TracebackStyle\n",
        "+\n",
        "+    entrysep: ClassVar = \"_ \"\n",
        "+\n",
        "+    def toterminal(self, tw: TerminalWriter) -> None:\n",
        "+        # The entries might have different styles.\n",
        "+        for i, entry in enumerate(self.reprentries):\n",
        "+            if entry.style == \"long\":\n",
        "+                tw.line(\"\")\n",
        "+            entry.toterminal(tw)\n",
        "+            if i < len(self.reprentries) - 1:\n",
        "+                next_entry = self.reprentries[i + 1]\n",
        "+                if entry.style == \"long\" or (\n",
        "+                    entry.style == \"short\" and next_entry.style == \"long\"\n",
        "+                ):\n",
        "+                    tw.sep(self.entrysep)\n",
        "+\n",
        "+        if self.extraline:\n",
        "+            tw.line(self.extraline)\n",
        "+\n",
        "+\n",
        "+class ReprTracebackNative(ReprTraceback):\n",
        "+    def __init__(self, tblines: Sequence[str]) -> None:\n",
        "+        self.reprentries = [ReprEntryNative(tblines)]\n",
        "+        self.extraline = None\n",
        "+        self.style = \"native\"\n",
        "+\n",
        "+\n",
        "+@dataclasses.dataclass(eq=False)\n",
        "+class ReprEntryNative(TerminalRepr):\n",
        "+    lines: Sequence[str]\n",
        "+\n",
        "+    style: ClassVar[TracebackStyle] = \"native\"\n",
        "+\n",
        "+    def toterminal(self, tw: TerminalWriter) -> None:\n",
        "+        tw.write(\"\".join(self.lines))\n",
        "+\n",
        "+\n",
        "+@dataclasses.dataclass(eq=False)\n",
        "+class ReprEntry(TerminalRepr):\n",
        "+    lines: Sequence[str]\n",
        "+    reprfuncargs: ReprFuncArgs | None\n",
        "+    reprlocals: ReprLocals | None\n",
        "+    reprfileloc: ReprFileLocation | None\n",
        "+    style: TracebackStyle\n",
        "+\n",
        "+    def _write_entry_lines(self, tw: TerminalWriter) -> None:\n",
        "+        \"\"\"Write the source code portions of a list of traceback entries with syntax highlighting.\n",
        "+\n",
        "+        Usually entries are lines like these:\n",
        "+\n",
        "+            \"     x = 1\"\n",
        "+            \">    assert x == 2\"\n",
        "+            \"E    assert 1 == 2\"\n",
        "+\n",
        "+        This function takes care of rendering the \"source\" portions of it (the lines without\n",
        "+        the \"E\" prefix) using syntax highlighting, taking care to not highlighting the \">\"\n",
        "+        character, as doing so might break line continuations.\n",
        "+        \"\"\"\n",
        "+        if not self.lines:\n",
        "+            return\n",
        "+\n",
        "+        if self.style == \"value\":\n",
        "+            # Using tw.write instead of tw.line for testing purposes due to TWMock implementation;\n",
        "+            # lines written with TWMock.line and TWMock._write_source cannot be distinguished\n",
        "+            # from each other, whereas lines written with TWMock.write are marked with TWMock.WRITE\n",
        "+            for line in self.lines:\n",
        "+                tw.write(line)\n",
        "+                tw.write(\"\\n\")\n",
        "+            return\n",
        "+\n",
        "+        # separate indents and source lines that are not failures: we want to\n",
        "+        # highlight the code but not the indentation, which may contain markers\n",
        "+        # such as \">   assert 0\"\n",
        "+        fail_marker = f\"{FormattedExcinfo.fail_marker}   \"\n",
        "+        indent_size = len(fail_marker)\n",
        "+        indents: list[str] = []\n",
        "+        source_lines: list[str] = []\n",
        "+        failure_lines: list[str] = []\n",
        "+        for index, line in enumerate(self.lines):\n",
        "+            is_failure_line = line.startswith(fail_marker)\n",
        "+            if is_failure_line:\n",
        "+                # from this point on all lines are considered part of the failure\n",
        "+                failure_lines.extend(self.lines[index:])\n",
        "+                break\n",
        "+            else:\n",
        "+                indents.append(line[:indent_size])\n",
        "+                source_lines.append(line[indent_size:])\n",
        "+\n",
        "+        tw._write_source(source_lines, indents)\n",
        "+\n",
        "+        # failure lines are always completely red and bold\n",
        "+        for line in failure_lines:\n",
        "+            tw.line(line, bold=True, red=True)\n",
        "+\n",
        "+    def toterminal(self, tw: TerminalWriter) -> None:\n",
        "+        if self.style == \"short\":\n",
        "+            if self.reprfileloc:\n",
        "+                self.reprfileloc.toterminal(tw)\n",
        "+            self._write_entry_lines(tw)\n",
        "+            if self.reprlocals:\n",
        "+                self.reprlocals.toterminal(tw, indent=\" \" * 8)\n",
        "+            return\n",
        "+\n",
        "+        if self.reprfuncargs:\n",
        "+            self.reprfuncargs.toterminal(tw)\n",
        "+\n",
        "+        self._write_entry_lines(tw)\n",
        "+\n",
        "+        if self.reprlocals:\n",
        "+            tw.line(\"\")\n",
        "+            self.reprlocals.toterminal(tw)\n",
        "+        if self.reprfileloc:\n",
        "+            if self.lines:\n",
        "+                tw.line(\"\")\n",
        "+            self.reprfileloc.toterminal(tw)\n",
        "+\n",
        "+    def __str__(self) -> str:\n",
        "+        return \"{}\\n{}\\n{}\".format(\n",
        "+            \"\\n\".join(self.lines), self.reprlocals, self.reprfileloc\n",
        "+        )\n",
        "+\n",
        "+\n",
        "+@dataclasses.dataclass(eq=False)\n",
        "+class ReprFileLocation(TerminalRepr):\n",
        "+    path: str\n",
        "+    lineno: int\n",
        "+    message: str\n",
        "+\n",
        "+    def __post_init__(self) -> None:\n",
        "+        self.path = str(self.path)\n",
        "+\n",
        "+    def toterminal(self, tw: TerminalWriter) -> None:\n",
        "+        # Filename and lineno output for each entry, using an output format\n",
        "+        # that most editors understand.\n",
        "+        msg = self.message\n",
        "+        i = msg.find(\"\\n\")\n",
        "+        if i != -1:\n",
        "+            msg = msg[:i]\n",
        "+        tw.write(self.path, bold=True, red=True)\n",
        "+        tw.line(f\":{self.lineno}: {msg}\")\n",
        "+\n",
        "+\n",
        "+@dataclasses.dataclass(eq=False)\n",
        "+class ReprLocals(TerminalRepr):\n",
        "+    lines: Sequence[str]\n",
        "+\n",
        "+    def toterminal(self, tw: TerminalWriter, indent=\"\") -> None:\n",
        "+        for line in self.lines:\n",
        "+            tw.line(indent + line)\n",
        "+\n",
        "+\n",
        "+@dataclasses.dataclass(eq=False)\n",
        "+class ReprFuncArgs(TerminalRepr):\n",
        "+    args: Sequence[tuple[str, object]]\n",
        "+\n",
        "+    def toterminal(self, tw: TerminalWriter) -> None:\n",
        "+        if self.args:\n",
        "+            linesofar = \"\"\n",
        "+            for name, value in self.args:\n",
        "+                ns = f\"{name} = {value}\"\n",
        "+                if len(ns) + len(linesofar) + 2 > tw.fullwidth:\n",
        "+                    if linesofar:\n",
        "+                        tw.line(linesofar)\n",
        "+                    linesofar = ns\n",
        "+                else:\n",
        "+                    if linesofar:\n",
        "+                        linesofar += \", \" + ns\n",
        "+                    else:\n",
        "+                        linesofar = ns\n",
        "+            if linesofar:\n",
        "+                tw.line(linesofar)\n",
        "+            tw.line(\"\")\n",
        "+\n",
        "+\n",
        "+def getfslineno(obj: object) -> tuple[str | Path, int]:\n",
        "+    \"\"\"Return source location (path, lineno) for the given object.\n",
        "+\n",
        "+    If the source cannot be determined return (\"\", -1).\n",
        "+\n",
        "+    The line number is 0-based.\n",
        "+    \"\"\"\n",
        "+    # xxx let decorators etc specify a sane ordering\n",
        "+    # NOTE: this used to be done in _pytest.compat.getfslineno, initially added\n",
        "+    #       in 6ec13a2b9.  It (\"place_as\") appears to be something very custom.\n",
        "+    obj = get_real_func(obj)\n",
        "+    if hasattr(obj, \"place_as\"):\n",
        "+        obj = obj.place_as\n",
        "+\n",
        "+    try:\n",
        "+        code = Code.from_function(obj)\n",
        "+    except TypeError:\n",
        "+        try:\n",
        "+            fn = inspect.getsourcefile(obj) or inspect.getfile(obj)  # type: ignore[arg-type]\n",
        "+        except TypeError:\n",
        "+            return \"\", -1\n",
        "+\n",
        "+        fspath = (fn and absolutepath(fn)) or \"\"\n",
        "+        lineno = -1\n",
        "+        if fspath:\n",
        "+            try:\n",
        "+                _, lineno = findsource(obj)\n",
        "+            except OSError:\n",
        "+                pass\n",
        "+        return fspath, lineno\n",
        "+\n",
        "+    return code.path, code.firstlineno\n",
        "+\n",
        "+\n",
        "+def _byte_offset_to_character_offset(str, offset):\n",
        "+    \"\"\"Converts a byte based offset in a string to a code-point.\"\"\"\n",
        "+    as_utf8 = str.encode(\"utf-8\")\n",
        "+    return len(as_utf8[:offset].decode(\"utf-8\", errors=\"replace\"))\n",
        "+\n",
        "+\n",
        "+# Relative paths that we use to filter traceback entries from appearing to the user;\n",
        "+# see filter_traceback.\n",
        "+# note: if we need to add more paths than what we have now we should probably use a list\n",
        "+# for better maintenance.\n",
        "+\n",
        "+_PLUGGY_DIR = Path(pluggy.__file__.rstrip(\"oc\"))\n",
        "+# pluggy is either a package or a single module depending on the version\n",
        "+if _PLUGGY_DIR.name == \"__init__.py\":\n",
        "+    _PLUGGY_DIR = _PLUGGY_DIR.parent\n",
        "+_PYTEST_DIR = Path(_pytest.__file__).parent\n",
        "+\n",
        "+\n",
        "+def filter_traceback(entry: TracebackEntry) -> bool:\n",
        "+    \"\"\"Return True if a TracebackEntry instance should be included in tracebacks.\n",
        "+\n",
        "+    We hide traceback entries of:\n",
        "+\n",
        "+    * dynamically generated code (no code to show up for it);\n",
        "+    * internal traceback from pytest or its internal libraries, py and pluggy.\n",
        "+    \"\"\"\n",
        "+    # entry.path might sometimes return a str object when the entry\n",
        "+    # points to dynamically generated code.\n",
        "+    # See https://bitbucket.org/pytest-dev/py/issues/71.\n",
        "+    raw_filename = entry.frame.code.raw.co_filename\n",
        "+    is_generated = \"<\" in raw_filename and \">\" in raw_filename\n",
        "+    if is_generated:\n",
        "+        return False\n",
        "+\n",
        "+    # entry.path might point to a non-existing file, in which case it will\n",
        "+    # also return a str object. See #1133.\n",
        "+    p = Path(entry.path)\n",
        "+\n",
        "+    parents = p.parents\n",
        "+    if _PLUGGY_DIR in parents:\n",
        "+        return False\n",
        "+    if _PYTEST_DIR in parents:\n",
        "+        return False\n",
        "+\n",
        "+    return True\n",
        "+\n",
        "+\n",
        "+def filter_excinfo_traceback(\n",
        "+    tbfilter: TracebackFilter, excinfo: ExceptionInfo[BaseException]\n",
        "+) -> Traceback:\n",
        "+    \"\"\"Filter the exception traceback in ``excinfo`` according to ``tbfilter``.\"\"\"\n",
        "+    if callable(tbfilter):\n",
        "+        return tbfilter(excinfo)\n",
        "+    elif tbfilter:\n",
        "+        return excinfo.traceback.filter(excinfo)\n",
        "+    else:\n",
        "+        return excinfo.traceback\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/_code/source.py",
      "status": "added",
      "additions": 225,
      "deletions": 0,
      "patch": "@@ -0,0 +1,225 @@\n+# mypy: allow-untyped-defs\n+from __future__ import annotations\n+\n+import ast\n+from bisect import bisect_right\n+from collections.abc import Iterable\n+from collections.abc import Iterator\n+import inspect\n+import textwrap\n+import tokenize\n+import types\n+from typing import overload\n+import warnings\n+\n+\n+class Source:\n+    \"\"\"An immutable object holding a source code fragment.\n+\n+    When using Source(...), the source lines are deindented.\n+    \"\"\"\n+\n+    def __init__(self, obj: object = None) -> None:\n+        if not obj:\n+            self.lines: list[str] = []\n+            self.raw_lines: list[str] = []\n+        elif isinstance(obj, Source):\n+            self.lines = obj.lines\n+            self.raw_lines = obj.raw_lines\n+        elif isinstance(obj, (tuple, list)):\n+            self.lines = deindent(x.rstrip(\"\\n\") for x in obj)\n+            self.raw_lines = list(x.rstrip(\"\\n\") for x in obj)\n+        elif isinstance(obj, str):\n+            self.lines = deindent(obj.split(\"\\n\"))\n+            self.raw_lines = obj.split(\"\\n\")\n+        else:\n+            try:\n+                rawcode = getrawcode(obj)\n+                src = inspect.getsource(rawcode)\n+            except TypeError:\n+                src = inspect.getsource(obj)  # type: ignore[arg-type]\n+            self.lines = deindent(src.split(\"\\n\"))\n+            self.raw_lines = src.split(\"\\n\")\n+\n+    def __eq__(self, other: object) -> bool:\n+        if not isinstance(other, Source):\n+            return NotImplemented\n+        return self.lines == other.lines\n+\n+    # Ignore type because of https://github.com/python/mypy/issues/4266.\n+    __hash__ = None  # type: ignore\n+\n+    @overload\n+    def __getitem__(self, key: int) -> str: ...\n+\n+    @overload\n+    def __getitem__(self, key: slice) -> Source: ...\n+\n+    def __getitem__(self, key: int | slice) -> str | Source:\n+        if isinstance(key, int):\n+            return self.lines[key]\n+        else:\n+            if key.step not in (None, 1):\n+                raise IndexError(\"cannot slice a Source with a step\")\n+            newsource = Source()\n+            newsource.lines = self.lines[key.start : key.stop]\n+            newsource.raw_lines = self.raw_lines[key.start : key.stop]\n+            return newsource\n+\n+    def __iter__(self) -> Iterator[str]:\n+        return iter(self.lines)\n+\n+    def __len__(self) -> int:\n+        return len(self.lines)\n+\n+    def strip(self) -> Source:\n+        \"\"\"Return new Source object with trailing and leading blank lines removed.\"\"\"\n+        start, end = 0, len(self)\n+        while start < end and not self.lines[start].strip():\n+            start += 1\n+        while end > start and not self.lines[end - 1].strip():\n+            end -= 1\n+        source = Source()\n+        source.raw_lines = self.raw_lines\n+        source.lines[:] = self.lines[start:end]\n+        return source\n+\n+    def indent(self, indent: str = \" \" * 4) -> Source:\n+        \"\"\"Return a copy of the source object with all lines indented by the\n+        given indent-string.\"\"\"\n+        newsource = Source()\n+        newsource.raw_lines = self.raw_lines\n+        newsource.lines = [(indent + line) for line in self.lines]\n+        return newsource\n+\n+    def getstatement(self, lineno: int) -> Source:\n+        \"\"\"Return Source statement which contains the given linenumber\n+        (counted from 0).\"\"\"\n+        start, end = self.getstatementrange(lineno)\n+        return self[start:end]\n+\n+    def getstatementrange(self, lineno: int) -> tuple[int, int]:\n+        \"\"\"Return (start, end) tuple which spans the minimal statement region\n+        which containing the given lineno.\"\"\"\n+        if not (0 <= lineno < len(self)):\n+            raise IndexError(\"lineno out of range\")\n+        ast, start, end = getstatementrange_ast(lineno, self)\n+        return start, end\n+\n+    def deindent(self) -> Source:\n+        \"\"\"Return a new Source object deindented.\"\"\"\n+        newsource = Source()\n+        newsource.lines[:] = deindent(self.lines)\n+        newsource.raw_lines = self.raw_lines\n+        return newsource\n+\n+    def __str__(self) -> str:\n+        return \"\\n\".join(self.lines)\n+\n+\n+#\n+# helper functions\n+#\n+\n+\n+def findsource(obj) -> tuple[Source | None, int]:\n+    try:\n+        sourcelines, lineno = inspect.findsource(obj)\n+    except Exception:\n+        return None, -1\n+    source = Source()\n+    source.lines = [line.rstrip() for line in sourcelines]\n+    source.raw_lines = sourcelines\n+    return source, lineno\n+\n+\n+def getrawcode(obj: object, trycall: bool = True) -> types.CodeType:\n+    \"\"\"Return code object for given function.\"\"\"\n+    try:\n+        return obj.__code__  # type: ignore[attr-defined,no-any-return]\n+    except AttributeError:\n+        pass\n+    if trycall:\n+        call = getattr(obj, \"__call__\", None)\n+        if call and not isinstance(obj, type):\n+            return getrawcode(call, trycall=False)\n+    raise TypeError(f\"could not get code object for {obj!r}\")\n+\n+\n+def deindent(lines: Iterable[str]) -> list[str]:\n+    return textwrap.dedent(\"\\n\".join(lines)).splitlines()\n+\n+\n+def get_statement_startend2(lineno: int, node: ast.AST) -> tuple[int, int | None]:\n+    # Flatten all statements and except handlers into one lineno-list.\n+    # AST's line numbers start indexing at 1.\n+    values: list[int] = []\n+    for x in ast.walk(node):\n+        if isinstance(x, (ast.stmt, ast.ExceptHandler)):\n+            # The lineno points to the class/def, so need to include the decorators.\n+            if isinstance(x, (ast.ClassDef, ast.FunctionDef, ast.AsyncFunctionDef)):\n+                for d in x.decorator_list:\n+                    values.append(d.lineno - 1)\n+            values.append(x.lineno - 1)\n+            for name in (\"finalbody\", \"orelse\"):\n+                val: list[ast.stmt] | None = getattr(x, name, None)\n+                if val:\n+                    # Treat the finally/orelse part as its own statement.\n+                    values.append(val[0].lineno - 1 - 1)\n+    values.sort()\n+    insert_index = bisect_right(values, lineno)\n+    start = values[insert_index - 1]\n+    if insert_index >= len(values):\n+        end = None\n+    else:\n+        end = values[insert_index]\n+    return start, end\n+\n+\n+def getstatementrange_ast(\n+    lineno: int,\n+    source: Source,\n+    assertion: bool = False,\n+    astnode: ast.AST | None = None,\n+) -> tuple[ast.AST, int, int]:\n+    if astnode is None:\n+        content = str(source)\n+        # See #4260:\n+        # Don't produce duplicate warnings when compiling source to find AST.\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"ignore\")\n+            astnode = ast.parse(content, \"source\", \"exec\")\n+\n+    start, end = get_statement_startend2(lineno, astnode)\n+    # We need to correct the end:\n+    # - ast-parsing strips comments\n+    # - there might be empty lines\n+    # - we might have lesser indented code blocks at the end\n+    if end is None:\n+        end = len(source.lines)\n+\n+    if end > start + 1:\n+        # Make sure we don't span differently indented code blocks\n+        # by using the BlockFinder helper used which inspect.getsource() uses itself.\n+        block_finder = inspect.BlockFinder()\n+        # If we start with an indented line, put blockfinder to \"started\" mode.\n+        block_finder.started = (\n+            bool(source.lines[start]) and source.lines[start][0].isspace()\n+        )\n+        it = ((x + \"\\n\") for x in source.lines[start:end])\n+        try:\n+            for tok in tokenize.generate_tokens(lambda: next(it)):\n+                block_finder.tokeneater(*tok)\n+        except (inspect.EndOfBlock, IndentationError):\n+            end = block_finder.last + start\n+        except Exception:\n+            pass\n+\n+    # The end might still point to a comment or empty line, correct it.\n+    while end:\n+        line = source.lines[end - 1].lstrip()\n+        if line.startswith(\"#\") or not line:\n+            end -= 1\n+        else:\n+            break\n+    return astnode, start, end",
      "patch_lines": [
        "@@ -0,0 +1,225 @@\n",
        "+# mypy: allow-untyped-defs\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+import ast\n",
        "+from bisect import bisect_right\n",
        "+from collections.abc import Iterable\n",
        "+from collections.abc import Iterator\n",
        "+import inspect\n",
        "+import textwrap\n",
        "+import tokenize\n",
        "+import types\n",
        "+from typing import overload\n",
        "+import warnings\n",
        "+\n",
        "+\n",
        "+class Source:\n",
        "+    \"\"\"An immutable object holding a source code fragment.\n",
        "+\n",
        "+    When using Source(...), the source lines are deindented.\n",
        "+    \"\"\"\n",
        "+\n",
        "+    def __init__(self, obj: object = None) -> None:\n",
        "+        if not obj:\n",
        "+            self.lines: list[str] = []\n",
        "+            self.raw_lines: list[str] = []\n",
        "+        elif isinstance(obj, Source):\n",
        "+            self.lines = obj.lines\n",
        "+            self.raw_lines = obj.raw_lines\n",
        "+        elif isinstance(obj, (tuple, list)):\n",
        "+            self.lines = deindent(x.rstrip(\"\\n\") for x in obj)\n",
        "+            self.raw_lines = list(x.rstrip(\"\\n\") for x in obj)\n",
        "+        elif isinstance(obj, str):\n",
        "+            self.lines = deindent(obj.split(\"\\n\"))\n",
        "+            self.raw_lines = obj.split(\"\\n\")\n",
        "+        else:\n",
        "+            try:\n",
        "+                rawcode = getrawcode(obj)\n",
        "+                src = inspect.getsource(rawcode)\n",
        "+            except TypeError:\n",
        "+                src = inspect.getsource(obj)  # type: ignore[arg-type]\n",
        "+            self.lines = deindent(src.split(\"\\n\"))\n",
        "+            self.raw_lines = src.split(\"\\n\")\n",
        "+\n",
        "+    def __eq__(self, other: object) -> bool:\n",
        "+        if not isinstance(other, Source):\n",
        "+            return NotImplemented\n",
        "+        return self.lines == other.lines\n",
        "+\n",
        "+    # Ignore type because of https://github.com/python/mypy/issues/4266.\n",
        "+    __hash__ = None  # type: ignore\n",
        "+\n",
        "+    @overload\n",
        "+    def __getitem__(self, key: int) -> str: ...\n",
        "+\n",
        "+    @overload\n",
        "+    def __getitem__(self, key: slice) -> Source: ...\n",
        "+\n",
        "+    def __getitem__(self, key: int | slice) -> str | Source:\n",
        "+        if isinstance(key, int):\n",
        "+            return self.lines[key]\n",
        "+        else:\n",
        "+            if key.step not in (None, 1):\n",
        "+                raise IndexError(\"cannot slice a Source with a step\")\n",
        "+            newsource = Source()\n",
        "+            newsource.lines = self.lines[key.start : key.stop]\n",
        "+            newsource.raw_lines = self.raw_lines[key.start : key.stop]\n",
        "+            return newsource\n",
        "+\n",
        "+    def __iter__(self) -> Iterator[str]:\n",
        "+        return iter(self.lines)\n",
        "+\n",
        "+    def __len__(self) -> int:\n",
        "+        return len(self.lines)\n",
        "+\n",
        "+    def strip(self) -> Source:\n",
        "+        \"\"\"Return new Source object with trailing and leading blank lines removed.\"\"\"\n",
        "+        start, end = 0, len(self)\n",
        "+        while start < end and not self.lines[start].strip():\n",
        "+            start += 1\n",
        "+        while end > start and not self.lines[end - 1].strip():\n",
        "+            end -= 1\n",
        "+        source = Source()\n",
        "+        source.raw_lines = self.raw_lines\n",
        "+        source.lines[:] = self.lines[start:end]\n",
        "+        return source\n",
        "+\n",
        "+    def indent(self, indent: str = \" \" * 4) -> Source:\n",
        "+        \"\"\"Return a copy of the source object with all lines indented by the\n",
        "+        given indent-string.\"\"\"\n",
        "+        newsource = Source()\n",
        "+        newsource.raw_lines = self.raw_lines\n",
        "+        newsource.lines = [(indent + line) for line in self.lines]\n",
        "+        return newsource\n",
        "+\n",
        "+    def getstatement(self, lineno: int) -> Source:\n",
        "+        \"\"\"Return Source statement which contains the given linenumber\n",
        "+        (counted from 0).\"\"\"\n",
        "+        start, end = self.getstatementrange(lineno)\n",
        "+        return self[start:end]\n",
        "+\n",
        "+    def getstatementrange(self, lineno: int) -> tuple[int, int]:\n",
        "+        \"\"\"Return (start, end) tuple which spans the minimal statement region\n",
        "+        which containing the given lineno.\"\"\"\n",
        "+        if not (0 <= lineno < len(self)):\n",
        "+            raise IndexError(\"lineno out of range\")\n",
        "+        ast, start, end = getstatementrange_ast(lineno, self)\n",
        "+        return start, end\n",
        "+\n",
        "+    def deindent(self) -> Source:\n",
        "+        \"\"\"Return a new Source object deindented.\"\"\"\n",
        "+        newsource = Source()\n",
        "+        newsource.lines[:] = deindent(self.lines)\n",
        "+        newsource.raw_lines = self.raw_lines\n",
        "+        return newsource\n",
        "+\n",
        "+    def __str__(self) -> str:\n",
        "+        return \"\\n\".join(self.lines)\n",
        "+\n",
        "+\n",
        "+#\n",
        "+# helper functions\n",
        "+#\n",
        "+\n",
        "+\n",
        "+def findsource(obj) -> tuple[Source | None, int]:\n",
        "+    try:\n",
        "+        sourcelines, lineno = inspect.findsource(obj)\n",
        "+    except Exception:\n",
        "+        return None, -1\n",
        "+    source = Source()\n",
        "+    source.lines = [line.rstrip() for line in sourcelines]\n",
        "+    source.raw_lines = sourcelines\n",
        "+    return source, lineno\n",
        "+\n",
        "+\n",
        "+def getrawcode(obj: object, trycall: bool = True) -> types.CodeType:\n",
        "+    \"\"\"Return code object for given function.\"\"\"\n",
        "+    try:\n",
        "+        return obj.__code__  # type: ignore[attr-defined,no-any-return]\n",
        "+    except AttributeError:\n",
        "+        pass\n",
        "+    if trycall:\n",
        "+        call = getattr(obj, \"__call__\", None)\n",
        "+        if call and not isinstance(obj, type):\n",
        "+            return getrawcode(call, trycall=False)\n",
        "+    raise TypeError(f\"could not get code object for {obj!r}\")\n",
        "+\n",
        "+\n",
        "+def deindent(lines: Iterable[str]) -> list[str]:\n",
        "+    return textwrap.dedent(\"\\n\".join(lines)).splitlines()\n",
        "+\n",
        "+\n",
        "+def get_statement_startend2(lineno: int, node: ast.AST) -> tuple[int, int | None]:\n",
        "+    # Flatten all statements and except handlers into one lineno-list.\n",
        "+    # AST's line numbers start indexing at 1.\n",
        "+    values: list[int] = []\n",
        "+    for x in ast.walk(node):\n",
        "+        if isinstance(x, (ast.stmt, ast.ExceptHandler)):\n",
        "+            # The lineno points to the class/def, so need to include the decorators.\n",
        "+            if isinstance(x, (ast.ClassDef, ast.FunctionDef, ast.AsyncFunctionDef)):\n",
        "+                for d in x.decorator_list:\n",
        "+                    values.append(d.lineno - 1)\n",
        "+            values.append(x.lineno - 1)\n",
        "+            for name in (\"finalbody\", \"orelse\"):\n",
        "+                val: list[ast.stmt] | None = getattr(x, name, None)\n",
        "+                if val:\n",
        "+                    # Treat the finally/orelse part as its own statement.\n",
        "+                    values.append(val[0].lineno - 1 - 1)\n",
        "+    values.sort()\n",
        "+    insert_index = bisect_right(values, lineno)\n",
        "+    start = values[insert_index - 1]\n",
        "+    if insert_index >= len(values):\n",
        "+        end = None\n",
        "+    else:\n",
        "+        end = values[insert_index]\n",
        "+    return start, end\n",
        "+\n",
        "+\n",
        "+def getstatementrange_ast(\n",
        "+    lineno: int,\n",
        "+    source: Source,\n",
        "+    assertion: bool = False,\n",
        "+    astnode: ast.AST | None = None,\n",
        "+) -> tuple[ast.AST, int, int]:\n",
        "+    if astnode is None:\n",
        "+        content = str(source)\n",
        "+        # See #4260:\n",
        "+        # Don't produce duplicate warnings when compiling source to find AST.\n",
        "+        with warnings.catch_warnings():\n",
        "+            warnings.simplefilter(\"ignore\")\n",
        "+            astnode = ast.parse(content, \"source\", \"exec\")\n",
        "+\n",
        "+    start, end = get_statement_startend2(lineno, astnode)\n",
        "+    # We need to correct the end:\n",
        "+    # - ast-parsing strips comments\n",
        "+    # - there might be empty lines\n",
        "+    # - we might have lesser indented code blocks at the end\n",
        "+    if end is None:\n",
        "+        end = len(source.lines)\n",
        "+\n",
        "+    if end > start + 1:\n",
        "+        # Make sure we don't span differently indented code blocks\n",
        "+        # by using the BlockFinder helper used which inspect.getsource() uses itself.\n",
        "+        block_finder = inspect.BlockFinder()\n",
        "+        # If we start with an indented line, put blockfinder to \"started\" mode.\n",
        "+        block_finder.started = (\n",
        "+            bool(source.lines[start]) and source.lines[start][0].isspace()\n",
        "+        )\n",
        "+        it = ((x + \"\\n\") for x in source.lines[start:end])\n",
        "+        try:\n",
        "+            for tok in tokenize.generate_tokens(lambda: next(it)):\n",
        "+                block_finder.tokeneater(*tok)\n",
        "+        except (inspect.EndOfBlock, IndentationError):\n",
        "+            end = block_finder.last + start\n",
        "+        except Exception:\n",
        "+            pass\n",
        "+\n",
        "+    # The end might still point to a comment or empty line, correct it.\n",
        "+    while end:\n",
        "+        line = source.lines[end - 1].lstrip()\n",
        "+        if line.startswith(\"#\") or not line:\n",
        "+            end -= 1\n",
        "+        else:\n",
        "+            break\n",
        "+    return astnode, start, end\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/_io/__init__.py",
      "status": "added",
      "additions": 10,
      "deletions": 0,
      "patch": "@@ -0,0 +1,10 @@\n+from __future__ import annotations\n+\n+from .terminalwriter import get_terminal_width\n+from .terminalwriter import TerminalWriter\n+\n+\n+__all__ = [\n+    \"TerminalWriter\",\n+    \"get_terminal_width\",\n+]",
      "patch_lines": [
        "@@ -0,0 +1,10 @@\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+from .terminalwriter import get_terminal_width\n",
        "+from .terminalwriter import TerminalWriter\n",
        "+\n",
        "+\n",
        "+__all__ = [\n",
        "+    \"TerminalWriter\",\n",
        "+    \"get_terminal_width\",\n",
        "+]\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/_io/pprint.py",
      "status": "added",
      "additions": 673,
      "deletions": 0,
      "patch": "@@ -0,0 +1,673 @@\n+# mypy: allow-untyped-defs\n+# This module was imported from the cpython standard library\n+# (https://github.com/python/cpython/) at commit\n+# c5140945c723ae6c4b7ee81ff720ac8ea4b52cfd (python3.12).\n+#\n+#\n+#  Original Author:      Fred L. Drake, Jr.\n+#                        fdrake@acm.org\n+#\n+#  This is a simple little module I wrote to make life easier.  I didn't\n+#  see anything quite like it in the library, though I may have overlooked\n+#  something.  I wrote this when I was trying to read some heavily nested\n+#  tuples with fairly non-descriptive content.  This is modeled very much\n+#  after Lisp/Scheme - style pretty-printing of lists.  If you find it\n+#  useful, thank small children who sleep at night.\n+from __future__ import annotations\n+\n+import collections as _collections\n+from collections.abc import Callable\n+from collections.abc import Iterator\n+import dataclasses as _dataclasses\n+from io import StringIO as _StringIO\n+import re\n+import types as _types\n+from typing import Any\n+from typing import IO\n+\n+\n+class _safe_key:\n+    \"\"\"Helper function for key functions when sorting unorderable objects.\n+\n+    The wrapped-object will fallback to a Py2.x style comparison for\n+    unorderable types (sorting first comparing the type name and then by\n+    the obj ids).  Does not work recursively, so dict.items() must have\n+    _safe_key applied to both the key and the value.\n+\n+    \"\"\"\n+\n+    __slots__ = [\"obj\"]\n+\n+    def __init__(self, obj):\n+        self.obj = obj\n+\n+    def __lt__(self, other):\n+        try:\n+            return self.obj < other.obj\n+        except TypeError:\n+            return (str(type(self.obj)), id(self.obj)) < (\n+                str(type(other.obj)),\n+                id(other.obj),\n+            )\n+\n+\n+def _safe_tuple(t):\n+    \"\"\"Helper function for comparing 2-tuples\"\"\"\n+    return _safe_key(t[0]), _safe_key(t[1])\n+\n+\n+class PrettyPrinter:\n+    def __init__(\n+        self,\n+        indent: int = 4,\n+        width: int = 80,\n+        depth: int | None = None,\n+    ) -> None:\n+        \"\"\"Handle pretty printing operations onto a stream using a set of\n+        configured parameters.\n+\n+        indent\n+            Number of spaces to indent for each level of nesting.\n+\n+        width\n+            Attempted maximum number of columns in the output.\n+\n+        depth\n+            The maximum depth to print out nested structures.\n+\n+        \"\"\"\n+        if indent < 0:\n+            raise ValueError(\"indent must be >= 0\")\n+        if depth is not None and depth <= 0:\n+            raise ValueError(\"depth must be > 0\")\n+        if not width:\n+            raise ValueError(\"width must be != 0\")\n+        self._depth = depth\n+        self._indent_per_level = indent\n+        self._width = width\n+\n+    def pformat(self, object: Any) -> str:\n+        sio = _StringIO()\n+        self._format(object, sio, 0, 0, set(), 0)\n+        return sio.getvalue()\n+\n+    def _format(\n+        self,\n+        object: Any,\n+        stream: IO[str],\n+        indent: int,\n+        allowance: int,\n+        context: set[int],\n+        level: int,\n+    ) -> None:\n+        objid = id(object)\n+        if objid in context:\n+            stream.write(_recursion(object))\n+            return\n+\n+        p = self._dispatch.get(type(object).__repr__, None)\n+        if p is not None:\n+            context.add(objid)\n+            p(self, object, stream, indent, allowance, context, level + 1)\n+            context.remove(objid)\n+        elif (\n+            _dataclasses.is_dataclass(object)\n+            and not isinstance(object, type)\n+            and object.__dataclass_params__.repr  # type:ignore[attr-defined]\n+            and\n+            # Check dataclass has generated repr method.\n+            hasattr(object.__repr__, \"__wrapped__\")\n+            and \"__create_fn__\" in object.__repr__.__wrapped__.__qualname__\n+        ):\n+            context.add(objid)\n+            self._pprint_dataclass(\n+                object, stream, indent, allowance, context, level + 1\n+            )\n+            context.remove(objid)\n+        else:\n+            stream.write(self._repr(object, context, level))\n+\n+    def _pprint_dataclass(\n+        self,\n+        object: Any,\n+        stream: IO[str],\n+        indent: int,\n+        allowance: int,\n+        context: set[int],\n+        level: int,\n+    ) -> None:\n+        cls_name = object.__class__.__name__\n+        items = [\n+            (f.name, getattr(object, f.name))\n+            for f in _dataclasses.fields(object)\n+            if f.repr\n+        ]\n+        stream.write(cls_name + \"(\")\n+        self._format_namespace_items(items, stream, indent, allowance, context, level)\n+        stream.write(\")\")\n+\n+    _dispatch: dict[\n+        Callable[..., str],\n+        Callable[[PrettyPrinter, Any, IO[str], int, int, set[int], int], None],\n+    ] = {}\n+\n+    def _pprint_dict(\n+        self,\n+        object: Any,\n+        stream: IO[str],\n+        indent: int,\n+        allowance: int,\n+        context: set[int],\n+        level: int,\n+    ) -> None:\n+        write = stream.write\n+        write(\"{\")\n+        items = sorted(object.items(), key=_safe_tuple)\n+        self._format_dict_items(items, stream, indent, allowance, context, level)\n+        write(\"}\")\n+\n+    _dispatch[dict.__repr__] = _pprint_dict\n+\n+    def _pprint_ordered_dict(\n+        self,\n+        object: Any,\n+        stream: IO[str],\n+        indent: int,\n+        allowance: int,\n+        context: set[int],\n+        level: int,\n+    ) -> None:\n+        if not len(object):\n+            stream.write(repr(object))\n+            return\n+        cls = object.__class__\n+        stream.write(cls.__name__ + \"(\")\n+        self._pprint_dict(object, stream, indent, allowance, context, level)\n+        stream.write(\")\")\n+\n+    _dispatch[_collections.OrderedDict.__repr__] = _pprint_ordered_dict\n+\n+    def _pprint_list(\n+        self,\n+        object: Any,\n+        stream: IO[str],\n+        indent: int,\n+        allowance: int,\n+        context: set[int],\n+        level: int,\n+    ) -> None:\n+        stream.write(\"[\")\n+        self._format_items(object, stream, indent, allowance, context, level)\n+        stream.write(\"]\")\n+\n+    _dispatch[list.__repr__] = _pprint_list\n+\n+    def _pprint_tuple(\n+        self,\n+        object: Any,\n+        stream: IO[str],\n+        indent: int,\n+        allowance: int,\n+        context: set[int],\n+        level: int,\n+    ) -> None:\n+        stream.write(\"(\")\n+        self._format_items(object, stream, indent, allowance, context, level)\n+        stream.write(\")\")\n+\n+    _dispatch[tuple.__repr__] = _pprint_tuple\n+\n+    def _pprint_set(\n+        self,\n+        object: Any,\n+        stream: IO[str],\n+        indent: int,\n+        allowance: int,\n+        context: set[int],\n+        level: int,\n+    ) -> None:\n+        if not len(object):\n+            stream.write(repr(object))\n+            return\n+        typ = object.__class__\n+        if typ is set:\n+            stream.write(\"{\")\n+            endchar = \"}\"\n+        else:\n+            stream.write(typ.__name__ + \"({\")\n+            endchar = \"})\"\n+        object = sorted(object, key=_safe_key)\n+        self._format_items(object, stream, indent, allowance, context, level)\n+        stream.write(endchar)\n+\n+    _dispatch[set.__repr__] = _pprint_set\n+    _dispatch[frozenset.__repr__] = _pprint_set\n+\n+    def _pprint_str(\n+        self,\n+        object: Any,\n+        stream: IO[str],\n+        indent: int,\n+        allowance: int,\n+        context: set[int],\n+        level: int,\n+    ) -> None:\n+        write = stream.write\n+        if not len(object):\n+            write(repr(object))\n+            return\n+        chunks = []\n+        lines = object.splitlines(True)\n+        if level == 1:\n+            indent += 1\n+            allowance += 1\n+        max_width1 = max_width = self._width - indent\n+        for i, line in enumerate(lines):\n+            rep = repr(line)\n+            if i == len(lines) - 1:\n+                max_width1 -= allowance\n+            if len(rep) <= max_width1:\n+                chunks.append(rep)\n+            else:\n+                # A list of alternating (non-space, space) strings\n+                parts = re.findall(r\"\\S*\\s*\", line)\n+                assert parts\n+                assert not parts[-1]\n+                parts.pop()  # drop empty last part\n+                max_width2 = max_width\n+                current = \"\"\n+                for j, part in enumerate(parts):\n+                    candidate = current + part\n+                    if j == len(parts) - 1 and i == len(lines) - 1:\n+                        max_width2 -= allowance\n+                    if len(repr(candidate)) > max_width2:\n+                        if current:\n+                            chunks.append(repr(current))\n+                        current = part\n+                    else:\n+                        current = candidate\n+                if current:\n+                    chunks.append(repr(current))\n+        if len(chunks) == 1:\n+            write(rep)\n+            return\n+        if level == 1:\n+            write(\"(\")\n+        for i, rep in enumerate(chunks):\n+            if i > 0:\n+                write(\"\\n\" + \" \" * indent)\n+            write(rep)\n+        if level == 1:\n+            write(\")\")\n+\n+    _dispatch[str.__repr__] = _pprint_str\n+\n+    def _pprint_bytes(\n+        self,\n+        object: Any,\n+        stream: IO[str],\n+        indent: int,\n+        allowance: int,\n+        context: set[int],\n+        level: int,\n+    ) -> None:\n+        write = stream.write\n+        if len(object) <= 4:\n+            write(repr(object))\n+            return\n+        parens = level == 1\n+        if parens:\n+            indent += 1\n+            allowance += 1\n+            write(\"(\")\n+        delim = \"\"\n+        for rep in _wrap_bytes_repr(object, self._width - indent, allowance):\n+            write(delim)\n+            write(rep)\n+            if not delim:\n+                delim = \"\\n\" + \" \" * indent\n+        if parens:\n+            write(\")\")\n+\n+    _dispatch[bytes.__repr__] = _pprint_bytes\n+\n+    def _pprint_bytearray(\n+        self,\n+        object: Any,\n+        stream: IO[str],\n+        indent: int,\n+        allowance: int,\n+        context: set[int],\n+        level: int,\n+    ) -> None:\n+        write = stream.write\n+        write(\"bytearray(\")\n+        self._pprint_bytes(\n+            bytes(object), stream, indent + 10, allowance + 1, context, level + 1\n+        )\n+        write(\")\")\n+\n+    _dispatch[bytearray.__repr__] = _pprint_bytearray\n+\n+    def _pprint_mappingproxy(\n+        self,\n+        object: Any,\n+        stream: IO[str],\n+        indent: int,\n+        allowance: int,\n+        context: set[int],\n+        level: int,\n+    ) -> None:\n+        stream.write(\"mappingproxy(\")\n+        self._format(object.copy(), stream, indent, allowance, context, level)\n+        stream.write(\")\")\n+\n+    _dispatch[_types.MappingProxyType.__repr__] = _pprint_mappingproxy\n+\n+    def _pprint_simplenamespace(\n+        self,\n+        object: Any,\n+        stream: IO[str],\n+        indent: int,\n+        allowance: int,\n+        context: set[int],\n+        level: int,\n+    ) -> None:\n+        if type(object) is _types.SimpleNamespace:\n+            # The SimpleNamespace repr is \"namespace\" instead of the class\n+            # name, so we do the same here. For subclasses; use the class name.\n+            cls_name = \"namespace\"\n+        else:\n+            cls_name = object.__class__.__name__\n+        items = object.__dict__.items()\n+        stream.write(cls_name + \"(\")\n+        self._format_namespace_items(items, stream, indent, allowance, context, level)\n+        stream.write(\")\")\n+\n+    _dispatch[_types.SimpleNamespace.__repr__] = _pprint_simplenamespace\n+\n+    def _format_dict_items(\n+        self,\n+        items: list[tuple[Any, Any]],\n+        stream: IO[str],\n+        indent: int,\n+        allowance: int,\n+        context: set[int],\n+        level: int,\n+    ) -> None:\n+        if not items:\n+            return\n+\n+        write = stream.write\n+        item_indent = indent + self._indent_per_level\n+        delimnl = \"\\n\" + \" \" * item_indent\n+        for key, ent in items:\n+            write(delimnl)\n+            write(self._repr(key, context, level))\n+            write(\": \")\n+            self._format(ent, stream, item_indent, 1, context, level)\n+            write(\",\")\n+\n+        write(\"\\n\" + \" \" * indent)\n+\n+    def _format_namespace_items(\n+        self,\n+        items: list[tuple[Any, Any]],\n+        stream: IO[str],\n+        indent: int,\n+        allowance: int,\n+        context: set[int],\n+        level: int,\n+    ) -> None:\n+        if not items:\n+            return\n+\n+        write = stream.write\n+        item_indent = indent + self._indent_per_level\n+        delimnl = \"\\n\" + \" \" * item_indent\n+        for key, ent in items:\n+            write(delimnl)\n+            write(key)\n+            write(\"=\")\n+            if id(ent) in context:\n+                # Special-case representation of recursion to match standard\n+                # recursive dataclass repr.\n+                write(\"...\")\n+            else:\n+                self._format(\n+                    ent,\n+                    stream,\n+                    item_indent + len(key) + 1,\n+                    1,\n+                    context,\n+                    level,\n+                )\n+\n+            write(\",\")\n+\n+        write(\"\\n\" + \" \" * indent)\n+\n+    def _format_items(\n+        self,\n+        items: list[Any],\n+        stream: IO[str],\n+        indent: int,\n+        allowance: int,\n+        context: set[int],\n+        level: int,\n+    ) -> None:\n+        if not items:\n+            return\n+\n+        write = stream.write\n+        item_indent = indent + self._indent_per_level\n+        delimnl = \"\\n\" + \" \" * item_indent\n+\n+        for item in items:\n+            write(delimnl)\n+            self._format(item, stream, item_indent, 1, context, level)\n+            write(\",\")\n+\n+        write(\"\\n\" + \" \" * indent)\n+\n+    def _repr(self, object: Any, context: set[int], level: int) -> str:\n+        return self._safe_repr(object, context.copy(), self._depth, level)\n+\n+    def _pprint_default_dict(\n+        self,\n+        object: Any,\n+        stream: IO[str],\n+        indent: int,\n+        allowance: int,\n+        context: set[int],\n+        level: int,\n+    ) -> None:\n+        rdf = self._repr(object.default_factory, context, level)\n+        stream.write(f\"{object.__class__.__name__}({rdf}, \")\n+        self._pprint_dict(object, stream, indent, allowance, context, level)\n+        stream.write(\")\")\n+\n+    _dispatch[_collections.defaultdict.__repr__] = _pprint_default_dict\n+\n+    def _pprint_counter(\n+        self,\n+        object: Any,\n+        stream: IO[str],\n+        indent: int,\n+        allowance: int,\n+        context: set[int],\n+        level: int,\n+    ) -> None:\n+        stream.write(object.__class__.__name__ + \"(\")\n+\n+        if object:\n+            stream.write(\"{\")\n+            items = object.most_common()\n+            self._format_dict_items(items, stream, indent, allowance, context, level)\n+            stream.write(\"}\")\n+\n+        stream.write(\")\")\n+\n+    _dispatch[_collections.Counter.__repr__] = _pprint_counter\n+\n+    def _pprint_chain_map(\n+        self,\n+        object: Any,\n+        stream: IO[str],\n+        indent: int,\n+        allowance: int,\n+        context: set[int],\n+        level: int,\n+    ) -> None:\n+        if not len(object.maps) or (len(object.maps) == 1 and not len(object.maps[0])):\n+            stream.write(repr(object))\n+            return\n+\n+        stream.write(object.__class__.__name__ + \"(\")\n+        self._format_items(object.maps, stream, indent, allowance, context, level)\n+        stream.write(\")\")\n+\n+    _dispatch[_collections.ChainMap.__repr__] = _pprint_chain_map\n+\n+    def _pprint_deque(\n+        self,\n+        object: Any,\n+        stream: IO[str],\n+        indent: int,\n+        allowance: int,\n+        context: set[int],\n+        level: int,\n+    ) -> None:\n+        stream.write(object.__class__.__name__ + \"(\")\n+        if object.maxlen is not None:\n+            stream.write(f\"maxlen={object.maxlen}, \")\n+        stream.write(\"[\")\n+\n+        self._format_items(object, stream, indent, allowance + 1, context, level)\n+        stream.write(\"])\")\n+\n+    _dispatch[_collections.deque.__repr__] = _pprint_deque\n+\n+    def _pprint_user_dict(\n+        self,\n+        object: Any,\n+        stream: IO[str],\n+        indent: int,\n+        allowance: int,\n+        context: set[int],\n+        level: int,\n+    ) -> None:\n+        self._format(object.data, stream, indent, allowance, context, level - 1)\n+\n+    _dispatch[_collections.UserDict.__repr__] = _pprint_user_dict\n+\n+    def _pprint_user_list(\n+        self,\n+        object: Any,\n+        stream: IO[str],\n+        indent: int,\n+        allowance: int,\n+        context: set[int],\n+        level: int,\n+    ) -> None:\n+        self._format(object.data, stream, indent, allowance, context, level - 1)\n+\n+    _dispatch[_collections.UserList.__repr__] = _pprint_user_list\n+\n+    def _pprint_user_string(\n+        self,\n+        object: Any,\n+        stream: IO[str],\n+        indent: int,\n+        allowance: int,\n+        context: set[int],\n+        level: int,\n+    ) -> None:\n+        self._format(object.data, stream, indent, allowance, context, level - 1)\n+\n+    _dispatch[_collections.UserString.__repr__] = _pprint_user_string\n+\n+    def _safe_repr(\n+        self, object: Any, context: set[int], maxlevels: int | None, level: int\n+    ) -> str:\n+        typ = type(object)\n+        if typ in _builtin_scalars:\n+            return repr(object)\n+\n+        r = getattr(typ, \"__repr__\", None)\n+\n+        if issubclass(typ, dict) and r is dict.__repr__:\n+            if not object:\n+                return \"{}\"\n+            objid = id(object)\n+            if maxlevels and level >= maxlevels:\n+                return \"{...}\"\n+            if objid in context:\n+                return _recursion(object)\n+            context.add(objid)\n+            components: list[str] = []\n+            append = components.append\n+            level += 1\n+            for k, v in sorted(object.items(), key=_safe_tuple):\n+                krepr = self._safe_repr(k, context, maxlevels, level)\n+                vrepr = self._safe_repr(v, context, maxlevels, level)\n+                append(f\"{krepr}: {vrepr}\")\n+            context.remove(objid)\n+            return \"{{{}}}\".format(\", \".join(components))\n+\n+        if (issubclass(typ, list) and r is list.__repr__) or (\n+            issubclass(typ, tuple) and r is tuple.__repr__\n+        ):\n+            if issubclass(typ, list):\n+                if not object:\n+                    return \"[]\"\n+                format = \"[%s]\"\n+            elif len(object) == 1:\n+                format = \"(%s,)\"\n+            else:\n+                if not object:\n+                    return \"()\"\n+                format = \"(%s)\"\n+            objid = id(object)\n+            if maxlevels and level >= maxlevels:\n+                return format % \"...\"\n+            if objid in context:\n+                return _recursion(object)\n+            context.add(objid)\n+            components = []\n+            append = components.append\n+            level += 1\n+            for o in object:\n+                orepr = self._safe_repr(o, context, maxlevels, level)\n+                append(orepr)\n+            context.remove(objid)\n+            return format % \", \".join(components)\n+\n+        return repr(object)\n+\n+\n+_builtin_scalars = frozenset(\n+    {str, bytes, bytearray, float, complex, bool, type(None), int}\n+)\n+\n+\n+def _recursion(object: Any) -> str:\n+    return f\"<Recursion on {type(object).__name__} with id={id(object)}>\"\n+\n+\n+def _wrap_bytes_repr(object: Any, width: int, allowance: int) -> Iterator[str]:\n+    current = b\"\"\n+    last = len(object) // 4 * 4\n+    for i in range(0, len(object), 4):\n+        part = object[i : i + 4]\n+        candidate = current + part\n+        if i == last:\n+            width -= allowance\n+        if len(repr(candidate)) > width:\n+            if current:\n+                yield repr(current)\n+            current = part\n+        else:\n+            current = candidate\n+    if current:\n+        yield repr(current)",
      "patch_lines": [
        "@@ -0,0 +1,673 @@\n",
        "+# mypy: allow-untyped-defs\n",
        "+# This module was imported from the cpython standard library\n",
        "+# (https://github.com/python/cpython/) at commit\n",
        "+# c5140945c723ae6c4b7ee81ff720ac8ea4b52cfd (python3.12).\n",
        "+#\n",
        "+#\n",
        "+#  Original Author:      Fred L. Drake, Jr.\n",
        "+#                        fdrake@acm.org\n",
        "+#\n",
        "+#  This is a simple little module I wrote to make life easier.  I didn't\n",
        "+#  see anything quite like it in the library, though I may have overlooked\n",
        "+#  something.  I wrote this when I was trying to read some heavily nested\n",
        "+#  tuples with fairly non-descriptive content.  This is modeled very much\n",
        "+#  after Lisp/Scheme - style pretty-printing of lists.  If you find it\n",
        "+#  useful, thank small children who sleep at night.\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+import collections as _collections\n",
        "+from collections.abc import Callable\n",
        "+from collections.abc import Iterator\n",
        "+import dataclasses as _dataclasses\n",
        "+from io import StringIO as _StringIO\n",
        "+import re\n",
        "+import types as _types\n",
        "+from typing import Any\n",
        "+from typing import IO\n",
        "+\n",
        "+\n",
        "+class _safe_key:\n",
        "+    \"\"\"Helper function for key functions when sorting unorderable objects.\n",
        "+\n",
        "+    The wrapped-object will fallback to a Py2.x style comparison for\n",
        "+    unorderable types (sorting first comparing the type name and then by\n",
        "+    the obj ids).  Does not work recursively, so dict.items() must have\n",
        "+    _safe_key applied to both the key and the value.\n",
        "+\n",
        "+    \"\"\"\n",
        "+\n",
        "+    __slots__ = [\"obj\"]\n",
        "+\n",
        "+    def __init__(self, obj):\n",
        "+        self.obj = obj\n",
        "+\n",
        "+    def __lt__(self, other):\n",
        "+        try:\n",
        "+            return self.obj < other.obj\n",
        "+        except TypeError:\n",
        "+            return (str(type(self.obj)), id(self.obj)) < (\n",
        "+                str(type(other.obj)),\n",
        "+                id(other.obj),\n",
        "+            )\n",
        "+\n",
        "+\n",
        "+def _safe_tuple(t):\n",
        "+    \"\"\"Helper function for comparing 2-tuples\"\"\"\n",
        "+    return _safe_key(t[0]), _safe_key(t[1])\n",
        "+\n",
        "+\n",
        "+class PrettyPrinter:\n",
        "+    def __init__(\n",
        "+        self,\n",
        "+        indent: int = 4,\n",
        "+        width: int = 80,\n",
        "+        depth: int | None = None,\n",
        "+    ) -> None:\n",
        "+        \"\"\"Handle pretty printing operations onto a stream using a set of\n",
        "+        configured parameters.\n",
        "+\n",
        "+        indent\n",
        "+            Number of spaces to indent for each level of nesting.\n",
        "+\n",
        "+        width\n",
        "+            Attempted maximum number of columns in the output.\n",
        "+\n",
        "+        depth\n",
        "+            The maximum depth to print out nested structures.\n",
        "+\n",
        "+        \"\"\"\n",
        "+        if indent < 0:\n",
        "+            raise ValueError(\"indent must be >= 0\")\n",
        "+        if depth is not None and depth <= 0:\n",
        "+            raise ValueError(\"depth must be > 0\")\n",
        "+        if not width:\n",
        "+            raise ValueError(\"width must be != 0\")\n",
        "+        self._depth = depth\n",
        "+        self._indent_per_level = indent\n",
        "+        self._width = width\n",
        "+\n",
        "+    def pformat(self, object: Any) -> str:\n",
        "+        sio = _StringIO()\n",
        "+        self._format(object, sio, 0, 0, set(), 0)\n",
        "+        return sio.getvalue()\n",
        "+\n",
        "+    def _format(\n",
        "+        self,\n",
        "+        object: Any,\n",
        "+        stream: IO[str],\n",
        "+        indent: int,\n",
        "+        allowance: int,\n",
        "+        context: set[int],\n",
        "+        level: int,\n",
        "+    ) -> None:\n",
        "+        objid = id(object)\n",
        "+        if objid in context:\n",
        "+            stream.write(_recursion(object))\n",
        "+            return\n",
        "+\n",
        "+        p = self._dispatch.get(type(object).__repr__, None)\n",
        "+        if p is not None:\n",
        "+            context.add(objid)\n",
        "+            p(self, object, stream, indent, allowance, context, level + 1)\n",
        "+            context.remove(objid)\n",
        "+        elif (\n",
        "+            _dataclasses.is_dataclass(object)\n",
        "+            and not isinstance(object, type)\n",
        "+            and object.__dataclass_params__.repr  # type:ignore[attr-defined]\n",
        "+            and\n",
        "+            # Check dataclass has generated repr method.\n",
        "+            hasattr(object.__repr__, \"__wrapped__\")\n",
        "+            and \"__create_fn__\" in object.__repr__.__wrapped__.__qualname__\n",
        "+        ):\n",
        "+            context.add(objid)\n",
        "+            self._pprint_dataclass(\n",
        "+                object, stream, indent, allowance, context, level + 1\n",
        "+            )\n",
        "+            context.remove(objid)\n",
        "+        else:\n",
        "+            stream.write(self._repr(object, context, level))\n",
        "+\n",
        "+    def _pprint_dataclass(\n",
        "+        self,\n",
        "+        object: Any,\n",
        "+        stream: IO[str],\n",
        "+        indent: int,\n",
        "+        allowance: int,\n",
        "+        context: set[int],\n",
        "+        level: int,\n",
        "+    ) -> None:\n",
        "+        cls_name = object.__class__.__name__\n",
        "+        items = [\n",
        "+            (f.name, getattr(object, f.name))\n",
        "+            for f in _dataclasses.fields(object)\n",
        "+            if f.repr\n",
        "+        ]\n",
        "+        stream.write(cls_name + \"(\")\n",
        "+        self._format_namespace_items(items, stream, indent, allowance, context, level)\n",
        "+        stream.write(\")\")\n",
        "+\n",
        "+    _dispatch: dict[\n",
        "+        Callable[..., str],\n",
        "+        Callable[[PrettyPrinter, Any, IO[str], int, int, set[int], int], None],\n",
        "+    ] = {}\n",
        "+\n",
        "+    def _pprint_dict(\n",
        "+        self,\n",
        "+        object: Any,\n",
        "+        stream: IO[str],\n",
        "+        indent: int,\n",
        "+        allowance: int,\n",
        "+        context: set[int],\n",
        "+        level: int,\n",
        "+    ) -> None:\n",
        "+        write = stream.write\n",
        "+        write(\"{\")\n",
        "+        items = sorted(object.items(), key=_safe_tuple)\n",
        "+        self._format_dict_items(items, stream, indent, allowance, context, level)\n",
        "+        write(\"}\")\n",
        "+\n",
        "+    _dispatch[dict.__repr__] = _pprint_dict\n",
        "+\n",
        "+    def _pprint_ordered_dict(\n",
        "+        self,\n",
        "+        object: Any,\n",
        "+        stream: IO[str],\n",
        "+        indent: int,\n",
        "+        allowance: int,\n",
        "+        context: set[int],\n",
        "+        level: int,\n",
        "+    ) -> None:\n",
        "+        if not len(object):\n",
        "+            stream.write(repr(object))\n",
        "+            return\n",
        "+        cls = object.__class__\n",
        "+        stream.write(cls.__name__ + \"(\")\n",
        "+        self._pprint_dict(object, stream, indent, allowance, context, level)\n",
        "+        stream.write(\")\")\n",
        "+\n",
        "+    _dispatch[_collections.OrderedDict.__repr__] = _pprint_ordered_dict\n",
        "+\n",
        "+    def _pprint_list(\n",
        "+        self,\n",
        "+        object: Any,\n",
        "+        stream: IO[str],\n",
        "+        indent: int,\n",
        "+        allowance: int,\n",
        "+        context: set[int],\n",
        "+        level: int,\n",
        "+    ) -> None:\n",
        "+        stream.write(\"[\")\n",
        "+        self._format_items(object, stream, indent, allowance, context, level)\n",
        "+        stream.write(\"]\")\n",
        "+\n",
        "+    _dispatch[list.__repr__] = _pprint_list\n",
        "+\n",
        "+    def _pprint_tuple(\n",
        "+        self,\n",
        "+        object: Any,\n",
        "+        stream: IO[str],\n",
        "+        indent: int,\n",
        "+        allowance: int,\n",
        "+        context: set[int],\n",
        "+        level: int,\n",
        "+    ) -> None:\n",
        "+        stream.write(\"(\")\n",
        "+        self._format_items(object, stream, indent, allowance, context, level)\n",
        "+        stream.write(\")\")\n",
        "+\n",
        "+    _dispatch[tuple.__repr__] = _pprint_tuple\n",
        "+\n",
        "+    def _pprint_set(\n",
        "+        self,\n",
        "+        object: Any,\n",
        "+        stream: IO[str],\n",
        "+        indent: int,\n",
        "+        allowance: int,\n",
        "+        context: set[int],\n",
        "+        level: int,\n",
        "+    ) -> None:\n",
        "+        if not len(object):\n",
        "+            stream.write(repr(object))\n",
        "+            return\n",
        "+        typ = object.__class__\n",
        "+        if typ is set:\n",
        "+            stream.write(\"{\")\n",
        "+            endchar = \"}\"\n",
        "+        else:\n",
        "+            stream.write(typ.__name__ + \"({\")\n",
        "+            endchar = \"})\"\n",
        "+        object = sorted(object, key=_safe_key)\n",
        "+        self._format_items(object, stream, indent, allowance, context, level)\n",
        "+        stream.write(endchar)\n",
        "+\n",
        "+    _dispatch[set.__repr__] = _pprint_set\n",
        "+    _dispatch[frozenset.__repr__] = _pprint_set\n",
        "+\n",
        "+    def _pprint_str(\n",
        "+        self,\n",
        "+        object: Any,\n",
        "+        stream: IO[str],\n",
        "+        indent: int,\n",
        "+        allowance: int,\n",
        "+        context: set[int],\n",
        "+        level: int,\n",
        "+    ) -> None:\n",
        "+        write = stream.write\n",
        "+        if not len(object):\n",
        "+            write(repr(object))\n",
        "+            return\n",
        "+        chunks = []\n",
        "+        lines = object.splitlines(True)\n",
        "+        if level == 1:\n",
        "+            indent += 1\n",
        "+            allowance += 1\n",
        "+        max_width1 = max_width = self._width - indent\n",
        "+        for i, line in enumerate(lines):\n",
        "+            rep = repr(line)\n",
        "+            if i == len(lines) - 1:\n",
        "+                max_width1 -= allowance\n",
        "+            if len(rep) <= max_width1:\n",
        "+                chunks.append(rep)\n",
        "+            else:\n",
        "+                # A list of alternating (non-space, space) strings\n",
        "+                parts = re.findall(r\"\\S*\\s*\", line)\n",
        "+                assert parts\n",
        "+                assert not parts[-1]\n",
        "+                parts.pop()  # drop empty last part\n",
        "+                max_width2 = max_width\n",
        "+                current = \"\"\n",
        "+                for j, part in enumerate(parts):\n",
        "+                    candidate = current + part\n",
        "+                    if j == len(parts) - 1 and i == len(lines) - 1:\n",
        "+                        max_width2 -= allowance\n",
        "+                    if len(repr(candidate)) > max_width2:\n",
        "+                        if current:\n",
        "+                            chunks.append(repr(current))\n",
        "+                        current = part\n",
        "+                    else:\n",
        "+                        current = candidate\n",
        "+                if current:\n",
        "+                    chunks.append(repr(current))\n",
        "+        if len(chunks) == 1:\n",
        "+            write(rep)\n",
        "+            return\n",
        "+        if level == 1:\n",
        "+            write(\"(\")\n",
        "+        for i, rep in enumerate(chunks):\n",
        "+            if i > 0:\n",
        "+                write(\"\\n\" + \" \" * indent)\n",
        "+            write(rep)\n",
        "+        if level == 1:\n",
        "+            write(\")\")\n",
        "+\n",
        "+    _dispatch[str.__repr__] = _pprint_str\n",
        "+\n",
        "+    def _pprint_bytes(\n",
        "+        self,\n",
        "+        object: Any,\n",
        "+        stream: IO[str],\n",
        "+        indent: int,\n",
        "+        allowance: int,\n",
        "+        context: set[int],\n",
        "+        level: int,\n",
        "+    ) -> None:\n",
        "+        write = stream.write\n",
        "+        if len(object) <= 4:\n",
        "+            write(repr(object))\n",
        "+            return\n",
        "+        parens = level == 1\n",
        "+        if parens:\n",
        "+            indent += 1\n",
        "+            allowance += 1\n",
        "+            write(\"(\")\n",
        "+        delim = \"\"\n",
        "+        for rep in _wrap_bytes_repr(object, self._width - indent, allowance):\n",
        "+            write(delim)\n",
        "+            write(rep)\n",
        "+            if not delim:\n",
        "+                delim = \"\\n\" + \" \" * indent\n",
        "+        if parens:\n",
        "+            write(\")\")\n",
        "+\n",
        "+    _dispatch[bytes.__repr__] = _pprint_bytes\n",
        "+\n",
        "+    def _pprint_bytearray(\n",
        "+        self,\n",
        "+        object: Any,\n",
        "+        stream: IO[str],\n",
        "+        indent: int,\n",
        "+        allowance: int,\n",
        "+        context: set[int],\n",
        "+        level: int,\n",
        "+    ) -> None:\n",
        "+        write = stream.write\n",
        "+        write(\"bytearray(\")\n",
        "+        self._pprint_bytes(\n",
        "+            bytes(object), stream, indent + 10, allowance + 1, context, level + 1\n",
        "+        )\n",
        "+        write(\")\")\n",
        "+\n",
        "+    _dispatch[bytearray.__repr__] = _pprint_bytearray\n",
        "+\n",
        "+    def _pprint_mappingproxy(\n",
        "+        self,\n",
        "+        object: Any,\n",
        "+        stream: IO[str],\n",
        "+        indent: int,\n",
        "+        allowance: int,\n",
        "+        context: set[int],\n",
        "+        level: int,\n",
        "+    ) -> None:\n",
        "+        stream.write(\"mappingproxy(\")\n",
        "+        self._format(object.copy(), stream, indent, allowance, context, level)\n",
        "+        stream.write(\")\")\n",
        "+\n",
        "+    _dispatch[_types.MappingProxyType.__repr__] = _pprint_mappingproxy\n",
        "+\n",
        "+    def _pprint_simplenamespace(\n",
        "+        self,\n",
        "+        object: Any,\n",
        "+        stream: IO[str],\n",
        "+        indent: int,\n",
        "+        allowance: int,\n",
        "+        context: set[int],\n",
        "+        level: int,\n",
        "+    ) -> None:\n",
        "+        if type(object) is _types.SimpleNamespace:\n",
        "+            # The SimpleNamespace repr is \"namespace\" instead of the class\n",
        "+            # name, so we do the same here. For subclasses; use the class name.\n",
        "+            cls_name = \"namespace\"\n",
        "+        else:\n",
        "+            cls_name = object.__class__.__name__\n",
        "+        items = object.__dict__.items()\n",
        "+        stream.write(cls_name + \"(\")\n",
        "+        self._format_namespace_items(items, stream, indent, allowance, context, level)\n",
        "+        stream.write(\")\")\n",
        "+\n",
        "+    _dispatch[_types.SimpleNamespace.__repr__] = _pprint_simplenamespace\n",
        "+\n",
        "+    def _format_dict_items(\n",
        "+        self,\n",
        "+        items: list[tuple[Any, Any]],\n",
        "+        stream: IO[str],\n",
        "+        indent: int,\n",
        "+        allowance: int,\n",
        "+        context: set[int],\n",
        "+        level: int,\n",
        "+    ) -> None:\n",
        "+        if not items:\n",
        "+            return\n",
        "+\n",
        "+        write = stream.write\n",
        "+        item_indent = indent + self._indent_per_level\n",
        "+        delimnl = \"\\n\" + \" \" * item_indent\n",
        "+        for key, ent in items:\n",
        "+            write(delimnl)\n",
        "+            write(self._repr(key, context, level))\n",
        "+            write(\": \")\n",
        "+            self._format(ent, stream, item_indent, 1, context, level)\n",
        "+            write(\",\")\n",
        "+\n",
        "+        write(\"\\n\" + \" \" * indent)\n",
        "+\n",
        "+    def _format_namespace_items(\n",
        "+        self,\n",
        "+        items: list[tuple[Any, Any]],\n",
        "+        stream: IO[str],\n",
        "+        indent: int,\n",
        "+        allowance: int,\n",
        "+        context: set[int],\n",
        "+        level: int,\n",
        "+    ) -> None:\n",
        "+        if not items:\n",
        "+            return\n",
        "+\n",
        "+        write = stream.write\n",
        "+        item_indent = indent + self._indent_per_level\n",
        "+        delimnl = \"\\n\" + \" \" * item_indent\n",
        "+        for key, ent in items:\n",
        "+            write(delimnl)\n",
        "+            write(key)\n",
        "+            write(\"=\")\n",
        "+            if id(ent) in context:\n",
        "+                # Special-case representation of recursion to match standard\n",
        "+                # recursive dataclass repr.\n",
        "+                write(\"...\")\n",
        "+            else:\n",
        "+                self._format(\n",
        "+                    ent,\n",
        "+                    stream,\n",
        "+                    item_indent + len(key) + 1,\n",
        "+                    1,\n",
        "+                    context,\n",
        "+                    level,\n",
        "+                )\n",
        "+\n",
        "+            write(\",\")\n",
        "+\n",
        "+        write(\"\\n\" + \" \" * indent)\n",
        "+\n",
        "+    def _format_items(\n",
        "+        self,\n",
        "+        items: list[Any],\n",
        "+        stream: IO[str],\n",
        "+        indent: int,\n",
        "+        allowance: int,\n",
        "+        context: set[int],\n",
        "+        level: int,\n",
        "+    ) -> None:\n",
        "+        if not items:\n",
        "+            return\n",
        "+\n",
        "+        write = stream.write\n",
        "+        item_indent = indent + self._indent_per_level\n",
        "+        delimnl = \"\\n\" + \" \" * item_indent\n",
        "+\n",
        "+        for item in items:\n",
        "+            write(delimnl)\n",
        "+            self._format(item, stream, item_indent, 1, context, level)\n",
        "+            write(\",\")\n",
        "+\n",
        "+        write(\"\\n\" + \" \" * indent)\n",
        "+\n",
        "+    def _repr(self, object: Any, context: set[int], level: int) -> str:\n",
        "+        return self._safe_repr(object, context.copy(), self._depth, level)\n",
        "+\n",
        "+    def _pprint_default_dict(\n",
        "+        self,\n",
        "+        object: Any,\n",
        "+        stream: IO[str],\n",
        "+        indent: int,\n",
        "+        allowance: int,\n",
        "+        context: set[int],\n",
        "+        level: int,\n",
        "+    ) -> None:\n",
        "+        rdf = self._repr(object.default_factory, context, level)\n",
        "+        stream.write(f\"{object.__class__.__name__}({rdf}, \")\n",
        "+        self._pprint_dict(object, stream, indent, allowance, context, level)\n",
        "+        stream.write(\")\")\n",
        "+\n",
        "+    _dispatch[_collections.defaultdict.__repr__] = _pprint_default_dict\n",
        "+\n",
        "+    def _pprint_counter(\n",
        "+        self,\n",
        "+        object: Any,\n",
        "+        stream: IO[str],\n",
        "+        indent: int,\n",
        "+        allowance: int,\n",
        "+        context: set[int],\n",
        "+        level: int,\n",
        "+    ) -> None:\n",
        "+        stream.write(object.__class__.__name__ + \"(\")\n",
        "+\n",
        "+        if object:\n",
        "+            stream.write(\"{\")\n",
        "+            items = object.most_common()\n",
        "+            self._format_dict_items(items, stream, indent, allowance, context, level)\n",
        "+            stream.write(\"}\")\n",
        "+\n",
        "+        stream.write(\")\")\n",
        "+\n",
        "+    _dispatch[_collections.Counter.__repr__] = _pprint_counter\n",
        "+\n",
        "+    def _pprint_chain_map(\n",
        "+        self,\n",
        "+        object: Any,\n",
        "+        stream: IO[str],\n",
        "+        indent: int,\n",
        "+        allowance: int,\n",
        "+        context: set[int],\n",
        "+        level: int,\n",
        "+    ) -> None:\n",
        "+        if not len(object.maps) or (len(object.maps) == 1 and not len(object.maps[0])):\n",
        "+            stream.write(repr(object))\n",
        "+            return\n",
        "+\n",
        "+        stream.write(object.__class__.__name__ + \"(\")\n",
        "+        self._format_items(object.maps, stream, indent, allowance, context, level)\n",
        "+        stream.write(\")\")\n",
        "+\n",
        "+    _dispatch[_collections.ChainMap.__repr__] = _pprint_chain_map\n",
        "+\n",
        "+    def _pprint_deque(\n",
        "+        self,\n",
        "+        object: Any,\n",
        "+        stream: IO[str],\n",
        "+        indent: int,\n",
        "+        allowance: int,\n",
        "+        context: set[int],\n",
        "+        level: int,\n",
        "+    ) -> None:\n",
        "+        stream.write(object.__class__.__name__ + \"(\")\n",
        "+        if object.maxlen is not None:\n",
        "+            stream.write(f\"maxlen={object.maxlen}, \")\n",
        "+        stream.write(\"[\")\n",
        "+\n",
        "+        self._format_items(object, stream, indent, allowance + 1, context, level)\n",
        "+        stream.write(\"])\")\n",
        "+\n",
        "+    _dispatch[_collections.deque.__repr__] = _pprint_deque\n",
        "+\n",
        "+    def _pprint_user_dict(\n",
        "+        self,\n",
        "+        object: Any,\n",
        "+        stream: IO[str],\n",
        "+        indent: int,\n",
        "+        allowance: int,\n",
        "+        context: set[int],\n",
        "+        level: int,\n",
        "+    ) -> None:\n",
        "+        self._format(object.data, stream, indent, allowance, context, level - 1)\n",
        "+\n",
        "+    _dispatch[_collections.UserDict.__repr__] = _pprint_user_dict\n",
        "+\n",
        "+    def _pprint_user_list(\n",
        "+        self,\n",
        "+        object: Any,\n",
        "+        stream: IO[str],\n",
        "+        indent: int,\n",
        "+        allowance: int,\n",
        "+        context: set[int],\n",
        "+        level: int,\n",
        "+    ) -> None:\n",
        "+        self._format(object.data, stream, indent, allowance, context, level - 1)\n",
        "+\n",
        "+    _dispatch[_collections.UserList.__repr__] = _pprint_user_list\n",
        "+\n",
        "+    def _pprint_user_string(\n",
        "+        self,\n",
        "+        object: Any,\n",
        "+        stream: IO[str],\n",
        "+        indent: int,\n",
        "+        allowance: int,\n",
        "+        context: set[int],\n",
        "+        level: int,\n",
        "+    ) -> None:\n",
        "+        self._format(object.data, stream, indent, allowance, context, level - 1)\n",
        "+\n",
        "+    _dispatch[_collections.UserString.__repr__] = _pprint_user_string\n",
        "+\n",
        "+    def _safe_repr(\n",
        "+        self, object: Any, context: set[int], maxlevels: int | None, level: int\n",
        "+    ) -> str:\n",
        "+        typ = type(object)\n",
        "+        if typ in _builtin_scalars:\n",
        "+            return repr(object)\n",
        "+\n",
        "+        r = getattr(typ, \"__repr__\", None)\n",
        "+\n",
        "+        if issubclass(typ, dict) and r is dict.__repr__:\n",
        "+            if not object:\n",
        "+                return \"{}\"\n",
        "+            objid = id(object)\n",
        "+            if maxlevels and level >= maxlevels:\n",
        "+                return \"{...}\"\n",
        "+            if objid in context:\n",
        "+                return _recursion(object)\n",
        "+            context.add(objid)\n",
        "+            components: list[str] = []\n",
        "+            append = components.append\n",
        "+            level += 1\n",
        "+            for k, v in sorted(object.items(), key=_safe_tuple):\n",
        "+                krepr = self._safe_repr(k, context, maxlevels, level)\n",
        "+                vrepr = self._safe_repr(v, context, maxlevels, level)\n",
        "+                append(f\"{krepr}: {vrepr}\")\n",
        "+            context.remove(objid)\n",
        "+            return \"{{{}}}\".format(\", \".join(components))\n",
        "+\n",
        "+        if (issubclass(typ, list) and r is list.__repr__) or (\n",
        "+            issubclass(typ, tuple) and r is tuple.__repr__\n",
        "+        ):\n",
        "+            if issubclass(typ, list):\n",
        "+                if not object:\n",
        "+                    return \"[]\"\n",
        "+                format = \"[%s]\"\n",
        "+            elif len(object) == 1:\n",
        "+                format = \"(%s,)\"\n",
        "+            else:\n",
        "+                if not object:\n",
        "+                    return \"()\"\n",
        "+                format = \"(%s)\"\n",
        "+            objid = id(object)\n",
        "+            if maxlevels and level >= maxlevels:\n",
        "+                return format % \"...\"\n",
        "+            if objid in context:\n",
        "+                return _recursion(object)\n",
        "+            context.add(objid)\n",
        "+            components = []\n",
        "+            append = components.append\n",
        "+            level += 1\n",
        "+            for o in object:\n",
        "+                orepr = self._safe_repr(o, context, maxlevels, level)\n",
        "+                append(orepr)\n",
        "+            context.remove(objid)\n",
        "+            return format % \", \".join(components)\n",
        "+\n",
        "+        return repr(object)\n",
        "+\n",
        "+\n",
        "+_builtin_scalars = frozenset(\n",
        "+    {str, bytes, bytearray, float, complex, bool, type(None), int}\n",
        "+)\n",
        "+\n",
        "+\n",
        "+def _recursion(object: Any) -> str:\n",
        "+    return f\"<Recursion on {type(object).__name__} with id={id(object)}>\"\n",
        "+\n",
        "+\n",
        "+def _wrap_bytes_repr(object: Any, width: int, allowance: int) -> Iterator[str]:\n",
        "+    current = b\"\"\n",
        "+    last = len(object) // 4 * 4\n",
        "+    for i in range(0, len(object), 4):\n",
        "+        part = object[i : i + 4]\n",
        "+        candidate = current + part\n",
        "+        if i == last:\n",
        "+            width -= allowance\n",
        "+        if len(repr(candidate)) > width:\n",
        "+            if current:\n",
        "+                yield repr(current)\n",
        "+            current = part\n",
        "+        else:\n",
        "+            current = candidate\n",
        "+    if current:\n",
        "+        yield repr(current)\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/_io/saferepr.py",
      "status": "added",
      "additions": 130,
      "deletions": 0,
      "patch": "@@ -0,0 +1,130 @@\n+from __future__ import annotations\n+\n+import pprint\n+import reprlib\n+\n+\n+def _try_repr_or_str(obj: object) -> str:\n+    try:\n+        return repr(obj)\n+    except (KeyboardInterrupt, SystemExit):\n+        raise\n+    except BaseException:\n+        return f'{type(obj).__name__}(\"{obj}\")'\n+\n+\n+def _format_repr_exception(exc: BaseException, obj: object) -> str:\n+    try:\n+        exc_info = _try_repr_or_str(exc)\n+    except (KeyboardInterrupt, SystemExit):\n+        raise\n+    except BaseException as inner_exc:\n+        exc_info = f\"unpresentable exception ({_try_repr_or_str(inner_exc)})\"\n+    return (\n+        f\"<[{exc_info} raised in repr()] {type(obj).__name__} object at 0x{id(obj):x}>\"\n+    )\n+\n+\n+def _ellipsize(s: str, maxsize: int) -> str:\n+    if len(s) > maxsize:\n+        i = max(0, (maxsize - 3) // 2)\n+        j = max(0, maxsize - 3 - i)\n+        return s[:i] + \"...\" + s[len(s) - j :]\n+    return s\n+\n+\n+class SafeRepr(reprlib.Repr):\n+    \"\"\"\n+    repr.Repr that limits the resulting size of repr() and includes\n+    information on exceptions raised during the call.\n+    \"\"\"\n+\n+    def __init__(self, maxsize: int | None, use_ascii: bool = False) -> None:\n+        \"\"\"\n+        :param maxsize:\n+            If not None, will truncate the resulting repr to that specific size, using ellipsis\n+            somewhere in the middle to hide the extra text.\n+            If None, will not impose any size limits on the returning repr.\n+        \"\"\"\n+        super().__init__()\n+        # ``maxstring`` is used by the superclass, and needs to be an int; using a\n+        # very large number in case maxsize is None, meaning we want to disable\n+        # truncation.\n+        self.maxstring = maxsize if maxsize is not None else 1_000_000_000\n+        self.maxsize = maxsize\n+        self.use_ascii = use_ascii\n+\n+    def repr(self, x: object) -> str:\n+        try:\n+            if self.use_ascii:\n+                s = ascii(x)\n+            else:\n+                s = super().repr(x)\n+        except (KeyboardInterrupt, SystemExit):\n+            raise\n+        except BaseException as exc:\n+            s = _format_repr_exception(exc, x)\n+        if self.maxsize is not None:\n+            s = _ellipsize(s, self.maxsize)\n+        return s\n+\n+    def repr_instance(self, x: object, level: int) -> str:\n+        try:\n+            s = repr(x)\n+        except (KeyboardInterrupt, SystemExit):\n+            raise\n+        except BaseException as exc:\n+            s = _format_repr_exception(exc, x)\n+        if self.maxsize is not None:\n+            s = _ellipsize(s, self.maxsize)\n+        return s\n+\n+\n+def safeformat(obj: object) -> str:\n+    \"\"\"Return a pretty printed string for the given object.\n+\n+    Failing __repr__ functions of user instances will be represented\n+    with a short exception info.\n+    \"\"\"\n+    try:\n+        return pprint.pformat(obj)\n+    except Exception as exc:\n+        return _format_repr_exception(exc, obj)\n+\n+\n+# Maximum size of overall repr of objects to display during assertion errors.\n+DEFAULT_REPR_MAX_SIZE = 240\n+\n+\n+def saferepr(\n+    obj: object, maxsize: int | None = DEFAULT_REPR_MAX_SIZE, use_ascii: bool = False\n+) -> str:\n+    \"\"\"Return a size-limited safe repr-string for the given object.\n+\n+    Failing __repr__ functions of user instances will be represented\n+    with a short exception info and 'saferepr' generally takes\n+    care to never raise exceptions itself.\n+\n+    This function is a wrapper around the Repr/reprlib functionality of the\n+    stdlib.\n+    \"\"\"\n+    return SafeRepr(maxsize, use_ascii).repr(obj)\n+\n+\n+def saferepr_unlimited(obj: object, use_ascii: bool = True) -> str:\n+    \"\"\"Return an unlimited-size safe repr-string for the given object.\n+\n+    As with saferepr, failing __repr__ functions of user instances\n+    will be represented with a short exception info.\n+\n+    This function is a wrapper around simple repr.\n+\n+    Note: a cleaner solution would be to alter ``saferepr``this way\n+    when maxsize=None, but that might affect some other code.\n+    \"\"\"\n+    try:\n+        if use_ascii:\n+            return ascii(obj)\n+        return repr(obj)\n+    except Exception as exc:\n+        return _format_repr_exception(exc, obj)",
      "patch_lines": [
        "@@ -0,0 +1,130 @@\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+import pprint\n",
        "+import reprlib\n",
        "+\n",
        "+\n",
        "+def _try_repr_or_str(obj: object) -> str:\n",
        "+    try:\n",
        "+        return repr(obj)\n",
        "+    except (KeyboardInterrupt, SystemExit):\n",
        "+        raise\n",
        "+    except BaseException:\n",
        "+        return f'{type(obj).__name__}(\"{obj}\")'\n",
        "+\n",
        "+\n",
        "+def _format_repr_exception(exc: BaseException, obj: object) -> str:\n",
        "+    try:\n",
        "+        exc_info = _try_repr_or_str(exc)\n",
        "+    except (KeyboardInterrupt, SystemExit):\n",
        "+        raise\n",
        "+    except BaseException as inner_exc:\n",
        "+        exc_info = f\"unpresentable exception ({_try_repr_or_str(inner_exc)})\"\n",
        "+    return (\n",
        "+        f\"<[{exc_info} raised in repr()] {type(obj).__name__} object at 0x{id(obj):x}>\"\n",
        "+    )\n",
        "+\n",
        "+\n",
        "+def _ellipsize(s: str, maxsize: int) -> str:\n",
        "+    if len(s) > maxsize:\n",
        "+        i = max(0, (maxsize - 3) // 2)\n",
        "+        j = max(0, maxsize - 3 - i)\n",
        "+        return s[:i] + \"...\" + s[len(s) - j :]\n",
        "+    return s\n",
        "+\n",
        "+\n",
        "+class SafeRepr(reprlib.Repr):\n",
        "+    \"\"\"\n",
        "+    repr.Repr that limits the resulting size of repr() and includes\n",
        "+    information on exceptions raised during the call.\n",
        "+    \"\"\"\n",
        "+\n",
        "+    def __init__(self, maxsize: int | None, use_ascii: bool = False) -> None:\n",
        "+        \"\"\"\n",
        "+        :param maxsize:\n",
        "+            If not None, will truncate the resulting repr to that specific size, using ellipsis\n",
        "+            somewhere in the middle to hide the extra text.\n",
        "+            If None, will not impose any size limits on the returning repr.\n",
        "+        \"\"\"\n",
        "+        super().__init__()\n",
        "+        # ``maxstring`` is used by the superclass, and needs to be an int; using a\n",
        "+        # very large number in case maxsize is None, meaning we want to disable\n",
        "+        # truncation.\n",
        "+        self.maxstring = maxsize if maxsize is not None else 1_000_000_000\n",
        "+        self.maxsize = maxsize\n",
        "+        self.use_ascii = use_ascii\n",
        "+\n",
        "+    def repr(self, x: object) -> str:\n",
        "+        try:\n",
        "+            if self.use_ascii:\n",
        "+                s = ascii(x)\n",
        "+            else:\n",
        "+                s = super().repr(x)\n",
        "+        except (KeyboardInterrupt, SystemExit):\n",
        "+            raise\n",
        "+        except BaseException as exc:\n",
        "+            s = _format_repr_exception(exc, x)\n",
        "+        if self.maxsize is not None:\n",
        "+            s = _ellipsize(s, self.maxsize)\n",
        "+        return s\n",
        "+\n",
        "+    def repr_instance(self, x: object, level: int) -> str:\n",
        "+        try:\n",
        "+            s = repr(x)\n",
        "+        except (KeyboardInterrupt, SystemExit):\n",
        "+            raise\n",
        "+        except BaseException as exc:\n",
        "+            s = _format_repr_exception(exc, x)\n",
        "+        if self.maxsize is not None:\n",
        "+            s = _ellipsize(s, self.maxsize)\n",
        "+        return s\n",
        "+\n",
        "+\n",
        "+def safeformat(obj: object) -> str:\n",
        "+    \"\"\"Return a pretty printed string for the given object.\n",
        "+\n",
        "+    Failing __repr__ functions of user instances will be represented\n",
        "+    with a short exception info.\n",
        "+    \"\"\"\n",
        "+    try:\n",
        "+        return pprint.pformat(obj)\n",
        "+    except Exception as exc:\n",
        "+        return _format_repr_exception(exc, obj)\n",
        "+\n",
        "+\n",
        "+# Maximum size of overall repr of objects to display during assertion errors.\n",
        "+DEFAULT_REPR_MAX_SIZE = 240\n",
        "+\n",
        "+\n",
        "+def saferepr(\n",
        "+    obj: object, maxsize: int | None = DEFAULT_REPR_MAX_SIZE, use_ascii: bool = False\n",
        "+) -> str:\n",
        "+    \"\"\"Return a size-limited safe repr-string for the given object.\n",
        "+\n",
        "+    Failing __repr__ functions of user instances will be represented\n",
        "+    with a short exception info and 'saferepr' generally takes\n",
        "+    care to never raise exceptions itself.\n",
        "+\n",
        "+    This function is a wrapper around the Repr/reprlib functionality of the\n",
        "+    stdlib.\n",
        "+    \"\"\"\n",
        "+    return SafeRepr(maxsize, use_ascii).repr(obj)\n",
        "+\n",
        "+\n",
        "+def saferepr_unlimited(obj: object, use_ascii: bool = True) -> str:\n",
        "+    \"\"\"Return an unlimited-size safe repr-string for the given object.\n",
        "+\n",
        "+    As with saferepr, failing __repr__ functions of user instances\n",
        "+    will be represented with a short exception info.\n",
        "+\n",
        "+    This function is a wrapper around simple repr.\n",
        "+\n",
        "+    Note: a cleaner solution would be to alter ``saferepr``this way\n",
        "+    when maxsize=None, but that might affect some other code.\n",
        "+    \"\"\"\n",
        "+    try:\n",
        "+        if use_ascii:\n",
        "+            return ascii(obj)\n",
        "+        return repr(obj)\n",
        "+    except Exception as exc:\n",
        "+        return _format_repr_exception(exc, obj)\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/_io/terminalwriter.py",
      "status": "added",
      "additions": 254,
      "deletions": 0,
      "patch": "@@ -0,0 +1,254 @@\n+\"\"\"Helper functions for writing to terminals and files.\"\"\"\n+\n+from __future__ import annotations\n+\n+from collections.abc import Sequence\n+import os\n+import shutil\n+import sys\n+from typing import final\n+from typing import Literal\n+from typing import TextIO\n+\n+import pygments\n+from pygments.formatters.terminal import TerminalFormatter\n+from pygments.lexer import Lexer\n+from pygments.lexers.diff import DiffLexer\n+from pygments.lexers.python import PythonLexer\n+\n+from ..compat import assert_never\n+from .wcwidth import wcswidth\n+\n+\n+# This code was initially copied from py 1.8.1, file _io/terminalwriter.py.\n+\n+\n+def get_terminal_width() -> int:\n+    width, _ = shutil.get_terminal_size(fallback=(80, 24))\n+\n+    # The Windows get_terminal_size may be bogus, let's sanify a bit.\n+    if width < 40:\n+        width = 80\n+\n+    return width\n+\n+\n+def should_do_markup(file: TextIO) -> bool:\n+    if os.environ.get(\"PY_COLORS\") == \"1\":\n+        return True\n+    if os.environ.get(\"PY_COLORS\") == \"0\":\n+        return False\n+    if os.environ.get(\"NO_COLOR\"):\n+        return False\n+    if os.environ.get(\"FORCE_COLOR\"):\n+        return True\n+    return (\n+        hasattr(file, \"isatty\") and file.isatty() and os.environ.get(\"TERM\") != \"dumb\"\n+    )\n+\n+\n+@final\n+class TerminalWriter:\n+    _esctable = dict(\n+        black=30,\n+        red=31,\n+        green=32,\n+        yellow=33,\n+        blue=34,\n+        purple=35,\n+        cyan=36,\n+        white=37,\n+        Black=40,\n+        Red=41,\n+        Green=42,\n+        Yellow=43,\n+        Blue=44,\n+        Purple=45,\n+        Cyan=46,\n+        White=47,\n+        bold=1,\n+        light=2,\n+        blink=5,\n+        invert=7,\n+    )\n+\n+    def __init__(self, file: TextIO | None = None) -> None:\n+        if file is None:\n+            file = sys.stdout\n+        if hasattr(file, \"isatty\") and file.isatty() and sys.platform == \"win32\":\n+            try:\n+                import colorama\n+            except ImportError:\n+                pass\n+            else:\n+                file = colorama.AnsiToWin32(file).stream\n+                assert file is not None\n+        self._file = file\n+        self.hasmarkup = should_do_markup(file)\n+        self._current_line = \"\"\n+        self._terminal_width: int | None = None\n+        self.code_highlight = True\n+\n+    @property\n+    def fullwidth(self) -> int:\n+        if self._terminal_width is not None:\n+            return self._terminal_width\n+        return get_terminal_width()\n+\n+    @fullwidth.setter\n+    def fullwidth(self, value: int) -> None:\n+        self._terminal_width = value\n+\n+    @property\n+    def width_of_current_line(self) -> int:\n+        \"\"\"Return an estimate of the width so far in the current line.\"\"\"\n+        return wcswidth(self._current_line)\n+\n+    def markup(self, text: str, **markup: bool) -> str:\n+        for name in markup:\n+            if name not in self._esctable:\n+                raise ValueError(f\"unknown markup: {name!r}\")\n+        if self.hasmarkup:\n+            esc = [self._esctable[name] for name, on in markup.items() if on]\n+            if esc:\n+                text = \"\".join(f\"\\x1b[{cod}m\" for cod in esc) + text + \"\\x1b[0m\"\n+        return text\n+\n+    def sep(\n+        self,\n+        sepchar: str,\n+        title: str | None = None,\n+        fullwidth: int | None = None,\n+        **markup: bool,\n+    ) -> None:\n+        if fullwidth is None:\n+            fullwidth = self.fullwidth\n+        # The goal is to have the line be as long as possible\n+        # under the condition that len(line) <= fullwidth.\n+        if sys.platform == \"win32\":\n+            # If we print in the last column on windows we are on a\n+            # new line but there is no way to verify/neutralize this\n+            # (we may not know the exact line width).\n+            # So let's be defensive to avoid empty lines in the output.\n+            fullwidth -= 1\n+        if title is not None:\n+            # we want 2 + 2*len(fill) + len(title) <= fullwidth\n+            # i.e.    2 + 2*len(sepchar)*N + len(title) <= fullwidth\n+            #         2*len(sepchar)*N <= fullwidth - len(title) - 2\n+            #         N <= (fullwidth - len(title) - 2) // (2*len(sepchar))\n+            N = max((fullwidth - len(title) - 2) // (2 * len(sepchar)), 1)\n+            fill = sepchar * N\n+            line = f\"{fill} {title} {fill}\"\n+        else:\n+            # we want len(sepchar)*N <= fullwidth\n+            # i.e.    N <= fullwidth // len(sepchar)\n+            line = sepchar * (fullwidth // len(sepchar))\n+        # In some situations there is room for an extra sepchar at the right,\n+        # in particular if we consider that with a sepchar like \"_ \" the\n+        # trailing space is not important at the end of the line.\n+        if len(line) + len(sepchar.rstrip()) <= fullwidth:\n+            line += sepchar.rstrip()\n+\n+        self.line(line, **markup)\n+\n+    def write(self, msg: str, *, flush: bool = False, **markup: bool) -> None:\n+        if msg:\n+            current_line = msg.rsplit(\"\\n\", 1)[-1]\n+            if \"\\n\" in msg:\n+                self._current_line = current_line\n+            else:\n+                self._current_line += current_line\n+\n+            msg = self.markup(msg, **markup)\n+\n+            try:\n+                self._file.write(msg)\n+            except UnicodeEncodeError:\n+                # Some environments don't support printing general Unicode\n+                # strings, due to misconfiguration or otherwise; in that case,\n+                # print the string escaped to ASCII.\n+                # When the Unicode situation improves we should consider\n+                # letting the error propagate instead of masking it (see #7475\n+                # for one brief attempt).\n+                msg = msg.encode(\"unicode-escape\").decode(\"ascii\")\n+                self._file.write(msg)\n+\n+            if flush:\n+                self.flush()\n+\n+    def line(self, s: str = \"\", **markup: bool) -> None:\n+        self.write(s, **markup)\n+        self.write(\"\\n\")\n+\n+    def flush(self) -> None:\n+        self._file.flush()\n+\n+    def _write_source(self, lines: Sequence[str], indents: Sequence[str] = ()) -> None:\n+        \"\"\"Write lines of source code possibly highlighted.\n+\n+        Keeping this private for now because the API is clunky. We should discuss how\n+        to evolve the terminal writer so we can have more precise color support, for example\n+        being able to write part of a line in one color and the rest in another, and so on.\n+        \"\"\"\n+        if indents and len(indents) != len(lines):\n+            raise ValueError(\n+                f\"indents size ({len(indents)}) should have same size as lines ({len(lines)})\"\n+            )\n+        if not indents:\n+            indents = [\"\"] * len(lines)\n+        source = \"\\n\".join(lines)\n+        new_lines = self._highlight(source).splitlines()\n+        for indent, new_line in zip(indents, new_lines):\n+            self.line(indent + new_line)\n+\n+    def _get_pygments_lexer(self, lexer: Literal[\"python\", \"diff\"]) -> Lexer:\n+        if lexer == \"python\":\n+            return PythonLexer()\n+        elif lexer == \"diff\":\n+            return DiffLexer()\n+        else:\n+            assert_never(lexer)\n+\n+    def _get_pygments_formatter(self) -> TerminalFormatter:\n+        from _pytest.config.exceptions import UsageError\n+\n+        theme = os.getenv(\"PYTEST_THEME\")\n+        theme_mode = os.getenv(\"PYTEST_THEME_MODE\", \"dark\")\n+\n+        try:\n+            return TerminalFormatter(bg=theme_mode, style=theme)\n+        except pygments.util.ClassNotFound as e:\n+            raise UsageError(\n+                f\"PYTEST_THEME environment variable has an invalid value: '{theme}'. \"\n+                \"Hint: See available pygments styles with `pygmentize -L styles`.\"\n+            ) from e\n+        except pygments.util.OptionError as e:\n+            raise UsageError(\n+                f\"PYTEST_THEME_MODE environment variable has an invalid value: '{theme_mode}'. \"\n+                \"The allowed values are 'dark' (default) and 'light'.\"\n+            ) from e\n+\n+    def _highlight(\n+        self, source: str, lexer: Literal[\"diff\", \"python\"] = \"python\"\n+    ) -> str:\n+        \"\"\"Highlight the given source if we have markup support.\"\"\"\n+        if not source or not self.hasmarkup or not self.code_highlight:\n+            return source\n+\n+        pygments_lexer = self._get_pygments_lexer(lexer)\n+        pygments_formatter = self._get_pygments_formatter()\n+\n+        highlighted: str = pygments.highlight(\n+            source, pygments_lexer, pygments_formatter\n+        )\n+        # pygments terminal formatter may add a newline when there wasn't one.\n+        # We don't want this, remove.\n+        if highlighted[-1] == \"\\n\" and source[-1] != \"\\n\":\n+            highlighted = highlighted[:-1]\n+\n+        # Some lexers will not set the initial color explicitly\n+        # which may lead to the previous color being propagated to the\n+        # start of the expression, so reset first.\n+        highlighted = \"\\x1b[0m\" + highlighted\n+\n+        return highlighted",
      "patch_lines": [
        "@@ -0,0 +1,254 @@\n",
        "+\"\"\"Helper functions for writing to terminals and files.\"\"\"\n",
        "+\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+from collections.abc import Sequence\n",
        "+import os\n",
        "+import shutil\n",
        "+import sys\n",
        "+from typing import final\n",
        "+from typing import Literal\n",
        "+from typing import TextIO\n",
        "+\n",
        "+import pygments\n",
        "+from pygments.formatters.terminal import TerminalFormatter\n",
        "+from pygments.lexer import Lexer\n",
        "+from pygments.lexers.diff import DiffLexer\n",
        "+from pygments.lexers.python import PythonLexer\n",
        "+\n",
        "+from ..compat import assert_never\n",
        "+from .wcwidth import wcswidth\n",
        "+\n",
        "+\n",
        "+# This code was initially copied from py 1.8.1, file _io/terminalwriter.py.\n",
        "+\n",
        "+\n",
        "+def get_terminal_width() -> int:\n",
        "+    width, _ = shutil.get_terminal_size(fallback=(80, 24))\n",
        "+\n",
        "+    # The Windows get_terminal_size may be bogus, let's sanify a bit.\n",
        "+    if width < 40:\n",
        "+        width = 80\n",
        "+\n",
        "+    return width\n",
        "+\n",
        "+\n",
        "+def should_do_markup(file: TextIO) -> bool:\n",
        "+    if os.environ.get(\"PY_COLORS\") == \"1\":\n",
        "+        return True\n",
        "+    if os.environ.get(\"PY_COLORS\") == \"0\":\n",
        "+        return False\n",
        "+    if os.environ.get(\"NO_COLOR\"):\n",
        "+        return False\n",
        "+    if os.environ.get(\"FORCE_COLOR\"):\n",
        "+        return True\n",
        "+    return (\n",
        "+        hasattr(file, \"isatty\") and file.isatty() and os.environ.get(\"TERM\") != \"dumb\"\n",
        "+    )\n",
        "+\n",
        "+\n",
        "+@final\n",
        "+class TerminalWriter:\n",
        "+    _esctable = dict(\n",
        "+        black=30,\n",
        "+        red=31,\n",
        "+        green=32,\n",
        "+        yellow=33,\n",
        "+        blue=34,\n",
        "+        purple=35,\n",
        "+        cyan=36,\n",
        "+        white=37,\n",
        "+        Black=40,\n",
        "+        Red=41,\n",
        "+        Green=42,\n",
        "+        Yellow=43,\n",
        "+        Blue=44,\n",
        "+        Purple=45,\n",
        "+        Cyan=46,\n",
        "+        White=47,\n",
        "+        bold=1,\n",
        "+        light=2,\n",
        "+        blink=5,\n",
        "+        invert=7,\n",
        "+    )\n",
        "+\n",
        "+    def __init__(self, file: TextIO | None = None) -> None:\n",
        "+        if file is None:\n",
        "+            file = sys.stdout\n",
        "+        if hasattr(file, \"isatty\") and file.isatty() and sys.platform == \"win32\":\n",
        "+            try:\n",
        "+                import colorama\n",
        "+            except ImportError:\n",
        "+                pass\n",
        "+            else:\n",
        "+                file = colorama.AnsiToWin32(file).stream\n",
        "+                assert file is not None\n",
        "+        self._file = file\n",
        "+        self.hasmarkup = should_do_markup(file)\n",
        "+        self._current_line = \"\"\n",
        "+        self._terminal_width: int | None = None\n",
        "+        self.code_highlight = True\n",
        "+\n",
        "+    @property\n",
        "+    def fullwidth(self) -> int:\n",
        "+        if self._terminal_width is not None:\n",
        "+            return self._terminal_width\n",
        "+        return get_terminal_width()\n",
        "+\n",
        "+    @fullwidth.setter\n",
        "+    def fullwidth(self, value: int) -> None:\n",
        "+        self._terminal_width = value\n",
        "+\n",
        "+    @property\n",
        "+    def width_of_current_line(self) -> int:\n",
        "+        \"\"\"Return an estimate of the width so far in the current line.\"\"\"\n",
        "+        return wcswidth(self._current_line)\n",
        "+\n",
        "+    def markup(self, text: str, **markup: bool) -> str:\n",
        "+        for name in markup:\n",
        "+            if name not in self._esctable:\n",
        "+                raise ValueError(f\"unknown markup: {name!r}\")\n",
        "+        if self.hasmarkup:\n",
        "+            esc = [self._esctable[name] for name, on in markup.items() if on]\n",
        "+            if esc:\n",
        "+                text = \"\".join(f\"\\x1b[{cod}m\" for cod in esc) + text + \"\\x1b[0m\"\n",
        "+        return text\n",
        "+\n",
        "+    def sep(\n",
        "+        self,\n",
        "+        sepchar: str,\n",
        "+        title: str | None = None,\n",
        "+        fullwidth: int | None = None,\n",
        "+        **markup: bool,\n",
        "+    ) -> None:\n",
        "+        if fullwidth is None:\n",
        "+            fullwidth = self.fullwidth\n",
        "+        # The goal is to have the line be as long as possible\n",
        "+        # under the condition that len(line) <= fullwidth.\n",
        "+        if sys.platform == \"win32\":\n",
        "+            # If we print in the last column on windows we are on a\n",
        "+            # new line but there is no way to verify/neutralize this\n",
        "+            # (we may not know the exact line width).\n",
        "+            # So let's be defensive to avoid empty lines in the output.\n",
        "+            fullwidth -= 1\n",
        "+        if title is not None:\n",
        "+            # we want 2 + 2*len(fill) + len(title) <= fullwidth\n",
        "+            # i.e.    2 + 2*len(sepchar)*N + len(title) <= fullwidth\n",
        "+            #         2*len(sepchar)*N <= fullwidth - len(title) - 2\n",
        "+            #         N <= (fullwidth - len(title) - 2) // (2*len(sepchar))\n",
        "+            N = max((fullwidth - len(title) - 2) // (2 * len(sepchar)), 1)\n",
        "+            fill = sepchar * N\n",
        "+            line = f\"{fill} {title} {fill}\"\n",
        "+        else:\n",
        "+            # we want len(sepchar)*N <= fullwidth\n",
        "+            # i.e.    N <= fullwidth // len(sepchar)\n",
        "+            line = sepchar * (fullwidth // len(sepchar))\n",
        "+        # In some situations there is room for an extra sepchar at the right,\n",
        "+        # in particular if we consider that with a sepchar like \"_ \" the\n",
        "+        # trailing space is not important at the end of the line.\n",
        "+        if len(line) + len(sepchar.rstrip()) <= fullwidth:\n",
        "+            line += sepchar.rstrip()\n",
        "+\n",
        "+        self.line(line, **markup)\n",
        "+\n",
        "+    def write(self, msg: str, *, flush: bool = False, **markup: bool) -> None:\n",
        "+        if msg:\n",
        "+            current_line = msg.rsplit(\"\\n\", 1)[-1]\n",
        "+            if \"\\n\" in msg:\n",
        "+                self._current_line = current_line\n",
        "+            else:\n",
        "+                self._current_line += current_line\n",
        "+\n",
        "+            msg = self.markup(msg, **markup)\n",
        "+\n",
        "+            try:\n",
        "+                self._file.write(msg)\n",
        "+            except UnicodeEncodeError:\n",
        "+                # Some environments don't support printing general Unicode\n",
        "+                # strings, due to misconfiguration or otherwise; in that case,\n",
        "+                # print the string escaped to ASCII.\n",
        "+                # When the Unicode situation improves we should consider\n",
        "+                # letting the error propagate instead of masking it (see #7475\n",
        "+                # for one brief attempt).\n",
        "+                msg = msg.encode(\"unicode-escape\").decode(\"ascii\")\n",
        "+                self._file.write(msg)\n",
        "+\n",
        "+            if flush:\n",
        "+                self.flush()\n",
        "+\n",
        "+    def line(self, s: str = \"\", **markup: bool) -> None:\n",
        "+        self.write(s, **markup)\n",
        "+        self.write(\"\\n\")\n",
        "+\n",
        "+    def flush(self) -> None:\n",
        "+        self._file.flush()\n",
        "+\n",
        "+    def _write_source(self, lines: Sequence[str], indents: Sequence[str] = ()) -> None:\n",
        "+        \"\"\"Write lines of source code possibly highlighted.\n",
        "+\n",
        "+        Keeping this private for now because the API is clunky. We should discuss how\n",
        "+        to evolve the terminal writer so we can have more precise color support, for example\n",
        "+        being able to write part of a line in one color and the rest in another, and so on.\n",
        "+        \"\"\"\n",
        "+        if indents and len(indents) != len(lines):\n",
        "+            raise ValueError(\n",
        "+                f\"indents size ({len(indents)}) should have same size as lines ({len(lines)})\"\n",
        "+            )\n",
        "+        if not indents:\n",
        "+            indents = [\"\"] * len(lines)\n",
        "+        source = \"\\n\".join(lines)\n",
        "+        new_lines = self._highlight(source).splitlines()\n",
        "+        for indent, new_line in zip(indents, new_lines):\n",
        "+            self.line(indent + new_line)\n",
        "+\n",
        "+    def _get_pygments_lexer(self, lexer: Literal[\"python\", \"diff\"]) -> Lexer:\n",
        "+        if lexer == \"python\":\n",
        "+            return PythonLexer()\n",
        "+        elif lexer == \"diff\":\n",
        "+            return DiffLexer()\n",
        "+        else:\n",
        "+            assert_never(lexer)\n",
        "+\n",
        "+    def _get_pygments_formatter(self) -> TerminalFormatter:\n",
        "+        from _pytest.config.exceptions import UsageError\n",
        "+\n",
        "+        theme = os.getenv(\"PYTEST_THEME\")\n",
        "+        theme_mode = os.getenv(\"PYTEST_THEME_MODE\", \"dark\")\n",
        "+\n",
        "+        try:\n",
        "+            return TerminalFormatter(bg=theme_mode, style=theme)\n",
        "+        except pygments.util.ClassNotFound as e:\n",
        "+            raise UsageError(\n",
        "+                f\"PYTEST_THEME environment variable has an invalid value: '{theme}'. \"\n",
        "+                \"Hint: See available pygments styles with `pygmentize -L styles`.\"\n",
        "+            ) from e\n",
        "+        except pygments.util.OptionError as e:\n",
        "+            raise UsageError(\n",
        "+                f\"PYTEST_THEME_MODE environment variable has an invalid value: '{theme_mode}'. \"\n",
        "+                \"The allowed values are 'dark' (default) and 'light'.\"\n",
        "+            ) from e\n",
        "+\n",
        "+    def _highlight(\n",
        "+        self, source: str, lexer: Literal[\"diff\", \"python\"] = \"python\"\n",
        "+    ) -> str:\n",
        "+        \"\"\"Highlight the given source if we have markup support.\"\"\"\n",
        "+        if not source or not self.hasmarkup or not self.code_highlight:\n",
        "+            return source\n",
        "+\n",
        "+        pygments_lexer = self._get_pygments_lexer(lexer)\n",
        "+        pygments_formatter = self._get_pygments_formatter()\n",
        "+\n",
        "+        highlighted: str = pygments.highlight(\n",
        "+            source, pygments_lexer, pygments_formatter\n",
        "+        )\n",
        "+        # pygments terminal formatter may add a newline when there wasn't one.\n",
        "+        # We don't want this, remove.\n",
        "+        if highlighted[-1] == \"\\n\" and source[-1] != \"\\n\":\n",
        "+            highlighted = highlighted[:-1]\n",
        "+\n",
        "+        # Some lexers will not set the initial color explicitly\n",
        "+        # which may lead to the previous color being propagated to the\n",
        "+        # start of the expression, so reset first.\n",
        "+        highlighted = \"\\x1b[0m\" + highlighted\n",
        "+\n",
        "+        return highlighted\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/_io/wcwidth.py",
      "status": "added",
      "additions": 57,
      "deletions": 0,
      "patch": "@@ -0,0 +1,57 @@\n+from __future__ import annotations\n+\n+from functools import lru_cache\n+import unicodedata\n+\n+\n+@lru_cache(100)\n+def wcwidth(c: str) -> int:\n+    \"\"\"Determine how many columns are needed to display a character in a terminal.\n+\n+    Returns -1 if the character is not printable.\n+    Returns 0, 1 or 2 for other characters.\n+    \"\"\"\n+    o = ord(c)\n+\n+    # ASCII fast path.\n+    if 0x20 <= o < 0x07F:\n+        return 1\n+\n+    # Some Cf/Zp/Zl characters which should be zero-width.\n+    if (\n+        o == 0x0000\n+        or 0x200B <= o <= 0x200F\n+        or 0x2028 <= o <= 0x202E\n+        or 0x2060 <= o <= 0x2063\n+    ):\n+        return 0\n+\n+    category = unicodedata.category(c)\n+\n+    # Control characters.\n+    if category == \"Cc\":\n+        return -1\n+\n+    # Combining characters with zero width.\n+    if category in (\"Me\", \"Mn\"):\n+        return 0\n+\n+    # Full/Wide east asian characters.\n+    if unicodedata.east_asian_width(c) in (\"F\", \"W\"):\n+        return 2\n+\n+    return 1\n+\n+\n+def wcswidth(s: str) -> int:\n+    \"\"\"Determine how many columns are needed to display a string in a terminal.\n+\n+    Returns -1 if the string contains non-printable characters.\n+    \"\"\"\n+    width = 0\n+    for c in unicodedata.normalize(\"NFC\", s):\n+        wc = wcwidth(c)\n+        if wc < 0:\n+            return -1\n+        width += wc\n+    return width",
      "patch_lines": [
        "@@ -0,0 +1,57 @@\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+from functools import lru_cache\n",
        "+import unicodedata\n",
        "+\n",
        "+\n",
        "+@lru_cache(100)\n",
        "+def wcwidth(c: str) -> int:\n",
        "+    \"\"\"Determine how many columns are needed to display a character in a terminal.\n",
        "+\n",
        "+    Returns -1 if the character is not printable.\n",
        "+    Returns 0, 1 or 2 for other characters.\n",
        "+    \"\"\"\n",
        "+    o = ord(c)\n",
        "+\n",
        "+    # ASCII fast path.\n",
        "+    if 0x20 <= o < 0x07F:\n",
        "+        return 1\n",
        "+\n",
        "+    # Some Cf/Zp/Zl characters which should be zero-width.\n",
        "+    if (\n",
        "+        o == 0x0000\n",
        "+        or 0x200B <= o <= 0x200F\n",
        "+        or 0x2028 <= o <= 0x202E\n",
        "+        or 0x2060 <= o <= 0x2063\n",
        "+    ):\n",
        "+        return 0\n",
        "+\n",
        "+    category = unicodedata.category(c)\n",
        "+\n",
        "+    # Control characters.\n",
        "+    if category == \"Cc\":\n",
        "+        return -1\n",
        "+\n",
        "+    # Combining characters with zero width.\n",
        "+    if category in (\"Me\", \"Mn\"):\n",
        "+        return 0\n",
        "+\n",
        "+    # Full/Wide east asian characters.\n",
        "+    if unicodedata.east_asian_width(c) in (\"F\", \"W\"):\n",
        "+        return 2\n",
        "+\n",
        "+    return 1\n",
        "+\n",
        "+\n",
        "+def wcswidth(s: str) -> int:\n",
        "+    \"\"\"Determine how many columns are needed to display a string in a terminal.\n",
        "+\n",
        "+    Returns -1 if the string contains non-printable characters.\n",
        "+    \"\"\"\n",
        "+    width = 0\n",
        "+    for c in unicodedata.normalize(\"NFC\", s):\n",
        "+        wc = wcwidth(c)\n",
        "+        if wc < 0:\n",
        "+            return -1\n",
        "+        width += wc\n",
        "+    return width\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/_py/__init__.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/_py/error.py",
      "status": "added",
      "additions": 119,
      "deletions": 0,
      "patch": "@@ -0,0 +1,119 @@\n+\"\"\"create errno-specific classes for IO or os calls.\"\"\"\n+\n+from __future__ import annotations\n+\n+from collections.abc import Callable\n+import errno\n+import os\n+import sys\n+from typing import TYPE_CHECKING\n+from typing import TypeVar\n+\n+\n+if TYPE_CHECKING:\n+    from typing_extensions import ParamSpec\n+\n+    P = ParamSpec(\"P\")\n+\n+R = TypeVar(\"R\")\n+\n+\n+class Error(EnvironmentError):\n+    def __repr__(self) -> str:\n+        return \"{}.{} {!r}: {} \".format(\n+            self.__class__.__module__,\n+            self.__class__.__name__,\n+            self.__class__.__doc__,\n+            \" \".join(map(str, self.args)),\n+            # repr(self.args)\n+        )\n+\n+    def __str__(self) -> str:\n+        s = \"[{}]: {}\".format(\n+            self.__class__.__doc__,\n+            \" \".join(map(str, self.args)),\n+        )\n+        return s\n+\n+\n+_winerrnomap = {\n+    2: errno.ENOENT,\n+    3: errno.ENOENT,\n+    17: errno.EEXIST,\n+    18: errno.EXDEV,\n+    13: errno.EBUSY,  # empty cd drive, but ENOMEDIUM seems unavailable\n+    22: errno.ENOTDIR,\n+    20: errno.ENOTDIR,\n+    267: errno.ENOTDIR,\n+    5: errno.EACCES,  # anything better?\n+}\n+\n+\n+class ErrorMaker:\n+    \"\"\"lazily provides Exception classes for each possible POSIX errno\n+    (as defined per the 'errno' module).  All such instances\n+    subclass EnvironmentError.\n+    \"\"\"\n+\n+    _errno2class: dict[int, type[Error]] = {}\n+\n+    def __getattr__(self, name: str) -> type[Error]:\n+        if name[0] == \"_\":\n+            raise AttributeError(name)\n+        eno = getattr(errno, name)\n+        cls = self._geterrnoclass(eno)\n+        setattr(self, name, cls)\n+        return cls\n+\n+    def _geterrnoclass(self, eno: int) -> type[Error]:\n+        try:\n+            return self._errno2class[eno]\n+        except KeyError:\n+            clsname = errno.errorcode.get(eno, f\"UnknownErrno{eno}\")\n+            errorcls = type(\n+                clsname,\n+                (Error,),\n+                {\"__module__\": \"py.error\", \"__doc__\": os.strerror(eno)},\n+            )\n+            self._errno2class[eno] = errorcls\n+            return errorcls\n+\n+    def checked_call(\n+        self, func: Callable[P, R], *args: P.args, **kwargs: P.kwargs\n+    ) -> R:\n+        \"\"\"Call a function and raise an errno-exception if applicable.\"\"\"\n+        __tracebackhide__ = True\n+        try:\n+            return func(*args, **kwargs)\n+        except Error:\n+            raise\n+        except OSError as value:\n+            if not hasattr(value, \"errno\"):\n+                raise\n+            if sys.platform == \"win32\":\n+                try:\n+                    # error: Invalid index type \"Optional[int]\" for \"dict[int, int]\"; expected type \"int\"  [index]\n+                    # OK to ignore because we catch the KeyError below.\n+                    cls = self._geterrnoclass(_winerrnomap[value.errno])  # type:ignore[index]\n+                except KeyError:\n+                    raise value\n+            else:\n+                # we are not on Windows, or we got a proper OSError\n+                if value.errno is None:\n+                    cls = type(\n+                        \"UnknownErrnoNone\",\n+                        (Error,),\n+                        {\"__module__\": \"py.error\", \"__doc__\": None},\n+                    )\n+                else:\n+                    cls = self._geterrnoclass(value.errno)\n+\n+            raise cls(f\"{func.__name__}{args!r}\")\n+\n+\n+_error_maker = ErrorMaker()\n+checked_call = _error_maker.checked_call\n+\n+\n+def __getattr__(attr: str) -> type[Error]:\n+    return getattr(_error_maker, attr)  # type: ignore[no-any-return]",
      "patch_lines": [
        "@@ -0,0 +1,119 @@\n",
        "+\"\"\"create errno-specific classes for IO or os calls.\"\"\"\n",
        "+\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+from collections.abc import Callable\n",
        "+import errno\n",
        "+import os\n",
        "+import sys\n",
        "+from typing import TYPE_CHECKING\n",
        "+from typing import TypeVar\n",
        "+\n",
        "+\n",
        "+if TYPE_CHECKING:\n",
        "+    from typing_extensions import ParamSpec\n",
        "+\n",
        "+    P = ParamSpec(\"P\")\n",
        "+\n",
        "+R = TypeVar(\"R\")\n",
        "+\n",
        "+\n",
        "+class Error(EnvironmentError):\n",
        "+    def __repr__(self) -> str:\n",
        "+        return \"{}.{} {!r}: {} \".format(\n",
        "+            self.__class__.__module__,\n",
        "+            self.__class__.__name__,\n",
        "+            self.__class__.__doc__,\n",
        "+            \" \".join(map(str, self.args)),\n",
        "+            # repr(self.args)\n",
        "+        )\n",
        "+\n",
        "+    def __str__(self) -> str:\n",
        "+        s = \"[{}]: {}\".format(\n",
        "+            self.__class__.__doc__,\n",
        "+            \" \".join(map(str, self.args)),\n",
        "+        )\n",
        "+        return s\n",
        "+\n",
        "+\n",
        "+_winerrnomap = {\n",
        "+    2: errno.ENOENT,\n",
        "+    3: errno.ENOENT,\n",
        "+    17: errno.EEXIST,\n",
        "+    18: errno.EXDEV,\n",
        "+    13: errno.EBUSY,  # empty cd drive, but ENOMEDIUM seems unavailable\n",
        "+    22: errno.ENOTDIR,\n",
        "+    20: errno.ENOTDIR,\n",
        "+    267: errno.ENOTDIR,\n",
        "+    5: errno.EACCES,  # anything better?\n",
        "+}\n",
        "+\n",
        "+\n",
        "+class ErrorMaker:\n",
        "+    \"\"\"lazily provides Exception classes for each possible POSIX errno\n",
        "+    (as defined per the 'errno' module).  All such instances\n",
        "+    subclass EnvironmentError.\n",
        "+    \"\"\"\n",
        "+\n",
        "+    _errno2class: dict[int, type[Error]] = {}\n",
        "+\n",
        "+    def __getattr__(self, name: str) -> type[Error]:\n",
        "+        if name[0] == \"_\":\n",
        "+            raise AttributeError(name)\n",
        "+        eno = getattr(errno, name)\n",
        "+        cls = self._geterrnoclass(eno)\n",
        "+        setattr(self, name, cls)\n",
        "+        return cls\n",
        "+\n",
        "+    def _geterrnoclass(self, eno: int) -> type[Error]:\n",
        "+        try:\n",
        "+            return self._errno2class[eno]\n",
        "+        except KeyError:\n",
        "+            clsname = errno.errorcode.get(eno, f\"UnknownErrno{eno}\")\n",
        "+            errorcls = type(\n",
        "+                clsname,\n",
        "+                (Error,),\n",
        "+                {\"__module__\": \"py.error\", \"__doc__\": os.strerror(eno)},\n",
        "+            )\n",
        "+            self._errno2class[eno] = errorcls\n",
        "+            return errorcls\n",
        "+\n",
        "+    def checked_call(\n",
        "+        self, func: Callable[P, R], *args: P.args, **kwargs: P.kwargs\n",
        "+    ) -> R:\n",
        "+        \"\"\"Call a function and raise an errno-exception if applicable.\"\"\"\n",
        "+        __tracebackhide__ = True\n",
        "+        try:\n",
        "+            return func(*args, **kwargs)\n",
        "+        except Error:\n",
        "+            raise\n",
        "+        except OSError as value:\n",
        "+            if not hasattr(value, \"errno\"):\n",
        "+                raise\n",
        "+            if sys.platform == \"win32\":\n",
        "+                try:\n",
        "+                    # error: Invalid index type \"Optional[int]\" for \"dict[int, int]\"; expected type \"int\"  [index]\n",
        "+                    # OK to ignore because we catch the KeyError below.\n",
        "+                    cls = self._geterrnoclass(_winerrnomap[value.errno])  # type:ignore[index]\n",
        "+                except KeyError:\n",
        "+                    raise value\n",
        "+            else:\n",
        "+                # we are not on Windows, or we got a proper OSError\n",
        "+                if value.errno is None:\n",
        "+                    cls = type(\n",
        "+                        \"UnknownErrnoNone\",\n",
        "+                        (Error,),\n",
        "+                        {\"__module__\": \"py.error\", \"__doc__\": None},\n",
        "+                    )\n",
        "+                else:\n",
        "+                    cls = self._geterrnoclass(value.errno)\n",
        "+\n",
        "+            raise cls(f\"{func.__name__}{args!r}\")\n",
        "+\n",
        "+\n",
        "+_error_maker = ErrorMaker()\n",
        "+checked_call = _error_maker.checked_call\n",
        "+\n",
        "+\n",
        "+def __getattr__(attr: str) -> type[Error]:\n",
        "+    return getattr(_error_maker, attr)  # type: ignore[no-any-return]\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/_py/path.py",
      "status": "added",
      "additions": 1475,
      "deletions": 0,
      "patch": "@@ -0,0 +1,1475 @@\n+# mypy: allow-untyped-defs\n+\"\"\"local path implementation.\"\"\"\n+\n+from __future__ import annotations\n+\n+import atexit\n+from collections.abc import Callable\n+from contextlib import contextmanager\n+import fnmatch\n+import importlib.util\n+import io\n+import os\n+from os.path import abspath\n+from os.path import dirname\n+from os.path import exists\n+from os.path import isabs\n+from os.path import isdir\n+from os.path import isfile\n+from os.path import islink\n+from os.path import normpath\n+import posixpath\n+from stat import S_ISDIR\n+from stat import S_ISLNK\n+from stat import S_ISREG\n+import sys\n+from typing import Any\n+from typing import cast\n+from typing import Literal\n+from typing import overload\n+from typing import TYPE_CHECKING\n+import uuid\n+import warnings\n+\n+from . import error\n+\n+\n+# Moved from local.py.\n+iswin32 = sys.platform == \"win32\" or (getattr(os, \"_name\", False) == \"nt\")\n+\n+\n+class Checkers:\n+    _depend_on_existence = \"exists\", \"link\", \"dir\", \"file\"\n+\n+    def __init__(self, path):\n+        self.path = path\n+\n+    def dotfile(self):\n+        return self.path.basename.startswith(\".\")\n+\n+    def ext(self, arg):\n+        if not arg.startswith(\".\"):\n+            arg = \".\" + arg\n+        return self.path.ext == arg\n+\n+    def basename(self, arg):\n+        return self.path.basename == arg\n+\n+    def basestarts(self, arg):\n+        return self.path.basename.startswith(arg)\n+\n+    def relto(self, arg):\n+        return self.path.relto(arg)\n+\n+    def fnmatch(self, arg):\n+        return self.path.fnmatch(arg)\n+\n+    def endswith(self, arg):\n+        return str(self.path).endswith(arg)\n+\n+    def _evaluate(self, kw):\n+        from .._code.source import getrawcode\n+\n+        for name, value in kw.items():\n+            invert = False\n+            meth = None\n+            try:\n+                meth = getattr(self, name)\n+            except AttributeError:\n+                if name[:3] == \"not\":\n+                    invert = True\n+                    try:\n+                        meth = getattr(self, name[3:])\n+                    except AttributeError:\n+                        pass\n+            if meth is None:\n+                raise TypeError(f\"no {name!r} checker available for {self.path!r}\")\n+            try:\n+                if getrawcode(meth).co_argcount > 1:\n+                    if (not meth(value)) ^ invert:\n+                        return False\n+                else:\n+                    if bool(value) ^ bool(meth()) ^ invert:\n+                        return False\n+            except (error.ENOENT, error.ENOTDIR, error.EBUSY):\n+                # EBUSY feels not entirely correct,\n+                # but its kind of necessary since ENOMEDIUM\n+                # is not accessible in python\n+                for name in self._depend_on_existence:\n+                    if name in kw:\n+                        if kw.get(name):\n+                            return False\n+                    name = \"not\" + name\n+                    if name in kw:\n+                        if not kw.get(name):\n+                            return False\n+        return True\n+\n+    _statcache: Stat\n+\n+    def _stat(self) -> Stat:\n+        try:\n+            return self._statcache\n+        except AttributeError:\n+            try:\n+                self._statcache = self.path.stat()\n+            except error.ELOOP:\n+                self._statcache = self.path.lstat()\n+            return self._statcache\n+\n+    def dir(self):\n+        return S_ISDIR(self._stat().mode)\n+\n+    def file(self):\n+        return S_ISREG(self._stat().mode)\n+\n+    def exists(self):\n+        return self._stat()\n+\n+    def link(self):\n+        st = self.path.lstat()\n+        return S_ISLNK(st.mode)\n+\n+\n+class NeverRaised(Exception):\n+    pass\n+\n+\n+class Visitor:\n+    def __init__(self, fil, rec, ignore, bf, sort):\n+        if isinstance(fil, str):\n+            fil = FNMatcher(fil)\n+        if isinstance(rec, str):\n+            self.rec: Callable[[LocalPath], bool] = FNMatcher(rec)\n+        elif not hasattr(rec, \"__call__\") and rec:\n+            self.rec = lambda path: True\n+        else:\n+            self.rec = rec\n+        self.fil = fil\n+        self.ignore = ignore\n+        self.breadthfirst = bf\n+        self.optsort = cast(Callable[[Any], Any], sorted) if sort else (lambda x: x)\n+\n+    def gen(self, path):\n+        try:\n+            entries = path.listdir()\n+        except self.ignore:\n+            return\n+        rec = self.rec\n+        dirs = self.optsort(\n+            [p for p in entries if p.check(dir=1) and (rec is None or rec(p))]\n+        )\n+        if not self.breadthfirst:\n+            for subdir in dirs:\n+                yield from self.gen(subdir)\n+        for p in self.optsort(entries):\n+            if self.fil is None or self.fil(p):\n+                yield p\n+        if self.breadthfirst:\n+            for subdir in dirs:\n+                yield from self.gen(subdir)\n+\n+\n+class FNMatcher:\n+    def __init__(self, pattern):\n+        self.pattern = pattern\n+\n+    def __call__(self, path):\n+        pattern = self.pattern\n+\n+        if (\n+            pattern.find(path.sep) == -1\n+            and iswin32\n+            and pattern.find(posixpath.sep) != -1\n+        ):\n+            # Running on Windows, the pattern has no Windows path separators,\n+            # and the pattern has one or more Posix path separators. Replace\n+            # the Posix path separators with the Windows path separator.\n+            pattern = pattern.replace(posixpath.sep, path.sep)\n+\n+        if pattern.find(path.sep) == -1:\n+            name = path.basename\n+        else:\n+            name = str(path)  # path.strpath # XXX svn?\n+            if not os.path.isabs(pattern):\n+                pattern = \"*\" + path.sep + pattern\n+        return fnmatch.fnmatch(name, pattern)\n+\n+\n+def map_as_list(func, iter):\n+    return list(map(func, iter))\n+\n+\n+class Stat:\n+    if TYPE_CHECKING:\n+\n+        @property\n+        def size(self) -> int: ...\n+\n+        @property\n+        def mtime(self) -> float: ...\n+\n+    def __getattr__(self, name: str) -> Any:\n+        return getattr(self._osstatresult, \"st_\" + name)\n+\n+    def __init__(self, path, osstatresult):\n+        self.path = path\n+        self._osstatresult = osstatresult\n+\n+    @property\n+    def owner(self):\n+        if iswin32:\n+            raise NotImplementedError(\"XXX win32\")\n+        import pwd\n+\n+        entry = error.checked_call(pwd.getpwuid, self.uid)  # type:ignore[attr-defined,unused-ignore]\n+        return entry[0]\n+\n+    @property\n+    def group(self):\n+        \"\"\"Return group name of file.\"\"\"\n+        if iswin32:\n+            raise NotImplementedError(\"XXX win32\")\n+        import grp\n+\n+        entry = error.checked_call(grp.getgrgid, self.gid)  # type:ignore[attr-defined,unused-ignore]\n+        return entry[0]\n+\n+    def isdir(self):\n+        return S_ISDIR(self._osstatresult.st_mode)\n+\n+    def isfile(self):\n+        return S_ISREG(self._osstatresult.st_mode)\n+\n+    def islink(self):\n+        self.path.lstat()\n+        return S_ISLNK(self._osstatresult.st_mode)\n+\n+\n+def getuserid(user):\n+    import pwd\n+\n+    if not isinstance(user, int):\n+        user = pwd.getpwnam(user)[2]  # type:ignore[attr-defined,unused-ignore]\n+    return user\n+\n+\n+def getgroupid(group):\n+    import grp\n+\n+    if not isinstance(group, int):\n+        group = grp.getgrnam(group)[2]  # type:ignore[attr-defined,unused-ignore]\n+    return group\n+\n+\n+class LocalPath:\n+    \"\"\"Object oriented interface to os.path and other local filesystem\n+    related information.\n+    \"\"\"\n+\n+    class ImportMismatchError(ImportError):\n+        \"\"\"raised on pyimport() if there is a mismatch of __file__'s\"\"\"\n+\n+    sep = os.sep\n+\n+    def __init__(self, path=None, expanduser=False):\n+        \"\"\"Initialize and return a local Path instance.\n+\n+        Path can be relative to the current directory.\n+        If path is None it defaults to the current working directory.\n+        If expanduser is True, tilde-expansion is performed.\n+        Note that Path instances always carry an absolute path.\n+        Note also that passing in a local path object will simply return\n+        the exact same path object. Use new() to get a new copy.\n+        \"\"\"\n+        if path is None:\n+            self.strpath = error.checked_call(os.getcwd)\n+        else:\n+            try:\n+                path = os.fspath(path)\n+            except TypeError:\n+                raise ValueError(\n+                    \"can only pass None, Path instances \"\n+                    \"or non-empty strings to LocalPath\"\n+                )\n+            if expanduser:\n+                path = os.path.expanduser(path)\n+            self.strpath = abspath(path)\n+\n+    if sys.platform != \"win32\":\n+\n+        def chown(self, user, group, rec=0):\n+            \"\"\"Change ownership to the given user and group.\n+            user and group may be specified by a number or\n+            by a name.  if rec is True change ownership\n+            recursively.\n+            \"\"\"\n+            uid = getuserid(user)\n+            gid = getgroupid(group)\n+            if rec:\n+                for x in self.visit(rec=lambda x: x.check(link=0)):\n+                    if x.check(link=0):\n+                        error.checked_call(os.chown, str(x), uid, gid)\n+            error.checked_call(os.chown, str(self), uid, gid)\n+\n+        def readlink(self) -> str:\n+            \"\"\"Return value of a symbolic link.\"\"\"\n+            # https://github.com/python/mypy/issues/12278\n+            return error.checked_call(os.readlink, self.strpath)  # type: ignore[arg-type,return-value,unused-ignore]\n+\n+        def mklinkto(self, oldname):\n+            \"\"\"Posix style hard link to another name.\"\"\"\n+            error.checked_call(os.link, str(oldname), str(self))\n+\n+        def mksymlinkto(self, value, absolute=1):\n+            \"\"\"Create a symbolic link with the given value (pointing to another name).\"\"\"\n+            if absolute:\n+                error.checked_call(os.symlink, str(value), self.strpath)\n+            else:\n+                base = self.common(value)\n+                # with posix local paths '/' is always a common base\n+                relsource = self.__class__(value).relto(base)\n+                reldest = self.relto(base)\n+                n = reldest.count(self.sep)\n+                target = self.sep.join((\"..\",) * n + (relsource,))\n+                error.checked_call(os.symlink, target, self.strpath)\n+\n+    def __div__(self, other):\n+        return self.join(os.fspath(other))\n+\n+    __truediv__ = __div__  # py3k\n+\n+    @property\n+    def basename(self):\n+        \"\"\"Basename part of path.\"\"\"\n+        return self._getbyspec(\"basename\")[0]\n+\n+    @property\n+    def dirname(self):\n+        \"\"\"Dirname part of path.\"\"\"\n+        return self._getbyspec(\"dirname\")[0]\n+\n+    @property\n+    def purebasename(self):\n+        \"\"\"Pure base name of the path.\"\"\"\n+        return self._getbyspec(\"purebasename\")[0]\n+\n+    @property\n+    def ext(self):\n+        \"\"\"Extension of the path (including the '.').\"\"\"\n+        return self._getbyspec(\"ext\")[0]\n+\n+    def read_binary(self):\n+        \"\"\"Read and return a bytestring from reading the path.\"\"\"\n+        with self.open(\"rb\") as f:\n+            return f.read()\n+\n+    def read_text(self, encoding):\n+        \"\"\"Read and return a Unicode string from reading the path.\"\"\"\n+        with self.open(\"r\", encoding=encoding) as f:\n+            return f.read()\n+\n+    def read(self, mode=\"r\"):\n+        \"\"\"Read and return a bytestring from reading the path.\"\"\"\n+        with self.open(mode) as f:\n+            return f.read()\n+\n+    def readlines(self, cr=1):\n+        \"\"\"Read and return a list of lines from the path. if cr is False, the\n+        newline will be removed from the end of each line.\"\"\"\n+        mode = \"r\"\n+\n+        if not cr:\n+            content = self.read(mode)\n+            return content.split(\"\\n\")\n+        else:\n+            f = self.open(mode)\n+            try:\n+                return f.readlines()\n+            finally:\n+                f.close()\n+\n+    def load(self):\n+        \"\"\"(deprecated) return object unpickled from self.read()\"\"\"\n+        f = self.open(\"rb\")\n+        try:\n+            import pickle\n+\n+            return error.checked_call(pickle.load, f)\n+        finally:\n+            f.close()\n+\n+    def move(self, target):\n+        \"\"\"Move this path to target.\"\"\"\n+        if target.relto(self):\n+            raise error.EINVAL(target, \"cannot move path into a subdirectory of itself\")\n+        try:\n+            self.rename(target)\n+        except error.EXDEV:  # invalid cross-device link\n+            self.copy(target)\n+            self.remove()\n+\n+    def fnmatch(self, pattern):\n+        \"\"\"Return true if the basename/fullname matches the glob-'pattern'.\n+\n+        valid pattern characters::\n+\n+            *       matches everything\n+            ?       matches any single character\n+            [seq]   matches any character in seq\n+            [!seq]  matches any char not in seq\n+\n+        If the pattern contains a path-separator then the full path\n+        is used for pattern matching and a '*' is prepended to the\n+        pattern.\n+\n+        if the pattern doesn't contain a path-separator the pattern\n+        is only matched against the basename.\n+        \"\"\"\n+        return FNMatcher(pattern)(self)\n+\n+    def relto(self, relpath):\n+        \"\"\"Return a string which is the relative part of the path\n+        to the given 'relpath'.\n+        \"\"\"\n+        if not isinstance(relpath, (str, LocalPath)):\n+            raise TypeError(f\"{relpath!r}: not a string or path object\")\n+        strrelpath = str(relpath)\n+        if strrelpath and strrelpath[-1] != self.sep:\n+            strrelpath += self.sep\n+        # assert strrelpath[-1] == self.sep\n+        # assert strrelpath[-2] != self.sep\n+        strself = self.strpath\n+        if sys.platform == \"win32\" or getattr(os, \"_name\", None) == \"nt\":\n+            if os.path.normcase(strself).startswith(os.path.normcase(strrelpath)):\n+                return strself[len(strrelpath) :]\n+        elif strself.startswith(strrelpath):\n+            return strself[len(strrelpath) :]\n+        return \"\"\n+\n+    def ensure_dir(self, *args):\n+        \"\"\"Ensure the path joined with args is a directory.\"\"\"\n+        return self.ensure(*args, dir=True)\n+\n+    def bestrelpath(self, dest):\n+        \"\"\"Return a string which is a relative path from self\n+        (assumed to be a directory) to dest such that\n+        self.join(bestrelpath) == dest and if not such\n+        path can be determined return dest.\n+        \"\"\"\n+        try:\n+            if self == dest:\n+                return os.curdir\n+            base = self.common(dest)\n+            if not base:  # can be the case on windows\n+                return str(dest)\n+            self2base = self.relto(base)\n+            reldest = dest.relto(base)\n+            if self2base:\n+                n = self2base.count(self.sep) + 1\n+            else:\n+                n = 0\n+            lst = [os.pardir] * n\n+            if reldest:\n+                lst.append(reldest)\n+            target = dest.sep.join(lst)\n+            return target\n+        except AttributeError:\n+            return str(dest)\n+\n+    def exists(self):\n+        return self.check()\n+\n+    def isdir(self):\n+        return self.check(dir=1)\n+\n+    def isfile(self):\n+        return self.check(file=1)\n+\n+    def parts(self, reverse=False):\n+        \"\"\"Return a root-first list of all ancestor directories\n+        plus the path itself.\n+        \"\"\"\n+        current = self\n+        lst = [self]\n+        while 1:\n+            last = current\n+            current = current.dirpath()\n+            if last == current:\n+                break\n+            lst.append(current)\n+        if not reverse:\n+            lst.reverse()\n+        return lst\n+\n+    def common(self, other):\n+        \"\"\"Return the common part shared with the other path\n+        or None if there is no common part.\n+        \"\"\"\n+        last = None\n+        for x, y in zip(self.parts(), other.parts()):\n+            if x != y:\n+                return last\n+            last = x\n+        return last\n+\n+    def __add__(self, other):\n+        \"\"\"Return new path object with 'other' added to the basename\"\"\"\n+        return self.new(basename=self.basename + str(other))\n+\n+    def visit(self, fil=None, rec=None, ignore=NeverRaised, bf=False, sort=False):\n+        \"\"\"Yields all paths below the current one\n+\n+        fil is a filter (glob pattern or callable), if not matching the\n+        path will not be yielded, defaulting to None (everything is\n+        returned)\n+\n+        rec is a filter (glob pattern or callable) that controls whether\n+        a node is descended, defaulting to None\n+\n+        ignore is an Exception class that is ignoredwhen calling dirlist()\n+        on any of the paths (by default, all exceptions are reported)\n+\n+        bf if True will cause a breadthfirst search instead of the\n+        default depthfirst. Default: False\n+\n+        sort if True will sort entries within each directory level.\n+        \"\"\"\n+        yield from Visitor(fil, rec, ignore, bf, sort).gen(self)\n+\n+    def _sortlist(self, res, sort):\n+        if sort:\n+            if hasattr(sort, \"__call__\"):\n+                warnings.warn(\n+                    DeprecationWarning(\n+                        \"listdir(sort=callable) is deprecated and breaks on python3\"\n+                    ),\n+                    stacklevel=3,\n+                )\n+                res.sort(sort)\n+            else:\n+                res.sort()\n+\n+    def __fspath__(self):\n+        return self.strpath\n+\n+    def __hash__(self):\n+        s = self.strpath\n+        if iswin32:\n+            s = s.lower()\n+        return hash(s)\n+\n+    def __eq__(self, other):\n+        s1 = os.fspath(self)\n+        try:\n+            s2 = os.fspath(other)\n+        except TypeError:\n+            return False\n+        if iswin32:\n+            s1 = s1.lower()\n+            try:\n+                s2 = s2.lower()\n+            except AttributeError:\n+                return False\n+        return s1 == s2\n+\n+    def __ne__(self, other):\n+        return not (self == other)\n+\n+    def __lt__(self, other):\n+        return os.fspath(self) < os.fspath(other)\n+\n+    def __gt__(self, other):\n+        return os.fspath(self) > os.fspath(other)\n+\n+    def samefile(self, other):\n+        \"\"\"Return True if 'other' references the same file as 'self'.\"\"\"\n+        other = os.fspath(other)\n+        if not isabs(other):\n+            other = abspath(other)\n+        if self == other:\n+            return True\n+        if not hasattr(os.path, \"samefile\"):\n+            return False\n+        return error.checked_call(os.path.samefile, self.strpath, other)\n+\n+    def remove(self, rec=1, ignore_errors=False):\n+        \"\"\"Remove a file or directory (or a directory tree if rec=1).\n+        if ignore_errors is True, errors while removing directories will\n+        be ignored.\n+        \"\"\"\n+        if self.check(dir=1, link=0):\n+            if rec:\n+                # force remove of readonly files on windows\n+                if iswin32:\n+                    self.chmod(0o700, rec=1)\n+                import shutil\n+\n+                error.checked_call(\n+                    shutil.rmtree, self.strpath, ignore_errors=ignore_errors\n+                )\n+            else:\n+                error.checked_call(os.rmdir, self.strpath)\n+        else:\n+            if iswin32:\n+                self.chmod(0o700)\n+            error.checked_call(os.remove, self.strpath)\n+\n+    def computehash(self, hashtype=\"md5\", chunksize=524288):\n+        \"\"\"Return hexdigest of hashvalue for this file.\"\"\"\n+        try:\n+            try:\n+                import hashlib as mod\n+            except ImportError:\n+                if hashtype == \"sha1\":\n+                    hashtype = \"sha\"\n+                mod = __import__(hashtype)\n+            hash = getattr(mod, hashtype)()\n+        except (AttributeError, ImportError):\n+            raise ValueError(f\"Don't know how to compute {hashtype!r} hash\")\n+        f = self.open(\"rb\")\n+        try:\n+            while 1:\n+                buf = f.read(chunksize)\n+                if not buf:\n+                    return hash.hexdigest()\n+                hash.update(buf)\n+        finally:\n+            f.close()\n+\n+    def new(self, **kw):\n+        \"\"\"Create a modified version of this path.\n+        the following keyword arguments modify various path parts::\n+\n+          a:/some/path/to/a/file.ext\n+          xx                           drive\n+          xxxxxxxxxxxxxxxxx            dirname\n+                            xxxxxxxx   basename\n+                            xxxx       purebasename\n+                                 xxx   ext\n+        \"\"\"\n+        obj = object.__new__(self.__class__)\n+        if not kw:\n+            obj.strpath = self.strpath\n+            return obj\n+        drive, dirname, basename, purebasename, ext = self._getbyspec(\n+            \"drive,dirname,basename,purebasename,ext\"\n+        )\n+        if \"basename\" in kw:\n+            if \"purebasename\" in kw or \"ext\" in kw:\n+                raise ValueError(f\"invalid specification {kw!r}\")\n+        else:\n+            pb = kw.setdefault(\"purebasename\", purebasename)\n+            try:\n+                ext = kw[\"ext\"]\n+            except KeyError:\n+                pass\n+            else:\n+                if ext and not ext.startswith(\".\"):\n+                    ext = \".\" + ext\n+            kw[\"basename\"] = pb + ext\n+\n+        if \"dirname\" in kw and not kw[\"dirname\"]:\n+            kw[\"dirname\"] = drive\n+        else:\n+            kw.setdefault(\"dirname\", dirname)\n+        kw.setdefault(\"sep\", self.sep)\n+        obj.strpath = normpath(\"{dirname}{sep}{basename}\".format(**kw))\n+        return obj\n+\n+    def _getbyspec(self, spec: str) -> list[str]:\n+        \"\"\"See new for what 'spec' can be.\"\"\"\n+        res = []\n+        parts = self.strpath.split(self.sep)\n+\n+        args = filter(None, spec.split(\",\"))\n+        for name in args:\n+            if name == \"drive\":\n+                res.append(parts[0])\n+            elif name == \"dirname\":\n+                res.append(self.sep.join(parts[:-1]))\n+            else:\n+                basename = parts[-1]\n+                if name == \"basename\":\n+                    res.append(basename)\n+                else:\n+                    i = basename.rfind(\".\")\n+                    if i == -1:\n+                        purebasename, ext = basename, \"\"\n+                    else:\n+                        purebasename, ext = basename[:i], basename[i:]\n+                    if name == \"purebasename\":\n+                        res.append(purebasename)\n+                    elif name == \"ext\":\n+                        res.append(ext)\n+                    else:\n+                        raise ValueError(f\"invalid part specification {name!r}\")\n+        return res\n+\n+    def dirpath(self, *args, **kwargs):\n+        \"\"\"Return the directory path joined with any given path arguments.\"\"\"\n+        if not kwargs:\n+            path = object.__new__(self.__class__)\n+            path.strpath = dirname(self.strpath)\n+            if args:\n+                path = path.join(*args)\n+            return path\n+        return self.new(basename=\"\").join(*args, **kwargs)\n+\n+    def join(self, *args: os.PathLike[str], abs: bool = False) -> LocalPath:\n+        \"\"\"Return a new path by appending all 'args' as path\n+        components.  if abs=1 is used restart from root if any\n+        of the args is an absolute path.\n+        \"\"\"\n+        sep = self.sep\n+        strargs = [os.fspath(arg) for arg in args]\n+        strpath = self.strpath\n+        if abs:\n+            newargs: list[str] = []\n+            for arg in reversed(strargs):\n+                if isabs(arg):\n+                    strpath = arg\n+                    strargs = newargs\n+                    break\n+                newargs.insert(0, arg)\n+        # special case for when we have e.g. strpath == \"/\"\n+        actual_sep = \"\" if strpath.endswith(sep) else sep\n+        for arg in strargs:\n+            arg = arg.strip(sep)\n+            if iswin32:\n+                # allow unix style paths even on windows.\n+                arg = arg.strip(\"/\")\n+                arg = arg.replace(\"/\", sep)\n+            strpath = strpath + actual_sep + arg\n+            actual_sep = sep\n+        obj = object.__new__(self.__class__)\n+        obj.strpath = normpath(strpath)\n+        return obj\n+\n+    def open(self, mode=\"r\", ensure=False, encoding=None):\n+        \"\"\"Return an opened file with the given mode.\n+\n+        If ensure is True, create parent directories if needed.\n+        \"\"\"\n+        if ensure:\n+            self.dirpath().ensure(dir=1)\n+        if encoding:\n+            return error.checked_call(\n+                io.open,\n+                self.strpath,\n+                mode,\n+                encoding=encoding,\n+            )\n+        return error.checked_call(open, self.strpath, mode)\n+\n+    def _fastjoin(self, name):\n+        child = object.__new__(self.__class__)\n+        child.strpath = self.strpath + self.sep + name\n+        return child\n+\n+    def islink(self):\n+        return islink(self.strpath)\n+\n+    def check(self, **kw):\n+        \"\"\"Check a path for existence and properties.\n+\n+        Without arguments, return True if the path exists, otherwise False.\n+\n+        valid checkers::\n+\n+            file = 1  # is a file\n+            file = 0  # is not a file (may not even exist)\n+            dir = 1  # is a dir\n+            link = 1  # is a link\n+            exists = 1  # exists\n+\n+        You can specify multiple checker definitions, for example::\n+\n+            path.check(file=1, link=1)  # a link pointing to a file\n+        \"\"\"\n+        if not kw:\n+            return exists(self.strpath)\n+        if len(kw) == 1:\n+            if \"dir\" in kw:\n+                return not kw[\"dir\"] ^ isdir(self.strpath)\n+            if \"file\" in kw:\n+                return not kw[\"file\"] ^ isfile(self.strpath)\n+        if not kw:\n+            kw = {\"exists\": 1}\n+        return Checkers(self)._evaluate(kw)\n+\n+    _patternchars = set(\"*?[\" + os.sep)\n+\n+    def listdir(self, fil=None, sort=None):\n+        \"\"\"List directory contents, possibly filter by the given fil func\n+        and possibly sorted.\n+        \"\"\"\n+        if fil is None and sort is None:\n+            names = error.checked_call(os.listdir, self.strpath)\n+            return map_as_list(self._fastjoin, names)\n+        if isinstance(fil, str):\n+            if not self._patternchars.intersection(fil):\n+                child = self._fastjoin(fil)\n+                if exists(child.strpath):\n+                    return [child]\n+                return []\n+            fil = FNMatcher(fil)\n+        names = error.checked_call(os.listdir, self.strpath)\n+        res = []\n+        for name in names:\n+            child = self._fastjoin(name)\n+            if fil is None or fil(child):\n+                res.append(child)\n+        self._sortlist(res, sort)\n+        return res\n+\n+    def size(self) -> int:\n+        \"\"\"Return size of the underlying file object\"\"\"\n+        return self.stat().size\n+\n+    def mtime(self) -> float:\n+        \"\"\"Return last modification time of the path.\"\"\"\n+        return self.stat().mtime\n+\n+    def copy(self, target, mode=False, stat=False):\n+        \"\"\"Copy path to target.\n+\n+        If mode is True, will copy permission from path to target.\n+        If stat is True, copy permission, last modification\n+        time, last access time, and flags from path to target.\n+        \"\"\"\n+        if self.check(file=1):\n+            if target.check(dir=1):\n+                target = target.join(self.basename)\n+            assert self != target\n+            copychunked(self, target)\n+            if mode:\n+                copymode(self.strpath, target.strpath)\n+            if stat:\n+                copystat(self, target)\n+        else:\n+\n+            def rec(p):\n+                return p.check(link=0)\n+\n+            for x in self.visit(rec=rec):\n+                relpath = x.relto(self)\n+                newx = target.join(relpath)\n+                newx.dirpath().ensure(dir=1)\n+                if x.check(link=1):\n+                    newx.mksymlinkto(x.readlink())\n+                    continue\n+                elif x.check(file=1):\n+                    copychunked(x, newx)\n+                elif x.check(dir=1):\n+                    newx.ensure(dir=1)\n+                if mode:\n+                    copymode(x.strpath, newx.strpath)\n+                if stat:\n+                    copystat(x, newx)\n+\n+    def rename(self, target):\n+        \"\"\"Rename this path to target.\"\"\"\n+        target = os.fspath(target)\n+        return error.checked_call(os.rename, self.strpath, target)\n+\n+    def dump(self, obj, bin=1):\n+        \"\"\"Pickle object into path location\"\"\"\n+        f = self.open(\"wb\")\n+        import pickle\n+\n+        try:\n+            error.checked_call(pickle.dump, obj, f, bin)\n+        finally:\n+            f.close()\n+\n+    def mkdir(self, *args):\n+        \"\"\"Create & return the directory joined with args.\"\"\"\n+        p = self.join(*args)\n+        error.checked_call(os.mkdir, os.fspath(p))\n+        return p\n+\n+    def write_binary(self, data, ensure=False):\n+        \"\"\"Write binary data into path.   If ensure is True create\n+        missing parent directories.\n+        \"\"\"\n+        if ensure:\n+            self.dirpath().ensure(dir=1)\n+        with self.open(\"wb\") as f:\n+            f.write(data)\n+\n+    def write_text(self, data, encoding, ensure=False):\n+        \"\"\"Write text data into path using the specified encoding.\n+        If ensure is True create missing parent directories.\n+        \"\"\"\n+        if ensure:\n+            self.dirpath().ensure(dir=1)\n+        with self.open(\"w\", encoding=encoding) as f:\n+            f.write(data)\n+\n+    def write(self, data, mode=\"w\", ensure=False):\n+        \"\"\"Write data into path.   If ensure is True create\n+        missing parent directories.\n+        \"\"\"\n+        if ensure:\n+            self.dirpath().ensure(dir=1)\n+        if \"b\" in mode:\n+            if not isinstance(data, bytes):\n+                raise ValueError(\"can only process bytes\")\n+        else:\n+            if not isinstance(data, str):\n+                if not isinstance(data, bytes):\n+                    data = str(data)\n+                else:\n+                    data = data.decode(sys.getdefaultencoding())\n+        f = self.open(mode)\n+        try:\n+            f.write(data)\n+        finally:\n+            f.close()\n+\n+    def _ensuredirs(self):\n+        parent = self.dirpath()\n+        if parent == self:\n+            return self\n+        if parent.check(dir=0):\n+            parent._ensuredirs()\n+        if self.check(dir=0):\n+            try:\n+                self.mkdir()\n+            except error.EEXIST:\n+                # race condition: file/dir created by another thread/process.\n+                # complain if it is not a dir\n+                if self.check(dir=0):\n+                    raise\n+        return self\n+\n+    def ensure(self, *args, **kwargs):\n+        \"\"\"Ensure that an args-joined path exists (by default as\n+        a file). if you specify a keyword argument 'dir=True'\n+        then the path is forced to be a directory path.\n+        \"\"\"\n+        p = self.join(*args)\n+        if kwargs.get(\"dir\", 0):\n+            return p._ensuredirs()\n+        else:\n+            p.dirpath()._ensuredirs()\n+            if not p.check(file=1):\n+                p.open(\"wb\").close()\n+            return p\n+\n+    @overload\n+    def stat(self, raising: Literal[True] = ...) -> Stat: ...\n+\n+    @overload\n+    def stat(self, raising: Literal[False]) -> Stat | None: ...\n+\n+    def stat(self, raising: bool = True) -> Stat | None:\n+        \"\"\"Return an os.stat() tuple.\"\"\"\n+        if raising:\n+            return Stat(self, error.checked_call(os.stat, self.strpath))\n+        try:\n+            return Stat(self, os.stat(self.strpath))\n+        except KeyboardInterrupt:\n+            raise\n+        except Exception:\n+            return None\n+\n+    def lstat(self) -> Stat:\n+        \"\"\"Return an os.lstat() tuple.\"\"\"\n+        return Stat(self, error.checked_call(os.lstat, self.strpath))\n+\n+    def setmtime(self, mtime=None):\n+        \"\"\"Set modification time for the given path.  if 'mtime' is None\n+        (the default) then the file's mtime is set to current time.\n+\n+        Note that the resolution for 'mtime' is platform dependent.\n+        \"\"\"\n+        if mtime is None:\n+            return error.checked_call(os.utime, self.strpath, mtime)\n+        try:\n+            return error.checked_call(os.utime, self.strpath, (-1, mtime))\n+        except error.EINVAL:\n+            return error.checked_call(os.utime, self.strpath, (self.atime(), mtime))\n+\n+    def chdir(self):\n+        \"\"\"Change directory to self and return old current directory\"\"\"\n+        try:\n+            old = self.__class__()\n+        except error.ENOENT:\n+            old = None\n+        error.checked_call(os.chdir, self.strpath)\n+        return old\n+\n+    @contextmanager\n+    def as_cwd(self):\n+        \"\"\"\n+        Return a context manager, which changes to the path's dir during the\n+        managed \"with\" context.\n+        On __enter__ it returns the old dir, which might be ``None``.\n+        \"\"\"\n+        old = self.chdir()\n+        try:\n+            yield old\n+        finally:\n+            if old is not None:\n+                old.chdir()\n+\n+    def realpath(self):\n+        \"\"\"Return a new path which contains no symbolic links.\"\"\"\n+        return self.__class__(os.path.realpath(self.strpath))\n+\n+    def atime(self):\n+        \"\"\"Return last access time of the path.\"\"\"\n+        return self.stat().atime\n+\n+    def __repr__(self):\n+        return f\"local({self.strpath!r})\"\n+\n+    def __str__(self):\n+        \"\"\"Return string representation of the Path.\"\"\"\n+        return self.strpath\n+\n+    def chmod(self, mode, rec=0):\n+        \"\"\"Change permissions to the given mode. If mode is an\n+        integer it directly encodes the os-specific modes.\n+        if rec is True perform recursively.\n+        \"\"\"\n+        if not isinstance(mode, int):\n+            raise TypeError(f\"mode {mode!r} must be an integer\")\n+        if rec:\n+            for x in self.visit(rec=rec):\n+                error.checked_call(os.chmod, str(x), mode)\n+        error.checked_call(os.chmod, self.strpath, mode)\n+\n+    def pypkgpath(self):\n+        \"\"\"Return the Python package path by looking for the last\n+        directory upwards which still contains an __init__.py.\n+        Return None if a pkgpath cannot be determined.\n+        \"\"\"\n+        pkgpath = None\n+        for parent in self.parts(reverse=True):\n+            if parent.isdir():\n+                if not parent.join(\"__init__.py\").exists():\n+                    break\n+                if not isimportable(parent.basename):\n+                    break\n+                pkgpath = parent\n+        return pkgpath\n+\n+    def _ensuresyspath(self, ensuremode, path):\n+        if ensuremode:\n+            s = str(path)\n+            if ensuremode == \"append\":\n+                if s not in sys.path:\n+                    sys.path.append(s)\n+            else:\n+                if s != sys.path[0]:\n+                    sys.path.insert(0, s)\n+\n+    def pyimport(self, modname=None, ensuresyspath=True):\n+        \"\"\"Return path as an imported python module.\n+\n+        If modname is None, look for the containing package\n+        and construct an according module name.\n+        The module will be put/looked up in sys.modules.\n+        if ensuresyspath is True then the root dir for importing\n+        the file (taking __init__.py files into account) will\n+        be prepended to sys.path if it isn't there already.\n+        If ensuresyspath==\"append\" the root dir will be appended\n+        if it isn't already contained in sys.path.\n+        if ensuresyspath is False no modification of syspath happens.\n+\n+        Special value of ensuresyspath==\"importlib\" is intended\n+        purely for using in pytest, it is capable only of importing\n+        separate .py files outside packages, e.g. for test suite\n+        without any __init__.py file. It effectively allows having\n+        same-named test modules in different places and offers\n+        mild opt-in via this option. Note that it works only in\n+        recent versions of python.\n+        \"\"\"\n+        if not self.check():\n+            raise error.ENOENT(self)\n+\n+        if ensuresyspath == \"importlib\":\n+            if modname is None:\n+                modname = self.purebasename\n+            spec = importlib.util.spec_from_file_location(modname, str(self))\n+            if spec is None or spec.loader is None:\n+                raise ImportError(f\"Can't find module {modname} at location {self!s}\")\n+            mod = importlib.util.module_from_spec(spec)\n+            spec.loader.exec_module(mod)\n+            return mod\n+\n+        pkgpath = None\n+        if modname is None:\n+            pkgpath = self.pypkgpath()\n+            if pkgpath is not None:\n+                pkgroot = pkgpath.dirpath()\n+                names = self.new(ext=\"\").relto(pkgroot).split(self.sep)\n+                if names[-1] == \"__init__\":\n+                    names.pop()\n+                modname = \".\".join(names)\n+            else:\n+                pkgroot = self.dirpath()\n+                modname = self.purebasename\n+\n+            self._ensuresyspath(ensuresyspath, pkgroot)\n+            __import__(modname)\n+            mod = sys.modules[modname]\n+            if self.basename == \"__init__.py\":\n+                return mod  # we don't check anything as we might\n+                # be in a namespace package ... too icky to check\n+            modfile = mod.__file__\n+            assert modfile is not None\n+            if modfile[-4:] in (\".pyc\", \".pyo\"):\n+                modfile = modfile[:-1]\n+            elif modfile.endswith(\"$py.class\"):\n+                modfile = modfile[:-9] + \".py\"\n+            if modfile.endswith(os.sep + \"__init__.py\"):\n+                if self.basename != \"__init__.py\":\n+                    modfile = modfile[:-12]\n+            try:\n+                issame = self.samefile(modfile)\n+            except error.ENOENT:\n+                issame = False\n+            if not issame:\n+                ignore = os.getenv(\"PY_IGNORE_IMPORTMISMATCH\")\n+                if ignore != \"1\":\n+                    raise self.ImportMismatchError(modname, modfile, self)\n+            return mod\n+        else:\n+            try:\n+                return sys.modules[modname]\n+            except KeyError:\n+                # we have a custom modname, do a pseudo-import\n+                import types\n+\n+                mod = types.ModuleType(modname)\n+                mod.__file__ = str(self)\n+                sys.modules[modname] = mod\n+                try:\n+                    with open(str(self), \"rb\") as f:\n+                        exec(f.read(), mod.__dict__)\n+                except BaseException:\n+                    del sys.modules[modname]\n+                    raise\n+                return mod\n+\n+    def sysexec(self, *argv: os.PathLike[str], **popen_opts: Any) -> str:\n+        \"\"\"Return stdout text from executing a system child process,\n+        where the 'self' path points to executable.\n+        The process is directly invoked and not through a system shell.\n+        \"\"\"\n+        from subprocess import PIPE\n+        from subprocess import Popen\n+\n+        popen_opts.pop(\"stdout\", None)\n+        popen_opts.pop(\"stderr\", None)\n+        proc = Popen(\n+            [str(self)] + [str(arg) for arg in argv],\n+            **popen_opts,\n+            stdout=PIPE,\n+            stderr=PIPE,\n+        )\n+        stdout: str | bytes\n+        stdout, stderr = proc.communicate()\n+        ret = proc.wait()\n+        if isinstance(stdout, bytes):\n+            stdout = stdout.decode(sys.getdefaultencoding())\n+        if ret != 0:\n+            if isinstance(stderr, bytes):\n+                stderr = stderr.decode(sys.getdefaultencoding())\n+            raise RuntimeError(\n+                ret,\n+                ret,\n+                str(self),\n+                stdout,\n+                stderr,\n+            )\n+        return stdout\n+\n+    @classmethod\n+    def sysfind(cls, name, checker=None, paths=None):\n+        \"\"\"Return a path object found by looking at the systems\n+        underlying PATH specification. If the checker is not None\n+        it will be invoked to filter matching paths.  If a binary\n+        cannot be found, None is returned\n+        Note: This is probably not working on plain win32 systems\n+        but may work on cygwin.\n+        \"\"\"\n+        if isabs(name):\n+            p = local(name)\n+            if p.check(file=1):\n+                return p\n+        else:\n+            if paths is None:\n+                if iswin32:\n+                    paths = os.environ[\"Path\"].split(\";\")\n+                    if \"\" not in paths and \".\" not in paths:\n+                        paths.append(\".\")\n+                    try:\n+                        systemroot = os.environ[\"SYSTEMROOT\"]\n+                    except KeyError:\n+                        pass\n+                    else:\n+                        paths = [\n+                            path.replace(\"%SystemRoot%\", systemroot) for path in paths\n+                        ]\n+                else:\n+                    paths = os.environ[\"PATH\"].split(\":\")\n+            tryadd = []\n+            if iswin32:\n+                tryadd += os.environ[\"PATHEXT\"].split(os.pathsep)\n+            tryadd.append(\"\")\n+\n+            for x in paths:\n+                for addext in tryadd:\n+                    p = local(x).join(name, abs=True) + addext\n+                    try:\n+                        if p.check(file=1):\n+                            if checker:\n+                                if not checker(p):\n+                                    continue\n+                            return p\n+                    except error.EACCES:\n+                        pass\n+        return None\n+\n+    @classmethod\n+    def _gethomedir(cls):\n+        try:\n+            x = os.environ[\"HOME\"]\n+        except KeyError:\n+            try:\n+                x = os.environ[\"HOMEDRIVE\"] + os.environ[\"HOMEPATH\"]\n+            except KeyError:\n+                return None\n+        return cls(x)\n+\n+    # \"\"\"\n+    # special class constructors for local filesystem paths\n+    # \"\"\"\n+    @classmethod\n+    def get_temproot(cls):\n+        \"\"\"Return the system's temporary directory\n+        (where tempfiles are usually created in)\n+        \"\"\"\n+        import tempfile\n+\n+        return local(tempfile.gettempdir())\n+\n+    @classmethod\n+    def mkdtemp(cls, rootdir=None):\n+        \"\"\"Return a Path object pointing to a fresh new temporary directory\n+        (which we created ourselves).\n+        \"\"\"\n+        import tempfile\n+\n+        if rootdir is None:\n+            rootdir = cls.get_temproot()\n+        path = error.checked_call(tempfile.mkdtemp, dir=str(rootdir))\n+        return cls(path)\n+\n+    @classmethod\n+    def make_numbered_dir(\n+        cls, prefix=\"session-\", rootdir=None, keep=3, lock_timeout=172800\n+    ):  # two days\n+        \"\"\"Return unique directory with a number greater than the current\n+        maximum one.  The number is assumed to start directly after prefix.\n+        if keep is true directories with a number less than (maxnum-keep)\n+        will be removed. If .lock files are used (lock_timeout non-zero),\n+        algorithm is multi-process safe.\n+        \"\"\"\n+        if rootdir is None:\n+            rootdir = cls.get_temproot()\n+\n+        nprefix = prefix.lower()\n+\n+        def parse_num(path):\n+            \"\"\"Parse the number out of a path (if it matches the prefix)\"\"\"\n+            nbasename = path.basename.lower()\n+            if nbasename.startswith(nprefix):\n+                try:\n+                    return int(nbasename[len(nprefix) :])\n+                except ValueError:\n+                    pass\n+\n+        def create_lockfile(path):\n+            \"\"\"Exclusively create lockfile. Throws when failed\"\"\"\n+            mypid = os.getpid()\n+            lockfile = path.join(\".lock\")\n+            if hasattr(lockfile, \"mksymlinkto\"):\n+                lockfile.mksymlinkto(str(mypid))\n+            else:\n+                fd = error.checked_call(\n+                    os.open, str(lockfile), os.O_WRONLY | os.O_CREAT | os.O_EXCL, 0o644\n+                )\n+                with os.fdopen(fd, \"w\") as f:\n+                    f.write(str(mypid))\n+            return lockfile\n+\n+        def atexit_remove_lockfile(lockfile):\n+            \"\"\"Ensure lockfile is removed at process exit\"\"\"\n+            mypid = os.getpid()\n+\n+            def try_remove_lockfile():\n+                # in a fork() situation, only the last process should\n+                # remove the .lock, otherwise the other processes run the\n+                # risk of seeing their temporary dir disappear.  For now\n+                # we remove the .lock in the parent only (i.e. we assume\n+                # that the children finish before the parent).\n+                if os.getpid() != mypid:\n+                    return\n+                try:\n+                    lockfile.remove()\n+                except error.Error:\n+                    pass\n+\n+            atexit.register(try_remove_lockfile)\n+\n+        # compute the maximum number currently in use with the prefix\n+        lastmax = None\n+        while True:\n+            maxnum = -1\n+            for path in rootdir.listdir():\n+                num = parse_num(path)\n+                if num is not None:\n+                    maxnum = max(maxnum, num)\n+\n+            # make the new directory\n+            try:\n+                udir = rootdir.mkdir(prefix + str(maxnum + 1))\n+                if lock_timeout:\n+                    lockfile = create_lockfile(udir)\n+                    atexit_remove_lockfile(lockfile)\n+            except (error.EEXIST, error.ENOENT, error.EBUSY):\n+                # race condition (1): another thread/process created the dir\n+                #                     in the meantime - try again\n+                # race condition (2): another thread/process spuriously acquired\n+                #                     lock treating empty directory as candidate\n+                #                     for removal - try again\n+                # race condition (3): another thread/process tried to create the lock at\n+                #                     the same time (happened in Python 3.3 on Windows)\n+                # https://ci.appveyor.com/project/pytestbot/py/build/1.0.21/job/ffi85j4c0lqwsfwa\n+                if lastmax == maxnum:\n+                    raise\n+                lastmax = maxnum\n+                continue\n+            break\n+\n+        def get_mtime(path):\n+            \"\"\"Read file modification time\"\"\"\n+            try:\n+                return path.lstat().mtime\n+            except error.Error:\n+                pass\n+\n+        garbage_prefix = prefix + \"garbage-\"\n+\n+        def is_garbage(path):\n+            \"\"\"Check if path denotes directory scheduled for removal\"\"\"\n+            bn = path.basename\n+            return bn.startswith(garbage_prefix)\n+\n+        # prune old directories\n+        udir_time = get_mtime(udir)\n+        if keep and udir_time:\n+            for path in rootdir.listdir():\n+                num = parse_num(path)\n+                if num is not None and num <= (maxnum - keep):\n+                    try:\n+                        # try acquiring lock to remove directory as exclusive user\n+                        if lock_timeout:\n+                            create_lockfile(path)\n+                    except (error.EEXIST, error.ENOENT, error.EBUSY):\n+                        path_time = get_mtime(path)\n+                        if not path_time:\n+                            # assume directory doesn't exist now\n+                            continue\n+                        if abs(udir_time - path_time) < lock_timeout:\n+                            # assume directory with lockfile exists\n+                            # and lock timeout hasn't expired yet\n+                            continue\n+\n+                    # path dir locked for exclusive use\n+                    # and scheduled for removal to avoid another thread/process\n+                    # treating it as a new directory or removal candidate\n+                    garbage_path = rootdir.join(garbage_prefix + str(uuid.uuid4()))\n+                    try:\n+                        path.rename(garbage_path)\n+                        garbage_path.remove(rec=1)\n+                    except KeyboardInterrupt:\n+                        raise\n+                    except Exception:  # this might be error.Error, WindowsError ...\n+                        pass\n+                if is_garbage(path):\n+                    try:\n+                        path.remove(rec=1)\n+                    except KeyboardInterrupt:\n+                        raise\n+                    except Exception:  # this might be error.Error, WindowsError ...\n+                        pass\n+\n+        # make link...\n+        try:\n+            username = os.environ[\"USER\"]  # linux, et al\n+        except KeyError:\n+            try:\n+                username = os.environ[\"USERNAME\"]  # windows\n+            except KeyError:\n+                username = \"current\"\n+\n+        src = str(udir)\n+        dest = src[: src.rfind(\"-\")] + \"-\" + username\n+        try:\n+            os.unlink(dest)\n+        except OSError:\n+            pass\n+        try:\n+            os.symlink(src, dest)\n+        except (OSError, AttributeError, NotImplementedError):\n+            pass\n+\n+        return udir\n+\n+\n+def copymode(src, dest):\n+    \"\"\"Copy permission from src to dst.\"\"\"\n+    import shutil\n+\n+    shutil.copymode(src, dest)\n+\n+\n+def copystat(src, dest):\n+    \"\"\"Copy permission,  last modification time,\n+    last access time, and flags from src to dst.\"\"\"\n+    import shutil\n+\n+    shutil.copystat(str(src), str(dest))\n+\n+\n+def copychunked(src, dest):\n+    chunksize = 524288  # half a meg of bytes\n+    fsrc = src.open(\"rb\")\n+    try:\n+        fdest = dest.open(\"wb\")\n+        try:\n+            while 1:\n+                buf = fsrc.read(chunksize)\n+                if not buf:\n+                    break\n+                fdest.write(buf)\n+        finally:\n+            fdest.close()\n+    finally:\n+        fsrc.close()\n+\n+\n+def isimportable(name):\n+    if name and (name[0].isalpha() or name[0] == \"_\"):\n+        name = name.replace(\"_\", \"\")\n+        return not name or name.isalnum()\n+\n+\n+local = LocalPath",
      "patch_lines": [
        "@@ -0,0 +1,1475 @@\n",
        "+# mypy: allow-untyped-defs\n",
        "+\"\"\"local path implementation.\"\"\"\n",
        "+\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+import atexit\n",
        "+from collections.abc import Callable\n",
        "+from contextlib import contextmanager\n",
        "+import fnmatch\n",
        "+import importlib.util\n",
        "+import io\n",
        "+import os\n",
        "+from os.path import abspath\n",
        "+from os.path import dirname\n",
        "+from os.path import exists\n",
        "+from os.path import isabs\n",
        "+from os.path import isdir\n",
        "+from os.path import isfile\n",
        "+from os.path import islink\n",
        "+from os.path import normpath\n",
        "+import posixpath\n",
        "+from stat import S_ISDIR\n",
        "+from stat import S_ISLNK\n",
        "+from stat import S_ISREG\n",
        "+import sys\n",
        "+from typing import Any\n",
        "+from typing import cast\n",
        "+from typing import Literal\n",
        "+from typing import overload\n",
        "+from typing import TYPE_CHECKING\n",
        "+import uuid\n",
        "+import warnings\n",
        "+\n",
        "+from . import error\n",
        "+\n",
        "+\n",
        "+# Moved from local.py.\n",
        "+iswin32 = sys.platform == \"win32\" or (getattr(os, \"_name\", False) == \"nt\")\n",
        "+\n",
        "+\n",
        "+class Checkers:\n",
        "+    _depend_on_existence = \"exists\", \"link\", \"dir\", \"file\"\n",
        "+\n",
        "+    def __init__(self, path):\n",
        "+        self.path = path\n",
        "+\n",
        "+    def dotfile(self):\n",
        "+        return self.path.basename.startswith(\".\")\n",
        "+\n",
        "+    def ext(self, arg):\n",
        "+        if not arg.startswith(\".\"):\n",
        "+            arg = \".\" + arg\n",
        "+        return self.path.ext == arg\n",
        "+\n",
        "+    def basename(self, arg):\n",
        "+        return self.path.basename == arg\n",
        "+\n",
        "+    def basestarts(self, arg):\n",
        "+        return self.path.basename.startswith(arg)\n",
        "+\n",
        "+    def relto(self, arg):\n",
        "+        return self.path.relto(arg)\n",
        "+\n",
        "+    def fnmatch(self, arg):\n",
        "+        return self.path.fnmatch(arg)\n",
        "+\n",
        "+    def endswith(self, arg):\n",
        "+        return str(self.path).endswith(arg)\n",
        "+\n",
        "+    def _evaluate(self, kw):\n",
        "+        from .._code.source import getrawcode\n",
        "+\n",
        "+        for name, value in kw.items():\n",
        "+            invert = False\n",
        "+            meth = None\n",
        "+            try:\n",
        "+                meth = getattr(self, name)\n",
        "+            except AttributeError:\n",
        "+                if name[:3] == \"not\":\n",
        "+                    invert = True\n",
        "+                    try:\n",
        "+                        meth = getattr(self, name[3:])\n",
        "+                    except AttributeError:\n",
        "+                        pass\n",
        "+            if meth is None:\n",
        "+                raise TypeError(f\"no {name!r} checker available for {self.path!r}\")\n",
        "+            try:\n",
        "+                if getrawcode(meth).co_argcount > 1:\n",
        "+                    if (not meth(value)) ^ invert:\n",
        "+                        return False\n",
        "+                else:\n",
        "+                    if bool(value) ^ bool(meth()) ^ invert:\n",
        "+                        return False\n",
        "+            except (error.ENOENT, error.ENOTDIR, error.EBUSY):\n",
        "+                # EBUSY feels not entirely correct,\n",
        "+                # but its kind of necessary since ENOMEDIUM\n",
        "+                # is not accessible in python\n",
        "+                for name in self._depend_on_existence:\n",
        "+                    if name in kw:\n",
        "+                        if kw.get(name):\n",
        "+                            return False\n",
        "+                    name = \"not\" + name\n",
        "+                    if name in kw:\n",
        "+                        if not kw.get(name):\n",
        "+                            return False\n",
        "+        return True\n",
        "+\n",
        "+    _statcache: Stat\n",
        "+\n",
        "+    def _stat(self) -> Stat:\n",
        "+        try:\n",
        "+            return self._statcache\n",
        "+        except AttributeError:\n",
        "+            try:\n",
        "+                self._statcache = self.path.stat()\n",
        "+            except error.ELOOP:\n",
        "+                self._statcache = self.path.lstat()\n",
        "+            return self._statcache\n",
        "+\n",
        "+    def dir(self):\n",
        "+        return S_ISDIR(self._stat().mode)\n",
        "+\n",
        "+    def file(self):\n",
        "+        return S_ISREG(self._stat().mode)\n",
        "+\n",
        "+    def exists(self):\n",
        "+        return self._stat()\n",
        "+\n",
        "+    def link(self):\n",
        "+        st = self.path.lstat()\n",
        "+        return S_ISLNK(st.mode)\n",
        "+\n",
        "+\n",
        "+class NeverRaised(Exception):\n",
        "+    pass\n",
        "+\n",
        "+\n",
        "+class Visitor:\n",
        "+    def __init__(self, fil, rec, ignore, bf, sort):\n",
        "+        if isinstance(fil, str):\n",
        "+            fil = FNMatcher(fil)\n",
        "+        if isinstance(rec, str):\n",
        "+            self.rec: Callable[[LocalPath], bool] = FNMatcher(rec)\n",
        "+        elif not hasattr(rec, \"__call__\") and rec:\n",
        "+            self.rec = lambda path: True\n",
        "+        else:\n",
        "+            self.rec = rec\n",
        "+        self.fil = fil\n",
        "+        self.ignore = ignore\n",
        "+        self.breadthfirst = bf\n",
        "+        self.optsort = cast(Callable[[Any], Any], sorted) if sort else (lambda x: x)\n",
        "+\n",
        "+    def gen(self, path):\n",
        "+        try:\n",
        "+            entries = path.listdir()\n",
        "+        except self.ignore:\n",
        "+            return\n",
        "+        rec = self.rec\n",
        "+        dirs = self.optsort(\n",
        "+            [p for p in entries if p.check(dir=1) and (rec is None or rec(p))]\n",
        "+        )\n",
        "+        if not self.breadthfirst:\n",
        "+            for subdir in dirs:\n",
        "+                yield from self.gen(subdir)\n",
        "+        for p in self.optsort(entries):\n",
        "+            if self.fil is None or self.fil(p):\n",
        "+                yield p\n",
        "+        if self.breadthfirst:\n",
        "+            for subdir in dirs:\n",
        "+                yield from self.gen(subdir)\n",
        "+\n",
        "+\n",
        "+class FNMatcher:\n",
        "+    def __init__(self, pattern):\n",
        "+        self.pattern = pattern\n",
        "+\n",
        "+    def __call__(self, path):\n",
        "+        pattern = self.pattern\n",
        "+\n",
        "+        if (\n",
        "+            pattern.find(path.sep) == -1\n",
        "+            and iswin32\n",
        "+            and pattern.find(posixpath.sep) != -1\n",
        "+        ):\n",
        "+            # Running on Windows, the pattern has no Windows path separators,\n",
        "+            # and the pattern has one or more Posix path separators. Replace\n",
        "+            # the Posix path separators with the Windows path separator.\n",
        "+            pattern = pattern.replace(posixpath.sep, path.sep)\n",
        "+\n",
        "+        if pattern.find(path.sep) == -1:\n",
        "+            name = path.basename\n",
        "+        else:\n",
        "+            name = str(path)  # path.strpath # XXX svn?\n",
        "+            if not os.path.isabs(pattern):\n",
        "+                pattern = \"*\" + path.sep + pattern\n",
        "+        return fnmatch.fnmatch(name, pattern)\n",
        "+\n",
        "+\n",
        "+def map_as_list(func, iter):\n",
        "+    return list(map(func, iter))\n",
        "+\n",
        "+\n",
        "+class Stat:\n",
        "+    if TYPE_CHECKING:\n",
        "+\n",
        "+        @property\n",
        "+        def size(self) -> int: ...\n",
        "+\n",
        "+        @property\n",
        "+        def mtime(self) -> float: ...\n",
        "+\n",
        "+    def __getattr__(self, name: str) -> Any:\n",
        "+        return getattr(self._osstatresult, \"st_\" + name)\n",
        "+\n",
        "+    def __init__(self, path, osstatresult):\n",
        "+        self.path = path\n",
        "+        self._osstatresult = osstatresult\n",
        "+\n",
        "+    @property\n",
        "+    def owner(self):\n",
        "+        if iswin32:\n",
        "+            raise NotImplementedError(\"XXX win32\")\n",
        "+        import pwd\n",
        "+\n",
        "+        entry = error.checked_call(pwd.getpwuid, self.uid)  # type:ignore[attr-defined,unused-ignore]\n",
        "+        return entry[0]\n",
        "+\n",
        "+    @property\n",
        "+    def group(self):\n",
        "+        \"\"\"Return group name of file.\"\"\"\n",
        "+        if iswin32:\n",
        "+            raise NotImplementedError(\"XXX win32\")\n",
        "+        import grp\n",
        "+\n",
        "+        entry = error.checked_call(grp.getgrgid, self.gid)  # type:ignore[attr-defined,unused-ignore]\n",
        "+        return entry[0]\n",
        "+\n",
        "+    def isdir(self):\n",
        "+        return S_ISDIR(self._osstatresult.st_mode)\n",
        "+\n",
        "+    def isfile(self):\n",
        "+        return S_ISREG(self._osstatresult.st_mode)\n",
        "+\n",
        "+    def islink(self):\n",
        "+        self.path.lstat()\n",
        "+        return S_ISLNK(self._osstatresult.st_mode)\n",
        "+\n",
        "+\n",
        "+def getuserid(user):\n",
        "+    import pwd\n",
        "+\n",
        "+    if not isinstance(user, int):\n",
        "+        user = pwd.getpwnam(user)[2]  # type:ignore[attr-defined,unused-ignore]\n",
        "+    return user\n",
        "+\n",
        "+\n",
        "+def getgroupid(group):\n",
        "+    import grp\n",
        "+\n",
        "+    if not isinstance(group, int):\n",
        "+        group = grp.getgrnam(group)[2]  # type:ignore[attr-defined,unused-ignore]\n",
        "+    return group\n",
        "+\n",
        "+\n",
        "+class LocalPath:\n",
        "+    \"\"\"Object oriented interface to os.path and other local filesystem\n",
        "+    related information.\n",
        "+    \"\"\"\n",
        "+\n",
        "+    class ImportMismatchError(ImportError):\n",
        "+        \"\"\"raised on pyimport() if there is a mismatch of __file__'s\"\"\"\n",
        "+\n",
        "+    sep = os.sep\n",
        "+\n",
        "+    def __init__(self, path=None, expanduser=False):\n",
        "+        \"\"\"Initialize and return a local Path instance.\n",
        "+\n",
        "+        Path can be relative to the current directory.\n",
        "+        If path is None it defaults to the current working directory.\n",
        "+        If expanduser is True, tilde-expansion is performed.\n",
        "+        Note that Path instances always carry an absolute path.\n",
        "+        Note also that passing in a local path object will simply return\n",
        "+        the exact same path object. Use new() to get a new copy.\n",
        "+        \"\"\"\n",
        "+        if path is None:\n",
        "+            self.strpath = error.checked_call(os.getcwd)\n",
        "+        else:\n",
        "+            try:\n",
        "+                path = os.fspath(path)\n",
        "+            except TypeError:\n",
        "+                raise ValueError(\n",
        "+                    \"can only pass None, Path instances \"\n",
        "+                    \"or non-empty strings to LocalPath\"\n",
        "+                )\n",
        "+            if expanduser:\n",
        "+                path = os.path.expanduser(path)\n",
        "+            self.strpath = abspath(path)\n",
        "+\n",
        "+    if sys.platform != \"win32\":\n",
        "+\n",
        "+        def chown(self, user, group, rec=0):\n",
        "+            \"\"\"Change ownership to the given user and group.\n",
        "+            user and group may be specified by a number or\n",
        "+            by a name.  if rec is True change ownership\n",
        "+            recursively.\n",
        "+            \"\"\"\n",
        "+            uid = getuserid(user)\n",
        "+            gid = getgroupid(group)\n",
        "+            if rec:\n",
        "+                for x in self.visit(rec=lambda x: x.check(link=0)):\n",
        "+                    if x.check(link=0):\n",
        "+                        error.checked_call(os.chown, str(x), uid, gid)\n",
        "+            error.checked_call(os.chown, str(self), uid, gid)\n",
        "+\n",
        "+        def readlink(self) -> str:\n",
        "+            \"\"\"Return value of a symbolic link.\"\"\"\n",
        "+            # https://github.com/python/mypy/issues/12278\n",
        "+            return error.checked_call(os.readlink, self.strpath)  # type: ignore[arg-type,return-value,unused-ignore]\n",
        "+\n",
        "+        def mklinkto(self, oldname):\n",
        "+            \"\"\"Posix style hard link to another name.\"\"\"\n",
        "+            error.checked_call(os.link, str(oldname), str(self))\n",
        "+\n",
        "+        def mksymlinkto(self, value, absolute=1):\n",
        "+            \"\"\"Create a symbolic link with the given value (pointing to another name).\"\"\"\n",
        "+            if absolute:\n",
        "+                error.checked_call(os.symlink, str(value), self.strpath)\n",
        "+            else:\n",
        "+                base = self.common(value)\n",
        "+                # with posix local paths '/' is always a common base\n",
        "+                relsource = self.__class__(value).relto(base)\n",
        "+                reldest = self.relto(base)\n",
        "+                n = reldest.count(self.sep)\n",
        "+                target = self.sep.join((\"..\",) * n + (relsource,))\n",
        "+                error.checked_call(os.symlink, target, self.strpath)\n",
        "+\n",
        "+    def __div__(self, other):\n",
        "+        return self.join(os.fspath(other))\n",
        "+\n",
        "+    __truediv__ = __div__  # py3k\n",
        "+\n",
        "+    @property\n",
        "+    def basename(self):\n",
        "+        \"\"\"Basename part of path.\"\"\"\n",
        "+        return self._getbyspec(\"basename\")[0]\n",
        "+\n",
        "+    @property\n",
        "+    def dirname(self):\n",
        "+        \"\"\"Dirname part of path.\"\"\"\n",
        "+        return self._getbyspec(\"dirname\")[0]\n",
        "+\n",
        "+    @property\n",
        "+    def purebasename(self):\n",
        "+        \"\"\"Pure base name of the path.\"\"\"\n",
        "+        return self._getbyspec(\"purebasename\")[0]\n",
        "+\n",
        "+    @property\n",
        "+    def ext(self):\n",
        "+        \"\"\"Extension of the path (including the '.').\"\"\"\n",
        "+        return self._getbyspec(\"ext\")[0]\n",
        "+\n",
        "+    def read_binary(self):\n",
        "+        \"\"\"Read and return a bytestring from reading the path.\"\"\"\n",
        "+        with self.open(\"rb\") as f:\n",
        "+            return f.read()\n",
        "+\n",
        "+    def read_text(self, encoding):\n",
        "+        \"\"\"Read and return a Unicode string from reading the path.\"\"\"\n",
        "+        with self.open(\"r\", encoding=encoding) as f:\n",
        "+            return f.read()\n",
        "+\n",
        "+    def read(self, mode=\"r\"):\n",
        "+        \"\"\"Read and return a bytestring from reading the path.\"\"\"\n",
        "+        with self.open(mode) as f:\n",
        "+            return f.read()\n",
        "+\n",
        "+    def readlines(self, cr=1):\n",
        "+        \"\"\"Read and return a list of lines from the path. if cr is False, the\n",
        "+        newline will be removed from the end of each line.\"\"\"\n",
        "+        mode = \"r\"\n",
        "+\n",
        "+        if not cr:\n",
        "+            content = self.read(mode)\n",
        "+            return content.split(\"\\n\")\n",
        "+        else:\n",
        "+            f = self.open(mode)\n",
        "+            try:\n",
        "+                return f.readlines()\n",
        "+            finally:\n",
        "+                f.close()\n",
        "+\n",
        "+    def load(self):\n",
        "+        \"\"\"(deprecated) return object unpickled from self.read()\"\"\"\n",
        "+        f = self.open(\"rb\")\n",
        "+        try:\n",
        "+            import pickle\n",
        "+\n",
        "+            return error.checked_call(pickle.load, f)\n",
        "+        finally:\n",
        "+            f.close()\n",
        "+\n",
        "+    def move(self, target):\n",
        "+        \"\"\"Move this path to target.\"\"\"\n",
        "+        if target.relto(self):\n",
        "+            raise error.EINVAL(target, \"cannot move path into a subdirectory of itself\")\n",
        "+        try:\n",
        "+            self.rename(target)\n",
        "+        except error.EXDEV:  # invalid cross-device link\n",
        "+            self.copy(target)\n",
        "+            self.remove()\n",
        "+\n",
        "+    def fnmatch(self, pattern):\n",
        "+        \"\"\"Return true if the basename/fullname matches the glob-'pattern'.\n",
        "+\n",
        "+        valid pattern characters::\n",
        "+\n",
        "+            *       matches everything\n",
        "+            ?       matches any single character\n",
        "+            [seq]   matches any character in seq\n",
        "+            [!seq]  matches any char not in seq\n",
        "+\n",
        "+        If the pattern contains a path-separator then the full path\n",
        "+        is used for pattern matching and a '*' is prepended to the\n",
        "+        pattern.\n",
        "+\n",
        "+        if the pattern doesn't contain a path-separator the pattern\n",
        "+        is only matched against the basename.\n",
        "+        \"\"\"\n",
        "+        return FNMatcher(pattern)(self)\n",
        "+\n",
        "+    def relto(self, relpath):\n",
        "+        \"\"\"Return a string which is the relative part of the path\n",
        "+        to the given 'relpath'.\n",
        "+        \"\"\"\n",
        "+        if not isinstance(relpath, (str, LocalPath)):\n",
        "+            raise TypeError(f\"{relpath!r}: not a string or path object\")\n",
        "+        strrelpath = str(relpath)\n",
        "+        if strrelpath and strrelpath[-1] != self.sep:\n",
        "+            strrelpath += self.sep\n",
        "+        # assert strrelpath[-1] == self.sep\n",
        "+        # assert strrelpath[-2] != self.sep\n",
        "+        strself = self.strpath\n",
        "+        if sys.platform == \"win32\" or getattr(os, \"_name\", None) == \"nt\":\n",
        "+            if os.path.normcase(strself).startswith(os.path.normcase(strrelpath)):\n",
        "+                return strself[len(strrelpath) :]\n",
        "+        elif strself.startswith(strrelpath):\n",
        "+            return strself[len(strrelpath) :]\n",
        "+        return \"\"\n",
        "+\n",
        "+    def ensure_dir(self, *args):\n",
        "+        \"\"\"Ensure the path joined with args is a directory.\"\"\"\n",
        "+        return self.ensure(*args, dir=True)\n",
        "+\n",
        "+    def bestrelpath(self, dest):\n",
        "+        \"\"\"Return a string which is a relative path from self\n",
        "+        (assumed to be a directory) to dest such that\n",
        "+        self.join(bestrelpath) == dest and if not such\n",
        "+        path can be determined return dest.\n",
        "+        \"\"\"\n",
        "+        try:\n",
        "+            if self == dest:\n",
        "+                return os.curdir\n",
        "+            base = self.common(dest)\n",
        "+            if not base:  # can be the case on windows\n",
        "+                return str(dest)\n",
        "+            self2base = self.relto(base)\n",
        "+            reldest = dest.relto(base)\n",
        "+            if self2base:\n",
        "+                n = self2base.count(self.sep) + 1\n",
        "+            else:\n",
        "+                n = 0\n",
        "+            lst = [os.pardir] * n\n",
        "+            if reldest:\n",
        "+                lst.append(reldest)\n",
        "+            target = dest.sep.join(lst)\n",
        "+            return target\n",
        "+        except AttributeError:\n",
        "+            return str(dest)\n",
        "+\n",
        "+    def exists(self):\n",
        "+        return self.check()\n",
        "+\n",
        "+    def isdir(self):\n",
        "+        return self.check(dir=1)\n",
        "+\n",
        "+    def isfile(self):\n",
        "+        return self.check(file=1)\n",
        "+\n",
        "+    def parts(self, reverse=False):\n",
        "+        \"\"\"Return a root-first list of all ancestor directories\n",
        "+        plus the path itself.\n",
        "+        \"\"\"\n",
        "+        current = self\n",
        "+        lst = [self]\n",
        "+        while 1:\n",
        "+            last = current\n",
        "+            current = current.dirpath()\n",
        "+            if last == current:\n",
        "+                break\n",
        "+            lst.append(current)\n",
        "+        if not reverse:\n",
        "+            lst.reverse()\n",
        "+        return lst\n",
        "+\n",
        "+    def common(self, other):\n",
        "+        \"\"\"Return the common part shared with the other path\n",
        "+        or None if there is no common part.\n",
        "+        \"\"\"\n",
        "+        last = None\n",
        "+        for x, y in zip(self.parts(), other.parts()):\n",
        "+            if x != y:\n",
        "+                return last\n",
        "+            last = x\n",
        "+        return last\n",
        "+\n",
        "+    def __add__(self, other):\n",
        "+        \"\"\"Return new path object with 'other' added to the basename\"\"\"\n",
        "+        return self.new(basename=self.basename + str(other))\n",
        "+\n",
        "+    def visit(self, fil=None, rec=None, ignore=NeverRaised, bf=False, sort=False):\n",
        "+        \"\"\"Yields all paths below the current one\n",
        "+\n",
        "+        fil is a filter (glob pattern or callable), if not matching the\n",
        "+        path will not be yielded, defaulting to None (everything is\n",
        "+        returned)\n",
        "+\n",
        "+        rec is a filter (glob pattern or callable) that controls whether\n",
        "+        a node is descended, defaulting to None\n",
        "+\n",
        "+        ignore is an Exception class that is ignoredwhen calling dirlist()\n",
        "+        on any of the paths (by default, all exceptions are reported)\n",
        "+\n",
        "+        bf if True will cause a breadthfirst search instead of the\n",
        "+        default depthfirst. Default: False\n",
        "+\n",
        "+        sort if True will sort entries within each directory level.\n",
        "+        \"\"\"\n",
        "+        yield from Visitor(fil, rec, ignore, bf, sort).gen(self)\n",
        "+\n",
        "+    def _sortlist(self, res, sort):\n",
        "+        if sort:\n",
        "+            if hasattr(sort, \"__call__\"):\n",
        "+                warnings.warn(\n",
        "+                    DeprecationWarning(\n",
        "+                        \"listdir(sort=callable) is deprecated and breaks on python3\"\n",
        "+                    ),\n",
        "+                    stacklevel=3,\n",
        "+                )\n",
        "+                res.sort(sort)\n",
        "+            else:\n",
        "+                res.sort()\n",
        "+\n",
        "+    def __fspath__(self):\n",
        "+        return self.strpath\n",
        "+\n",
        "+    def __hash__(self):\n",
        "+        s = self.strpath\n",
        "+        if iswin32:\n",
        "+            s = s.lower()\n",
        "+        return hash(s)\n",
        "+\n",
        "+    def __eq__(self, other):\n",
        "+        s1 = os.fspath(self)\n",
        "+        try:\n",
        "+            s2 = os.fspath(other)\n",
        "+        except TypeError:\n",
        "+            return False\n",
        "+        if iswin32:\n",
        "+            s1 = s1.lower()\n",
        "+            try:\n",
        "+                s2 = s2.lower()\n",
        "+            except AttributeError:\n",
        "+                return False\n",
        "+        return s1 == s2\n",
        "+\n",
        "+    def __ne__(self, other):\n",
        "+        return not (self == other)\n",
        "+\n",
        "+    def __lt__(self, other):\n",
        "+        return os.fspath(self) < os.fspath(other)\n",
        "+\n",
        "+    def __gt__(self, other):\n",
        "+        return os.fspath(self) > os.fspath(other)\n",
        "+\n",
        "+    def samefile(self, other):\n",
        "+        \"\"\"Return True if 'other' references the same file as 'self'.\"\"\"\n",
        "+        other = os.fspath(other)\n",
        "+        if not isabs(other):\n",
        "+            other = abspath(other)\n",
        "+        if self == other:\n",
        "+            return True\n",
        "+        if not hasattr(os.path, \"samefile\"):\n",
        "+            return False\n",
        "+        return error.checked_call(os.path.samefile, self.strpath, other)\n",
        "+\n",
        "+    def remove(self, rec=1, ignore_errors=False):\n",
        "+        \"\"\"Remove a file or directory (or a directory tree if rec=1).\n",
        "+        if ignore_errors is True, errors while removing directories will\n",
        "+        be ignored.\n",
        "+        \"\"\"\n",
        "+        if self.check(dir=1, link=0):\n",
        "+            if rec:\n",
        "+                # force remove of readonly files on windows\n",
        "+                if iswin32:\n",
        "+                    self.chmod(0o700, rec=1)\n",
        "+                import shutil\n",
        "+\n",
        "+                error.checked_call(\n",
        "+                    shutil.rmtree, self.strpath, ignore_errors=ignore_errors\n",
        "+                )\n",
        "+            else:\n",
        "+                error.checked_call(os.rmdir, self.strpath)\n",
        "+        else:\n",
        "+            if iswin32:\n",
        "+                self.chmod(0o700)\n",
        "+            error.checked_call(os.remove, self.strpath)\n",
        "+\n",
        "+    def computehash(self, hashtype=\"md5\", chunksize=524288):\n",
        "+        \"\"\"Return hexdigest of hashvalue for this file.\"\"\"\n",
        "+        try:\n",
        "+            try:\n",
        "+                import hashlib as mod\n",
        "+            except ImportError:\n",
        "+                if hashtype == \"sha1\":\n",
        "+                    hashtype = \"sha\"\n",
        "+                mod = __import__(hashtype)\n",
        "+            hash = getattr(mod, hashtype)()\n",
        "+        except (AttributeError, ImportError):\n",
        "+            raise ValueError(f\"Don't know how to compute {hashtype!r} hash\")\n",
        "+        f = self.open(\"rb\")\n",
        "+        try:\n",
        "+            while 1:\n",
        "+                buf = f.read(chunksize)\n",
        "+                if not buf:\n",
        "+                    return hash.hexdigest()\n",
        "+                hash.update(buf)\n",
        "+        finally:\n",
        "+            f.close()\n",
        "+\n",
        "+    def new(self, **kw):\n",
        "+        \"\"\"Create a modified version of this path.\n",
        "+        the following keyword arguments modify various path parts::\n",
        "+\n",
        "+          a:/some/path/to/a/file.ext\n",
        "+          xx                           drive\n",
        "+          xxxxxxxxxxxxxxxxx            dirname\n",
        "+                            xxxxxxxx   basename\n",
        "+                            xxxx       purebasename\n",
        "+                                 xxx   ext\n",
        "+        \"\"\"\n",
        "+        obj = object.__new__(self.__class__)\n",
        "+        if not kw:\n",
        "+            obj.strpath = self.strpath\n",
        "+            return obj\n",
        "+        drive, dirname, basename, purebasename, ext = self._getbyspec(\n",
        "+            \"drive,dirname,basename,purebasename,ext\"\n",
        "+        )\n",
        "+        if \"basename\" in kw:\n",
        "+            if \"purebasename\" in kw or \"ext\" in kw:\n",
        "+                raise ValueError(f\"invalid specification {kw!r}\")\n",
        "+        else:\n",
        "+            pb = kw.setdefault(\"purebasename\", purebasename)\n",
        "+            try:\n",
        "+                ext = kw[\"ext\"]\n",
        "+            except KeyError:\n",
        "+                pass\n",
        "+            else:\n",
        "+                if ext and not ext.startswith(\".\"):\n",
        "+                    ext = \".\" + ext\n",
        "+            kw[\"basename\"] = pb + ext\n",
        "+\n",
        "+        if \"dirname\" in kw and not kw[\"dirname\"]:\n",
        "+            kw[\"dirname\"] = drive\n",
        "+        else:\n",
        "+            kw.setdefault(\"dirname\", dirname)\n",
        "+        kw.setdefault(\"sep\", self.sep)\n",
        "+        obj.strpath = normpath(\"{dirname}{sep}{basename}\".format(**kw))\n",
        "+        return obj\n",
        "+\n",
        "+    def _getbyspec(self, spec: str) -> list[str]:\n",
        "+        \"\"\"See new for what 'spec' can be.\"\"\"\n",
        "+        res = []\n",
        "+        parts = self.strpath.split(self.sep)\n",
        "+\n",
        "+        args = filter(None, spec.split(\",\"))\n",
        "+        for name in args:\n",
        "+            if name == \"drive\":\n",
        "+                res.append(parts[0])\n",
        "+            elif name == \"dirname\":\n",
        "+                res.append(self.sep.join(parts[:-1]))\n",
        "+            else:\n",
        "+                basename = parts[-1]\n",
        "+                if name == \"basename\":\n",
        "+                    res.append(basename)\n",
        "+                else:\n",
        "+                    i = basename.rfind(\".\")\n",
        "+                    if i == -1:\n",
        "+                        purebasename, ext = basename, \"\"\n",
        "+                    else:\n",
        "+                        purebasename, ext = basename[:i], basename[i:]\n",
        "+                    if name == \"purebasename\":\n",
        "+                        res.append(purebasename)\n",
        "+                    elif name == \"ext\":\n",
        "+                        res.append(ext)\n",
        "+                    else:\n",
        "+                        raise ValueError(f\"invalid part specification {name!r}\")\n",
        "+        return res\n",
        "+\n",
        "+    def dirpath(self, *args, **kwargs):\n",
        "+        \"\"\"Return the directory path joined with any given path arguments.\"\"\"\n",
        "+        if not kwargs:\n",
        "+            path = object.__new__(self.__class__)\n",
        "+            path.strpath = dirname(self.strpath)\n",
        "+            if args:\n",
        "+                path = path.join(*args)\n",
        "+            return path\n",
        "+        return self.new(basename=\"\").join(*args, **kwargs)\n",
        "+\n",
        "+    def join(self, *args: os.PathLike[str], abs: bool = False) -> LocalPath:\n",
        "+        \"\"\"Return a new path by appending all 'args' as path\n",
        "+        components.  if abs=1 is used restart from root if any\n",
        "+        of the args is an absolute path.\n",
        "+        \"\"\"\n",
        "+        sep = self.sep\n",
        "+        strargs = [os.fspath(arg) for arg in args]\n",
        "+        strpath = self.strpath\n",
        "+        if abs:\n",
        "+            newargs: list[str] = []\n",
        "+            for arg in reversed(strargs):\n",
        "+                if isabs(arg):\n",
        "+                    strpath = arg\n",
        "+                    strargs = newargs\n",
        "+                    break\n",
        "+                newargs.insert(0, arg)\n",
        "+        # special case for when we have e.g. strpath == \"/\"\n",
        "+        actual_sep = \"\" if strpath.endswith(sep) else sep\n",
        "+        for arg in strargs:\n",
        "+            arg = arg.strip(sep)\n",
        "+            if iswin32:\n",
        "+                # allow unix style paths even on windows.\n",
        "+                arg = arg.strip(\"/\")\n",
        "+                arg = arg.replace(\"/\", sep)\n",
        "+            strpath = strpath + actual_sep + arg\n",
        "+            actual_sep = sep\n",
        "+        obj = object.__new__(self.__class__)\n",
        "+        obj.strpath = normpath(strpath)\n",
        "+        return obj\n",
        "+\n",
        "+    def open(self, mode=\"r\", ensure=False, encoding=None):\n",
        "+        \"\"\"Return an opened file with the given mode.\n",
        "+\n",
        "+        If ensure is True, create parent directories if needed.\n",
        "+        \"\"\"\n",
        "+        if ensure:\n",
        "+            self.dirpath().ensure(dir=1)\n",
        "+        if encoding:\n",
        "+            return error.checked_call(\n",
        "+                io.open,\n",
        "+                self.strpath,\n",
        "+                mode,\n",
        "+                encoding=encoding,\n",
        "+            )\n",
        "+        return error.checked_call(open, self.strpath, mode)\n",
        "+\n",
        "+    def _fastjoin(self, name):\n",
        "+        child = object.__new__(self.__class__)\n",
        "+        child.strpath = self.strpath + self.sep + name\n",
        "+        return child\n",
        "+\n",
        "+    def islink(self):\n",
        "+        return islink(self.strpath)\n",
        "+\n",
        "+    def check(self, **kw):\n",
        "+        \"\"\"Check a path for existence and properties.\n",
        "+\n",
        "+        Without arguments, return True if the path exists, otherwise False.\n",
        "+\n",
        "+        valid checkers::\n",
        "+\n",
        "+            file = 1  # is a file\n",
        "+            file = 0  # is not a file (may not even exist)\n",
        "+            dir = 1  # is a dir\n",
        "+            link = 1  # is a link\n",
        "+            exists = 1  # exists\n",
        "+\n",
        "+        You can specify multiple checker definitions, for example::\n",
        "+\n",
        "+            path.check(file=1, link=1)  # a link pointing to a file\n",
        "+        \"\"\"\n",
        "+        if not kw:\n",
        "+            return exists(self.strpath)\n",
        "+        if len(kw) == 1:\n",
        "+            if \"dir\" in kw:\n",
        "+                return not kw[\"dir\"] ^ isdir(self.strpath)\n",
        "+            if \"file\" in kw:\n",
        "+                return not kw[\"file\"] ^ isfile(self.strpath)\n",
        "+        if not kw:\n",
        "+            kw = {\"exists\": 1}\n",
        "+        return Checkers(self)._evaluate(kw)\n",
        "+\n",
        "+    _patternchars = set(\"*?[\" + os.sep)\n",
        "+\n",
        "+    def listdir(self, fil=None, sort=None):\n",
        "+        \"\"\"List directory contents, possibly filter by the given fil func\n",
        "+        and possibly sorted.\n",
        "+        \"\"\"\n",
        "+        if fil is None and sort is None:\n",
        "+            names = error.checked_call(os.listdir, self.strpath)\n",
        "+            return map_as_list(self._fastjoin, names)\n",
        "+        if isinstance(fil, str):\n",
        "+            if not self._patternchars.intersection(fil):\n",
        "+                child = self._fastjoin(fil)\n",
        "+                if exists(child.strpath):\n",
        "+                    return [child]\n",
        "+                return []\n",
        "+            fil = FNMatcher(fil)\n",
        "+        names = error.checked_call(os.listdir, self.strpath)\n",
        "+        res = []\n",
        "+        for name in names:\n",
        "+            child = self._fastjoin(name)\n",
        "+            if fil is None or fil(child):\n",
        "+                res.append(child)\n",
        "+        self._sortlist(res, sort)\n",
        "+        return res\n",
        "+\n",
        "+    def size(self) -> int:\n",
        "+        \"\"\"Return size of the underlying file object\"\"\"\n",
        "+        return self.stat().size\n",
        "+\n",
        "+    def mtime(self) -> float:\n",
        "+        \"\"\"Return last modification time of the path.\"\"\"\n",
        "+        return self.stat().mtime\n",
        "+\n",
        "+    def copy(self, target, mode=False, stat=False):\n",
        "+        \"\"\"Copy path to target.\n",
        "+\n",
        "+        If mode is True, will copy permission from path to target.\n",
        "+        If stat is True, copy permission, last modification\n",
        "+        time, last access time, and flags from path to target.\n",
        "+        \"\"\"\n",
        "+        if self.check(file=1):\n",
        "+            if target.check(dir=1):\n",
        "+                target = target.join(self.basename)\n",
        "+            assert self != target\n",
        "+            copychunked(self, target)\n",
        "+            if mode:\n",
        "+                copymode(self.strpath, target.strpath)\n",
        "+            if stat:\n",
        "+                copystat(self, target)\n",
        "+        else:\n",
        "+\n",
        "+            def rec(p):\n",
        "+                return p.check(link=0)\n",
        "+\n",
        "+            for x in self.visit(rec=rec):\n",
        "+                relpath = x.relto(self)\n",
        "+                newx = target.join(relpath)\n",
        "+                newx.dirpath().ensure(dir=1)\n",
        "+                if x.check(link=1):\n",
        "+                    newx.mksymlinkto(x.readlink())\n",
        "+                    continue\n",
        "+                elif x.check(file=1):\n",
        "+                    copychunked(x, newx)\n",
        "+                elif x.check(dir=1):\n",
        "+                    newx.ensure(dir=1)\n",
        "+                if mode:\n",
        "+                    copymode(x.strpath, newx.strpath)\n",
        "+                if stat:\n",
        "+                    copystat(x, newx)\n",
        "+\n",
        "+    def rename(self, target):\n",
        "+        \"\"\"Rename this path to target.\"\"\"\n",
        "+        target = os.fspath(target)\n",
        "+        return error.checked_call(os.rename, self.strpath, target)\n",
        "+\n",
        "+    def dump(self, obj, bin=1):\n",
        "+        \"\"\"Pickle object into path location\"\"\"\n",
        "+        f = self.open(\"wb\")\n",
        "+        import pickle\n",
        "+\n",
        "+        try:\n",
        "+            error.checked_call(pickle.dump, obj, f, bin)\n",
        "+        finally:\n",
        "+            f.close()\n",
        "+\n",
        "+    def mkdir(self, *args):\n",
        "+        \"\"\"Create & return the directory joined with args.\"\"\"\n",
        "+        p = self.join(*args)\n",
        "+        error.checked_call(os.mkdir, os.fspath(p))\n",
        "+        return p\n",
        "+\n",
        "+    def write_binary(self, data, ensure=False):\n",
        "+        \"\"\"Write binary data into path.   If ensure is True create\n",
        "+        missing parent directories.\n",
        "+        \"\"\"\n",
        "+        if ensure:\n",
        "+            self.dirpath().ensure(dir=1)\n",
        "+        with self.open(\"wb\") as f:\n",
        "+            f.write(data)\n",
        "+\n",
        "+    def write_text(self, data, encoding, ensure=False):\n",
        "+        \"\"\"Write text data into path using the specified encoding.\n",
        "+        If ensure is True create missing parent directories.\n",
        "+        \"\"\"\n",
        "+        if ensure:\n",
        "+            self.dirpath().ensure(dir=1)\n",
        "+        with self.open(\"w\", encoding=encoding) as f:\n",
        "+            f.write(data)\n",
        "+\n",
        "+    def write(self, data, mode=\"w\", ensure=False):\n",
        "+        \"\"\"Write data into path.   If ensure is True create\n",
        "+        missing parent directories.\n",
        "+        \"\"\"\n",
        "+        if ensure:\n",
        "+            self.dirpath().ensure(dir=1)\n",
        "+        if \"b\" in mode:\n",
        "+            if not isinstance(data, bytes):\n",
        "+                raise ValueError(\"can only process bytes\")\n",
        "+        else:\n",
        "+            if not isinstance(data, str):\n",
        "+                if not isinstance(data, bytes):\n",
        "+                    data = str(data)\n",
        "+                else:\n",
        "+                    data = data.decode(sys.getdefaultencoding())\n",
        "+        f = self.open(mode)\n",
        "+        try:\n",
        "+            f.write(data)\n",
        "+        finally:\n",
        "+            f.close()\n",
        "+\n",
        "+    def _ensuredirs(self):\n",
        "+        parent = self.dirpath()\n",
        "+        if parent == self:\n",
        "+            return self\n",
        "+        if parent.check(dir=0):\n",
        "+            parent._ensuredirs()\n",
        "+        if self.check(dir=0):\n",
        "+            try:\n",
        "+                self.mkdir()\n",
        "+            except error.EEXIST:\n",
        "+                # race condition: file/dir created by another thread/process.\n",
        "+                # complain if it is not a dir\n",
        "+                if self.check(dir=0):\n",
        "+                    raise\n",
        "+        return self\n",
        "+\n",
        "+    def ensure(self, *args, **kwargs):\n",
        "+        \"\"\"Ensure that an args-joined path exists (by default as\n",
        "+        a file). if you specify a keyword argument 'dir=True'\n",
        "+        then the path is forced to be a directory path.\n",
        "+        \"\"\"\n",
        "+        p = self.join(*args)\n",
        "+        if kwargs.get(\"dir\", 0):\n",
        "+            return p._ensuredirs()\n",
        "+        else:\n",
        "+            p.dirpath()._ensuredirs()\n",
        "+            if not p.check(file=1):\n",
        "+                p.open(\"wb\").close()\n",
        "+            return p\n",
        "+\n",
        "+    @overload\n",
        "+    def stat(self, raising: Literal[True] = ...) -> Stat: ...\n",
        "+\n",
        "+    @overload\n",
        "+    def stat(self, raising: Literal[False]) -> Stat | None: ...\n",
        "+\n",
        "+    def stat(self, raising: bool = True) -> Stat | None:\n",
        "+        \"\"\"Return an os.stat() tuple.\"\"\"\n",
        "+        if raising:\n",
        "+            return Stat(self, error.checked_call(os.stat, self.strpath))\n",
        "+        try:\n",
        "+            return Stat(self, os.stat(self.strpath))\n",
        "+        except KeyboardInterrupt:\n",
        "+            raise\n",
        "+        except Exception:\n",
        "+            return None\n",
        "+\n",
        "+    def lstat(self) -> Stat:\n",
        "+        \"\"\"Return an os.lstat() tuple.\"\"\"\n",
        "+        return Stat(self, error.checked_call(os.lstat, self.strpath))\n",
        "+\n",
        "+    def setmtime(self, mtime=None):\n",
        "+        \"\"\"Set modification time for the given path.  if 'mtime' is None\n",
        "+        (the default) then the file's mtime is set to current time.\n",
        "+\n",
        "+        Note that the resolution for 'mtime' is platform dependent.\n",
        "+        \"\"\"\n",
        "+        if mtime is None:\n",
        "+            return error.checked_call(os.utime, self.strpath, mtime)\n",
        "+        try:\n",
        "+            return error.checked_call(os.utime, self.strpath, (-1, mtime))\n",
        "+        except error.EINVAL:\n",
        "+            return error.checked_call(os.utime, self.strpath, (self.atime(), mtime))\n",
        "+\n",
        "+    def chdir(self):\n",
        "+        \"\"\"Change directory to self and return old current directory\"\"\"\n",
        "+        try:\n",
        "+            old = self.__class__()\n",
        "+        except error.ENOENT:\n",
        "+            old = None\n",
        "+        error.checked_call(os.chdir, self.strpath)\n",
        "+        return old\n",
        "+\n",
        "+    @contextmanager\n",
        "+    def as_cwd(self):\n",
        "+        \"\"\"\n",
        "+        Return a context manager, which changes to the path's dir during the\n",
        "+        managed \"with\" context.\n",
        "+        On __enter__ it returns the old dir, which might be ``None``.\n",
        "+        \"\"\"\n",
        "+        old = self.chdir()\n",
        "+        try:\n",
        "+            yield old\n",
        "+        finally:\n",
        "+            if old is not None:\n",
        "+                old.chdir()\n",
        "+\n",
        "+    def realpath(self):\n",
        "+        \"\"\"Return a new path which contains no symbolic links.\"\"\"\n",
        "+        return self.__class__(os.path.realpath(self.strpath))\n",
        "+\n",
        "+    def atime(self):\n",
        "+        \"\"\"Return last access time of the path.\"\"\"\n",
        "+        return self.stat().atime\n",
        "+\n",
        "+    def __repr__(self):\n",
        "+        return f\"local({self.strpath!r})\"\n",
        "+\n",
        "+    def __str__(self):\n",
        "+        \"\"\"Return string representation of the Path.\"\"\"\n",
        "+        return self.strpath\n",
        "+\n",
        "+    def chmod(self, mode, rec=0):\n",
        "+        \"\"\"Change permissions to the given mode. If mode is an\n",
        "+        integer it directly encodes the os-specific modes.\n",
        "+        if rec is True perform recursively.\n",
        "+        \"\"\"\n",
        "+        if not isinstance(mode, int):\n",
        "+            raise TypeError(f\"mode {mode!r} must be an integer\")\n",
        "+        if rec:\n",
        "+            for x in self.visit(rec=rec):\n",
        "+                error.checked_call(os.chmod, str(x), mode)\n",
        "+        error.checked_call(os.chmod, self.strpath, mode)\n",
        "+\n",
        "+    def pypkgpath(self):\n",
        "+        \"\"\"Return the Python package path by looking for the last\n",
        "+        directory upwards which still contains an __init__.py.\n",
        "+        Return None if a pkgpath cannot be determined.\n",
        "+        \"\"\"\n",
        "+        pkgpath = None\n",
        "+        for parent in self.parts(reverse=True):\n",
        "+            if parent.isdir():\n",
        "+                if not parent.join(\"__init__.py\").exists():\n",
        "+                    break\n",
        "+                if not isimportable(parent.basename):\n",
        "+                    break\n",
        "+                pkgpath = parent\n",
        "+        return pkgpath\n",
        "+\n",
        "+    def _ensuresyspath(self, ensuremode, path):\n",
        "+        if ensuremode:\n",
        "+            s = str(path)\n",
        "+            if ensuremode == \"append\":\n",
        "+                if s not in sys.path:\n",
        "+                    sys.path.append(s)\n",
        "+            else:\n",
        "+                if s != sys.path[0]:\n",
        "+                    sys.path.insert(0, s)\n",
        "+\n",
        "+    def pyimport(self, modname=None, ensuresyspath=True):\n",
        "+        \"\"\"Return path as an imported python module.\n",
        "+\n",
        "+        If modname is None, look for the containing package\n",
        "+        and construct an according module name.\n",
        "+        The module will be put/looked up in sys.modules.\n",
        "+        if ensuresyspath is True then the root dir for importing\n",
        "+        the file (taking __init__.py files into account) will\n",
        "+        be prepended to sys.path if it isn't there already.\n",
        "+        If ensuresyspath==\"append\" the root dir will be appended\n",
        "+        if it isn't already contained in sys.path.\n",
        "+        if ensuresyspath is False no modification of syspath happens.\n",
        "+\n",
        "+        Special value of ensuresyspath==\"importlib\" is intended\n",
        "+        purely for using in pytest, it is capable only of importing\n",
        "+        separate .py files outside packages, e.g. for test suite\n",
        "+        without any __init__.py file. It effectively allows having\n",
        "+        same-named test modules in different places and offers\n",
        "+        mild opt-in via this option. Note that it works only in\n",
        "+        recent versions of python.\n",
        "+        \"\"\"\n",
        "+        if not self.check():\n",
        "+            raise error.ENOENT(self)\n",
        "+\n",
        "+        if ensuresyspath == \"importlib\":\n",
        "+            if modname is None:\n",
        "+                modname = self.purebasename\n",
        "+            spec = importlib.util.spec_from_file_location(modname, str(self))\n",
        "+            if spec is None or spec.loader is None:\n",
        "+                raise ImportError(f\"Can't find module {modname} at location {self!s}\")\n",
        "+            mod = importlib.util.module_from_spec(spec)\n",
        "+            spec.loader.exec_module(mod)\n",
        "+            return mod\n",
        "+\n",
        "+        pkgpath = None\n",
        "+        if modname is None:\n",
        "+            pkgpath = self.pypkgpath()\n",
        "+            if pkgpath is not None:\n",
        "+                pkgroot = pkgpath.dirpath()\n",
        "+                names = self.new(ext=\"\").relto(pkgroot).split(self.sep)\n",
        "+                if names[-1] == \"__init__\":\n",
        "+                    names.pop()\n",
        "+                modname = \".\".join(names)\n",
        "+            else:\n",
        "+                pkgroot = self.dirpath()\n",
        "+                modname = self.purebasename\n",
        "+\n",
        "+            self._ensuresyspath(ensuresyspath, pkgroot)\n",
        "+            __import__(modname)\n",
        "+            mod = sys.modules[modname]\n",
        "+            if self.basename == \"__init__.py\":\n",
        "+                return mod  # we don't check anything as we might\n",
        "+                # be in a namespace package ... too icky to check\n",
        "+            modfile = mod.__file__\n",
        "+            assert modfile is not None\n",
        "+            if modfile[-4:] in (\".pyc\", \".pyo\"):\n",
        "+                modfile = modfile[:-1]\n",
        "+            elif modfile.endswith(\"$py.class\"):\n",
        "+                modfile = modfile[:-9] + \".py\"\n",
        "+            if modfile.endswith(os.sep + \"__init__.py\"):\n",
        "+                if self.basename != \"__init__.py\":\n",
        "+                    modfile = modfile[:-12]\n",
        "+            try:\n",
        "+                issame = self.samefile(modfile)\n",
        "+            except error.ENOENT:\n",
        "+                issame = False\n",
        "+            if not issame:\n",
        "+                ignore = os.getenv(\"PY_IGNORE_IMPORTMISMATCH\")\n",
        "+                if ignore != \"1\":\n",
        "+                    raise self.ImportMismatchError(modname, modfile, self)\n",
        "+            return mod\n",
        "+        else:\n",
        "+            try:\n",
        "+                return sys.modules[modname]\n",
        "+            except KeyError:\n",
        "+                # we have a custom modname, do a pseudo-import\n",
        "+                import types\n",
        "+\n",
        "+                mod = types.ModuleType(modname)\n",
        "+                mod.__file__ = str(self)\n",
        "+                sys.modules[modname] = mod\n",
        "+                try:\n",
        "+                    with open(str(self), \"rb\") as f:\n",
        "+                        exec(f.read(), mod.__dict__)\n",
        "+                except BaseException:\n",
        "+                    del sys.modules[modname]\n",
        "+                    raise\n",
        "+                return mod\n",
        "+\n",
        "+    def sysexec(self, *argv: os.PathLike[str], **popen_opts: Any) -> str:\n",
        "+        \"\"\"Return stdout text from executing a system child process,\n",
        "+        where the 'self' path points to executable.\n",
        "+        The process is directly invoked and not through a system shell.\n",
        "+        \"\"\"\n",
        "+        from subprocess import PIPE\n",
        "+        from subprocess import Popen\n",
        "+\n",
        "+        popen_opts.pop(\"stdout\", None)\n",
        "+        popen_opts.pop(\"stderr\", None)\n",
        "+        proc = Popen(\n",
        "+            [str(self)] + [str(arg) for arg in argv],\n",
        "+            **popen_opts,\n",
        "+            stdout=PIPE,\n",
        "+            stderr=PIPE,\n",
        "+        )\n",
        "+        stdout: str | bytes\n",
        "+        stdout, stderr = proc.communicate()\n",
        "+        ret = proc.wait()\n",
        "+        if isinstance(stdout, bytes):\n",
        "+            stdout = stdout.decode(sys.getdefaultencoding())\n",
        "+        if ret != 0:\n",
        "+            if isinstance(stderr, bytes):\n",
        "+                stderr = stderr.decode(sys.getdefaultencoding())\n",
        "+            raise RuntimeError(\n",
        "+                ret,\n",
        "+                ret,\n",
        "+                str(self),\n",
        "+                stdout,\n",
        "+                stderr,\n",
        "+            )\n",
        "+        return stdout\n",
        "+\n",
        "+    @classmethod\n",
        "+    def sysfind(cls, name, checker=None, paths=None):\n",
        "+        \"\"\"Return a path object found by looking at the systems\n",
        "+        underlying PATH specification. If the checker is not None\n",
        "+        it will be invoked to filter matching paths.  If a binary\n",
        "+        cannot be found, None is returned\n",
        "+        Note: This is probably not working on plain win32 systems\n",
        "+        but may work on cygwin.\n",
        "+        \"\"\"\n",
        "+        if isabs(name):\n",
        "+            p = local(name)\n",
        "+            if p.check(file=1):\n",
        "+                return p\n",
        "+        else:\n",
        "+            if paths is None:\n",
        "+                if iswin32:\n",
        "+                    paths = os.environ[\"Path\"].split(\";\")\n",
        "+                    if \"\" not in paths and \".\" not in paths:\n",
        "+                        paths.append(\".\")\n",
        "+                    try:\n",
        "+                        systemroot = os.environ[\"SYSTEMROOT\"]\n",
        "+                    except KeyError:\n",
        "+                        pass\n",
        "+                    else:\n",
        "+                        paths = [\n",
        "+                            path.replace(\"%SystemRoot%\", systemroot) for path in paths\n",
        "+                        ]\n",
        "+                else:\n",
        "+                    paths = os.environ[\"PATH\"].split(\":\")\n",
        "+            tryadd = []\n",
        "+            if iswin32:\n",
        "+                tryadd += os.environ[\"PATHEXT\"].split(os.pathsep)\n",
        "+            tryadd.append(\"\")\n",
        "+\n",
        "+            for x in paths:\n",
        "+                for addext in tryadd:\n",
        "+                    p = local(x).join(name, abs=True) + addext\n",
        "+                    try:\n",
        "+                        if p.check(file=1):\n",
        "+                            if checker:\n",
        "+                                if not checker(p):\n",
        "+                                    continue\n",
        "+                            return p\n",
        "+                    except error.EACCES:\n",
        "+                        pass\n",
        "+        return None\n",
        "+\n",
        "+    @classmethod\n",
        "+    def _gethomedir(cls):\n",
        "+        try:\n",
        "+            x = os.environ[\"HOME\"]\n",
        "+        except KeyError:\n",
        "+            try:\n",
        "+                x = os.environ[\"HOMEDRIVE\"] + os.environ[\"HOMEPATH\"]\n",
        "+            except KeyError:\n",
        "+                return None\n",
        "+        return cls(x)\n",
        "+\n",
        "+    # \"\"\"\n",
        "+    # special class constructors for local filesystem paths\n",
        "+    # \"\"\"\n",
        "+    @classmethod\n",
        "+    def get_temproot(cls):\n",
        "+        \"\"\"Return the system's temporary directory\n",
        "+        (where tempfiles are usually created in)\n",
        "+        \"\"\"\n",
        "+        import tempfile\n",
        "+\n",
        "+        return local(tempfile.gettempdir())\n",
        "+\n",
        "+    @classmethod\n",
        "+    def mkdtemp(cls, rootdir=None):\n",
        "+        \"\"\"Return a Path object pointing to a fresh new temporary directory\n",
        "+        (which we created ourselves).\n",
        "+        \"\"\"\n",
        "+        import tempfile\n",
        "+\n",
        "+        if rootdir is None:\n",
        "+            rootdir = cls.get_temproot()\n",
        "+        path = error.checked_call(tempfile.mkdtemp, dir=str(rootdir))\n",
        "+        return cls(path)\n",
        "+\n",
        "+    @classmethod\n",
        "+    def make_numbered_dir(\n",
        "+        cls, prefix=\"session-\", rootdir=None, keep=3, lock_timeout=172800\n",
        "+    ):  # two days\n",
        "+        \"\"\"Return unique directory with a number greater than the current\n",
        "+        maximum one.  The number is assumed to start directly after prefix.\n",
        "+        if keep is true directories with a number less than (maxnum-keep)\n",
        "+        will be removed. If .lock files are used (lock_timeout non-zero),\n",
        "+        algorithm is multi-process safe.\n",
        "+        \"\"\"\n",
        "+        if rootdir is None:\n",
        "+            rootdir = cls.get_temproot()\n",
        "+\n",
        "+        nprefix = prefix.lower()\n",
        "+\n",
        "+        def parse_num(path):\n",
        "+            \"\"\"Parse the number out of a path (if it matches the prefix)\"\"\"\n",
        "+            nbasename = path.basename.lower()\n",
        "+            if nbasename.startswith(nprefix):\n",
        "+                try:\n",
        "+                    return int(nbasename[len(nprefix) :])\n",
        "+                except ValueError:\n",
        "+                    pass\n",
        "+\n",
        "+        def create_lockfile(path):\n",
        "+            \"\"\"Exclusively create lockfile. Throws when failed\"\"\"\n",
        "+            mypid = os.getpid()\n",
        "+            lockfile = path.join(\".lock\")\n",
        "+            if hasattr(lockfile, \"mksymlinkto\"):\n",
        "+                lockfile.mksymlinkto(str(mypid))\n",
        "+            else:\n",
        "+                fd = error.checked_call(\n",
        "+                    os.open, str(lockfile), os.O_WRONLY | os.O_CREAT | os.O_EXCL, 0o644\n",
        "+                )\n",
        "+                with os.fdopen(fd, \"w\") as f:\n",
        "+                    f.write(str(mypid))\n",
        "+            return lockfile\n",
        "+\n",
        "+        def atexit_remove_lockfile(lockfile):\n",
        "+            \"\"\"Ensure lockfile is removed at process exit\"\"\"\n",
        "+            mypid = os.getpid()\n",
        "+\n",
        "+            def try_remove_lockfile():\n",
        "+                # in a fork() situation, only the last process should\n",
        "+                # remove the .lock, otherwise the other processes run the\n",
        "+                # risk of seeing their temporary dir disappear.  For now\n",
        "+                # we remove the .lock in the parent only (i.e. we assume\n",
        "+                # that the children finish before the parent).\n",
        "+                if os.getpid() != mypid:\n",
        "+                    return\n",
        "+                try:\n",
        "+                    lockfile.remove()\n",
        "+                except error.Error:\n",
        "+                    pass\n",
        "+\n",
        "+            atexit.register(try_remove_lockfile)\n",
        "+\n",
        "+        # compute the maximum number currently in use with the prefix\n",
        "+        lastmax = None\n",
        "+        while True:\n",
        "+            maxnum = -1\n",
        "+            for path in rootdir.listdir():\n",
        "+                num = parse_num(path)\n",
        "+                if num is not None:\n",
        "+                    maxnum = max(maxnum, num)\n",
        "+\n",
        "+            # make the new directory\n",
        "+            try:\n",
        "+                udir = rootdir.mkdir(prefix + str(maxnum + 1))\n",
        "+                if lock_timeout:\n",
        "+                    lockfile = create_lockfile(udir)\n",
        "+                    atexit_remove_lockfile(lockfile)\n",
        "+            except (error.EEXIST, error.ENOENT, error.EBUSY):\n",
        "+                # race condition (1): another thread/process created the dir\n",
        "+                #                     in the meantime - try again\n",
        "+                # race condition (2): another thread/process spuriously acquired\n",
        "+                #                     lock treating empty directory as candidate\n",
        "+                #                     for removal - try again\n",
        "+                # race condition (3): another thread/process tried to create the lock at\n",
        "+                #                     the same time (happened in Python 3.3 on Windows)\n",
        "+                # https://ci.appveyor.com/project/pytestbot/py/build/1.0.21/job/ffi85j4c0lqwsfwa\n",
        "+                if lastmax == maxnum:\n",
        "+                    raise\n",
        "+                lastmax = maxnum\n",
        "+                continue\n",
        "+            break\n",
        "+\n",
        "+        def get_mtime(path):\n",
        "+            \"\"\"Read file modification time\"\"\"\n",
        "+            try:\n",
        "+                return path.lstat().mtime\n",
        "+            except error.Error:\n",
        "+                pass\n",
        "+\n",
        "+        garbage_prefix = prefix + \"garbage-\"\n",
        "+\n",
        "+        def is_garbage(path):\n",
        "+            \"\"\"Check if path denotes directory scheduled for removal\"\"\"\n",
        "+            bn = path.basename\n",
        "+            return bn.startswith(garbage_prefix)\n",
        "+\n",
        "+        # prune old directories\n",
        "+        udir_time = get_mtime(udir)\n",
        "+        if keep and udir_time:\n",
        "+            for path in rootdir.listdir():\n",
        "+                num = parse_num(path)\n",
        "+                if num is not None and num <= (maxnum - keep):\n",
        "+                    try:\n",
        "+                        # try acquiring lock to remove directory as exclusive user\n",
        "+                        if lock_timeout:\n",
        "+                            create_lockfile(path)\n",
        "+                    except (error.EEXIST, error.ENOENT, error.EBUSY):\n",
        "+                        path_time = get_mtime(path)\n",
        "+                        if not path_time:\n",
        "+                            # assume directory doesn't exist now\n",
        "+                            continue\n",
        "+                        if abs(udir_time - path_time) < lock_timeout:\n",
        "+                            # assume directory with lockfile exists\n",
        "+                            # and lock timeout hasn't expired yet\n",
        "+                            continue\n",
        "+\n",
        "+                    # path dir locked for exclusive use\n",
        "+                    # and scheduled for removal to avoid another thread/process\n",
        "+                    # treating it as a new directory or removal candidate\n",
        "+                    garbage_path = rootdir.join(garbage_prefix + str(uuid.uuid4()))\n",
        "+                    try:\n",
        "+                        path.rename(garbage_path)\n",
        "+                        garbage_path.remove(rec=1)\n",
        "+                    except KeyboardInterrupt:\n",
        "+                        raise\n",
        "+                    except Exception:  # this might be error.Error, WindowsError ...\n",
        "+                        pass\n",
        "+                if is_garbage(path):\n",
        "+                    try:\n",
        "+                        path.remove(rec=1)\n",
        "+                    except KeyboardInterrupt:\n",
        "+                        raise\n",
        "+                    except Exception:  # this might be error.Error, WindowsError ...\n",
        "+                        pass\n",
        "+\n",
        "+        # make link...\n",
        "+        try:\n",
        "+            username = os.environ[\"USER\"]  # linux, et al\n",
        "+        except KeyError:\n",
        "+            try:\n",
        "+                username = os.environ[\"USERNAME\"]  # windows\n",
        "+            except KeyError:\n",
        "+                username = \"current\"\n",
        "+\n",
        "+        src = str(udir)\n",
        "+        dest = src[: src.rfind(\"-\")] + \"-\" + username\n",
        "+        try:\n",
        "+            os.unlink(dest)\n",
        "+        except OSError:\n",
        "+            pass\n",
        "+        try:\n",
        "+            os.symlink(src, dest)\n",
        "+        except (OSError, AttributeError, NotImplementedError):\n",
        "+            pass\n",
        "+\n",
        "+        return udir\n",
        "+\n",
        "+\n",
        "+def copymode(src, dest):\n",
        "+    \"\"\"Copy permission from src to dst.\"\"\"\n",
        "+    import shutil\n",
        "+\n",
        "+    shutil.copymode(src, dest)\n",
        "+\n",
        "+\n",
        "+def copystat(src, dest):\n",
        "+    \"\"\"Copy permission,  last modification time,\n",
        "+    last access time, and flags from src to dst.\"\"\"\n",
        "+    import shutil\n",
        "+\n",
        "+    shutil.copystat(str(src), str(dest))\n",
        "+\n",
        "+\n",
        "+def copychunked(src, dest):\n",
        "+    chunksize = 524288  # half a meg of bytes\n",
        "+    fsrc = src.open(\"rb\")\n",
        "+    try:\n",
        "+        fdest = dest.open(\"wb\")\n",
        "+        try:\n",
        "+            while 1:\n",
        "+                buf = fsrc.read(chunksize)\n",
        "+                if not buf:\n",
        "+                    break\n",
        "+                fdest.write(buf)\n",
        "+        finally:\n",
        "+            fdest.close()\n",
        "+    finally:\n",
        "+        fsrc.close()\n",
        "+\n",
        "+\n",
        "+def isimportable(name):\n",
        "+    if name and (name[0].isalpha() or name[0] == \"_\"):\n",
        "+        name = name.replace(\"_\", \"\")\n",
        "+        return not name or name.isalnum()\n",
        "+\n",
        "+\n",
        "+local = LocalPath\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/_version.py",
      "status": "added",
      "additions": 21,
      "deletions": 0,
      "patch": "@@ -0,0 +1,21 @@\n+# file generated by setuptools-scm\n+# don't change, don't track in version control\n+\n+__all__ = [\"__version__\", \"__version_tuple__\", \"version\", \"version_tuple\"]\n+\n+TYPE_CHECKING = False\n+if TYPE_CHECKING:\n+    from typing import Tuple\n+    from typing import Union\n+\n+    VERSION_TUPLE = Tuple[Union[int, str], ...]\n+else:\n+    VERSION_TUPLE = object\n+\n+version: str\n+__version__: str\n+__version_tuple__: VERSION_TUPLE\n+version_tuple: VERSION_TUPLE\n+\n+__version__ = version = '8.4.1'\n+__version_tuple__ = version_tuple = (8, 4, 1)",
      "patch_lines": [
        "@@ -0,0 +1,21 @@\n",
        "+# file generated by setuptools-scm\n",
        "+# don't change, don't track in version control\n",
        "+\n",
        "+__all__ = [\"__version__\", \"__version_tuple__\", \"version\", \"version_tuple\"]\n",
        "+\n",
        "+TYPE_CHECKING = False\n",
        "+if TYPE_CHECKING:\n",
        "+    from typing import Tuple\n",
        "+    from typing import Union\n",
        "+\n",
        "+    VERSION_TUPLE = Tuple[Union[int, str], ...]\n",
        "+else:\n",
        "+    VERSION_TUPLE = object\n",
        "+\n",
        "+version: str\n",
        "+__version__: str\n",
        "+__version_tuple__: VERSION_TUPLE\n",
        "+version_tuple: VERSION_TUPLE\n",
        "+\n",
        "+__version__ = version = '8.4.1'\n",
        "+__version_tuple__ = version_tuple = (8, 4, 1)\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/assertion/__init__.py",
      "status": "added",
      "additions": 208,
      "deletions": 0,
      "patch": "@@ -0,0 +1,208 @@\n+# mypy: allow-untyped-defs\n+\"\"\"Support for presenting detailed information in failing assertions.\"\"\"\n+\n+from __future__ import annotations\n+\n+from collections.abc import Generator\n+import sys\n+from typing import Any\n+from typing import Protocol\n+from typing import TYPE_CHECKING\n+\n+from _pytest.assertion import rewrite\n+from _pytest.assertion import truncate\n+from _pytest.assertion import util\n+from _pytest.assertion.rewrite import assertstate_key\n+from _pytest.config import Config\n+from _pytest.config import hookimpl\n+from _pytest.config.argparsing import Parser\n+from _pytest.nodes import Item\n+\n+\n+if TYPE_CHECKING:\n+    from _pytest.main import Session\n+\n+\n+def pytest_addoption(parser: Parser) -> None:\n+    group = parser.getgroup(\"debugconfig\")\n+    group.addoption(\n+        \"--assert\",\n+        action=\"store\",\n+        dest=\"assertmode\",\n+        choices=(\"rewrite\", \"plain\"),\n+        default=\"rewrite\",\n+        metavar=\"MODE\",\n+        help=(\n+            \"Control assertion debugging tools.\\n\"\n+            \"'plain' performs no assertion debugging.\\n\"\n+            \"'rewrite' (the default) rewrites assert statements in test modules\"\n+            \" on import to provide assert expression information.\"\n+        ),\n+    )\n+    parser.addini(\n+        \"enable_assertion_pass_hook\",\n+        type=\"bool\",\n+        default=False,\n+        help=\"Enables the pytest_assertion_pass hook. \"\n+        \"Make sure to delete any previously generated pyc cache files.\",\n+    )\n+\n+    parser.addini(\n+        \"truncation_limit_lines\",\n+        default=None,\n+        help=\"Set threshold of LINES after which truncation will take effect\",\n+    )\n+    parser.addini(\n+        \"truncation_limit_chars\",\n+        default=None,\n+        help=(\"Set threshold of CHARS after which truncation will take effect\"),\n+    )\n+\n+    Config._add_verbosity_ini(\n+        parser,\n+        Config.VERBOSITY_ASSERTIONS,\n+        help=(\n+            \"Specify a verbosity level for assertions, overriding the main level. \"\n+            \"Higher levels will provide more detailed explanation when an assertion fails.\"\n+        ),\n+    )\n+\n+\n+def register_assert_rewrite(*names: str) -> None:\n+    \"\"\"Register one or more module names to be rewritten on import.\n+\n+    This function will make sure that this module or all modules inside\n+    the package will get their assert statements rewritten.\n+    Thus you should make sure to call this before the module is\n+    actually imported, usually in your __init__.py if you are a plugin\n+    using a package.\n+\n+    :param names: The module names to register.\n+    \"\"\"\n+    for name in names:\n+        if not isinstance(name, str):\n+            msg = \"expected module names as *args, got {0} instead\"  # type: ignore[unreachable]\n+            raise TypeError(msg.format(repr(names)))\n+    rewrite_hook: RewriteHook\n+    for hook in sys.meta_path:\n+        if isinstance(hook, rewrite.AssertionRewritingHook):\n+            rewrite_hook = hook\n+            break\n+    else:\n+        rewrite_hook = DummyRewriteHook()\n+    rewrite_hook.mark_rewrite(*names)\n+\n+\n+class RewriteHook(Protocol):\n+    def mark_rewrite(self, *names: str) -> None: ...\n+\n+\n+class DummyRewriteHook:\n+    \"\"\"A no-op import hook for when rewriting is disabled.\"\"\"\n+\n+    def mark_rewrite(self, *names: str) -> None:\n+        pass\n+\n+\n+class AssertionState:\n+    \"\"\"State for the assertion plugin.\"\"\"\n+\n+    def __init__(self, config: Config, mode) -> None:\n+        self.mode = mode\n+        self.trace = config.trace.root.get(\"assertion\")\n+        self.hook: rewrite.AssertionRewritingHook | None = None\n+\n+\n+def install_importhook(config: Config) -> rewrite.AssertionRewritingHook:\n+    \"\"\"Try to install the rewrite hook, raise SystemError if it fails.\"\"\"\n+    config.stash[assertstate_key] = AssertionState(config, \"rewrite\")\n+    config.stash[assertstate_key].hook = hook = rewrite.AssertionRewritingHook(config)\n+    sys.meta_path.insert(0, hook)\n+    config.stash[assertstate_key].trace(\"installed rewrite import hook\")\n+\n+    def undo() -> None:\n+        hook = config.stash[assertstate_key].hook\n+        if hook is not None and hook in sys.meta_path:\n+            sys.meta_path.remove(hook)\n+\n+    config.add_cleanup(undo)\n+    return hook\n+\n+\n+def pytest_collection(session: Session) -> None:\n+    # This hook is only called when test modules are collected\n+    # so for example not in the managing process of pytest-xdist\n+    # (which does not collect test modules).\n+    assertstate = session.config.stash.get(assertstate_key, None)\n+    if assertstate:\n+        if assertstate.hook is not None:\n+            assertstate.hook.set_session(session)\n+\n+\n+@hookimpl(wrapper=True, tryfirst=True)\n+def pytest_runtest_protocol(item: Item) -> Generator[None, object, object]:\n+    \"\"\"Setup the pytest_assertrepr_compare and pytest_assertion_pass hooks.\n+\n+    The rewrite module will use util._reprcompare if it exists to use custom\n+    reporting via the pytest_assertrepr_compare hook.  This sets up this custom\n+    comparison for the test.\n+    \"\"\"\n+    ihook = item.ihook\n+\n+    def callbinrepr(op, left: object, right: object) -> str | None:\n+        \"\"\"Call the pytest_assertrepr_compare hook and prepare the result.\n+\n+        This uses the first result from the hook and then ensures the\n+        following:\n+        * Overly verbose explanations are truncated unless configured otherwise\n+          (eg. if running in verbose mode).\n+        * Embedded newlines are escaped to help util.format_explanation()\n+          later.\n+        * If the rewrite mode is used embedded %-characters are replaced\n+          to protect later % formatting.\n+\n+        The result can be formatted by util.format_explanation() for\n+        pretty printing.\n+        \"\"\"\n+        hook_result = ihook.pytest_assertrepr_compare(\n+            config=item.config, op=op, left=left, right=right\n+        )\n+        for new_expl in hook_result:\n+            if new_expl:\n+                new_expl = truncate.truncate_if_required(new_expl, item)\n+                new_expl = [line.replace(\"\\n\", \"\\\\n\") for line in new_expl]\n+                res = \"\\n~\".join(new_expl)\n+                if item.config.getvalue(\"assertmode\") == \"rewrite\":\n+                    res = res.replace(\"%\", \"%%\")\n+                return res\n+        return None\n+\n+    saved_assert_hooks = util._reprcompare, util._assertion_pass\n+    util._reprcompare = callbinrepr\n+    util._config = item.config\n+\n+    if ihook.pytest_assertion_pass.get_hookimpls():\n+\n+        def call_assertion_pass_hook(lineno: int, orig: str, expl: str) -> None:\n+            ihook.pytest_assertion_pass(item=item, lineno=lineno, orig=orig, expl=expl)\n+\n+        util._assertion_pass = call_assertion_pass_hook\n+\n+    try:\n+        return (yield)\n+    finally:\n+        util._reprcompare, util._assertion_pass = saved_assert_hooks\n+        util._config = None\n+\n+\n+def pytest_sessionfinish(session: Session) -> None:\n+    assertstate = session.config.stash.get(assertstate_key, None)\n+    if assertstate:\n+        if assertstate.hook is not None:\n+            assertstate.hook.set_session(None)\n+\n+\n+def pytest_assertrepr_compare(\n+    config: Config, op: str, left: Any, right: Any\n+) -> list[str] | None:\n+    return util.assertrepr_compare(config=config, op=op, left=left, right=right)",
      "patch_lines": [
        "@@ -0,0 +1,208 @@\n",
        "+# mypy: allow-untyped-defs\n",
        "+\"\"\"Support for presenting detailed information in failing assertions.\"\"\"\n",
        "+\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+from collections.abc import Generator\n",
        "+import sys\n",
        "+from typing import Any\n",
        "+from typing import Protocol\n",
        "+from typing import TYPE_CHECKING\n",
        "+\n",
        "+from _pytest.assertion import rewrite\n",
        "+from _pytest.assertion import truncate\n",
        "+from _pytest.assertion import util\n",
        "+from _pytest.assertion.rewrite import assertstate_key\n",
        "+from _pytest.config import Config\n",
        "+from _pytest.config import hookimpl\n",
        "+from _pytest.config.argparsing import Parser\n",
        "+from _pytest.nodes import Item\n",
        "+\n",
        "+\n",
        "+if TYPE_CHECKING:\n",
        "+    from _pytest.main import Session\n",
        "+\n",
        "+\n",
        "+def pytest_addoption(parser: Parser) -> None:\n",
        "+    group = parser.getgroup(\"debugconfig\")\n",
        "+    group.addoption(\n",
        "+        \"--assert\",\n",
        "+        action=\"store\",\n",
        "+        dest=\"assertmode\",\n",
        "+        choices=(\"rewrite\", \"plain\"),\n",
        "+        default=\"rewrite\",\n",
        "+        metavar=\"MODE\",\n",
        "+        help=(\n",
        "+            \"Control assertion debugging tools.\\n\"\n",
        "+            \"'plain' performs no assertion debugging.\\n\"\n",
        "+            \"'rewrite' (the default) rewrites assert statements in test modules\"\n",
        "+            \" on import to provide assert expression information.\"\n",
        "+        ),\n",
        "+    )\n",
        "+    parser.addini(\n",
        "+        \"enable_assertion_pass_hook\",\n",
        "+        type=\"bool\",\n",
        "+        default=False,\n",
        "+        help=\"Enables the pytest_assertion_pass hook. \"\n",
        "+        \"Make sure to delete any previously generated pyc cache files.\",\n",
        "+    )\n",
        "+\n",
        "+    parser.addini(\n",
        "+        \"truncation_limit_lines\",\n",
        "+        default=None,\n",
        "+        help=\"Set threshold of LINES after which truncation will take effect\",\n",
        "+    )\n",
        "+    parser.addini(\n",
        "+        \"truncation_limit_chars\",\n",
        "+        default=None,\n",
        "+        help=(\"Set threshold of CHARS after which truncation will take effect\"),\n",
        "+    )\n",
        "+\n",
        "+    Config._add_verbosity_ini(\n",
        "+        parser,\n",
        "+        Config.VERBOSITY_ASSERTIONS,\n",
        "+        help=(\n",
        "+            \"Specify a verbosity level for assertions, overriding the main level. \"\n",
        "+            \"Higher levels will provide more detailed explanation when an assertion fails.\"\n",
        "+        ),\n",
        "+    )\n",
        "+\n",
        "+\n",
        "+def register_assert_rewrite(*names: str) -> None:\n",
        "+    \"\"\"Register one or more module names to be rewritten on import.\n",
        "+\n",
        "+    This function will make sure that this module or all modules inside\n",
        "+    the package will get their assert statements rewritten.\n",
        "+    Thus you should make sure to call this before the module is\n",
        "+    actually imported, usually in your __init__.py if you are a plugin\n",
        "+    using a package.\n",
        "+\n",
        "+    :param names: The module names to register.\n",
        "+    \"\"\"\n",
        "+    for name in names:\n",
        "+        if not isinstance(name, str):\n",
        "+            msg = \"expected module names as *args, got {0} instead\"  # type: ignore[unreachable]\n",
        "+            raise TypeError(msg.format(repr(names)))\n",
        "+    rewrite_hook: RewriteHook\n",
        "+    for hook in sys.meta_path:\n",
        "+        if isinstance(hook, rewrite.AssertionRewritingHook):\n",
        "+            rewrite_hook = hook\n",
        "+            break\n",
        "+    else:\n",
        "+        rewrite_hook = DummyRewriteHook()\n",
        "+    rewrite_hook.mark_rewrite(*names)\n",
        "+\n",
        "+\n",
        "+class RewriteHook(Protocol):\n",
        "+    def mark_rewrite(self, *names: str) -> None: ...\n",
        "+\n",
        "+\n",
        "+class DummyRewriteHook:\n",
        "+    \"\"\"A no-op import hook for when rewriting is disabled.\"\"\"\n",
        "+\n",
        "+    def mark_rewrite(self, *names: str) -> None:\n",
        "+        pass\n",
        "+\n",
        "+\n",
        "+class AssertionState:\n",
        "+    \"\"\"State for the assertion plugin.\"\"\"\n",
        "+\n",
        "+    def __init__(self, config: Config, mode) -> None:\n",
        "+        self.mode = mode\n",
        "+        self.trace = config.trace.root.get(\"assertion\")\n",
        "+        self.hook: rewrite.AssertionRewritingHook | None = None\n",
        "+\n",
        "+\n",
        "+def install_importhook(config: Config) -> rewrite.AssertionRewritingHook:\n",
        "+    \"\"\"Try to install the rewrite hook, raise SystemError if it fails.\"\"\"\n",
        "+    config.stash[assertstate_key] = AssertionState(config, \"rewrite\")\n",
        "+    config.stash[assertstate_key].hook = hook = rewrite.AssertionRewritingHook(config)\n",
        "+    sys.meta_path.insert(0, hook)\n",
        "+    config.stash[assertstate_key].trace(\"installed rewrite import hook\")\n",
        "+\n",
        "+    def undo() -> None:\n",
        "+        hook = config.stash[assertstate_key].hook\n",
        "+        if hook is not None and hook in sys.meta_path:\n",
        "+            sys.meta_path.remove(hook)\n",
        "+\n",
        "+    config.add_cleanup(undo)\n",
        "+    return hook\n",
        "+\n",
        "+\n",
        "+def pytest_collection(session: Session) -> None:\n",
        "+    # This hook is only called when test modules are collected\n",
        "+    # so for example not in the managing process of pytest-xdist\n",
        "+    # (which does not collect test modules).\n",
        "+    assertstate = session.config.stash.get(assertstate_key, None)\n",
        "+    if assertstate:\n",
        "+        if assertstate.hook is not None:\n",
        "+            assertstate.hook.set_session(session)\n",
        "+\n",
        "+\n",
        "+@hookimpl(wrapper=True, tryfirst=True)\n",
        "+def pytest_runtest_protocol(item: Item) -> Generator[None, object, object]:\n",
        "+    \"\"\"Setup the pytest_assertrepr_compare and pytest_assertion_pass hooks.\n",
        "+\n",
        "+    The rewrite module will use util._reprcompare if it exists to use custom\n",
        "+    reporting via the pytest_assertrepr_compare hook.  This sets up this custom\n",
        "+    comparison for the test.\n",
        "+    \"\"\"\n",
        "+    ihook = item.ihook\n",
        "+\n",
        "+    def callbinrepr(op, left: object, right: object) -> str | None:\n",
        "+        \"\"\"Call the pytest_assertrepr_compare hook and prepare the result.\n",
        "+\n",
        "+        This uses the first result from the hook and then ensures the\n",
        "+        following:\n",
        "+        * Overly verbose explanations are truncated unless configured otherwise\n",
        "+          (eg. if running in verbose mode).\n",
        "+        * Embedded newlines are escaped to help util.format_explanation()\n",
        "+          later.\n",
        "+        * If the rewrite mode is used embedded %-characters are replaced\n",
        "+          to protect later % formatting.\n",
        "+\n",
        "+        The result can be formatted by util.format_explanation() for\n",
        "+        pretty printing.\n",
        "+        \"\"\"\n",
        "+        hook_result = ihook.pytest_assertrepr_compare(\n",
        "+            config=item.config, op=op, left=left, right=right\n",
        "+        )\n",
        "+        for new_expl in hook_result:\n",
        "+            if new_expl:\n",
        "+                new_expl = truncate.truncate_if_required(new_expl, item)\n",
        "+                new_expl = [line.replace(\"\\n\", \"\\\\n\") for line in new_expl]\n",
        "+                res = \"\\n~\".join(new_expl)\n",
        "+                if item.config.getvalue(\"assertmode\") == \"rewrite\":\n",
        "+                    res = res.replace(\"%\", \"%%\")\n",
        "+                return res\n",
        "+        return None\n",
        "+\n",
        "+    saved_assert_hooks = util._reprcompare, util._assertion_pass\n",
        "+    util._reprcompare = callbinrepr\n",
        "+    util._config = item.config\n",
        "+\n",
        "+    if ihook.pytest_assertion_pass.get_hookimpls():\n",
        "+\n",
        "+        def call_assertion_pass_hook(lineno: int, orig: str, expl: str) -> None:\n",
        "+            ihook.pytest_assertion_pass(item=item, lineno=lineno, orig=orig, expl=expl)\n",
        "+\n",
        "+        util._assertion_pass = call_assertion_pass_hook\n",
        "+\n",
        "+    try:\n",
        "+        return (yield)\n",
        "+    finally:\n",
        "+        util._reprcompare, util._assertion_pass = saved_assert_hooks\n",
        "+        util._config = None\n",
        "+\n",
        "+\n",
        "+def pytest_sessionfinish(session: Session) -> None:\n",
        "+    assertstate = session.config.stash.get(assertstate_key, None)\n",
        "+    if assertstate:\n",
        "+        if assertstate.hook is not None:\n",
        "+            assertstate.hook.set_session(None)\n",
        "+\n",
        "+\n",
        "+def pytest_assertrepr_compare(\n",
        "+    config: Config, op: str, left: Any, right: Any\n",
        "+) -> list[str] | None:\n",
        "+    return util.assertrepr_compare(config=config, op=op, left=left, right=right)\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/assertion/rewrite.py",
      "status": "added",
      "additions": 1216,
      "deletions": 0,
      "patch": "@@ -0,0 +1,1216 @@\n+\"\"\"Rewrite assertion AST to produce nice error messages.\"\"\"\n+\n+from __future__ import annotations\n+\n+import ast\n+from collections import defaultdict\n+from collections.abc import Callable\n+from collections.abc import Iterable\n+from collections.abc import Iterator\n+from collections.abc import Sequence\n+import errno\n+import functools\n+import importlib.abc\n+import importlib.machinery\n+import importlib.util\n+import io\n+import itertools\n+import marshal\n+import os\n+from pathlib import Path\n+from pathlib import PurePath\n+import struct\n+import sys\n+import tokenize\n+import types\n+from typing import IO\n+from typing import TYPE_CHECKING\n+\n+from _pytest._io.saferepr import DEFAULT_REPR_MAX_SIZE\n+from _pytest._io.saferepr import saferepr\n+from _pytest._io.saferepr import saferepr_unlimited\n+from _pytest._version import version\n+from _pytest.assertion import util\n+from _pytest.config import Config\n+from _pytest.fixtures import FixtureFunctionDefinition\n+from _pytest.main import Session\n+from _pytest.pathlib import absolutepath\n+from _pytest.pathlib import fnmatch_ex\n+from _pytest.stash import StashKey\n+\n+\n+# fmt: off\n+from _pytest.assertion.util import format_explanation as _format_explanation  # noqa:F401, isort:skip\n+# fmt:on\n+\n+if TYPE_CHECKING:\n+    from _pytest.assertion import AssertionState\n+\n+\n+class Sentinel:\n+    pass\n+\n+\n+assertstate_key = StashKey[\"AssertionState\"]()\n+\n+# pytest caches rewritten pycs in pycache dirs\n+PYTEST_TAG = f\"{sys.implementation.cache_tag}-pytest-{version}\"\n+PYC_EXT = \".py\" + ((__debug__ and \"c\") or \"o\")\n+PYC_TAIL = \".\" + PYTEST_TAG + PYC_EXT\n+\n+# Special marker that denotes we have just left a scope definition\n+_SCOPE_END_MARKER = Sentinel()\n+\n+\n+class AssertionRewritingHook(importlib.abc.MetaPathFinder, importlib.abc.Loader):\n+    \"\"\"PEP302/PEP451 import hook which rewrites asserts.\"\"\"\n+\n+    def __init__(self, config: Config) -> None:\n+        self.config = config\n+        try:\n+            self.fnpats = config.getini(\"python_files\")\n+        except ValueError:\n+            self.fnpats = [\"test_*.py\", \"*_test.py\"]\n+        self.session: Session | None = None\n+        self._rewritten_names: dict[str, Path] = {}\n+        self._must_rewrite: set[str] = set()\n+        # flag to guard against trying to rewrite a pyc file while we are already writing another pyc file,\n+        # which might result in infinite recursion (#3506)\n+        self._writing_pyc = False\n+        self._basenames_to_check_rewrite = {\"conftest\"}\n+        self._marked_for_rewrite_cache: dict[str, bool] = {}\n+        self._session_paths_checked = False\n+\n+    def set_session(self, session: Session | None) -> None:\n+        self.session = session\n+        self._session_paths_checked = False\n+\n+    # Indirection so we can mock calls to find_spec originated from the hook during testing\n+    _find_spec = importlib.machinery.PathFinder.find_spec\n+\n+    def find_spec(\n+        self,\n+        name: str,\n+        path: Sequence[str | bytes] | None = None,\n+        target: types.ModuleType | None = None,\n+    ) -> importlib.machinery.ModuleSpec | None:\n+        if self._writing_pyc:\n+            return None\n+        state = self.config.stash[assertstate_key]\n+        if self._early_rewrite_bailout(name, state):\n+            return None\n+        state.trace(f\"find_module called for: {name}\")\n+\n+        # Type ignored because mypy is confused about the `self` binding here.\n+        spec = self._find_spec(name, path)  # type: ignore\n+\n+        if spec is None and path is not None:\n+            # With --import-mode=importlib, PathFinder cannot find spec without modifying `sys.path`,\n+            # causing inability to assert rewriting (#12659).\n+            # At this point, try using the file path to find the module spec.\n+            for _path_str in path:\n+                spec = importlib.util.spec_from_file_location(name, _path_str)\n+                if spec is not None:\n+                    break\n+\n+        if (\n+            # the import machinery could not find a file to import\n+            spec is None\n+            # this is a namespace package (without `__init__.py`)\n+            # there's nothing to rewrite there\n+            or spec.origin is None\n+            # we can only rewrite source files\n+            or not isinstance(spec.loader, importlib.machinery.SourceFileLoader)\n+            # if the file doesn't exist, we can't rewrite it\n+            or not os.path.exists(spec.origin)\n+        ):\n+            return None\n+        else:\n+            fn = spec.origin\n+\n+        if not self._should_rewrite(name, fn, state):\n+            return None\n+\n+        return importlib.util.spec_from_file_location(\n+            name,\n+            fn,\n+            loader=self,\n+            submodule_search_locations=spec.submodule_search_locations,\n+        )\n+\n+    def create_module(\n+        self, spec: importlib.machinery.ModuleSpec\n+    ) -> types.ModuleType | None:\n+        return None  # default behaviour is fine\n+\n+    def exec_module(self, module: types.ModuleType) -> None:\n+        assert module.__spec__ is not None\n+        assert module.__spec__.origin is not None\n+        fn = Path(module.__spec__.origin)\n+        state = self.config.stash[assertstate_key]\n+\n+        self._rewritten_names[module.__name__] = fn\n+\n+        # The requested module looks like a test file, so rewrite it. This is\n+        # the most magical part of the process: load the source, rewrite the\n+        # asserts, and load the rewritten source. We also cache the rewritten\n+        # module code in a special pyc. We must be aware of the possibility of\n+        # concurrent pytest processes rewriting and loading pycs. To avoid\n+        # tricky race conditions, we maintain the following invariant: The\n+        # cached pyc is always a complete, valid pyc. Operations on it must be\n+        # atomic. POSIX's atomic rename comes in handy.\n+        write = not sys.dont_write_bytecode\n+        cache_dir = get_cache_dir(fn)\n+        if write:\n+            ok = try_makedirs(cache_dir)\n+            if not ok:\n+                write = False\n+                state.trace(f\"read only directory: {cache_dir}\")\n+\n+        cache_name = fn.name[:-3] + PYC_TAIL\n+        pyc = cache_dir / cache_name\n+        # Notice that even if we're in a read-only directory, I'm going\n+        # to check for a cached pyc. This may not be optimal...\n+        co = _read_pyc(fn, pyc, state.trace)\n+        if co is None:\n+            state.trace(f\"rewriting {fn!r}\")\n+            source_stat, co = _rewrite_test(fn, self.config)\n+            if write:\n+                self._writing_pyc = True\n+                try:\n+                    _write_pyc(state, co, source_stat, pyc)\n+                finally:\n+                    self._writing_pyc = False\n+        else:\n+            state.trace(f\"found cached rewritten pyc for {fn}\")\n+        exec(co, module.__dict__)\n+\n+    def _early_rewrite_bailout(self, name: str, state: AssertionState) -> bool:\n+        \"\"\"A fast way to get out of rewriting modules.\n+\n+        Profiling has shown that the call to PathFinder.find_spec (inside of\n+        the find_spec from this class) is a major slowdown, so, this method\n+        tries to filter what we're sure won't be rewritten before getting to\n+        it.\n+        \"\"\"\n+        if self.session is not None and not self._session_paths_checked:\n+            self._session_paths_checked = True\n+            for initial_path in self.session._initialpaths:\n+                # Make something as c:/projects/my_project/path.py ->\n+                #     ['c:', 'projects', 'my_project', 'path.py']\n+                parts = str(initial_path).split(os.sep)\n+                # add 'path' to basenames to be checked.\n+                self._basenames_to_check_rewrite.add(os.path.splitext(parts[-1])[0])\n+\n+        # Note: conftest already by default in _basenames_to_check_rewrite.\n+        parts = name.split(\".\")\n+        if parts[-1] in self._basenames_to_check_rewrite:\n+            return False\n+\n+        # For matching the name it must be as if it was a filename.\n+        path = PurePath(*parts).with_suffix(\".py\")\n+\n+        for pat in self.fnpats:\n+            # if the pattern contains subdirectories (\"tests/**.py\" for example) we can't bail out based\n+            # on the name alone because we need to match against the full path\n+            if os.path.dirname(pat):\n+                return False\n+            if fnmatch_ex(pat, path):\n+                return False\n+\n+        if self._is_marked_for_rewrite(name, state):\n+            return False\n+\n+        state.trace(f\"early skip of rewriting module: {name}\")\n+        return True\n+\n+    def _should_rewrite(self, name: str, fn: str, state: AssertionState) -> bool:\n+        # always rewrite conftest files\n+        if os.path.basename(fn) == \"conftest.py\":\n+            state.trace(f\"rewriting conftest file: {fn!r}\")\n+            return True\n+\n+        if self.session is not None:\n+            if self.session.isinitpath(absolutepath(fn)):\n+                state.trace(f\"matched test file (was specified on cmdline): {fn!r}\")\n+                return True\n+\n+        # modules not passed explicitly on the command line are only\n+        # rewritten if they match the naming convention for test files\n+        fn_path = PurePath(fn)\n+        for pat in self.fnpats:\n+            if fnmatch_ex(pat, fn_path):\n+                state.trace(f\"matched test file {fn!r}\")\n+                return True\n+\n+        return self._is_marked_for_rewrite(name, state)\n+\n+    def _is_marked_for_rewrite(self, name: str, state: AssertionState) -> bool:\n+        try:\n+            return self._marked_for_rewrite_cache[name]\n+        except KeyError:\n+            for marked in self._must_rewrite:\n+                if name == marked or name.startswith(marked + \".\"):\n+                    state.trace(f\"matched marked file {name!r} (from {marked!r})\")\n+                    self._marked_for_rewrite_cache[name] = True\n+                    return True\n+\n+            self._marked_for_rewrite_cache[name] = False\n+            return False\n+\n+    def mark_rewrite(self, *names: str) -> None:\n+        \"\"\"Mark import names as needing to be rewritten.\n+\n+        The named module or package as well as any nested modules will\n+        be rewritten on import.\n+        \"\"\"\n+        already_imported = (\n+            set(names).intersection(sys.modules).difference(self._rewritten_names)\n+        )\n+        for name in already_imported:\n+            mod = sys.modules[name]\n+            if not AssertionRewriter.is_rewrite_disabled(\n+                mod.__doc__ or \"\"\n+            ) and not isinstance(mod.__loader__, type(self)):\n+                self._warn_already_imported(name)\n+        self._must_rewrite.update(names)\n+        self._marked_for_rewrite_cache.clear()\n+\n+    def _warn_already_imported(self, name: str) -> None:\n+        from _pytest.warning_types import PytestAssertRewriteWarning\n+\n+        self.config.issue_config_time_warning(\n+            PytestAssertRewriteWarning(\n+                f\"Module already imported so cannot be rewritten; {name}\"\n+            ),\n+            stacklevel=5,\n+        )\n+\n+    def get_data(self, pathname: str | bytes) -> bytes:\n+        \"\"\"Optional PEP302 get_data API.\"\"\"\n+        with open(pathname, \"rb\") as f:\n+            return f.read()\n+\n+    if sys.version_info >= (3, 10):\n+        if sys.version_info >= (3, 12):\n+            from importlib.resources.abc import TraversableResources\n+        else:\n+            from importlib.abc import TraversableResources\n+\n+        def get_resource_reader(self, name: str) -> TraversableResources:\n+            if sys.version_info < (3, 11):\n+                from importlib.readers import FileReader\n+            else:\n+                from importlib.resources.readers import FileReader\n+\n+            return FileReader(types.SimpleNamespace(path=self._rewritten_names[name]))\n+\n+\n+def _write_pyc_fp(\n+    fp: IO[bytes], source_stat: os.stat_result, co: types.CodeType\n+) -> None:\n+    # Technically, we don't have to have the same pyc format as\n+    # (C)Python, since these \"pycs\" should never be seen by builtin\n+    # import. However, there's little reason to deviate.\n+    fp.write(importlib.util.MAGIC_NUMBER)\n+    # https://www.python.org/dev/peps/pep-0552/\n+    flags = b\"\\x00\\x00\\x00\\x00\"\n+    fp.write(flags)\n+    # as of now, bytecode header expects 32-bit numbers for size and mtime (#4903)\n+    mtime = int(source_stat.st_mtime) & 0xFFFFFFFF\n+    size = source_stat.st_size & 0xFFFFFFFF\n+    # \"<LL\" stands for 2 unsigned longs, little-endian.\n+    fp.write(struct.pack(\"<LL\", mtime, size))\n+    fp.write(marshal.dumps(co))\n+\n+\n+def _write_pyc(\n+    state: AssertionState,\n+    co: types.CodeType,\n+    source_stat: os.stat_result,\n+    pyc: Path,\n+) -> bool:\n+    proc_pyc = f\"{pyc}.{os.getpid()}\"\n+    try:\n+        with open(proc_pyc, \"wb\") as fp:\n+            _write_pyc_fp(fp, source_stat, co)\n+    except OSError as e:\n+        state.trace(f\"error writing pyc file at {proc_pyc}: errno={e.errno}\")\n+        return False\n+\n+    try:\n+        os.replace(proc_pyc, pyc)\n+    except OSError as e:\n+        state.trace(f\"error writing pyc file at {pyc}: {e}\")\n+        # we ignore any failure to write the cache file\n+        # there are many reasons, permission-denied, pycache dir being a\n+        # file etc.\n+        return False\n+    return True\n+\n+\n+def _rewrite_test(fn: Path, config: Config) -> tuple[os.stat_result, types.CodeType]:\n+    \"\"\"Read and rewrite *fn* and return the code object.\"\"\"\n+    stat = os.stat(fn)\n+    source = fn.read_bytes()\n+    strfn = str(fn)\n+    tree = ast.parse(source, filename=strfn)\n+    rewrite_asserts(tree, source, strfn, config)\n+    co = compile(tree, strfn, \"exec\", dont_inherit=True)\n+    return stat, co\n+\n+\n+def _read_pyc(\n+    source: Path, pyc: Path, trace: Callable[[str], None] = lambda x: None\n+) -> types.CodeType | None:\n+    \"\"\"Possibly read a pytest pyc containing rewritten code.\n+\n+    Return rewritten code if successful or None if not.\n+    \"\"\"\n+    try:\n+        fp = open(pyc, \"rb\")\n+    except OSError:\n+        return None\n+    with fp:\n+        try:\n+            stat_result = os.stat(source)\n+            mtime = int(stat_result.st_mtime)\n+            size = stat_result.st_size\n+            data = fp.read(16)\n+        except OSError as e:\n+            trace(f\"_read_pyc({source}): OSError {e}\")\n+            return None\n+        # Check for invalid or out of date pyc file.\n+        if len(data) != (16):\n+            trace(f\"_read_pyc({source}): invalid pyc (too short)\")\n+            return None\n+        if data[:4] != importlib.util.MAGIC_NUMBER:\n+            trace(f\"_read_pyc({source}): invalid pyc (bad magic number)\")\n+            return None\n+        if data[4:8] != b\"\\x00\\x00\\x00\\x00\":\n+            trace(f\"_read_pyc({source}): invalid pyc (unsupported flags)\")\n+            return None\n+        mtime_data = data[8:12]\n+        if int.from_bytes(mtime_data, \"little\") != mtime & 0xFFFFFFFF:\n+            trace(f\"_read_pyc({source}): out of date\")\n+            return None\n+        size_data = data[12:16]\n+        if int.from_bytes(size_data, \"little\") != size & 0xFFFFFFFF:\n+            trace(f\"_read_pyc({source}): invalid pyc (incorrect size)\")\n+            return None\n+        try:\n+            co = marshal.load(fp)\n+        except Exception as e:\n+            trace(f\"_read_pyc({source}): marshal.load error {e}\")\n+            return None\n+        if not isinstance(co, types.CodeType):\n+            trace(f\"_read_pyc({source}): not a code object\")\n+            return None\n+        return co\n+\n+\n+def rewrite_asserts(\n+    mod: ast.Module,\n+    source: bytes,\n+    module_path: str | None = None,\n+    config: Config | None = None,\n+) -> None:\n+    \"\"\"Rewrite the assert statements in mod.\"\"\"\n+    AssertionRewriter(module_path, config, source).run(mod)\n+\n+\n+def _saferepr(obj: object) -> str:\n+    r\"\"\"Get a safe repr of an object for assertion error messages.\n+\n+    The assertion formatting (util.format_explanation()) requires\n+    newlines to be escaped since they are a special character for it.\n+    Normally assertion.util.format_explanation() does this but for a\n+    custom repr it is possible to contain one of the special escape\n+    sequences, especially '\\n{' and '\\n}' are likely to be present in\n+    JSON reprs.\n+    \"\"\"\n+    if isinstance(obj, types.MethodType):\n+        # for bound methods, skip redundant <bound method ...> information\n+        return obj.__name__\n+\n+    maxsize = _get_maxsize_for_saferepr(util._config)\n+    if not maxsize:\n+        return saferepr_unlimited(obj).replace(\"\\n\", \"\\\\n\")\n+    return saferepr(obj, maxsize=maxsize).replace(\"\\n\", \"\\\\n\")\n+\n+\n+def _get_maxsize_for_saferepr(config: Config | None) -> int | None:\n+    \"\"\"Get `maxsize` configuration for saferepr based on the given config object.\"\"\"\n+    if config is None:\n+        verbosity = 0\n+    else:\n+        verbosity = config.get_verbosity(Config.VERBOSITY_ASSERTIONS)\n+    if verbosity >= 2:\n+        return None\n+    if verbosity >= 1:\n+        return DEFAULT_REPR_MAX_SIZE * 10\n+    return DEFAULT_REPR_MAX_SIZE\n+\n+\n+def _format_assertmsg(obj: object) -> str:\n+    r\"\"\"Format the custom assertion message given.\n+\n+    For strings this simply replaces newlines with '\\n~' so that\n+    util.format_explanation() will preserve them instead of escaping\n+    newlines.  For other objects saferepr() is used first.\n+    \"\"\"\n+    # reprlib appears to have a bug which means that if a string\n+    # contains a newline it gets escaped, however if an object has a\n+    # .__repr__() which contains newlines it does not get escaped.\n+    # However in either case we want to preserve the newline.\n+    replaces = [(\"\\n\", \"\\n~\"), (\"%\", \"%%\")]\n+    if not isinstance(obj, str):\n+        obj = saferepr(obj, _get_maxsize_for_saferepr(util._config))\n+        replaces.append((\"\\\\n\", \"\\n~\"))\n+\n+    for r1, r2 in replaces:\n+        obj = obj.replace(r1, r2)\n+\n+    return obj\n+\n+\n+def _should_repr_global_name(obj: object) -> bool:\n+    if callable(obj):\n+        # For pytest fixtures the __repr__ method provides more information than the function name.\n+        return isinstance(obj, FixtureFunctionDefinition)\n+\n+    try:\n+        return not hasattr(obj, \"__name__\")\n+    except Exception:\n+        return True\n+\n+\n+def _format_boolop(explanations: Iterable[str], is_or: bool) -> str:\n+    explanation = \"(\" + ((is_or and \" or \") or \" and \").join(explanations) + \")\"\n+    return explanation.replace(\"%\", \"%%\")\n+\n+\n+def _call_reprcompare(\n+    ops: Sequence[str],\n+    results: Sequence[bool],\n+    expls: Sequence[str],\n+    each_obj: Sequence[object],\n+) -> str:\n+    for i, res, expl in zip(range(len(ops)), results, expls):\n+        try:\n+            done = not res\n+        except Exception:\n+            done = True\n+        if done:\n+            break\n+    if util._reprcompare is not None:\n+        custom = util._reprcompare(ops[i], each_obj[i], each_obj[i + 1])\n+        if custom is not None:\n+            return custom\n+    return expl\n+\n+\n+def _call_assertion_pass(lineno: int, orig: str, expl: str) -> None:\n+    if util._assertion_pass is not None:\n+        util._assertion_pass(lineno, orig, expl)\n+\n+\n+def _check_if_assertion_pass_impl() -> bool:\n+    \"\"\"Check if any plugins implement the pytest_assertion_pass hook\n+    in order not to generate explanation unnecessarily (might be expensive).\"\"\"\n+    return True if util._assertion_pass else False\n+\n+\n+UNARY_MAP = {ast.Not: \"not %s\", ast.Invert: \"~%s\", ast.USub: \"-%s\", ast.UAdd: \"+%s\"}\n+\n+BINOP_MAP = {\n+    ast.BitOr: \"|\",\n+    ast.BitXor: \"^\",\n+    ast.BitAnd: \"&\",\n+    ast.LShift: \"<<\",\n+    ast.RShift: \">>\",\n+    ast.Add: \"+\",\n+    ast.Sub: \"-\",\n+    ast.Mult: \"*\",\n+    ast.Div: \"/\",\n+    ast.FloorDiv: \"//\",\n+    ast.Mod: \"%%\",  # escaped for string formatting\n+    ast.Eq: \"==\",\n+    ast.NotEq: \"!=\",\n+    ast.Lt: \"<\",\n+    ast.LtE: \"<=\",\n+    ast.Gt: \">\",\n+    ast.GtE: \">=\",\n+    ast.Pow: \"**\",\n+    ast.Is: \"is\",\n+    ast.IsNot: \"is not\",\n+    ast.In: \"in\",\n+    ast.NotIn: \"not in\",\n+    ast.MatMult: \"@\",\n+}\n+\n+\n+def traverse_node(node: ast.AST) -> Iterator[ast.AST]:\n+    \"\"\"Recursively yield node and all its children in depth-first order.\"\"\"\n+    yield node\n+    for child in ast.iter_child_nodes(node):\n+        yield from traverse_node(child)\n+\n+\n+@functools.lru_cache(maxsize=1)\n+def _get_assertion_exprs(src: bytes) -> dict[int, str]:\n+    \"\"\"Return a mapping from {lineno: \"assertion test expression\"}.\"\"\"\n+    ret: dict[int, str] = {}\n+\n+    depth = 0\n+    lines: list[str] = []\n+    assert_lineno: int | None = None\n+    seen_lines: set[int] = set()\n+\n+    def _write_and_reset() -> None:\n+        nonlocal depth, lines, assert_lineno, seen_lines\n+        assert assert_lineno is not None\n+        ret[assert_lineno] = \"\".join(lines).rstrip().rstrip(\"\\\\\")\n+        depth = 0\n+        lines = []\n+        assert_lineno = None\n+        seen_lines = set()\n+\n+    tokens = tokenize.tokenize(io.BytesIO(src).readline)\n+    for tp, source, (lineno, offset), _, line in tokens:\n+        if tp == tokenize.NAME and source == \"assert\":\n+            assert_lineno = lineno\n+        elif assert_lineno is not None:\n+            # keep track of depth for the assert-message `,` lookup\n+            if tp == tokenize.OP and source in \"([{\":\n+                depth += 1\n+            elif tp == tokenize.OP and source in \")]}\":\n+                depth -= 1\n+\n+            if not lines:\n+                lines.append(line[offset:])\n+                seen_lines.add(lineno)\n+            # a non-nested comma separates the expression from the message\n+            elif depth == 0 and tp == tokenize.OP and source == \",\":\n+                # one line assert with message\n+                if lineno in seen_lines and len(lines) == 1:\n+                    offset_in_trimmed = offset + len(lines[-1]) - len(line)\n+                    lines[-1] = lines[-1][:offset_in_trimmed]\n+                # multi-line assert with message\n+                elif lineno in seen_lines:\n+                    lines[-1] = lines[-1][:offset]\n+                # multi line assert with escaped newline before message\n+                else:\n+                    lines.append(line[:offset])\n+                _write_and_reset()\n+            elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n+                _write_and_reset()\n+            elif lines and lineno not in seen_lines:\n+                lines.append(line)\n+                seen_lines.add(lineno)\n+\n+    return ret\n+\n+\n+class AssertionRewriter(ast.NodeVisitor):\n+    \"\"\"Assertion rewriting implementation.\n+\n+    The main entrypoint is to call .run() with an ast.Module instance,\n+    this will then find all the assert statements and rewrite them to\n+    provide intermediate values and a detailed assertion error.  See\n+    http://pybites.blogspot.be/2011/07/behind-scenes-of-pytests-new-assertion.html\n+    for an overview of how this works.\n+\n+    The entry point here is .run() which will iterate over all the\n+    statements in an ast.Module and for each ast.Assert statement it\n+    finds call .visit() with it.  Then .visit_Assert() takes over and\n+    is responsible for creating new ast statements to replace the\n+    original assert statement: it rewrites the test of an assertion\n+    to provide intermediate values and replace it with an if statement\n+    which raises an assertion error with a detailed explanation in\n+    case the expression is false and calls pytest_assertion_pass hook\n+    if expression is true.\n+\n+    For this .visit_Assert() uses the visitor pattern to visit all the\n+    AST nodes of the ast.Assert.test field, each visit call returning\n+    an AST node and the corresponding explanation string.  During this\n+    state is kept in several instance attributes:\n+\n+    :statements: All the AST statements which will replace the assert\n+       statement.\n+\n+    :variables: This is populated by .variable() with each variable\n+       used by the statements so that they can all be set to None at\n+       the end of the statements.\n+\n+    :variable_counter: Counter to create new unique variables needed\n+       by statements.  Variables are created using .variable() and\n+       have the form of \"@py_assert0\".\n+\n+    :expl_stmts: The AST statements which will be executed to get\n+       data from the assertion.  This is the code which will construct\n+       the detailed assertion message that is used in the AssertionError\n+       or for the pytest_assertion_pass hook.\n+\n+    :explanation_specifiers: A dict filled by .explanation_param()\n+       with %-formatting placeholders and their corresponding\n+       expressions to use in the building of an assertion message.\n+       This is used by .pop_format_context() to build a message.\n+\n+    :stack: A stack of the explanation_specifiers dicts maintained by\n+       .push_format_context() and .pop_format_context() which allows\n+       to build another %-formatted string while already building one.\n+\n+    :scope: A tuple containing the current scope used for variables_overwrite.\n+\n+    :variables_overwrite: A dict filled with references to variables\n+       that change value within an assert. This happens when a variable is\n+       reassigned with the walrus operator\n+\n+    This state, except the variables_overwrite,  is reset on every new assert\n+    statement visited and used by the other visitors.\n+    \"\"\"\n+\n+    def __init__(\n+        self, module_path: str | None, config: Config | None, source: bytes\n+    ) -> None:\n+        super().__init__()\n+        self.module_path = module_path\n+        self.config = config\n+        if config is not None:\n+            self.enable_assertion_pass_hook = config.getini(\n+                \"enable_assertion_pass_hook\"\n+            )\n+        else:\n+            self.enable_assertion_pass_hook = False\n+        self.source = source\n+        self.scope: tuple[ast.AST, ...] = ()\n+        self.variables_overwrite: defaultdict[tuple[ast.AST, ...], dict[str, str]] = (\n+            defaultdict(dict)\n+        )\n+\n+    def run(self, mod: ast.Module) -> None:\n+        \"\"\"Find all assert statements in *mod* and rewrite them.\"\"\"\n+        if not mod.body:\n+            # Nothing to do.\n+            return\n+\n+        # We'll insert some special imports at the top of the module, but after any\n+        # docstrings and __future__ imports, so first figure out where that is.\n+        doc = getattr(mod, \"docstring\", None)\n+        expect_docstring = doc is None\n+        if doc is not None and self.is_rewrite_disabled(doc):\n+            return\n+        pos = 0\n+        item = None\n+        for item in mod.body:\n+            if (\n+                expect_docstring\n+                and isinstance(item, ast.Expr)\n+                and isinstance(item.value, ast.Constant)\n+                and isinstance(item.value.value, str)\n+            ):\n+                doc = item.value.value\n+                if self.is_rewrite_disabled(doc):\n+                    return\n+                expect_docstring = False\n+            elif (\n+                isinstance(item, ast.ImportFrom)\n+                and item.level == 0\n+                and item.module == \"__future__\"\n+            ):\n+                pass\n+            else:\n+                break\n+            pos += 1\n+        # Special case: for a decorated function, set the lineno to that of the\n+        # first decorator, not the `def`. Issue #4984.\n+        if isinstance(item, ast.FunctionDef) and item.decorator_list:\n+            lineno = item.decorator_list[0].lineno\n+        else:\n+            lineno = item.lineno\n+        # Now actually insert the special imports.\n+        if sys.version_info >= (3, 10):\n+            aliases = [\n+                ast.alias(\"builtins\", \"@py_builtins\", lineno=lineno, col_offset=0),\n+                ast.alias(\n+                    \"_pytest.assertion.rewrite\",\n+                    \"@pytest_ar\",\n+                    lineno=lineno,\n+                    col_offset=0,\n+                ),\n+            ]\n+        else:\n+            aliases = [\n+                ast.alias(\"builtins\", \"@py_builtins\"),\n+                ast.alias(\"_pytest.assertion.rewrite\", \"@pytest_ar\"),\n+            ]\n+        imports = [\n+            ast.Import([alias], lineno=lineno, col_offset=0) for alias in aliases\n+        ]\n+        mod.body[pos:pos] = imports\n+\n+        # Collect asserts.\n+        self.scope = (mod,)\n+        nodes: list[ast.AST | Sentinel] = [mod]\n+        while nodes:\n+            node = nodes.pop()\n+            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n+                self.scope = tuple((*self.scope, node))\n+                nodes.append(_SCOPE_END_MARKER)\n+            if node == _SCOPE_END_MARKER:\n+                self.scope = self.scope[:-1]\n+                continue\n+            assert isinstance(node, ast.AST)\n+            for name, field in ast.iter_fields(node):\n+                if isinstance(field, list):\n+                    new: list[ast.AST] = []\n+                    for i, child in enumerate(field):\n+                        if isinstance(child, ast.Assert):\n+                            # Transform assert.\n+                            new.extend(self.visit(child))\n+                        else:\n+                            new.append(child)\n+                            if isinstance(child, ast.AST):\n+                                nodes.append(child)\n+                    setattr(node, name, new)\n+                elif (\n+                    isinstance(field, ast.AST)\n+                    # Don't recurse into expressions as they can't contain\n+                    # asserts.\n+                    and not isinstance(field, ast.expr)\n+                ):\n+                    nodes.append(field)\n+\n+    @staticmethod\n+    def is_rewrite_disabled(docstring: str) -> bool:\n+        return \"PYTEST_DONT_REWRITE\" in docstring\n+\n+    def variable(self) -> str:\n+        \"\"\"Get a new variable.\"\"\"\n+        # Use a character invalid in python identifiers to avoid clashing.\n+        name = \"@py_assert\" + str(next(self.variable_counter))\n+        self.variables.append(name)\n+        return name\n+\n+    def assign(self, expr: ast.expr) -> ast.Name:\n+        \"\"\"Give *expr* a name.\"\"\"\n+        name = self.variable()\n+        self.statements.append(ast.Assign([ast.Name(name, ast.Store())], expr))\n+        return ast.copy_location(ast.Name(name, ast.Load()), expr)\n+\n+    def display(self, expr: ast.expr) -> ast.expr:\n+        \"\"\"Call saferepr on the expression.\"\"\"\n+        return self.helper(\"_saferepr\", expr)\n+\n+    def helper(self, name: str, *args: ast.expr) -> ast.expr:\n+        \"\"\"Call a helper in this module.\"\"\"\n+        py_name = ast.Name(\"@pytest_ar\", ast.Load())\n+        attr = ast.Attribute(py_name, name, ast.Load())\n+        return ast.Call(attr, list(args), [])\n+\n+    def builtin(self, name: str) -> ast.Attribute:\n+        \"\"\"Return the builtin called *name*.\"\"\"\n+        builtin_name = ast.Name(\"@py_builtins\", ast.Load())\n+        return ast.Attribute(builtin_name, name, ast.Load())\n+\n+    def explanation_param(self, expr: ast.expr) -> str:\n+        \"\"\"Return a new named %-formatting placeholder for expr.\n+\n+        This creates a %-formatting placeholder for expr in the\n+        current formatting context, e.g. ``%(py0)s``.  The placeholder\n+        and expr are placed in the current format context so that it\n+        can be used on the next call to .pop_format_context().\n+        \"\"\"\n+        specifier = \"py\" + str(next(self.variable_counter))\n+        self.explanation_specifiers[specifier] = expr\n+        return \"%(\" + specifier + \")s\"\n+\n+    def push_format_context(self) -> None:\n+        \"\"\"Create a new formatting context.\n+\n+        The format context is used for when an explanation wants to\n+        have a variable value formatted in the assertion message.  In\n+        this case the value required can be added using\n+        .explanation_param().  Finally .pop_format_context() is used\n+        to format a string of %-formatted values as added by\n+        .explanation_param().\n+        \"\"\"\n+        self.explanation_specifiers: dict[str, ast.expr] = {}\n+        self.stack.append(self.explanation_specifiers)\n+\n+    def pop_format_context(self, expl_expr: ast.expr) -> ast.Name:\n+        \"\"\"Format the %-formatted string with current format context.\n+\n+        The expl_expr should be an str ast.expr instance constructed from\n+        the %-placeholders created by .explanation_param().  This will\n+        add the required code to format said string to .expl_stmts and\n+        return the ast.Name instance of the formatted string.\n+        \"\"\"\n+        current = self.stack.pop()\n+        if self.stack:\n+            self.explanation_specifiers = self.stack[-1]\n+        keys: list[ast.expr | None] = [ast.Constant(key) for key in current.keys()]\n+        format_dict = ast.Dict(keys, list(current.values()))\n+        form = ast.BinOp(expl_expr, ast.Mod(), format_dict)\n+        name = \"@py_format\" + str(next(self.variable_counter))\n+        if self.enable_assertion_pass_hook:\n+            self.format_variables.append(name)\n+        self.expl_stmts.append(ast.Assign([ast.Name(name, ast.Store())], form))\n+        return ast.Name(name, ast.Load())\n+\n+    def generic_visit(self, node: ast.AST) -> tuple[ast.Name, str]:\n+        \"\"\"Handle expressions we don't have custom code for.\"\"\"\n+        assert isinstance(node, ast.expr)\n+        res = self.assign(node)\n+        return res, self.explanation_param(self.display(res))\n+\n+    def visit_Assert(self, assert_: ast.Assert) -> list[ast.stmt]:\n+        \"\"\"Return the AST statements to replace the ast.Assert instance.\n+\n+        This rewrites the test of an assertion to provide\n+        intermediate values and replace it with an if statement which\n+        raises an assertion error with a detailed explanation in case\n+        the expression is false.\n+        \"\"\"\n+        if isinstance(assert_.test, ast.Tuple) and len(assert_.test.elts) >= 1:\n+            import warnings\n+\n+            from _pytest.warning_types import PytestAssertRewriteWarning\n+\n+            # TODO: This assert should not be needed.\n+            assert self.module_path is not None\n+            warnings.warn_explicit(\n+                PytestAssertRewriteWarning(\n+                    \"assertion is always true, perhaps remove parentheses?\"\n+                ),\n+                category=None,\n+                filename=self.module_path,\n+                lineno=assert_.lineno,\n+            )\n+\n+        self.statements: list[ast.stmt] = []\n+        self.variables: list[str] = []\n+        self.variable_counter = itertools.count()\n+\n+        if self.enable_assertion_pass_hook:\n+            self.format_variables: list[str] = []\n+\n+        self.stack: list[dict[str, ast.expr]] = []\n+        self.expl_stmts: list[ast.stmt] = []\n+        self.push_format_context()\n+        # Rewrite assert into a bunch of statements.\n+        top_condition, explanation = self.visit(assert_.test)\n+\n+        negation = ast.UnaryOp(ast.Not(), top_condition)\n+\n+        if self.enable_assertion_pass_hook:  # Experimental pytest_assertion_pass hook\n+            msg = self.pop_format_context(ast.Constant(explanation))\n+\n+            # Failed\n+            if assert_.msg:\n+                assertmsg = self.helper(\"_format_assertmsg\", assert_.msg)\n+                gluestr = \"\\n>assert \"\n+            else:\n+                assertmsg = ast.Constant(\"\")\n+                gluestr = \"assert \"\n+            err_explanation = ast.BinOp(ast.Constant(gluestr), ast.Add(), msg)\n+            err_msg = ast.BinOp(assertmsg, ast.Add(), err_explanation)\n+            err_name = ast.Name(\"AssertionError\", ast.Load())\n+            fmt = self.helper(\"_format_explanation\", err_msg)\n+            exc = ast.Call(err_name, [fmt], [])\n+            raise_ = ast.Raise(exc, None)\n+            statements_fail = []\n+            statements_fail.extend(self.expl_stmts)\n+            statements_fail.append(raise_)\n+\n+            # Passed\n+            fmt_pass = self.helper(\"_format_explanation\", msg)\n+            orig = _get_assertion_exprs(self.source)[assert_.lineno]\n+            hook_call_pass = ast.Expr(\n+                self.helper(\n+                    \"_call_assertion_pass\",\n+                    ast.Constant(assert_.lineno),\n+                    ast.Constant(orig),\n+                    fmt_pass,\n+                )\n+            )\n+            # If any hooks implement assert_pass hook\n+            hook_impl_test = ast.If(\n+                self.helper(\"_check_if_assertion_pass_impl\"),\n+                [*self.expl_stmts, hook_call_pass],\n+                [],\n+            )\n+            statements_pass: list[ast.stmt] = [hook_impl_test]\n+\n+            # Test for assertion condition\n+            main_test = ast.If(negation, statements_fail, statements_pass)\n+            self.statements.append(main_test)\n+            if self.format_variables:\n+                variables: list[ast.expr] = [\n+                    ast.Name(name, ast.Store()) for name in self.format_variables\n+                ]\n+                clear_format = ast.Assign(variables, ast.Constant(None))\n+                self.statements.append(clear_format)\n+\n+        else:  # Original assertion rewriting\n+            # Create failure message.\n+            body = self.expl_stmts\n+            self.statements.append(ast.If(negation, body, []))\n+            if assert_.msg:\n+                assertmsg = self.helper(\"_format_assertmsg\", assert_.msg)\n+                explanation = \"\\n>assert \" + explanation\n+            else:\n+                assertmsg = ast.Constant(\"\")\n+                explanation = \"assert \" + explanation\n+            template = ast.BinOp(assertmsg, ast.Add(), ast.Constant(explanation))\n+            msg = self.pop_format_context(template)\n+            fmt = self.helper(\"_format_explanation\", msg)\n+            err_name = ast.Name(\"AssertionError\", ast.Load())\n+            exc = ast.Call(err_name, [fmt], [])\n+            raise_ = ast.Raise(exc, None)\n+\n+            body.append(raise_)\n+\n+        # Clear temporary variables by setting them to None.\n+        if self.variables:\n+            variables = [ast.Name(name, ast.Store()) for name in self.variables]\n+            clear = ast.Assign(variables, ast.Constant(None))\n+            self.statements.append(clear)\n+        # Fix locations (line numbers/column offsets).\n+        for stmt in self.statements:\n+            for node in traverse_node(stmt):\n+                if getattr(node, \"lineno\", None) is None:\n+                    # apply the assertion location to all generated ast nodes without source location\n+                    # and preserve the location of existing nodes or generated nodes with an correct location.\n+                    ast.copy_location(node, assert_)\n+        return self.statements\n+\n+    def visit_NamedExpr(self, name: ast.NamedExpr) -> tuple[ast.NamedExpr, str]:\n+        # This method handles the 'walrus operator' repr of the target\n+        # name if it's a local variable or _should_repr_global_name()\n+        # thinks it's acceptable.\n+        locs = ast.Call(self.builtin(\"locals\"), [], [])\n+        target_id = name.target.id\n+        inlocs = ast.Compare(ast.Constant(target_id), [ast.In()], [locs])\n+        dorepr = self.helper(\"_should_repr_global_name\", name)\n+        test = ast.BoolOp(ast.Or(), [inlocs, dorepr])\n+        expr = ast.IfExp(test, self.display(name), ast.Constant(target_id))\n+        return name, self.explanation_param(expr)\n+\n+    def visit_Name(self, name: ast.Name) -> tuple[ast.Name, str]:\n+        # Display the repr of the name if it's a local variable or\n+        # _should_repr_global_name() thinks it's acceptable.\n+        locs = ast.Call(self.builtin(\"locals\"), [], [])\n+        inlocs = ast.Compare(ast.Constant(name.id), [ast.In()], [locs])\n+        dorepr = self.helper(\"_should_repr_global_name\", name)\n+        test = ast.BoolOp(ast.Or(), [inlocs, dorepr])\n+        expr = ast.IfExp(test, self.display(name), ast.Constant(name.id))\n+        return name, self.explanation_param(expr)\n+\n+    def visit_BoolOp(self, boolop: ast.BoolOp) -> tuple[ast.Name, str]:\n+        res_var = self.variable()\n+        expl_list = self.assign(ast.List([], ast.Load()))\n+        app = ast.Attribute(expl_list, \"append\", ast.Load())\n+        is_or = int(isinstance(boolop.op, ast.Or))\n+        body = save = self.statements\n+        fail_save = self.expl_stmts\n+        levels = len(boolop.values) - 1\n+        self.push_format_context()\n+        # Process each operand, short-circuiting if needed.\n+        for i, v in enumerate(boolop.values):\n+            if i:\n+                fail_inner: list[ast.stmt] = []\n+                # cond is set in a prior loop iteration below\n+                self.expl_stmts.append(ast.If(cond, fail_inner, []))  # noqa: F821\n+                self.expl_stmts = fail_inner\n+                # Check if the left operand is a ast.NamedExpr and the value has already been visited\n+                if (\n+                    isinstance(v, ast.Compare)\n+                    and isinstance(v.left, ast.NamedExpr)\n+                    and v.left.target.id\n+                    in [\n+                        ast_expr.id\n+                        for ast_expr in boolop.values[:i]\n+                        if hasattr(ast_expr, \"id\")\n+                    ]\n+                ):\n+                    pytest_temp = self.variable()\n+                    self.variables_overwrite[self.scope][v.left.target.id] = v.left  # type:ignore[assignment]\n+                    v.left.target.id = pytest_temp\n+            self.push_format_context()\n+            res, expl = self.visit(v)\n+            body.append(ast.Assign([ast.Name(res_var, ast.Store())], res))\n+            expl_format = self.pop_format_context(ast.Constant(expl))\n+            call = ast.Call(app, [expl_format], [])\n+            self.expl_stmts.append(ast.Expr(call))\n+            if i < levels:\n+                cond: ast.expr = res\n+                if is_or:\n+                    cond = ast.UnaryOp(ast.Not(), cond)\n+                inner: list[ast.stmt] = []\n+                self.statements.append(ast.If(cond, inner, []))\n+                self.statements = body = inner\n+        self.statements = save\n+        self.expl_stmts = fail_save\n+        expl_template = self.helper(\"_format_boolop\", expl_list, ast.Constant(is_or))\n+        expl = self.pop_format_context(expl_template)\n+        return ast.Name(res_var, ast.Load()), self.explanation_param(expl)\n+\n+    def visit_UnaryOp(self, unary: ast.UnaryOp) -> tuple[ast.Name, str]:\n+        pattern = UNARY_MAP[unary.op.__class__]\n+        operand_res, operand_expl = self.visit(unary.operand)\n+        res = self.assign(ast.copy_location(ast.UnaryOp(unary.op, operand_res), unary))\n+        return res, pattern % (operand_expl,)\n+\n+    def visit_BinOp(self, binop: ast.BinOp) -> tuple[ast.Name, str]:\n+        symbol = BINOP_MAP[binop.op.__class__]\n+        left_expr, left_expl = self.visit(binop.left)\n+        right_expr, right_expl = self.visit(binop.right)\n+        explanation = f\"({left_expl} {symbol} {right_expl})\"\n+        res = self.assign(\n+            ast.copy_location(ast.BinOp(left_expr, binop.op, right_expr), binop)\n+        )\n+        return res, explanation\n+\n+    def visit_Call(self, call: ast.Call) -> tuple[ast.Name, str]:\n+        new_func, func_expl = self.visit(call.func)\n+        arg_expls = []\n+        new_args = []\n+        new_kwargs = []\n+        for arg in call.args:\n+            if isinstance(arg, ast.Name) and arg.id in self.variables_overwrite.get(\n+                self.scope, {}\n+            ):\n+                arg = self.variables_overwrite[self.scope][arg.id]  # type:ignore[assignment]\n+            res, expl = self.visit(arg)\n+            arg_expls.append(expl)\n+            new_args.append(res)\n+        for keyword in call.keywords:\n+            if isinstance(\n+                keyword.value, ast.Name\n+            ) and keyword.value.id in self.variables_overwrite.get(self.scope, {}):\n+                keyword.value = self.variables_overwrite[self.scope][keyword.value.id]  # type:ignore[assignment]\n+            res, expl = self.visit(keyword.value)\n+            new_kwargs.append(ast.keyword(keyword.arg, res))\n+            if keyword.arg:\n+                arg_expls.append(keyword.arg + \"=\" + expl)\n+            else:  # **args have `arg` keywords with an .arg of None\n+                arg_expls.append(\"**\" + expl)\n+\n+        expl = \"{}({})\".format(func_expl, \", \".join(arg_expls))\n+        new_call = ast.copy_location(ast.Call(new_func, new_args, new_kwargs), call)\n+        res = self.assign(new_call)\n+        res_expl = self.explanation_param(self.display(res))\n+        outer_expl = f\"{res_expl}\\n{{{res_expl} = {expl}\\n}}\"\n+        return res, outer_expl\n+\n+    def visit_Starred(self, starred: ast.Starred) -> tuple[ast.Starred, str]:\n+        # A Starred node can appear in a function call.\n+        res, expl = self.visit(starred.value)\n+        new_starred = ast.Starred(res, starred.ctx)\n+        return new_starred, \"*\" + expl\n+\n+    def visit_Attribute(self, attr: ast.Attribute) -> tuple[ast.Name, str]:\n+        if not isinstance(attr.ctx, ast.Load):\n+            return self.generic_visit(attr)\n+        value, value_expl = self.visit(attr.value)\n+        res = self.assign(\n+            ast.copy_location(ast.Attribute(value, attr.attr, ast.Load()), attr)\n+        )\n+        res_expl = self.explanation_param(self.display(res))\n+        pat = \"%s\\n{%s = %s.%s\\n}\"\n+        expl = pat % (res_expl, res_expl, value_expl, attr.attr)\n+        return res, expl\n+\n+    def visit_Compare(self, comp: ast.Compare) -> tuple[ast.expr, str]:\n+        self.push_format_context()\n+        # We first check if we have overwritten a variable in the previous assert\n+        if isinstance(\n+            comp.left, ast.Name\n+        ) and comp.left.id in self.variables_overwrite.get(self.scope, {}):\n+            comp.left = self.variables_overwrite[self.scope][comp.left.id]  # type:ignore[assignment]\n+        if isinstance(comp.left, ast.NamedExpr):\n+            self.variables_overwrite[self.scope][comp.left.target.id] = comp.left  # type:ignore[assignment]\n+        left_res, left_expl = self.visit(comp.left)\n+        if isinstance(comp.left, (ast.Compare, ast.BoolOp)):\n+            left_expl = f\"({left_expl})\"\n+        res_variables = [self.variable() for i in range(len(comp.ops))]\n+        load_names: list[ast.expr] = [ast.Name(v, ast.Load()) for v in res_variables]\n+        store_names = [ast.Name(v, ast.Store()) for v in res_variables]\n+        it = zip(range(len(comp.ops)), comp.ops, comp.comparators)\n+        expls: list[ast.expr] = []\n+        syms: list[ast.expr] = []\n+        results = [left_res]\n+        for i, op, next_operand in it:\n+            if (\n+                isinstance(next_operand, ast.NamedExpr)\n+                and isinstance(left_res, ast.Name)\n+                and next_operand.target.id == left_res.id\n+            ):\n+                next_operand.target.id = self.variable()\n+                self.variables_overwrite[self.scope][left_res.id] = next_operand  # type:ignore[assignment]\n+            next_res, next_expl = self.visit(next_operand)\n+            if isinstance(next_operand, (ast.Compare, ast.BoolOp)):\n+                next_expl = f\"({next_expl})\"\n+            results.append(next_res)\n+            sym = BINOP_MAP[op.__class__]\n+            syms.append(ast.Constant(sym))\n+            expl = f\"{left_expl} {sym} {next_expl}\"\n+            expls.append(ast.Constant(expl))\n+            res_expr = ast.copy_location(ast.Compare(left_res, [op], [next_res]), comp)\n+            self.statements.append(ast.Assign([store_names[i]], res_expr))\n+            left_res, left_expl = next_res, next_expl\n+        # Use pytest.assertion.util._reprcompare if that's available.\n+        expl_call = self.helper(\n+            \"_call_reprcompare\",\n+            ast.Tuple(syms, ast.Load()),\n+            ast.Tuple(load_names, ast.Load()),\n+            ast.Tuple(expls, ast.Load()),\n+            ast.Tuple(results, ast.Load()),\n+        )\n+        if len(comp.ops) > 1:\n+            res: ast.expr = ast.BoolOp(ast.And(), load_names)\n+        else:\n+            res = load_names[0]\n+\n+        return res, self.explanation_param(self.pop_format_context(expl_call))\n+\n+\n+def try_makedirs(cache_dir: Path) -> bool:\n+    \"\"\"Attempt to create the given directory and sub-directories exist.\n+\n+    Returns True if successful or if it already exists.\n+    \"\"\"\n+    try:\n+        os.makedirs(cache_dir, exist_ok=True)\n+    except (FileNotFoundError, NotADirectoryError, FileExistsError):\n+        # One of the path components was not a directory:\n+        # - we're in a zip file\n+        # - it is a file\n+        return False\n+    except PermissionError:\n+        return False\n+    except OSError as e:\n+        # as of now, EROFS doesn't have an equivalent OSError-subclass\n+        #\n+        # squashfuse_ll returns ENOSYS \"OSError: [Errno 38] Function not\n+        # implemented\" for a read-only error\n+        if e.errno in {errno.EROFS, errno.ENOSYS}:\n+            return False\n+        raise\n+    return True\n+\n+\n+def get_cache_dir(file_path: Path) -> Path:\n+    \"\"\"Return the cache directory to write .pyc files for the given .py file path.\"\"\"\n+    if sys.pycache_prefix:\n+        # given:\n+        #   prefix = '/tmp/pycs'\n+        #   path = '/home/user/proj/test_app.py'\n+        # we want:\n+        #   '/tmp/pycs/home/user/proj'\n+        return Path(sys.pycache_prefix) / Path(*file_path.parts[1:-1])\n+    else:\n+        # classic pycache directory\n+        return file_path.parent / \"__pycache__\"",
      "patch_lines": [
        "@@ -0,0 +1,1216 @@\n",
        "+\"\"\"Rewrite assertion AST to produce nice error messages.\"\"\"\n",
        "+\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+import ast\n",
        "+from collections import defaultdict\n",
        "+from collections.abc import Callable\n",
        "+from collections.abc import Iterable\n",
        "+from collections.abc import Iterator\n",
        "+from collections.abc import Sequence\n",
        "+import errno\n",
        "+import functools\n",
        "+import importlib.abc\n",
        "+import importlib.machinery\n",
        "+import importlib.util\n",
        "+import io\n",
        "+import itertools\n",
        "+import marshal\n",
        "+import os\n",
        "+from pathlib import Path\n",
        "+from pathlib import PurePath\n",
        "+import struct\n",
        "+import sys\n",
        "+import tokenize\n",
        "+import types\n",
        "+from typing import IO\n",
        "+from typing import TYPE_CHECKING\n",
        "+\n",
        "+from _pytest._io.saferepr import DEFAULT_REPR_MAX_SIZE\n",
        "+from _pytest._io.saferepr import saferepr\n",
        "+from _pytest._io.saferepr import saferepr_unlimited\n",
        "+from _pytest._version import version\n",
        "+from _pytest.assertion import util\n",
        "+from _pytest.config import Config\n",
        "+from _pytest.fixtures import FixtureFunctionDefinition\n",
        "+from _pytest.main import Session\n",
        "+from _pytest.pathlib import absolutepath\n",
        "+from _pytest.pathlib import fnmatch_ex\n",
        "+from _pytest.stash import StashKey\n",
        "+\n",
        "+\n",
        "+# fmt: off\n",
        "+from _pytest.assertion.util import format_explanation as _format_explanation  # noqa:F401, isort:skip\n",
        "+# fmt:on\n",
        "+\n",
        "+if TYPE_CHECKING:\n",
        "+    from _pytest.assertion import AssertionState\n",
        "+\n",
        "+\n",
        "+class Sentinel:\n",
        "+    pass\n",
        "+\n",
        "+\n",
        "+assertstate_key = StashKey[\"AssertionState\"]()\n",
        "+\n",
        "+# pytest caches rewritten pycs in pycache dirs\n",
        "+PYTEST_TAG = f\"{sys.implementation.cache_tag}-pytest-{version}\"\n",
        "+PYC_EXT = \".py\" + ((__debug__ and \"c\") or \"o\")\n",
        "+PYC_TAIL = \".\" + PYTEST_TAG + PYC_EXT\n",
        "+\n",
        "+# Special marker that denotes we have just left a scope definition\n",
        "+_SCOPE_END_MARKER = Sentinel()\n",
        "+\n",
        "+\n",
        "+class AssertionRewritingHook(importlib.abc.MetaPathFinder, importlib.abc.Loader):\n",
        "+    \"\"\"PEP302/PEP451 import hook which rewrites asserts.\"\"\"\n",
        "+\n",
        "+    def __init__(self, config: Config) -> None:\n",
        "+        self.config = config\n",
        "+        try:\n",
        "+            self.fnpats = config.getini(\"python_files\")\n",
        "+        except ValueError:\n",
        "+            self.fnpats = [\"test_*.py\", \"*_test.py\"]\n",
        "+        self.session: Session | None = None\n",
        "+        self._rewritten_names: dict[str, Path] = {}\n",
        "+        self._must_rewrite: set[str] = set()\n",
        "+        # flag to guard against trying to rewrite a pyc file while we are already writing another pyc file,\n",
        "+        # which might result in infinite recursion (#3506)\n",
        "+        self._writing_pyc = False\n",
        "+        self._basenames_to_check_rewrite = {\"conftest\"}\n",
        "+        self._marked_for_rewrite_cache: dict[str, bool] = {}\n",
        "+        self._session_paths_checked = False\n",
        "+\n",
        "+    def set_session(self, session: Session | None) -> None:\n",
        "+        self.session = session\n",
        "+        self._session_paths_checked = False\n",
        "+\n",
        "+    # Indirection so we can mock calls to find_spec originated from the hook during testing\n",
        "+    _find_spec = importlib.machinery.PathFinder.find_spec\n",
        "+\n",
        "+    def find_spec(\n",
        "+        self,\n",
        "+        name: str,\n",
        "+        path: Sequence[str | bytes] | None = None,\n",
        "+        target: types.ModuleType | None = None,\n",
        "+    ) -> importlib.machinery.ModuleSpec | None:\n",
        "+        if self._writing_pyc:\n",
        "+            return None\n",
        "+        state = self.config.stash[assertstate_key]\n",
        "+        if self._early_rewrite_bailout(name, state):\n",
        "+            return None\n",
        "+        state.trace(f\"find_module called for: {name}\")\n",
        "+\n",
        "+        # Type ignored because mypy is confused about the `self` binding here.\n",
        "+        spec = self._find_spec(name, path)  # type: ignore\n",
        "+\n",
        "+        if spec is None and path is not None:\n",
        "+            # With --import-mode=importlib, PathFinder cannot find spec without modifying `sys.path`,\n",
        "+            # causing inability to assert rewriting (#12659).\n",
        "+            # At this point, try using the file path to find the module spec.\n",
        "+            for _path_str in path:\n",
        "+                spec = importlib.util.spec_from_file_location(name, _path_str)\n",
        "+                if spec is not None:\n",
        "+                    break\n",
        "+\n",
        "+        if (\n",
        "+            # the import machinery could not find a file to import\n",
        "+            spec is None\n",
        "+            # this is a namespace package (without `__init__.py`)\n",
        "+            # there's nothing to rewrite there\n",
        "+            or spec.origin is None\n",
        "+            # we can only rewrite source files\n",
        "+            or not isinstance(spec.loader, importlib.machinery.SourceFileLoader)\n",
        "+            # if the file doesn't exist, we can't rewrite it\n",
        "+            or not os.path.exists(spec.origin)\n",
        "+        ):\n",
        "+            return None\n",
        "+        else:\n",
        "+            fn = spec.origin\n",
        "+\n",
        "+        if not self._should_rewrite(name, fn, state):\n",
        "+            return None\n",
        "+\n",
        "+        return importlib.util.spec_from_file_location(\n",
        "+            name,\n",
        "+            fn,\n",
        "+            loader=self,\n",
        "+            submodule_search_locations=spec.submodule_search_locations,\n",
        "+        )\n",
        "+\n",
        "+    def create_module(\n",
        "+        self, spec: importlib.machinery.ModuleSpec\n",
        "+    ) -> types.ModuleType | None:\n",
        "+        return None  # default behaviour is fine\n",
        "+\n",
        "+    def exec_module(self, module: types.ModuleType) -> None:\n",
        "+        assert module.__spec__ is not None\n",
        "+        assert module.__spec__.origin is not None\n",
        "+        fn = Path(module.__spec__.origin)\n",
        "+        state = self.config.stash[assertstate_key]\n",
        "+\n",
        "+        self._rewritten_names[module.__name__] = fn\n",
        "+\n",
        "+        # The requested module looks like a test file, so rewrite it. This is\n",
        "+        # the most magical part of the process: load the source, rewrite the\n",
        "+        # asserts, and load the rewritten source. We also cache the rewritten\n",
        "+        # module code in a special pyc. We must be aware of the possibility of\n",
        "+        # concurrent pytest processes rewriting and loading pycs. To avoid\n",
        "+        # tricky race conditions, we maintain the following invariant: The\n",
        "+        # cached pyc is always a complete, valid pyc. Operations on it must be\n",
        "+        # atomic. POSIX's atomic rename comes in handy.\n",
        "+        write = not sys.dont_write_bytecode\n",
        "+        cache_dir = get_cache_dir(fn)\n",
        "+        if write:\n",
        "+            ok = try_makedirs(cache_dir)\n",
        "+            if not ok:\n",
        "+                write = False\n",
        "+                state.trace(f\"read only directory: {cache_dir}\")\n",
        "+\n",
        "+        cache_name = fn.name[:-3] + PYC_TAIL\n",
        "+        pyc = cache_dir / cache_name\n",
        "+        # Notice that even if we're in a read-only directory, I'm going\n",
        "+        # to check for a cached pyc. This may not be optimal...\n",
        "+        co = _read_pyc(fn, pyc, state.trace)\n",
        "+        if co is None:\n",
        "+            state.trace(f\"rewriting {fn!r}\")\n",
        "+            source_stat, co = _rewrite_test(fn, self.config)\n",
        "+            if write:\n",
        "+                self._writing_pyc = True\n",
        "+                try:\n",
        "+                    _write_pyc(state, co, source_stat, pyc)\n",
        "+                finally:\n",
        "+                    self._writing_pyc = False\n",
        "+        else:\n",
        "+            state.trace(f\"found cached rewritten pyc for {fn}\")\n",
        "+        exec(co, module.__dict__)\n",
        "+\n",
        "+    def _early_rewrite_bailout(self, name: str, state: AssertionState) -> bool:\n",
        "+        \"\"\"A fast way to get out of rewriting modules.\n",
        "+\n",
        "+        Profiling has shown that the call to PathFinder.find_spec (inside of\n",
        "+        the find_spec from this class) is a major slowdown, so, this method\n",
        "+        tries to filter what we're sure won't be rewritten before getting to\n",
        "+        it.\n",
        "+        \"\"\"\n",
        "+        if self.session is not None and not self._session_paths_checked:\n",
        "+            self._session_paths_checked = True\n",
        "+            for initial_path in self.session._initialpaths:\n",
        "+                # Make something as c:/projects/my_project/path.py ->\n",
        "+                #     ['c:', 'projects', 'my_project', 'path.py']\n",
        "+                parts = str(initial_path).split(os.sep)\n",
        "+                # add 'path' to basenames to be checked.\n",
        "+                self._basenames_to_check_rewrite.add(os.path.splitext(parts[-1])[0])\n",
        "+\n",
        "+        # Note: conftest already by default in _basenames_to_check_rewrite.\n",
        "+        parts = name.split(\".\")\n",
        "+        if parts[-1] in self._basenames_to_check_rewrite:\n",
        "+            return False\n",
        "+\n",
        "+        # For matching the name it must be as if it was a filename.\n",
        "+        path = PurePath(*parts).with_suffix(\".py\")\n",
        "+\n",
        "+        for pat in self.fnpats:\n",
        "+            # if the pattern contains subdirectories (\"tests/**.py\" for example) we can't bail out based\n",
        "+            # on the name alone because we need to match against the full path\n",
        "+            if os.path.dirname(pat):\n",
        "+                return False\n",
        "+            if fnmatch_ex(pat, path):\n",
        "+                return False\n",
        "+\n",
        "+        if self._is_marked_for_rewrite(name, state):\n",
        "+            return False\n",
        "+\n",
        "+        state.trace(f\"early skip of rewriting module: {name}\")\n",
        "+        return True\n",
        "+\n",
        "+    def _should_rewrite(self, name: str, fn: str, state: AssertionState) -> bool:\n",
        "+        # always rewrite conftest files\n",
        "+        if os.path.basename(fn) == \"conftest.py\":\n",
        "+            state.trace(f\"rewriting conftest file: {fn!r}\")\n",
        "+            return True\n",
        "+\n",
        "+        if self.session is not None:\n",
        "+            if self.session.isinitpath(absolutepath(fn)):\n",
        "+                state.trace(f\"matched test file (was specified on cmdline): {fn!r}\")\n",
        "+                return True\n",
        "+\n",
        "+        # modules not passed explicitly on the command line are only\n",
        "+        # rewritten if they match the naming convention for test files\n",
        "+        fn_path = PurePath(fn)\n",
        "+        for pat in self.fnpats:\n",
        "+            if fnmatch_ex(pat, fn_path):\n",
        "+                state.trace(f\"matched test file {fn!r}\")\n",
        "+                return True\n",
        "+\n",
        "+        return self._is_marked_for_rewrite(name, state)\n",
        "+\n",
        "+    def _is_marked_for_rewrite(self, name: str, state: AssertionState) -> bool:\n",
        "+        try:\n",
        "+            return self._marked_for_rewrite_cache[name]\n",
        "+        except KeyError:\n",
        "+            for marked in self._must_rewrite:\n",
        "+                if name == marked or name.startswith(marked + \".\"):\n",
        "+                    state.trace(f\"matched marked file {name!r} (from {marked!r})\")\n",
        "+                    self._marked_for_rewrite_cache[name] = True\n",
        "+                    return True\n",
        "+\n",
        "+            self._marked_for_rewrite_cache[name] = False\n",
        "+            return False\n",
        "+\n",
        "+    def mark_rewrite(self, *names: str) -> None:\n",
        "+        \"\"\"Mark import names as needing to be rewritten.\n",
        "+\n",
        "+        The named module or package as well as any nested modules will\n",
        "+        be rewritten on import.\n",
        "+        \"\"\"\n",
        "+        already_imported = (\n",
        "+            set(names).intersection(sys.modules).difference(self._rewritten_names)\n",
        "+        )\n",
        "+        for name in already_imported:\n",
        "+            mod = sys.modules[name]\n",
        "+            if not AssertionRewriter.is_rewrite_disabled(\n",
        "+                mod.__doc__ or \"\"\n",
        "+            ) and not isinstance(mod.__loader__, type(self)):\n",
        "+                self._warn_already_imported(name)\n",
        "+        self._must_rewrite.update(names)\n",
        "+        self._marked_for_rewrite_cache.clear()\n",
        "+\n",
        "+    def _warn_already_imported(self, name: str) -> None:\n",
        "+        from _pytest.warning_types import PytestAssertRewriteWarning\n",
        "+\n",
        "+        self.config.issue_config_time_warning(\n",
        "+            PytestAssertRewriteWarning(\n",
        "+                f\"Module already imported so cannot be rewritten; {name}\"\n",
        "+            ),\n",
        "+            stacklevel=5,\n",
        "+        )\n",
        "+\n",
        "+    def get_data(self, pathname: str | bytes) -> bytes:\n",
        "+        \"\"\"Optional PEP302 get_data API.\"\"\"\n",
        "+        with open(pathname, \"rb\") as f:\n",
        "+            return f.read()\n",
        "+\n",
        "+    if sys.version_info >= (3, 10):\n",
        "+        if sys.version_info >= (3, 12):\n",
        "+            from importlib.resources.abc import TraversableResources\n",
        "+        else:\n",
        "+            from importlib.abc import TraversableResources\n",
        "+\n",
        "+        def get_resource_reader(self, name: str) -> TraversableResources:\n",
        "+            if sys.version_info < (3, 11):\n",
        "+                from importlib.readers import FileReader\n",
        "+            else:\n",
        "+                from importlib.resources.readers import FileReader\n",
        "+\n",
        "+            return FileReader(types.SimpleNamespace(path=self._rewritten_names[name]))\n",
        "+\n",
        "+\n",
        "+def _write_pyc_fp(\n",
        "+    fp: IO[bytes], source_stat: os.stat_result, co: types.CodeType\n",
        "+) -> None:\n",
        "+    # Technically, we don't have to have the same pyc format as\n",
        "+    # (C)Python, since these \"pycs\" should never be seen by builtin\n",
        "+    # import. However, there's little reason to deviate.\n",
        "+    fp.write(importlib.util.MAGIC_NUMBER)\n",
        "+    # https://www.python.org/dev/peps/pep-0552/\n",
        "+    flags = b\"\\x00\\x00\\x00\\x00\"\n",
        "+    fp.write(flags)\n",
        "+    # as of now, bytecode header expects 32-bit numbers for size and mtime (#4903)\n",
        "+    mtime = int(source_stat.st_mtime) & 0xFFFFFFFF\n",
        "+    size = source_stat.st_size & 0xFFFFFFFF\n",
        "+    # \"<LL\" stands for 2 unsigned longs, little-endian.\n",
        "+    fp.write(struct.pack(\"<LL\", mtime, size))\n",
        "+    fp.write(marshal.dumps(co))\n",
        "+\n",
        "+\n",
        "+def _write_pyc(\n",
        "+    state: AssertionState,\n",
        "+    co: types.CodeType,\n",
        "+    source_stat: os.stat_result,\n",
        "+    pyc: Path,\n",
        "+) -> bool:\n",
        "+    proc_pyc = f\"{pyc}.{os.getpid()}\"\n",
        "+    try:\n",
        "+        with open(proc_pyc, \"wb\") as fp:\n",
        "+            _write_pyc_fp(fp, source_stat, co)\n",
        "+    except OSError as e:\n",
        "+        state.trace(f\"error writing pyc file at {proc_pyc}: errno={e.errno}\")\n",
        "+        return False\n",
        "+\n",
        "+    try:\n",
        "+        os.replace(proc_pyc, pyc)\n",
        "+    except OSError as e:\n",
        "+        state.trace(f\"error writing pyc file at {pyc}: {e}\")\n",
        "+        # we ignore any failure to write the cache file\n",
        "+        # there are many reasons, permission-denied, pycache dir being a\n",
        "+        # file etc.\n",
        "+        return False\n",
        "+    return True\n",
        "+\n",
        "+\n",
        "+def _rewrite_test(fn: Path, config: Config) -> tuple[os.stat_result, types.CodeType]:\n",
        "+    \"\"\"Read and rewrite *fn* and return the code object.\"\"\"\n",
        "+    stat = os.stat(fn)\n",
        "+    source = fn.read_bytes()\n",
        "+    strfn = str(fn)\n",
        "+    tree = ast.parse(source, filename=strfn)\n",
        "+    rewrite_asserts(tree, source, strfn, config)\n",
        "+    co = compile(tree, strfn, \"exec\", dont_inherit=True)\n",
        "+    return stat, co\n",
        "+\n",
        "+\n",
        "+def _read_pyc(\n",
        "+    source: Path, pyc: Path, trace: Callable[[str], None] = lambda x: None\n",
        "+) -> types.CodeType | None:\n",
        "+    \"\"\"Possibly read a pytest pyc containing rewritten code.\n",
        "+\n",
        "+    Return rewritten code if successful or None if not.\n",
        "+    \"\"\"\n",
        "+    try:\n",
        "+        fp = open(pyc, \"rb\")\n",
        "+    except OSError:\n",
        "+        return None\n",
        "+    with fp:\n",
        "+        try:\n",
        "+            stat_result = os.stat(source)\n",
        "+            mtime = int(stat_result.st_mtime)\n",
        "+            size = stat_result.st_size\n",
        "+            data = fp.read(16)\n",
        "+        except OSError as e:\n",
        "+            trace(f\"_read_pyc({source}): OSError {e}\")\n",
        "+            return None\n",
        "+        # Check for invalid or out of date pyc file.\n",
        "+        if len(data) != (16):\n",
        "+            trace(f\"_read_pyc({source}): invalid pyc (too short)\")\n",
        "+            return None\n",
        "+        if data[:4] != importlib.util.MAGIC_NUMBER:\n",
        "+            trace(f\"_read_pyc({source}): invalid pyc (bad magic number)\")\n",
        "+            return None\n",
        "+        if data[4:8] != b\"\\x00\\x00\\x00\\x00\":\n",
        "+            trace(f\"_read_pyc({source}): invalid pyc (unsupported flags)\")\n",
        "+            return None\n",
        "+        mtime_data = data[8:12]\n",
        "+        if int.from_bytes(mtime_data, \"little\") != mtime & 0xFFFFFFFF:\n",
        "+            trace(f\"_read_pyc({source}): out of date\")\n",
        "+            return None\n",
        "+        size_data = data[12:16]\n",
        "+        if int.from_bytes(size_data, \"little\") != size & 0xFFFFFFFF:\n",
        "+            trace(f\"_read_pyc({source}): invalid pyc (incorrect size)\")\n",
        "+            return None\n",
        "+        try:\n",
        "+            co = marshal.load(fp)\n",
        "+        except Exception as e:\n",
        "+            trace(f\"_read_pyc({source}): marshal.load error {e}\")\n",
        "+            return None\n",
        "+        if not isinstance(co, types.CodeType):\n",
        "+            trace(f\"_read_pyc({source}): not a code object\")\n",
        "+            return None\n",
        "+        return co\n",
        "+\n",
        "+\n",
        "+def rewrite_asserts(\n",
        "+    mod: ast.Module,\n",
        "+    source: bytes,\n",
        "+    module_path: str | None = None,\n",
        "+    config: Config | None = None,\n",
        "+) -> None:\n",
        "+    \"\"\"Rewrite the assert statements in mod.\"\"\"\n",
        "+    AssertionRewriter(module_path, config, source).run(mod)\n",
        "+\n",
        "+\n",
        "+def _saferepr(obj: object) -> str:\n",
        "+    r\"\"\"Get a safe repr of an object for assertion error messages.\n",
        "+\n",
        "+    The assertion formatting (util.format_explanation()) requires\n",
        "+    newlines to be escaped since they are a special character for it.\n",
        "+    Normally assertion.util.format_explanation() does this but for a\n",
        "+    custom repr it is possible to contain one of the special escape\n",
        "+    sequences, especially '\\n{' and '\\n}' are likely to be present in\n",
        "+    JSON reprs.\n",
        "+    \"\"\"\n",
        "+    if isinstance(obj, types.MethodType):\n",
        "+        # for bound methods, skip redundant <bound method ...> information\n",
        "+        return obj.__name__\n",
        "+\n",
        "+    maxsize = _get_maxsize_for_saferepr(util._config)\n",
        "+    if not maxsize:\n",
        "+        return saferepr_unlimited(obj).replace(\"\\n\", \"\\\\n\")\n",
        "+    return saferepr(obj, maxsize=maxsize).replace(\"\\n\", \"\\\\n\")\n",
        "+\n",
        "+\n",
        "+def _get_maxsize_for_saferepr(config: Config | None) -> int | None:\n",
        "+    \"\"\"Get `maxsize` configuration for saferepr based on the given config object.\"\"\"\n",
        "+    if config is None:\n",
        "+        verbosity = 0\n",
        "+    else:\n",
        "+        verbosity = config.get_verbosity(Config.VERBOSITY_ASSERTIONS)\n",
        "+    if verbosity >= 2:\n",
        "+        return None\n",
        "+    if verbosity >= 1:\n",
        "+        return DEFAULT_REPR_MAX_SIZE * 10\n",
        "+    return DEFAULT_REPR_MAX_SIZE\n",
        "+\n",
        "+\n",
        "+def _format_assertmsg(obj: object) -> str:\n",
        "+    r\"\"\"Format the custom assertion message given.\n",
        "+\n",
        "+    For strings this simply replaces newlines with '\\n~' so that\n",
        "+    util.format_explanation() will preserve them instead of escaping\n",
        "+    newlines.  For other objects saferepr() is used first.\n",
        "+    \"\"\"\n",
        "+    # reprlib appears to have a bug which means that if a string\n",
        "+    # contains a newline it gets escaped, however if an object has a\n",
        "+    # .__repr__() which contains newlines it does not get escaped.\n",
        "+    # However in either case we want to preserve the newline.\n",
        "+    replaces = [(\"\\n\", \"\\n~\"), (\"%\", \"%%\")]\n",
        "+    if not isinstance(obj, str):\n",
        "+        obj = saferepr(obj, _get_maxsize_for_saferepr(util._config))\n",
        "+        replaces.append((\"\\\\n\", \"\\n~\"))\n",
        "+\n",
        "+    for r1, r2 in replaces:\n",
        "+        obj = obj.replace(r1, r2)\n",
        "+\n",
        "+    return obj\n",
        "+\n",
        "+\n",
        "+def _should_repr_global_name(obj: object) -> bool:\n",
        "+    if callable(obj):\n",
        "+        # For pytest fixtures the __repr__ method provides more information than the function name.\n",
        "+        return isinstance(obj, FixtureFunctionDefinition)\n",
        "+\n",
        "+    try:\n",
        "+        return not hasattr(obj, \"__name__\")\n",
        "+    except Exception:\n",
        "+        return True\n",
        "+\n",
        "+\n",
        "+def _format_boolop(explanations: Iterable[str], is_or: bool) -> str:\n",
        "+    explanation = \"(\" + ((is_or and \" or \") or \" and \").join(explanations) + \")\"\n",
        "+    return explanation.replace(\"%\", \"%%\")\n",
        "+\n",
        "+\n",
        "+def _call_reprcompare(\n",
        "+    ops: Sequence[str],\n",
        "+    results: Sequence[bool],\n",
        "+    expls: Sequence[str],\n",
        "+    each_obj: Sequence[object],\n",
        "+) -> str:\n",
        "+    for i, res, expl in zip(range(len(ops)), results, expls):\n",
        "+        try:\n",
        "+            done = not res\n",
        "+        except Exception:\n",
        "+            done = True\n",
        "+        if done:\n",
        "+            break\n",
        "+    if util._reprcompare is not None:\n",
        "+        custom = util._reprcompare(ops[i], each_obj[i], each_obj[i + 1])\n",
        "+        if custom is not None:\n",
        "+            return custom\n",
        "+    return expl\n",
        "+\n",
        "+\n",
        "+def _call_assertion_pass(lineno: int, orig: str, expl: str) -> None:\n",
        "+    if util._assertion_pass is not None:\n",
        "+        util._assertion_pass(lineno, orig, expl)\n",
        "+\n",
        "+\n",
        "+def _check_if_assertion_pass_impl() -> bool:\n",
        "+    \"\"\"Check if any plugins implement the pytest_assertion_pass hook\n",
        "+    in order not to generate explanation unnecessarily (might be expensive).\"\"\"\n",
        "+    return True if util._assertion_pass else False\n",
        "+\n",
        "+\n",
        "+UNARY_MAP = {ast.Not: \"not %s\", ast.Invert: \"~%s\", ast.USub: \"-%s\", ast.UAdd: \"+%s\"}\n",
        "+\n",
        "+BINOP_MAP = {\n",
        "+    ast.BitOr: \"|\",\n",
        "+    ast.BitXor: \"^\",\n",
        "+    ast.BitAnd: \"&\",\n",
        "+    ast.LShift: \"<<\",\n",
        "+    ast.RShift: \">>\",\n",
        "+    ast.Add: \"+\",\n",
        "+    ast.Sub: \"-\",\n",
        "+    ast.Mult: \"*\",\n",
        "+    ast.Div: \"/\",\n",
        "+    ast.FloorDiv: \"//\",\n",
        "+    ast.Mod: \"%%\",  # escaped for string formatting\n",
        "+    ast.Eq: \"==\",\n",
        "+    ast.NotEq: \"!=\",\n",
        "+    ast.Lt: \"<\",\n",
        "+    ast.LtE: \"<=\",\n",
        "+    ast.Gt: \">\",\n",
        "+    ast.GtE: \">=\",\n",
        "+    ast.Pow: \"**\",\n",
        "+    ast.Is: \"is\",\n",
        "+    ast.IsNot: \"is not\",\n",
        "+    ast.In: \"in\",\n",
        "+    ast.NotIn: \"not in\",\n",
        "+    ast.MatMult: \"@\",\n",
        "+}\n",
        "+\n",
        "+\n",
        "+def traverse_node(node: ast.AST) -> Iterator[ast.AST]:\n",
        "+    \"\"\"Recursively yield node and all its children in depth-first order.\"\"\"\n",
        "+    yield node\n",
        "+    for child in ast.iter_child_nodes(node):\n",
        "+        yield from traverse_node(child)\n",
        "+\n",
        "+\n",
        "+@functools.lru_cache(maxsize=1)\n",
        "+def _get_assertion_exprs(src: bytes) -> dict[int, str]:\n",
        "+    \"\"\"Return a mapping from {lineno: \"assertion test expression\"}.\"\"\"\n",
        "+    ret: dict[int, str] = {}\n",
        "+\n",
        "+    depth = 0\n",
        "+    lines: list[str] = []\n",
        "+    assert_lineno: int | None = None\n",
        "+    seen_lines: set[int] = set()\n",
        "+\n",
        "+    def _write_and_reset() -> None:\n",
        "+        nonlocal depth, lines, assert_lineno, seen_lines\n",
        "+        assert assert_lineno is not None\n",
        "+        ret[assert_lineno] = \"\".join(lines).rstrip().rstrip(\"\\\\\")\n",
        "+        depth = 0\n",
        "+        lines = []\n",
        "+        assert_lineno = None\n",
        "+        seen_lines = set()\n",
        "+\n",
        "+    tokens = tokenize.tokenize(io.BytesIO(src).readline)\n",
        "+    for tp, source, (lineno, offset), _, line in tokens:\n",
        "+        if tp == tokenize.NAME and source == \"assert\":\n",
        "+            assert_lineno = lineno\n",
        "+        elif assert_lineno is not None:\n",
        "+            # keep track of depth for the assert-message `,` lookup\n",
        "+            if tp == tokenize.OP and source in \"([{\":\n",
        "+                depth += 1\n",
        "+            elif tp == tokenize.OP and source in \")]}\":\n",
        "+                depth -= 1\n",
        "+\n",
        "+            if not lines:\n",
        "+                lines.append(line[offset:])\n",
        "+                seen_lines.add(lineno)\n",
        "+            # a non-nested comma separates the expression from the message\n",
        "+            elif depth == 0 and tp == tokenize.OP and source == \",\":\n",
        "+                # one line assert with message\n",
        "+                if lineno in seen_lines and len(lines) == 1:\n",
        "+                    offset_in_trimmed = offset + len(lines[-1]) - len(line)\n",
        "+                    lines[-1] = lines[-1][:offset_in_trimmed]\n",
        "+                # multi-line assert with message\n",
        "+                elif lineno in seen_lines:\n",
        "+                    lines[-1] = lines[-1][:offset]\n",
        "+                # multi line assert with escaped newline before message\n",
        "+                else:\n",
        "+                    lines.append(line[:offset])\n",
        "+                _write_and_reset()\n",
        "+            elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n",
        "+                _write_and_reset()\n",
        "+            elif lines and lineno not in seen_lines:\n",
        "+                lines.append(line)\n",
        "+                seen_lines.add(lineno)\n",
        "+\n",
        "+    return ret\n",
        "+\n",
        "+\n",
        "+class AssertionRewriter(ast.NodeVisitor):\n",
        "+    \"\"\"Assertion rewriting implementation.\n",
        "+\n",
        "+    The main entrypoint is to call .run() with an ast.Module instance,\n",
        "+    this will then find all the assert statements and rewrite them to\n",
        "+    provide intermediate values and a detailed assertion error.  See\n",
        "+    http://pybites.blogspot.be/2011/07/behind-scenes-of-pytests-new-assertion.html\n",
        "+    for an overview of how this works.\n",
        "+\n",
        "+    The entry point here is .run() which will iterate over all the\n",
        "+    statements in an ast.Module and for each ast.Assert statement it\n",
        "+    finds call .visit() with it.  Then .visit_Assert() takes over and\n",
        "+    is responsible for creating new ast statements to replace the\n",
        "+    original assert statement: it rewrites the test of an assertion\n",
        "+    to provide intermediate values and replace it with an if statement\n",
        "+    which raises an assertion error with a detailed explanation in\n",
        "+    case the expression is false and calls pytest_assertion_pass hook\n",
        "+    if expression is true.\n",
        "+\n",
        "+    For this .visit_Assert() uses the visitor pattern to visit all the\n",
        "+    AST nodes of the ast.Assert.test field, each visit call returning\n",
        "+    an AST node and the corresponding explanation string.  During this\n",
        "+    state is kept in several instance attributes:\n",
        "+\n",
        "+    :statements: All the AST statements which will replace the assert\n",
        "+       statement.\n",
        "+\n",
        "+    :variables: This is populated by .variable() with each variable\n",
        "+       used by the statements so that they can all be set to None at\n",
        "+       the end of the statements.\n",
        "+\n",
        "+    :variable_counter: Counter to create new unique variables needed\n",
        "+       by statements.  Variables are created using .variable() and\n",
        "+       have the form of \"@py_assert0\".\n",
        "+\n",
        "+    :expl_stmts: The AST statements which will be executed to get\n",
        "+       data from the assertion.  This is the code which will construct\n",
        "+       the detailed assertion message that is used in the AssertionError\n",
        "+       or for the pytest_assertion_pass hook.\n",
        "+\n",
        "+    :explanation_specifiers: A dict filled by .explanation_param()\n",
        "+       with %-formatting placeholders and their corresponding\n",
        "+       expressions to use in the building of an assertion message.\n",
        "+       This is used by .pop_format_context() to build a message.\n",
        "+\n",
        "+    :stack: A stack of the explanation_specifiers dicts maintained by\n",
        "+       .push_format_context() and .pop_format_context() which allows\n",
        "+       to build another %-formatted string while already building one.\n",
        "+\n",
        "+    :scope: A tuple containing the current scope used for variables_overwrite.\n",
        "+\n",
        "+    :variables_overwrite: A dict filled with references to variables\n",
        "+       that change value within an assert. This happens when a variable is\n",
        "+       reassigned with the walrus operator\n",
        "+\n",
        "+    This state, except the variables_overwrite,  is reset on every new assert\n",
        "+    statement visited and used by the other visitors.\n",
        "+    \"\"\"\n",
        "+\n",
        "+    def __init__(\n",
        "+        self, module_path: str | None, config: Config | None, source: bytes\n",
        "+    ) -> None:\n",
        "+        super().__init__()\n",
        "+        self.module_path = module_path\n",
        "+        self.config = config\n",
        "+        if config is not None:\n",
        "+            self.enable_assertion_pass_hook = config.getini(\n",
        "+                \"enable_assertion_pass_hook\"\n",
        "+            )\n",
        "+        else:\n",
        "+            self.enable_assertion_pass_hook = False\n",
        "+        self.source = source\n",
        "+        self.scope: tuple[ast.AST, ...] = ()\n",
        "+        self.variables_overwrite: defaultdict[tuple[ast.AST, ...], dict[str, str]] = (\n",
        "+            defaultdict(dict)\n",
        "+        )\n",
        "+\n",
        "+    def run(self, mod: ast.Module) -> None:\n",
        "+        \"\"\"Find all assert statements in *mod* and rewrite them.\"\"\"\n",
        "+        if not mod.body:\n",
        "+            # Nothing to do.\n",
        "+            return\n",
        "+\n",
        "+        # We'll insert some special imports at the top of the module, but after any\n",
        "+        # docstrings and __future__ imports, so first figure out where that is.\n",
        "+        doc = getattr(mod, \"docstring\", None)\n",
        "+        expect_docstring = doc is None\n",
        "+        if doc is not None and self.is_rewrite_disabled(doc):\n",
        "+            return\n",
        "+        pos = 0\n",
        "+        item = None\n",
        "+        for item in mod.body:\n",
        "+            if (\n",
        "+                expect_docstring\n",
        "+                and isinstance(item, ast.Expr)\n",
        "+                and isinstance(item.value, ast.Constant)\n",
        "+                and isinstance(item.value.value, str)\n",
        "+            ):\n",
        "+                doc = item.value.value\n",
        "+                if self.is_rewrite_disabled(doc):\n",
        "+                    return\n",
        "+                expect_docstring = False\n",
        "+            elif (\n",
        "+                isinstance(item, ast.ImportFrom)\n",
        "+                and item.level == 0\n",
        "+                and item.module == \"__future__\"\n",
        "+            ):\n",
        "+                pass\n",
        "+            else:\n",
        "+                break\n",
        "+            pos += 1\n",
        "+        # Special case: for a decorated function, set the lineno to that of the\n",
        "+        # first decorator, not the `def`. Issue #4984.\n",
        "+        if isinstance(item, ast.FunctionDef) and item.decorator_list:\n",
        "+            lineno = item.decorator_list[0].lineno\n",
        "+        else:\n",
        "+            lineno = item.lineno\n",
        "+        # Now actually insert the special imports.\n",
        "+        if sys.version_info >= (3, 10):\n",
        "+            aliases = [\n",
        "+                ast.alias(\"builtins\", \"@py_builtins\", lineno=lineno, col_offset=0),\n",
        "+                ast.alias(\n",
        "+                    \"_pytest.assertion.rewrite\",\n",
        "+                    \"@pytest_ar\",\n",
        "+                    lineno=lineno,\n",
        "+                    col_offset=0,\n",
        "+                ),\n",
        "+            ]\n",
        "+        else:\n",
        "+            aliases = [\n",
        "+                ast.alias(\"builtins\", \"@py_builtins\"),\n",
        "+                ast.alias(\"_pytest.assertion.rewrite\", \"@pytest_ar\"),\n",
        "+            ]\n",
        "+        imports = [\n",
        "+            ast.Import([alias], lineno=lineno, col_offset=0) for alias in aliases\n",
        "+        ]\n",
        "+        mod.body[pos:pos] = imports\n",
        "+\n",
        "+        # Collect asserts.\n",
        "+        self.scope = (mod,)\n",
        "+        nodes: list[ast.AST | Sentinel] = [mod]\n",
        "+        while nodes:\n",
        "+            node = nodes.pop()\n",
        "+            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n",
        "+                self.scope = tuple((*self.scope, node))\n",
        "+                nodes.append(_SCOPE_END_MARKER)\n",
        "+            if node == _SCOPE_END_MARKER:\n",
        "+                self.scope = self.scope[:-1]\n",
        "+                continue\n",
        "+            assert isinstance(node, ast.AST)\n",
        "+            for name, field in ast.iter_fields(node):\n",
        "+                if isinstance(field, list):\n",
        "+                    new: list[ast.AST] = []\n",
        "+                    for i, child in enumerate(field):\n",
        "+                        if isinstance(child, ast.Assert):\n",
        "+                            # Transform assert.\n",
        "+                            new.extend(self.visit(child))\n",
        "+                        else:\n",
        "+                            new.append(child)\n",
        "+                            if isinstance(child, ast.AST):\n",
        "+                                nodes.append(child)\n",
        "+                    setattr(node, name, new)\n",
        "+                elif (\n",
        "+                    isinstance(field, ast.AST)\n",
        "+                    # Don't recurse into expressions as they can't contain\n",
        "+                    # asserts.\n",
        "+                    and not isinstance(field, ast.expr)\n",
        "+                ):\n",
        "+                    nodes.append(field)\n",
        "+\n",
        "+    @staticmethod\n",
        "+    def is_rewrite_disabled(docstring: str) -> bool:\n",
        "+        return \"PYTEST_DONT_REWRITE\" in docstring\n",
        "+\n",
        "+    def variable(self) -> str:\n",
        "+        \"\"\"Get a new variable.\"\"\"\n",
        "+        # Use a character invalid in python identifiers to avoid clashing.\n",
        "+        name = \"@py_assert\" + str(next(self.variable_counter))\n",
        "+        self.variables.append(name)\n",
        "+        return name\n",
        "+\n",
        "+    def assign(self, expr: ast.expr) -> ast.Name:\n",
        "+        \"\"\"Give *expr* a name.\"\"\"\n",
        "+        name = self.variable()\n",
        "+        self.statements.append(ast.Assign([ast.Name(name, ast.Store())], expr))\n",
        "+        return ast.copy_location(ast.Name(name, ast.Load()), expr)\n",
        "+\n",
        "+    def display(self, expr: ast.expr) -> ast.expr:\n",
        "+        \"\"\"Call saferepr on the expression.\"\"\"\n",
        "+        return self.helper(\"_saferepr\", expr)\n",
        "+\n",
        "+    def helper(self, name: str, *args: ast.expr) -> ast.expr:\n",
        "+        \"\"\"Call a helper in this module.\"\"\"\n",
        "+        py_name = ast.Name(\"@pytest_ar\", ast.Load())\n",
        "+        attr = ast.Attribute(py_name, name, ast.Load())\n",
        "+        return ast.Call(attr, list(args), [])\n",
        "+\n",
        "+    def builtin(self, name: str) -> ast.Attribute:\n",
        "+        \"\"\"Return the builtin called *name*.\"\"\"\n",
        "+        builtin_name = ast.Name(\"@py_builtins\", ast.Load())\n",
        "+        return ast.Attribute(builtin_name, name, ast.Load())\n",
        "+\n",
        "+    def explanation_param(self, expr: ast.expr) -> str:\n",
        "+        \"\"\"Return a new named %-formatting placeholder for expr.\n",
        "+\n",
        "+        This creates a %-formatting placeholder for expr in the\n",
        "+        current formatting context, e.g. ``%(py0)s``.  The placeholder\n",
        "+        and expr are placed in the current format context so that it\n",
        "+        can be used on the next call to .pop_format_context().\n",
        "+        \"\"\"\n",
        "+        specifier = \"py\" + str(next(self.variable_counter))\n",
        "+        self.explanation_specifiers[specifier] = expr\n",
        "+        return \"%(\" + specifier + \")s\"\n",
        "+\n",
        "+    def push_format_context(self) -> None:\n",
        "+        \"\"\"Create a new formatting context.\n",
        "+\n",
        "+        The format context is used for when an explanation wants to\n",
        "+        have a variable value formatted in the assertion message.  In\n",
        "+        this case the value required can be added using\n",
        "+        .explanation_param().  Finally .pop_format_context() is used\n",
        "+        to format a string of %-formatted values as added by\n",
        "+        .explanation_param().\n",
        "+        \"\"\"\n",
        "+        self.explanation_specifiers: dict[str, ast.expr] = {}\n",
        "+        self.stack.append(self.explanation_specifiers)\n",
        "+\n",
        "+    def pop_format_context(self, expl_expr: ast.expr) -> ast.Name:\n",
        "+        \"\"\"Format the %-formatted string with current format context.\n",
        "+\n",
        "+        The expl_expr should be an str ast.expr instance constructed from\n",
        "+        the %-placeholders created by .explanation_param().  This will\n",
        "+        add the required code to format said string to .expl_stmts and\n",
        "+        return the ast.Name instance of the formatted string.\n",
        "+        \"\"\"\n",
        "+        current = self.stack.pop()\n",
        "+        if self.stack:\n",
        "+            self.explanation_specifiers = self.stack[-1]\n",
        "+        keys: list[ast.expr | None] = [ast.Constant(key) for key in current.keys()]\n",
        "+        format_dict = ast.Dict(keys, list(current.values()))\n",
        "+        form = ast.BinOp(expl_expr, ast.Mod(), format_dict)\n",
        "+        name = \"@py_format\" + str(next(self.variable_counter))\n",
        "+        if self.enable_assertion_pass_hook:\n",
        "+            self.format_variables.append(name)\n",
        "+        self.expl_stmts.append(ast.Assign([ast.Name(name, ast.Store())], form))\n",
        "+        return ast.Name(name, ast.Load())\n",
        "+\n",
        "+    def generic_visit(self, node: ast.AST) -> tuple[ast.Name, str]:\n",
        "+        \"\"\"Handle expressions we don't have custom code for.\"\"\"\n",
        "+        assert isinstance(node, ast.expr)\n",
        "+        res = self.assign(node)\n",
        "+        return res, self.explanation_param(self.display(res))\n",
        "+\n",
        "+    def visit_Assert(self, assert_: ast.Assert) -> list[ast.stmt]:\n",
        "+        \"\"\"Return the AST statements to replace the ast.Assert instance.\n",
        "+\n",
        "+        This rewrites the test of an assertion to provide\n",
        "+        intermediate values and replace it with an if statement which\n",
        "+        raises an assertion error with a detailed explanation in case\n",
        "+        the expression is false.\n",
        "+        \"\"\"\n",
        "+        if isinstance(assert_.test, ast.Tuple) and len(assert_.test.elts) >= 1:\n",
        "+            import warnings\n",
        "+\n",
        "+            from _pytest.warning_types import PytestAssertRewriteWarning\n",
        "+\n",
        "+            # TODO: This assert should not be needed.\n",
        "+            assert self.module_path is not None\n",
        "+            warnings.warn_explicit(\n",
        "+                PytestAssertRewriteWarning(\n",
        "+                    \"assertion is always true, perhaps remove parentheses?\"\n",
        "+                ),\n",
        "+                category=None,\n",
        "+                filename=self.module_path,\n",
        "+                lineno=assert_.lineno,\n",
        "+            )\n",
        "+\n",
        "+        self.statements: list[ast.stmt] = []\n",
        "+        self.variables: list[str] = []\n",
        "+        self.variable_counter = itertools.count()\n",
        "+\n",
        "+        if self.enable_assertion_pass_hook:\n",
        "+            self.format_variables: list[str] = []\n",
        "+\n",
        "+        self.stack: list[dict[str, ast.expr]] = []\n",
        "+        self.expl_stmts: list[ast.stmt] = []\n",
        "+        self.push_format_context()\n",
        "+        # Rewrite assert into a bunch of statements.\n",
        "+        top_condition, explanation = self.visit(assert_.test)\n",
        "+\n",
        "+        negation = ast.UnaryOp(ast.Not(), top_condition)\n",
        "+\n",
        "+        if self.enable_assertion_pass_hook:  # Experimental pytest_assertion_pass hook\n",
        "+            msg = self.pop_format_context(ast.Constant(explanation))\n",
        "+\n",
        "+            # Failed\n",
        "+            if assert_.msg:\n",
        "+                assertmsg = self.helper(\"_format_assertmsg\", assert_.msg)\n",
        "+                gluestr = \"\\n>assert \"\n",
        "+            else:\n",
        "+                assertmsg = ast.Constant(\"\")\n",
        "+                gluestr = \"assert \"\n",
        "+            err_explanation = ast.BinOp(ast.Constant(gluestr), ast.Add(), msg)\n",
        "+            err_msg = ast.BinOp(assertmsg, ast.Add(), err_explanation)\n",
        "+            err_name = ast.Name(\"AssertionError\", ast.Load())\n",
        "+            fmt = self.helper(\"_format_explanation\", err_msg)\n",
        "+            exc = ast.Call(err_name, [fmt], [])\n",
        "+            raise_ = ast.Raise(exc, None)\n",
        "+            statements_fail = []\n",
        "+            statements_fail.extend(self.expl_stmts)\n",
        "+            statements_fail.append(raise_)\n",
        "+\n",
        "+            # Passed\n",
        "+            fmt_pass = self.helper(\"_format_explanation\", msg)\n",
        "+            orig = _get_assertion_exprs(self.source)[assert_.lineno]\n",
        "+            hook_call_pass = ast.Expr(\n",
        "+                self.helper(\n",
        "+                    \"_call_assertion_pass\",\n",
        "+                    ast.Constant(assert_.lineno),\n",
        "+                    ast.Constant(orig),\n",
        "+                    fmt_pass,\n",
        "+                )\n",
        "+            )\n",
        "+            # If any hooks implement assert_pass hook\n",
        "+            hook_impl_test = ast.If(\n",
        "+                self.helper(\"_check_if_assertion_pass_impl\"),\n",
        "+                [*self.expl_stmts, hook_call_pass],\n",
        "+                [],\n",
        "+            )\n",
        "+            statements_pass: list[ast.stmt] = [hook_impl_test]\n",
        "+\n",
        "+            # Test for assertion condition\n",
        "+            main_test = ast.If(negation, statements_fail, statements_pass)\n",
        "+            self.statements.append(main_test)\n",
        "+            if self.format_variables:\n",
        "+                variables: list[ast.expr] = [\n",
        "+                    ast.Name(name, ast.Store()) for name in self.format_variables\n",
        "+                ]\n",
        "+                clear_format = ast.Assign(variables, ast.Constant(None))\n",
        "+                self.statements.append(clear_format)\n",
        "+\n",
        "+        else:  # Original assertion rewriting\n",
        "+            # Create failure message.\n",
        "+            body = self.expl_stmts\n",
        "+            self.statements.append(ast.If(negation, body, []))\n",
        "+            if assert_.msg:\n",
        "+                assertmsg = self.helper(\"_format_assertmsg\", assert_.msg)\n",
        "+                explanation = \"\\n>assert \" + explanation\n",
        "+            else:\n",
        "+                assertmsg = ast.Constant(\"\")\n",
        "+                explanation = \"assert \" + explanation\n",
        "+            template = ast.BinOp(assertmsg, ast.Add(), ast.Constant(explanation))\n",
        "+            msg = self.pop_format_context(template)\n",
        "+            fmt = self.helper(\"_format_explanation\", msg)\n",
        "+            err_name = ast.Name(\"AssertionError\", ast.Load())\n",
        "+            exc = ast.Call(err_name, [fmt], [])\n",
        "+            raise_ = ast.Raise(exc, None)\n",
        "+\n",
        "+            body.append(raise_)\n",
        "+\n",
        "+        # Clear temporary variables by setting them to None.\n",
        "+        if self.variables:\n",
        "+            variables = [ast.Name(name, ast.Store()) for name in self.variables]\n",
        "+            clear = ast.Assign(variables, ast.Constant(None))\n",
        "+            self.statements.append(clear)\n",
        "+        # Fix locations (line numbers/column offsets).\n",
        "+        for stmt in self.statements:\n",
        "+            for node in traverse_node(stmt):\n",
        "+                if getattr(node, \"lineno\", None) is None:\n",
        "+                    # apply the assertion location to all generated ast nodes without source location\n",
        "+                    # and preserve the location of existing nodes or generated nodes with an correct location.\n",
        "+                    ast.copy_location(node, assert_)\n",
        "+        return self.statements\n",
        "+\n",
        "+    def visit_NamedExpr(self, name: ast.NamedExpr) -> tuple[ast.NamedExpr, str]:\n",
        "+        # This method handles the 'walrus operator' repr of the target\n",
        "+        # name if it's a local variable or _should_repr_global_name()\n",
        "+        # thinks it's acceptable.\n",
        "+        locs = ast.Call(self.builtin(\"locals\"), [], [])\n",
        "+        target_id = name.target.id\n",
        "+        inlocs = ast.Compare(ast.Constant(target_id), [ast.In()], [locs])\n",
        "+        dorepr = self.helper(\"_should_repr_global_name\", name)\n",
        "+        test = ast.BoolOp(ast.Or(), [inlocs, dorepr])\n",
        "+        expr = ast.IfExp(test, self.display(name), ast.Constant(target_id))\n",
        "+        return name, self.explanation_param(expr)\n",
        "+\n",
        "+    def visit_Name(self, name: ast.Name) -> tuple[ast.Name, str]:\n",
        "+        # Display the repr of the name if it's a local variable or\n",
        "+        # _should_repr_global_name() thinks it's acceptable.\n",
        "+        locs = ast.Call(self.builtin(\"locals\"), [], [])\n",
        "+        inlocs = ast.Compare(ast.Constant(name.id), [ast.In()], [locs])\n",
        "+        dorepr = self.helper(\"_should_repr_global_name\", name)\n",
        "+        test = ast.BoolOp(ast.Or(), [inlocs, dorepr])\n",
        "+        expr = ast.IfExp(test, self.display(name), ast.Constant(name.id))\n",
        "+        return name, self.explanation_param(expr)\n",
        "+\n",
        "+    def visit_BoolOp(self, boolop: ast.BoolOp) -> tuple[ast.Name, str]:\n",
        "+        res_var = self.variable()\n",
        "+        expl_list = self.assign(ast.List([], ast.Load()))\n",
        "+        app = ast.Attribute(expl_list, \"append\", ast.Load())\n",
        "+        is_or = int(isinstance(boolop.op, ast.Or))\n",
        "+        body = save = self.statements\n",
        "+        fail_save = self.expl_stmts\n",
        "+        levels = len(boolop.values) - 1\n",
        "+        self.push_format_context()\n",
        "+        # Process each operand, short-circuiting if needed.\n",
        "+        for i, v in enumerate(boolop.values):\n",
        "+            if i:\n",
        "+                fail_inner: list[ast.stmt] = []\n",
        "+                # cond is set in a prior loop iteration below\n",
        "+                self.expl_stmts.append(ast.If(cond, fail_inner, []))  # noqa: F821\n",
        "+                self.expl_stmts = fail_inner\n",
        "+                # Check if the left operand is a ast.NamedExpr and the value has already been visited\n",
        "+                if (\n",
        "+                    isinstance(v, ast.Compare)\n",
        "+                    and isinstance(v.left, ast.NamedExpr)\n",
        "+                    and v.left.target.id\n",
        "+                    in [\n",
        "+                        ast_expr.id\n",
        "+                        for ast_expr in boolop.values[:i]\n",
        "+                        if hasattr(ast_expr, \"id\")\n",
        "+                    ]\n",
        "+                ):\n",
        "+                    pytest_temp = self.variable()\n",
        "+                    self.variables_overwrite[self.scope][v.left.target.id] = v.left  # type:ignore[assignment]\n",
        "+                    v.left.target.id = pytest_temp\n",
        "+            self.push_format_context()\n",
        "+            res, expl = self.visit(v)\n",
        "+            body.append(ast.Assign([ast.Name(res_var, ast.Store())], res))\n",
        "+            expl_format = self.pop_format_context(ast.Constant(expl))\n",
        "+            call = ast.Call(app, [expl_format], [])\n",
        "+            self.expl_stmts.append(ast.Expr(call))\n",
        "+            if i < levels:\n",
        "+                cond: ast.expr = res\n",
        "+                if is_or:\n",
        "+                    cond = ast.UnaryOp(ast.Not(), cond)\n",
        "+                inner: list[ast.stmt] = []\n",
        "+                self.statements.append(ast.If(cond, inner, []))\n",
        "+                self.statements = body = inner\n",
        "+        self.statements = save\n",
        "+        self.expl_stmts = fail_save\n",
        "+        expl_template = self.helper(\"_format_boolop\", expl_list, ast.Constant(is_or))\n",
        "+        expl = self.pop_format_context(expl_template)\n",
        "+        return ast.Name(res_var, ast.Load()), self.explanation_param(expl)\n",
        "+\n",
        "+    def visit_UnaryOp(self, unary: ast.UnaryOp) -> tuple[ast.Name, str]:\n",
        "+        pattern = UNARY_MAP[unary.op.__class__]\n",
        "+        operand_res, operand_expl = self.visit(unary.operand)\n",
        "+        res = self.assign(ast.copy_location(ast.UnaryOp(unary.op, operand_res), unary))\n",
        "+        return res, pattern % (operand_expl,)\n",
        "+\n",
        "+    def visit_BinOp(self, binop: ast.BinOp) -> tuple[ast.Name, str]:\n",
        "+        symbol = BINOP_MAP[binop.op.__class__]\n",
        "+        left_expr, left_expl = self.visit(binop.left)\n",
        "+        right_expr, right_expl = self.visit(binop.right)\n",
        "+        explanation = f\"({left_expl} {symbol} {right_expl})\"\n",
        "+        res = self.assign(\n",
        "+            ast.copy_location(ast.BinOp(left_expr, binop.op, right_expr), binop)\n",
        "+        )\n",
        "+        return res, explanation\n",
        "+\n",
        "+    def visit_Call(self, call: ast.Call) -> tuple[ast.Name, str]:\n",
        "+        new_func, func_expl = self.visit(call.func)\n",
        "+        arg_expls = []\n",
        "+        new_args = []\n",
        "+        new_kwargs = []\n",
        "+        for arg in call.args:\n",
        "+            if isinstance(arg, ast.Name) and arg.id in self.variables_overwrite.get(\n",
        "+                self.scope, {}\n",
        "+            ):\n",
        "+                arg = self.variables_overwrite[self.scope][arg.id]  # type:ignore[assignment]\n",
        "+            res, expl = self.visit(arg)\n",
        "+            arg_expls.append(expl)\n",
        "+            new_args.append(res)\n",
        "+        for keyword in call.keywords:\n",
        "+            if isinstance(\n",
        "+                keyword.value, ast.Name\n",
        "+            ) and keyword.value.id in self.variables_overwrite.get(self.scope, {}):\n",
        "+                keyword.value = self.variables_overwrite[self.scope][keyword.value.id]  # type:ignore[assignment]\n",
        "+            res, expl = self.visit(keyword.value)\n",
        "+            new_kwargs.append(ast.keyword(keyword.arg, res))\n",
        "+            if keyword.arg:\n",
        "+                arg_expls.append(keyword.arg + \"=\" + expl)\n",
        "+            else:  # **args have `arg` keywords with an .arg of None\n",
        "+                arg_expls.append(\"**\" + expl)\n",
        "+\n",
        "+        expl = \"{}({})\".format(func_expl, \", \".join(arg_expls))\n",
        "+        new_call = ast.copy_location(ast.Call(new_func, new_args, new_kwargs), call)\n",
        "+        res = self.assign(new_call)\n",
        "+        res_expl = self.explanation_param(self.display(res))\n",
        "+        outer_expl = f\"{res_expl}\\n{{{res_expl} = {expl}\\n}}\"\n",
        "+        return res, outer_expl\n",
        "+\n",
        "+    def visit_Starred(self, starred: ast.Starred) -> tuple[ast.Starred, str]:\n",
        "+        # A Starred node can appear in a function call.\n",
        "+        res, expl = self.visit(starred.value)\n",
        "+        new_starred = ast.Starred(res, starred.ctx)\n",
        "+        return new_starred, \"*\" + expl\n",
        "+\n",
        "+    def visit_Attribute(self, attr: ast.Attribute) -> tuple[ast.Name, str]:\n",
        "+        if not isinstance(attr.ctx, ast.Load):\n",
        "+            return self.generic_visit(attr)\n",
        "+        value, value_expl = self.visit(attr.value)\n",
        "+        res = self.assign(\n",
        "+            ast.copy_location(ast.Attribute(value, attr.attr, ast.Load()), attr)\n",
        "+        )\n",
        "+        res_expl = self.explanation_param(self.display(res))\n",
        "+        pat = \"%s\\n{%s = %s.%s\\n}\"\n",
        "+        expl = pat % (res_expl, res_expl, value_expl, attr.attr)\n",
        "+        return res, expl\n",
        "+\n",
        "+    def visit_Compare(self, comp: ast.Compare) -> tuple[ast.expr, str]:\n",
        "+        self.push_format_context()\n",
        "+        # We first check if we have overwritten a variable in the previous assert\n",
        "+        if isinstance(\n",
        "+            comp.left, ast.Name\n",
        "+        ) and comp.left.id in self.variables_overwrite.get(self.scope, {}):\n",
        "+            comp.left = self.variables_overwrite[self.scope][comp.left.id]  # type:ignore[assignment]\n",
        "+        if isinstance(comp.left, ast.NamedExpr):\n",
        "+            self.variables_overwrite[self.scope][comp.left.target.id] = comp.left  # type:ignore[assignment]\n",
        "+        left_res, left_expl = self.visit(comp.left)\n",
        "+        if isinstance(comp.left, (ast.Compare, ast.BoolOp)):\n",
        "+            left_expl = f\"({left_expl})\"\n",
        "+        res_variables = [self.variable() for i in range(len(comp.ops))]\n",
        "+        load_names: list[ast.expr] = [ast.Name(v, ast.Load()) for v in res_variables]\n",
        "+        store_names = [ast.Name(v, ast.Store()) for v in res_variables]\n",
        "+        it = zip(range(len(comp.ops)), comp.ops, comp.comparators)\n",
        "+        expls: list[ast.expr] = []\n",
        "+        syms: list[ast.expr] = []\n",
        "+        results = [left_res]\n",
        "+        for i, op, next_operand in it:\n",
        "+            if (\n",
        "+                isinstance(next_operand, ast.NamedExpr)\n",
        "+                and isinstance(left_res, ast.Name)\n",
        "+                and next_operand.target.id == left_res.id\n",
        "+            ):\n",
        "+                next_operand.target.id = self.variable()\n",
        "+                self.variables_overwrite[self.scope][left_res.id] = next_operand  # type:ignore[assignment]\n",
        "+            next_res, next_expl = self.visit(next_operand)\n",
        "+            if isinstance(next_operand, (ast.Compare, ast.BoolOp)):\n",
        "+                next_expl = f\"({next_expl})\"\n",
        "+            results.append(next_res)\n",
        "+            sym = BINOP_MAP[op.__class__]\n",
        "+            syms.append(ast.Constant(sym))\n",
        "+            expl = f\"{left_expl} {sym} {next_expl}\"\n",
        "+            expls.append(ast.Constant(expl))\n",
        "+            res_expr = ast.copy_location(ast.Compare(left_res, [op], [next_res]), comp)\n",
        "+            self.statements.append(ast.Assign([store_names[i]], res_expr))\n",
        "+            left_res, left_expl = next_res, next_expl\n",
        "+        # Use pytest.assertion.util._reprcompare if that's available.\n",
        "+        expl_call = self.helper(\n",
        "+            \"_call_reprcompare\",\n",
        "+            ast.Tuple(syms, ast.Load()),\n",
        "+            ast.Tuple(load_names, ast.Load()),\n",
        "+            ast.Tuple(expls, ast.Load()),\n",
        "+            ast.Tuple(results, ast.Load()),\n",
        "+        )\n",
        "+        if len(comp.ops) > 1:\n",
        "+            res: ast.expr = ast.BoolOp(ast.And(), load_names)\n",
        "+        else:\n",
        "+            res = load_names[0]\n",
        "+\n",
        "+        return res, self.explanation_param(self.pop_format_context(expl_call))\n",
        "+\n",
        "+\n",
        "+def try_makedirs(cache_dir: Path) -> bool:\n",
        "+    \"\"\"Attempt to create the given directory and sub-directories exist.\n",
        "+\n",
        "+    Returns True if successful or if it already exists.\n",
        "+    \"\"\"\n",
        "+    try:\n",
        "+        os.makedirs(cache_dir, exist_ok=True)\n",
        "+    except (FileNotFoundError, NotADirectoryError, FileExistsError):\n",
        "+        # One of the path components was not a directory:\n",
        "+        # - we're in a zip file\n",
        "+        # - it is a file\n",
        "+        return False\n",
        "+    except PermissionError:\n",
        "+        return False\n",
        "+    except OSError as e:\n",
        "+        # as of now, EROFS doesn't have an equivalent OSError-subclass\n",
        "+        #\n",
        "+        # squashfuse_ll returns ENOSYS \"OSError: [Errno 38] Function not\n",
        "+        # implemented\" for a read-only error\n",
        "+        if e.errno in {errno.EROFS, errno.ENOSYS}:\n",
        "+            return False\n",
        "+        raise\n",
        "+    return True\n",
        "+\n",
        "+\n",
        "+def get_cache_dir(file_path: Path) -> Path:\n",
        "+    \"\"\"Return the cache directory to write .pyc files for the given .py file path.\"\"\"\n",
        "+    if sys.pycache_prefix:\n",
        "+        # given:\n",
        "+        #   prefix = '/tmp/pycs'\n",
        "+        #   path = '/home/user/proj/test_app.py'\n",
        "+        # we want:\n",
        "+        #   '/tmp/pycs/home/user/proj'\n",
        "+        return Path(sys.pycache_prefix) / Path(*file_path.parts[1:-1])\n",
        "+    else:\n",
        "+        # classic pycache directory\n",
        "+        return file_path.parent / \"__pycache__\"\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/assertion/truncate.py",
      "status": "added",
      "additions": 137,
      "deletions": 0,
      "patch": "@@ -0,0 +1,137 @@\n+\"\"\"Utilities for truncating assertion output.\n+\n+Current default behaviour is to truncate assertion explanations at\n+terminal lines, unless running with an assertions verbosity level of at least 2 or running on CI.\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from _pytest.assertion import util\n+from _pytest.config import Config\n+from _pytest.nodes import Item\n+\n+\n+DEFAULT_MAX_LINES = 8\n+DEFAULT_MAX_CHARS = DEFAULT_MAX_LINES * 80\n+USAGE_MSG = \"use '-vv' to show\"\n+\n+\n+def truncate_if_required(explanation: list[str], item: Item) -> list[str]:\n+    \"\"\"Truncate this assertion explanation if the given test item is eligible.\"\"\"\n+    should_truncate, max_lines, max_chars = _get_truncation_parameters(item)\n+    if should_truncate:\n+        return _truncate_explanation(\n+            explanation,\n+            max_lines=max_lines,\n+            max_chars=max_chars,\n+        )\n+    return explanation\n+\n+\n+def _get_truncation_parameters(item: Item) -> tuple[bool, int, int]:\n+    \"\"\"Return the truncation parameters related to the given item, as (should truncate, max lines, max chars).\"\"\"\n+    # We do not need to truncate if one of conditions is met:\n+    # 1. Verbosity level is 2 or more;\n+    # 2. Test is being run in CI environment;\n+    # 3. Both truncation_limit_lines and truncation_limit_chars\n+    #    .ini parameters are set to 0 explicitly.\n+    max_lines = item.config.getini(\"truncation_limit_lines\")\n+    max_lines = int(max_lines if max_lines is not None else DEFAULT_MAX_LINES)\n+\n+    max_chars = item.config.getini(\"truncation_limit_chars\")\n+    max_chars = int(max_chars if max_chars is not None else DEFAULT_MAX_CHARS)\n+\n+    verbose = item.config.get_verbosity(Config.VERBOSITY_ASSERTIONS)\n+\n+    should_truncate = verbose < 2 and not util.running_on_ci()\n+    should_truncate = should_truncate and (max_lines > 0 or max_chars > 0)\n+\n+    return should_truncate, max_lines, max_chars\n+\n+\n+def _truncate_explanation(\n+    input_lines: list[str],\n+    max_lines: int,\n+    max_chars: int,\n+) -> list[str]:\n+    \"\"\"Truncate given list of strings that makes up the assertion explanation.\n+\n+    Truncates to either max_lines, or max_chars - whichever the input reaches\n+    first, taking the truncation explanation into account. The remaining lines\n+    will be replaced by a usage message.\n+    \"\"\"\n+    # Check if truncation required\n+    input_char_count = len(\"\".join(input_lines))\n+    # The length of the truncation explanation depends on the number of lines\n+    # removed but is at least 68 characters:\n+    # The real value is\n+    # 64 (for the base message:\n+    # '...\\n...Full output truncated (1 line hidden), use '-vv' to show\")'\n+    # )\n+    # + 1 (for plural)\n+    # + int(math.log10(len(input_lines) - max_lines)) (number of hidden line, at least 1)\n+    # + 3 for the '...' added to the truncated line\n+    # But if there's more than 100 lines it's very likely that we're going to\n+    # truncate, so we don't need the exact value using log10.\n+    tolerable_max_chars = (\n+        max_chars + 70  # 64 + 1 (for plural) + 2 (for '99') + 3 for '...'\n+    )\n+    # The truncation explanation add two lines to the output\n+    tolerable_max_lines = max_lines + 2\n+    if (\n+        len(input_lines) <= tolerable_max_lines\n+        and input_char_count <= tolerable_max_chars\n+    ):\n+        return input_lines\n+    # Truncate first to max_lines, and then truncate to max_chars if necessary\n+    if max_lines > 0:\n+        truncated_explanation = input_lines[:max_lines]\n+    else:\n+        truncated_explanation = input_lines\n+    truncated_char = True\n+    # We reevaluate the need to truncate chars following removal of some lines\n+    if len(\"\".join(truncated_explanation)) > tolerable_max_chars and max_chars > 0:\n+        truncated_explanation = _truncate_by_char_count(\n+            truncated_explanation, max_chars\n+        )\n+    else:\n+        truncated_char = False\n+\n+    if truncated_explanation == input_lines:\n+        # No truncation happened, so we do not need to add any explanations\n+        return truncated_explanation\n+\n+    truncated_line_count = len(input_lines) - len(truncated_explanation)\n+    if truncated_explanation[-1]:\n+        # Add ellipsis and take into account part-truncated final line\n+        truncated_explanation[-1] = truncated_explanation[-1] + \"...\"\n+        if truncated_char:\n+            # It's possible that we did not remove any char from this line\n+            truncated_line_count += 1\n+    else:\n+        # Add proper ellipsis when we were able to fit a full line exactly\n+        truncated_explanation[-1] = \"...\"\n+    return [\n+        *truncated_explanation,\n+        \"\",\n+        f\"...Full output truncated ({truncated_line_count} line\"\n+        f\"{'' if truncated_line_count == 1 else 's'} hidden), {USAGE_MSG}\",\n+    ]\n+\n+\n+def _truncate_by_char_count(input_lines: list[str], max_chars: int) -> list[str]:\n+    # Find point at which input length exceeds total allowed length\n+    iterated_char_count = 0\n+    for iterated_index, input_line in enumerate(input_lines):\n+        if iterated_char_count + len(input_line) > max_chars:\n+            break\n+        iterated_char_count += len(input_line)\n+\n+    # Create truncated explanation with modified final line\n+    truncated_result = input_lines[:iterated_index]\n+    final_line = input_lines[iterated_index]\n+    if final_line:\n+        final_line_truncate_point = max_chars - iterated_char_count\n+        final_line = final_line[:final_line_truncate_point]\n+    truncated_result.append(final_line)\n+    return truncated_result",
      "patch_lines": [
        "@@ -0,0 +1,137 @@\n",
        "+\"\"\"Utilities for truncating assertion output.\n",
        "+\n",
        "+Current default behaviour is to truncate assertion explanations at\n",
        "+terminal lines, unless running with an assertions verbosity level of at least 2 or running on CI.\n",
        "+\"\"\"\n",
        "+\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+from _pytest.assertion import util\n",
        "+from _pytest.config import Config\n",
        "+from _pytest.nodes import Item\n",
        "+\n",
        "+\n",
        "+DEFAULT_MAX_LINES = 8\n",
        "+DEFAULT_MAX_CHARS = DEFAULT_MAX_LINES * 80\n",
        "+USAGE_MSG = \"use '-vv' to show\"\n",
        "+\n",
        "+\n",
        "+def truncate_if_required(explanation: list[str], item: Item) -> list[str]:\n",
        "+    \"\"\"Truncate this assertion explanation if the given test item is eligible.\"\"\"\n",
        "+    should_truncate, max_lines, max_chars = _get_truncation_parameters(item)\n",
        "+    if should_truncate:\n",
        "+        return _truncate_explanation(\n",
        "+            explanation,\n",
        "+            max_lines=max_lines,\n",
        "+            max_chars=max_chars,\n",
        "+        )\n",
        "+    return explanation\n",
        "+\n",
        "+\n",
        "+def _get_truncation_parameters(item: Item) -> tuple[bool, int, int]:\n",
        "+    \"\"\"Return the truncation parameters related to the given item, as (should truncate, max lines, max chars).\"\"\"\n",
        "+    # We do not need to truncate if one of conditions is met:\n",
        "+    # 1. Verbosity level is 2 or more;\n",
        "+    # 2. Test is being run in CI environment;\n",
        "+    # 3. Both truncation_limit_lines and truncation_limit_chars\n",
        "+    #    .ini parameters are set to 0 explicitly.\n",
        "+    max_lines = item.config.getini(\"truncation_limit_lines\")\n",
        "+    max_lines = int(max_lines if max_lines is not None else DEFAULT_MAX_LINES)\n",
        "+\n",
        "+    max_chars = item.config.getini(\"truncation_limit_chars\")\n",
        "+    max_chars = int(max_chars if max_chars is not None else DEFAULT_MAX_CHARS)\n",
        "+\n",
        "+    verbose = item.config.get_verbosity(Config.VERBOSITY_ASSERTIONS)\n",
        "+\n",
        "+    should_truncate = verbose < 2 and not util.running_on_ci()\n",
        "+    should_truncate = should_truncate and (max_lines > 0 or max_chars > 0)\n",
        "+\n",
        "+    return should_truncate, max_lines, max_chars\n",
        "+\n",
        "+\n",
        "+def _truncate_explanation(\n",
        "+    input_lines: list[str],\n",
        "+    max_lines: int,\n",
        "+    max_chars: int,\n",
        "+) -> list[str]:\n",
        "+    \"\"\"Truncate given list of strings that makes up the assertion explanation.\n",
        "+\n",
        "+    Truncates to either max_lines, or max_chars - whichever the input reaches\n",
        "+    first, taking the truncation explanation into account. The remaining lines\n",
        "+    will be replaced by a usage message.\n",
        "+    \"\"\"\n",
        "+    # Check if truncation required\n",
        "+    input_char_count = len(\"\".join(input_lines))\n",
        "+    # The length of the truncation explanation depends on the number of lines\n",
        "+    # removed but is at least 68 characters:\n",
        "+    # The real value is\n",
        "+    # 64 (for the base message:\n",
        "+    # '...\\n...Full output truncated (1 line hidden), use '-vv' to show\")'\n",
        "+    # )\n",
        "+    # + 1 (for plural)\n",
        "+    # + int(math.log10(len(input_lines) - max_lines)) (number of hidden line, at least 1)\n",
        "+    # + 3 for the '...' added to the truncated line\n",
        "+    # But if there's more than 100 lines it's very likely that we're going to\n",
        "+    # truncate, so we don't need the exact value using log10.\n",
        "+    tolerable_max_chars = (\n",
        "+        max_chars + 70  # 64 + 1 (for plural) + 2 (for '99') + 3 for '...'\n",
        "+    )\n",
        "+    # The truncation explanation add two lines to the output\n",
        "+    tolerable_max_lines = max_lines + 2\n",
        "+    if (\n",
        "+        len(input_lines) <= tolerable_max_lines\n",
        "+        and input_char_count <= tolerable_max_chars\n",
        "+    ):\n",
        "+        return input_lines\n",
        "+    # Truncate first to max_lines, and then truncate to max_chars if necessary\n",
        "+    if max_lines > 0:\n",
        "+        truncated_explanation = input_lines[:max_lines]\n",
        "+    else:\n",
        "+        truncated_explanation = input_lines\n",
        "+    truncated_char = True\n",
        "+    # We reevaluate the need to truncate chars following removal of some lines\n",
        "+    if len(\"\".join(truncated_explanation)) > tolerable_max_chars and max_chars > 0:\n",
        "+        truncated_explanation = _truncate_by_char_count(\n",
        "+            truncated_explanation, max_chars\n",
        "+        )\n",
        "+    else:\n",
        "+        truncated_char = False\n",
        "+\n",
        "+    if truncated_explanation == input_lines:\n",
        "+        # No truncation happened, so we do not need to add any explanations\n",
        "+        return truncated_explanation\n",
        "+\n",
        "+    truncated_line_count = len(input_lines) - len(truncated_explanation)\n",
        "+    if truncated_explanation[-1]:\n",
        "+        # Add ellipsis and take into account part-truncated final line\n",
        "+        truncated_explanation[-1] = truncated_explanation[-1] + \"...\"\n",
        "+        if truncated_char:\n",
        "+            # It's possible that we did not remove any char from this line\n",
        "+            truncated_line_count += 1\n",
        "+    else:\n",
        "+        # Add proper ellipsis when we were able to fit a full line exactly\n",
        "+        truncated_explanation[-1] = \"...\"\n",
        "+    return [\n",
        "+        *truncated_explanation,\n",
        "+        \"\",\n",
        "+        f\"...Full output truncated ({truncated_line_count} line\"\n",
        "+        f\"{'' if truncated_line_count == 1 else 's'} hidden), {USAGE_MSG}\",\n",
        "+    ]\n",
        "+\n",
        "+\n",
        "+def _truncate_by_char_count(input_lines: list[str], max_chars: int) -> list[str]:\n",
        "+    # Find point at which input length exceeds total allowed length\n",
        "+    iterated_char_count = 0\n",
        "+    for iterated_index, input_line in enumerate(input_lines):\n",
        "+        if iterated_char_count + len(input_line) > max_chars:\n",
        "+            break\n",
        "+        iterated_char_count += len(input_line)\n",
        "+\n",
        "+    # Create truncated explanation with modified final line\n",
        "+    truncated_result = input_lines[:iterated_index]\n",
        "+    final_line = input_lines[iterated_index]\n",
        "+    if final_line:\n",
        "+        final_line_truncate_point = max_chars - iterated_char_count\n",
        "+        final_line = final_line[:final_line_truncate_point]\n",
        "+    truncated_result.append(final_line)\n",
        "+    return truncated_result\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/assertion/util.py",
      "status": "added",
      "additions": 621,
      "deletions": 0,
      "patch": "@@ -0,0 +1,621 @@\n+# mypy: allow-untyped-defs\n+\"\"\"Utilities for assertion debugging.\"\"\"\n+\n+from __future__ import annotations\n+\n+import collections.abc\n+from collections.abc import Callable\n+from collections.abc import Iterable\n+from collections.abc import Mapping\n+from collections.abc import Sequence\n+from collections.abc import Set as AbstractSet\n+import os\n+import pprint\n+from typing import Any\n+from typing import Literal\n+from typing import Protocol\n+from unicodedata import normalize\n+\n+from _pytest import outcomes\n+import _pytest._code\n+from _pytest._io.pprint import PrettyPrinter\n+from _pytest._io.saferepr import saferepr\n+from _pytest._io.saferepr import saferepr_unlimited\n+from _pytest.config import Config\n+\n+\n+# The _reprcompare attribute on the util module is used by the new assertion\n+# interpretation code and assertion rewriter to detect this plugin was\n+# loaded and in turn call the hooks defined here as part of the\n+# DebugInterpreter.\n+_reprcompare: Callable[[str, object, object], str | None] | None = None\n+\n+# Works similarly as _reprcompare attribute. Is populated with the hook call\n+# when pytest_runtest_setup is called.\n+_assertion_pass: Callable[[int, str, str], None] | None = None\n+\n+# Config object which is assigned during pytest_runtest_protocol.\n+_config: Config | None = None\n+\n+\n+class _HighlightFunc(Protocol):\n+    def __call__(self, source: str, lexer: Literal[\"diff\", \"python\"] = \"python\") -> str:\n+        \"\"\"Apply highlighting to the given source.\"\"\"\n+\n+\n+def dummy_highlighter(source: str, lexer: Literal[\"diff\", \"python\"] = \"python\") -> str:\n+    \"\"\"Dummy highlighter that returns the text unprocessed.\n+\n+    Needed for _notin_text, as the diff gets post-processed to only show the \"+\" part.\n+    \"\"\"\n+    return source\n+\n+\n+def format_explanation(explanation: str) -> str:\n+    r\"\"\"Format an explanation.\n+\n+    Normally all embedded newlines are escaped, however there are\n+    three exceptions: \\n{, \\n} and \\n~.  The first two are intended\n+    cover nested explanations, see function and attribute explanations\n+    for examples (.visit_Call(), visit_Attribute()).  The last one is\n+    for when one explanation needs to span multiple lines, e.g. when\n+    displaying diffs.\n+    \"\"\"\n+    lines = _split_explanation(explanation)\n+    result = _format_lines(lines)\n+    return \"\\n\".join(result)\n+\n+\n+def _split_explanation(explanation: str) -> list[str]:\n+    r\"\"\"Return a list of individual lines in the explanation.\n+\n+    This will return a list of lines split on '\\n{', '\\n}' and '\\n~'.\n+    Any other newlines will be escaped and appear in the line as the\n+    literal '\\n' characters.\n+    \"\"\"\n+    raw_lines = (explanation or \"\").split(\"\\n\")\n+    lines = [raw_lines[0]]\n+    for values in raw_lines[1:]:\n+        if values and values[0] in [\"{\", \"}\", \"~\", \">\"]:\n+            lines.append(values)\n+        else:\n+            lines[-1] += \"\\\\n\" + values\n+    return lines\n+\n+\n+def _format_lines(lines: Sequence[str]) -> list[str]:\n+    \"\"\"Format the individual lines.\n+\n+    This will replace the '{', '}' and '~' characters of our mini formatting\n+    language with the proper 'where ...', 'and ...' and ' + ...' text, taking\n+    care of indentation along the way.\n+\n+    Return a list of formatted lines.\n+    \"\"\"\n+    result = list(lines[:1])\n+    stack = [0]\n+    stackcnt = [0]\n+    for line in lines[1:]:\n+        if line.startswith(\"{\"):\n+            if stackcnt[-1]:\n+                s = \"and   \"\n+            else:\n+                s = \"where \"\n+            stack.append(len(result))\n+            stackcnt[-1] += 1\n+            stackcnt.append(0)\n+            result.append(\" +\" + \"  \" * (len(stack) - 1) + s + line[1:])\n+        elif line.startswith(\"}\"):\n+            stack.pop()\n+            stackcnt.pop()\n+            result[stack[-1]] += line[1:]\n+        else:\n+            assert line[0] in [\"~\", \">\"]\n+            stack[-1] += 1\n+            indent = len(stack) if line.startswith(\"~\") else len(stack) - 1\n+            result.append(\"  \" * indent + line[1:])\n+    assert len(stack) == 1\n+    return result\n+\n+\n+def issequence(x: Any) -> bool:\n+    return isinstance(x, collections.abc.Sequence) and not isinstance(x, str)\n+\n+\n+def istext(x: Any) -> bool:\n+    return isinstance(x, str)\n+\n+\n+def isdict(x: Any) -> bool:\n+    return isinstance(x, dict)\n+\n+\n+def isset(x: Any) -> bool:\n+    return isinstance(x, (set, frozenset))\n+\n+\n+def isnamedtuple(obj: Any) -> bool:\n+    return isinstance(obj, tuple) and getattr(obj, \"_fields\", None) is not None\n+\n+\n+def isdatacls(obj: Any) -> bool:\n+    return getattr(obj, \"__dataclass_fields__\", None) is not None\n+\n+\n+def isattrs(obj: Any) -> bool:\n+    return getattr(obj, \"__attrs_attrs__\", None) is not None\n+\n+\n+def isiterable(obj: Any) -> bool:\n+    try:\n+        iter(obj)\n+        return not istext(obj)\n+    except Exception:\n+        return False\n+\n+\n+def has_default_eq(\n+    obj: object,\n+) -> bool:\n+    \"\"\"Check if an instance of an object contains the default eq\n+\n+    First, we check if the object's __eq__ attribute has __code__,\n+    if so, we check the equally of the method code filename (__code__.co_filename)\n+    to the default one generated by the dataclass and attr module\n+    for dataclasses the default co_filename is <string>, for attrs class, the __eq__ should contain \"attrs eq generated\"\n+    \"\"\"\n+    # inspired from https://github.com/willmcgugan/rich/blob/07d51ffc1aee6f16bd2e5a25b4e82850fb9ed778/rich/pretty.py#L68\n+    if hasattr(obj.__eq__, \"__code__\") and hasattr(obj.__eq__.__code__, \"co_filename\"):\n+        code_filename = obj.__eq__.__code__.co_filename\n+\n+        if isattrs(obj):\n+            return \"attrs generated \" in code_filename\n+\n+        return code_filename == \"<string>\"  # data class\n+    return True\n+\n+\n+def assertrepr_compare(\n+    config, op: str, left: Any, right: Any, use_ascii: bool = False\n+) -> list[str] | None:\n+    \"\"\"Return specialised explanations for some operators/operands.\"\"\"\n+    verbose = config.get_verbosity(Config.VERBOSITY_ASSERTIONS)\n+\n+    # Strings which normalize equal are often hard to distinguish when printed; use ascii() to make this easier.\n+    # See issue #3246.\n+    use_ascii = (\n+        isinstance(left, str)\n+        and isinstance(right, str)\n+        and normalize(\"NFD\", left) == normalize(\"NFD\", right)\n+    )\n+\n+    if verbose > 1:\n+        left_repr = saferepr_unlimited(left, use_ascii=use_ascii)\n+        right_repr = saferepr_unlimited(right, use_ascii=use_ascii)\n+    else:\n+        # XXX: \"15 chars indentation\" is wrong\n+        #      (\"E       AssertionError: assert \"); should use term width.\n+        maxsize = (\n+            80 - 15 - len(op) - 2\n+        ) // 2  # 15 chars indentation, 1 space around op\n+\n+        left_repr = saferepr(left, maxsize=maxsize, use_ascii=use_ascii)\n+        right_repr = saferepr(right, maxsize=maxsize, use_ascii=use_ascii)\n+\n+    summary = f\"{left_repr} {op} {right_repr}\"\n+    highlighter = config.get_terminal_writer()._highlight\n+\n+    explanation = None\n+    try:\n+        if op == \"==\":\n+            explanation = _compare_eq_any(left, right, highlighter, verbose)\n+        elif op == \"not in\":\n+            if istext(left) and istext(right):\n+                explanation = _notin_text(left, right, verbose)\n+        elif op == \"!=\":\n+            if isset(left) and isset(right):\n+                explanation = [\"Both sets are equal\"]\n+        elif op == \">=\":\n+            if isset(left) and isset(right):\n+                explanation = _compare_gte_set(left, right, highlighter, verbose)\n+        elif op == \"<=\":\n+            if isset(left) and isset(right):\n+                explanation = _compare_lte_set(left, right, highlighter, verbose)\n+        elif op == \">\":\n+            if isset(left) and isset(right):\n+                explanation = _compare_gt_set(left, right, highlighter, verbose)\n+        elif op == \"<\":\n+            if isset(left) and isset(right):\n+                explanation = _compare_lt_set(left, right, highlighter, verbose)\n+\n+    except outcomes.Exit:\n+        raise\n+    except Exception:\n+        repr_crash = _pytest._code.ExceptionInfo.from_current()._getreprcrash()\n+        explanation = [\n+            f\"(pytest_assertion plugin: representation of details failed: {repr_crash}.\",\n+            \" Probably an object has a faulty __repr__.)\",\n+        ]\n+\n+    if not explanation:\n+        return None\n+\n+    if explanation[0] != \"\":\n+        explanation = [\"\", *explanation]\n+    return [summary, *explanation]\n+\n+\n+def _compare_eq_any(\n+    left: Any, right: Any, highlighter: _HighlightFunc, verbose: int = 0\n+) -> list[str]:\n+    explanation = []\n+    if istext(left) and istext(right):\n+        explanation = _diff_text(left, right, highlighter, verbose)\n+    else:\n+        from _pytest.python_api import ApproxBase\n+\n+        if isinstance(left, ApproxBase) or isinstance(right, ApproxBase):\n+            # Although the common order should be obtained == expected, this ensures both ways\n+            approx_side = left if isinstance(left, ApproxBase) else right\n+            other_side = right if isinstance(left, ApproxBase) else left\n+\n+            explanation = approx_side._repr_compare(other_side)\n+        elif type(left) is type(right) and (\n+            isdatacls(left) or isattrs(left) or isnamedtuple(left)\n+        ):\n+            # Note: unlike dataclasses/attrs, namedtuples compare only the\n+            # field values, not the type or field names. But this branch\n+            # intentionally only handles the same-type case, which was often\n+            # used in older code bases before dataclasses/attrs were available.\n+            explanation = _compare_eq_cls(left, right, highlighter, verbose)\n+        elif issequence(left) and issequence(right):\n+            explanation = _compare_eq_sequence(left, right, highlighter, verbose)\n+        elif isset(left) and isset(right):\n+            explanation = _compare_eq_set(left, right, highlighter, verbose)\n+        elif isdict(left) and isdict(right):\n+            explanation = _compare_eq_dict(left, right, highlighter, verbose)\n+\n+        if isiterable(left) and isiterable(right):\n+            expl = _compare_eq_iterable(left, right, highlighter, verbose)\n+            explanation.extend(expl)\n+\n+    return explanation\n+\n+\n+def _diff_text(\n+    left: str, right: str, highlighter: _HighlightFunc, verbose: int = 0\n+) -> list[str]:\n+    \"\"\"Return the explanation for the diff between text.\n+\n+    Unless --verbose is used this will skip leading and trailing\n+    characters which are identical to keep the diff minimal.\n+    \"\"\"\n+    from difflib import ndiff\n+\n+    explanation: list[str] = []\n+\n+    if verbose < 1:\n+        i = 0  # just in case left or right has zero length\n+        for i in range(min(len(left), len(right))):\n+            if left[i] != right[i]:\n+                break\n+        if i > 42:\n+            i -= 10  # Provide some context\n+            explanation = [\n+                f\"Skipping {i} identical leading characters in diff, use -v to show\"\n+            ]\n+            left = left[i:]\n+            right = right[i:]\n+        if len(left) == len(right):\n+            for i in range(len(left)):\n+                if left[-i] != right[-i]:\n+                    break\n+            if i > 42:\n+                i -= 10  # Provide some context\n+                explanation += [\n+                    f\"Skipping {i} identical trailing \"\n+                    \"characters in diff, use -v to show\"\n+                ]\n+                left = left[:-i]\n+                right = right[:-i]\n+    keepends = True\n+    if left.isspace() or right.isspace():\n+        left = repr(str(left))\n+        right = repr(str(right))\n+        explanation += [\"Strings contain only whitespace, escaping them using repr()\"]\n+    # \"right\" is the expected base against which we compare \"left\",\n+    # see https://github.com/pytest-dev/pytest/issues/3333\n+    explanation.extend(\n+        highlighter(\n+            \"\\n\".join(\n+                line.strip(\"\\n\")\n+                for line in ndiff(right.splitlines(keepends), left.splitlines(keepends))\n+            ),\n+            lexer=\"diff\",\n+        ).splitlines()\n+    )\n+    return explanation\n+\n+\n+def _compare_eq_iterable(\n+    left: Iterable[Any],\n+    right: Iterable[Any],\n+    highlighter: _HighlightFunc,\n+    verbose: int = 0,\n+) -> list[str]:\n+    if verbose <= 0 and not running_on_ci():\n+        return [\"Use -v to get more diff\"]\n+    # dynamic import to speedup pytest\n+    import difflib\n+\n+    left_formatting = PrettyPrinter().pformat(left).splitlines()\n+    right_formatting = PrettyPrinter().pformat(right).splitlines()\n+\n+    explanation = [\"\", \"Full diff:\"]\n+    # \"right\" is the expected base against which we compare \"left\",\n+    # see https://github.com/pytest-dev/pytest/issues/3333\n+    explanation.extend(\n+        highlighter(\n+            \"\\n\".join(\n+                line.rstrip()\n+                for line in difflib.ndiff(right_formatting, left_formatting)\n+            ),\n+            lexer=\"diff\",\n+        ).splitlines()\n+    )\n+    return explanation\n+\n+\n+def _compare_eq_sequence(\n+    left: Sequence[Any],\n+    right: Sequence[Any],\n+    highlighter: _HighlightFunc,\n+    verbose: int = 0,\n+) -> list[str]:\n+    comparing_bytes = isinstance(left, bytes) and isinstance(right, bytes)\n+    explanation: list[str] = []\n+    len_left = len(left)\n+    len_right = len(right)\n+    for i in range(min(len_left, len_right)):\n+        if left[i] != right[i]:\n+            if comparing_bytes:\n+                # when comparing bytes, we want to see their ascii representation\n+                # instead of their numeric values (#5260)\n+                # using a slice gives us the ascii representation:\n+                # >>> s = b'foo'\n+                # >>> s[0]\n+                # 102\n+                # >>> s[0:1]\n+                # b'f'\n+                left_value = left[i : i + 1]\n+                right_value = right[i : i + 1]\n+            else:\n+                left_value = left[i]\n+                right_value = right[i]\n+\n+            explanation.append(\n+                f\"At index {i} diff:\"\n+                f\" {highlighter(repr(left_value))} != {highlighter(repr(right_value))}\"\n+            )\n+            break\n+\n+    if comparing_bytes:\n+        # when comparing bytes, it doesn't help to show the \"sides contain one or more\n+        # items\" longer explanation, so skip it\n+\n+        return explanation\n+\n+    len_diff = len_left - len_right\n+    if len_diff:\n+        if len_diff > 0:\n+            dir_with_more = \"Left\"\n+            extra = saferepr(left[len_right])\n+        else:\n+            len_diff = 0 - len_diff\n+            dir_with_more = \"Right\"\n+            extra = saferepr(right[len_left])\n+\n+        if len_diff == 1:\n+            explanation += [\n+                f\"{dir_with_more} contains one more item: {highlighter(extra)}\"\n+            ]\n+        else:\n+            explanation += [\n+                f\"{dir_with_more} contains {len_diff} more items, first extra item: {highlighter(extra)}\"\n+            ]\n+    return explanation\n+\n+\n+def _compare_eq_set(\n+    left: AbstractSet[Any],\n+    right: AbstractSet[Any],\n+    highlighter: _HighlightFunc,\n+    verbose: int = 0,\n+) -> list[str]:\n+    explanation = []\n+    explanation.extend(_set_one_sided_diff(\"left\", left, right, highlighter))\n+    explanation.extend(_set_one_sided_diff(\"right\", right, left, highlighter))\n+    return explanation\n+\n+\n+def _compare_gt_set(\n+    left: AbstractSet[Any],\n+    right: AbstractSet[Any],\n+    highlighter: _HighlightFunc,\n+    verbose: int = 0,\n+) -> list[str]:\n+    explanation = _compare_gte_set(left, right, highlighter)\n+    if not explanation:\n+        return [\"Both sets are equal\"]\n+    return explanation\n+\n+\n+def _compare_lt_set(\n+    left: AbstractSet[Any],\n+    right: AbstractSet[Any],\n+    highlighter: _HighlightFunc,\n+    verbose: int = 0,\n+) -> list[str]:\n+    explanation = _compare_lte_set(left, right, highlighter)\n+    if not explanation:\n+        return [\"Both sets are equal\"]\n+    return explanation\n+\n+\n+def _compare_gte_set(\n+    left: AbstractSet[Any],\n+    right: AbstractSet[Any],\n+    highlighter: _HighlightFunc,\n+    verbose: int = 0,\n+) -> list[str]:\n+    return _set_one_sided_diff(\"right\", right, left, highlighter)\n+\n+\n+def _compare_lte_set(\n+    left: AbstractSet[Any],\n+    right: AbstractSet[Any],\n+    highlighter: _HighlightFunc,\n+    verbose: int = 0,\n+) -> list[str]:\n+    return _set_one_sided_diff(\"left\", left, right, highlighter)\n+\n+\n+def _set_one_sided_diff(\n+    posn: str,\n+    set1: AbstractSet[Any],\n+    set2: AbstractSet[Any],\n+    highlighter: _HighlightFunc,\n+) -> list[str]:\n+    explanation = []\n+    diff = set1 - set2\n+    if diff:\n+        explanation.append(f\"Extra items in the {posn} set:\")\n+        for item in diff:\n+            explanation.append(highlighter(saferepr(item)))\n+    return explanation\n+\n+\n+def _compare_eq_dict(\n+    left: Mapping[Any, Any],\n+    right: Mapping[Any, Any],\n+    highlighter: _HighlightFunc,\n+    verbose: int = 0,\n+) -> list[str]:\n+    explanation: list[str] = []\n+    set_left = set(left)\n+    set_right = set(right)\n+    common = set_left.intersection(set_right)\n+    same = {k: left[k] for k in common if left[k] == right[k]}\n+    if same and verbose < 2:\n+        explanation += [f\"Omitting {len(same)} identical items, use -vv to show\"]\n+    elif same:\n+        explanation += [\"Common items:\"]\n+        explanation += highlighter(pprint.pformat(same)).splitlines()\n+    diff = {k for k in common if left[k] != right[k]}\n+    if diff:\n+        explanation += [\"Differing items:\"]\n+        for k in diff:\n+            explanation += [\n+                highlighter(saferepr({k: left[k]}))\n+                + \" != \"\n+                + highlighter(saferepr({k: right[k]}))\n+            ]\n+    extra_left = set_left - set_right\n+    len_extra_left = len(extra_left)\n+    if len_extra_left:\n+        explanation.append(\n+            f\"Left contains {len_extra_left} more item{'' if len_extra_left == 1 else 's'}:\"\n+        )\n+        explanation.extend(\n+            highlighter(pprint.pformat({k: left[k] for k in extra_left})).splitlines()\n+        )\n+    extra_right = set_right - set_left\n+    len_extra_right = len(extra_right)\n+    if len_extra_right:\n+        explanation.append(\n+            f\"Right contains {len_extra_right} more item{'' if len_extra_right == 1 else 's'}:\"\n+        )\n+        explanation.extend(\n+            highlighter(pprint.pformat({k: right[k] for k in extra_right})).splitlines()\n+        )\n+    return explanation\n+\n+\n+def _compare_eq_cls(\n+    left: Any, right: Any, highlighter: _HighlightFunc, verbose: int\n+) -> list[str]:\n+    if not has_default_eq(left):\n+        return []\n+    if isdatacls(left):\n+        import dataclasses\n+\n+        all_fields = dataclasses.fields(left)\n+        fields_to_check = [info.name for info in all_fields if info.compare]\n+    elif isattrs(left):\n+        all_fields = left.__attrs_attrs__\n+        fields_to_check = [field.name for field in all_fields if getattr(field, \"eq\")]\n+    elif isnamedtuple(left):\n+        fields_to_check = left._fields\n+    else:\n+        assert False\n+\n+    indent = \"  \"\n+    same = []\n+    diff = []\n+    for field in fields_to_check:\n+        if getattr(left, field) == getattr(right, field):\n+            same.append(field)\n+        else:\n+            diff.append(field)\n+\n+    explanation = []\n+    if same or diff:\n+        explanation += [\"\"]\n+    if same and verbose < 2:\n+        explanation.append(f\"Omitting {len(same)} identical items, use -vv to show\")\n+    elif same:\n+        explanation += [\"Matching attributes:\"]\n+        explanation += highlighter(pprint.pformat(same)).splitlines()\n+    if diff:\n+        explanation += [\"Differing attributes:\"]\n+        explanation += highlighter(pprint.pformat(diff)).splitlines()\n+        for field in diff:\n+            field_left = getattr(left, field)\n+            field_right = getattr(right, field)\n+            explanation += [\n+                \"\",\n+                f\"Drill down into differing attribute {field}:\",\n+                f\"{indent}{field}: {highlighter(repr(field_left))} != {highlighter(repr(field_right))}\",\n+            ]\n+            explanation += [\n+                indent + line\n+                for line in _compare_eq_any(\n+                    field_left, field_right, highlighter, verbose\n+                )\n+            ]\n+    return explanation\n+\n+\n+def _notin_text(term: str, text: str, verbose: int = 0) -> list[str]:\n+    index = text.find(term)\n+    head = text[:index]\n+    tail = text[index + len(term) :]\n+    correct_text = head + tail\n+    diff = _diff_text(text, correct_text, dummy_highlighter, verbose)\n+    newdiff = [f\"{saferepr(term, maxsize=42)} is contained here:\"]\n+    for line in diff:\n+        if line.startswith(\"Skipping\"):\n+            continue\n+        if line.startswith(\"- \"):\n+            continue\n+        if line.startswith(\"+ \"):\n+            newdiff.append(\"  \" + line[2:])\n+        else:\n+            newdiff.append(line)\n+    return newdiff\n+\n+\n+def running_on_ci() -> bool:\n+    \"\"\"Check if we're currently running on a CI system.\"\"\"\n+    env_vars = [\"CI\", \"BUILD_NUMBER\"]\n+    return any(var in os.environ for var in env_vars)",
      "patch_lines": [
        "@@ -0,0 +1,621 @@\n",
        "+# mypy: allow-untyped-defs\n",
        "+\"\"\"Utilities for assertion debugging.\"\"\"\n",
        "+\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+import collections.abc\n",
        "+from collections.abc import Callable\n",
        "+from collections.abc import Iterable\n",
        "+from collections.abc import Mapping\n",
        "+from collections.abc import Sequence\n",
        "+from collections.abc import Set as AbstractSet\n",
        "+import os\n",
        "+import pprint\n",
        "+from typing import Any\n",
        "+from typing import Literal\n",
        "+from typing import Protocol\n",
        "+from unicodedata import normalize\n",
        "+\n",
        "+from _pytest import outcomes\n",
        "+import _pytest._code\n",
        "+from _pytest._io.pprint import PrettyPrinter\n",
        "+from _pytest._io.saferepr import saferepr\n",
        "+from _pytest._io.saferepr import saferepr_unlimited\n",
        "+from _pytest.config import Config\n",
        "+\n",
        "+\n",
        "+# The _reprcompare attribute on the util module is used by the new assertion\n",
        "+# interpretation code and assertion rewriter to detect this plugin was\n",
        "+# loaded and in turn call the hooks defined here as part of the\n",
        "+# DebugInterpreter.\n",
        "+_reprcompare: Callable[[str, object, object], str | None] | None = None\n",
        "+\n",
        "+# Works similarly as _reprcompare attribute. Is populated with the hook call\n",
        "+# when pytest_runtest_setup is called.\n",
        "+_assertion_pass: Callable[[int, str, str], None] | None = None\n",
        "+\n",
        "+# Config object which is assigned during pytest_runtest_protocol.\n",
        "+_config: Config | None = None\n",
        "+\n",
        "+\n",
        "+class _HighlightFunc(Protocol):\n",
        "+    def __call__(self, source: str, lexer: Literal[\"diff\", \"python\"] = \"python\") -> str:\n",
        "+        \"\"\"Apply highlighting to the given source.\"\"\"\n",
        "+\n",
        "+\n",
        "+def dummy_highlighter(source: str, lexer: Literal[\"diff\", \"python\"] = \"python\") -> str:\n",
        "+    \"\"\"Dummy highlighter that returns the text unprocessed.\n",
        "+\n",
        "+    Needed for _notin_text, as the diff gets post-processed to only show the \"+\" part.\n",
        "+    \"\"\"\n",
        "+    return source\n",
        "+\n",
        "+\n",
        "+def format_explanation(explanation: str) -> str:\n",
        "+    r\"\"\"Format an explanation.\n",
        "+\n",
        "+    Normally all embedded newlines are escaped, however there are\n",
        "+    three exceptions: \\n{, \\n} and \\n~.  The first two are intended\n",
        "+    cover nested explanations, see function and attribute explanations\n",
        "+    for examples (.visit_Call(), visit_Attribute()).  The last one is\n",
        "+    for when one explanation needs to span multiple lines, e.g. when\n",
        "+    displaying diffs.\n",
        "+    \"\"\"\n",
        "+    lines = _split_explanation(explanation)\n",
        "+    result = _format_lines(lines)\n",
        "+    return \"\\n\".join(result)\n",
        "+\n",
        "+\n",
        "+def _split_explanation(explanation: str) -> list[str]:\n",
        "+    r\"\"\"Return a list of individual lines in the explanation.\n",
        "+\n",
        "+    This will return a list of lines split on '\\n{', '\\n}' and '\\n~'.\n",
        "+    Any other newlines will be escaped and appear in the line as the\n",
        "+    literal '\\n' characters.\n",
        "+    \"\"\"\n",
        "+    raw_lines = (explanation or \"\").split(\"\\n\")\n",
        "+    lines = [raw_lines[0]]\n",
        "+    for values in raw_lines[1:]:\n",
        "+        if values and values[0] in [\"{\", \"}\", \"~\", \">\"]:\n",
        "+            lines.append(values)\n",
        "+        else:\n",
        "+            lines[-1] += \"\\\\n\" + values\n",
        "+    return lines\n",
        "+\n",
        "+\n",
        "+def _format_lines(lines: Sequence[str]) -> list[str]:\n",
        "+    \"\"\"Format the individual lines.\n",
        "+\n",
        "+    This will replace the '{', '}' and '~' characters of our mini formatting\n",
        "+    language with the proper 'where ...', 'and ...' and ' + ...' text, taking\n",
        "+    care of indentation along the way.\n",
        "+\n",
        "+    Return a list of formatted lines.\n",
        "+    \"\"\"\n",
        "+    result = list(lines[:1])\n",
        "+    stack = [0]\n",
        "+    stackcnt = [0]\n",
        "+    for line in lines[1:]:\n",
        "+        if line.startswith(\"{\"):\n",
        "+            if stackcnt[-1]:\n",
        "+                s = \"and   \"\n",
        "+            else:\n",
        "+                s = \"where \"\n",
        "+            stack.append(len(result))\n",
        "+            stackcnt[-1] += 1\n",
        "+            stackcnt.append(0)\n",
        "+            result.append(\" +\" + \"  \" * (len(stack) - 1) + s + line[1:])\n",
        "+        elif line.startswith(\"}\"):\n",
        "+            stack.pop()\n",
        "+            stackcnt.pop()\n",
        "+            result[stack[-1]] += line[1:]\n",
        "+        else:\n",
        "+            assert line[0] in [\"~\", \">\"]\n",
        "+            stack[-1] += 1\n",
        "+            indent = len(stack) if line.startswith(\"~\") else len(stack) - 1\n",
        "+            result.append(\"  \" * indent + line[1:])\n",
        "+    assert len(stack) == 1\n",
        "+    return result\n",
        "+\n",
        "+\n",
        "+def issequence(x: Any) -> bool:\n",
        "+    return isinstance(x, collections.abc.Sequence) and not isinstance(x, str)\n",
        "+\n",
        "+\n",
        "+def istext(x: Any) -> bool:\n",
        "+    return isinstance(x, str)\n",
        "+\n",
        "+\n",
        "+def isdict(x: Any) -> bool:\n",
        "+    return isinstance(x, dict)\n",
        "+\n",
        "+\n",
        "+def isset(x: Any) -> bool:\n",
        "+    return isinstance(x, (set, frozenset))\n",
        "+\n",
        "+\n",
        "+def isnamedtuple(obj: Any) -> bool:\n",
        "+    return isinstance(obj, tuple) and getattr(obj, \"_fields\", None) is not None\n",
        "+\n",
        "+\n",
        "+def isdatacls(obj: Any) -> bool:\n",
        "+    return getattr(obj, \"__dataclass_fields__\", None) is not None\n",
        "+\n",
        "+\n",
        "+def isattrs(obj: Any) -> bool:\n",
        "+    return getattr(obj, \"__attrs_attrs__\", None) is not None\n",
        "+\n",
        "+\n",
        "+def isiterable(obj: Any) -> bool:\n",
        "+    try:\n",
        "+        iter(obj)\n",
        "+        return not istext(obj)\n",
        "+    except Exception:\n",
        "+        return False\n",
        "+\n",
        "+\n",
        "+def has_default_eq(\n",
        "+    obj: object,\n",
        "+) -> bool:\n",
        "+    \"\"\"Check if an instance of an object contains the default eq\n",
        "+\n",
        "+    First, we check if the object's __eq__ attribute has __code__,\n",
        "+    if so, we check the equally of the method code filename (__code__.co_filename)\n",
        "+    to the default one generated by the dataclass and attr module\n",
        "+    for dataclasses the default co_filename is <string>, for attrs class, the __eq__ should contain \"attrs eq generated\"\n",
        "+    \"\"\"\n",
        "+    # inspired from https://github.com/willmcgugan/rich/blob/07d51ffc1aee6f16bd2e5a25b4e82850fb9ed778/rich/pretty.py#L68\n",
        "+    if hasattr(obj.__eq__, \"__code__\") and hasattr(obj.__eq__.__code__, \"co_filename\"):\n",
        "+        code_filename = obj.__eq__.__code__.co_filename\n",
        "+\n",
        "+        if isattrs(obj):\n",
        "+            return \"attrs generated \" in code_filename\n",
        "+\n",
        "+        return code_filename == \"<string>\"  # data class\n",
        "+    return True\n",
        "+\n",
        "+\n",
        "+def assertrepr_compare(\n",
        "+    config, op: str, left: Any, right: Any, use_ascii: bool = False\n",
        "+) -> list[str] | None:\n",
        "+    \"\"\"Return specialised explanations for some operators/operands.\"\"\"\n",
        "+    verbose = config.get_verbosity(Config.VERBOSITY_ASSERTIONS)\n",
        "+\n",
        "+    # Strings which normalize equal are often hard to distinguish when printed; use ascii() to make this easier.\n",
        "+    # See issue #3246.\n",
        "+    use_ascii = (\n",
        "+        isinstance(left, str)\n",
        "+        and isinstance(right, str)\n",
        "+        and normalize(\"NFD\", left) == normalize(\"NFD\", right)\n",
        "+    )\n",
        "+\n",
        "+    if verbose > 1:\n",
        "+        left_repr = saferepr_unlimited(left, use_ascii=use_ascii)\n",
        "+        right_repr = saferepr_unlimited(right, use_ascii=use_ascii)\n",
        "+    else:\n",
        "+        # XXX: \"15 chars indentation\" is wrong\n",
        "+        #      (\"E       AssertionError: assert \"); should use term width.\n",
        "+        maxsize = (\n",
        "+            80 - 15 - len(op) - 2\n",
        "+        ) // 2  # 15 chars indentation, 1 space around op\n",
        "+\n",
        "+        left_repr = saferepr(left, maxsize=maxsize, use_ascii=use_ascii)\n",
        "+        right_repr = saferepr(right, maxsize=maxsize, use_ascii=use_ascii)\n",
        "+\n",
        "+    summary = f\"{left_repr} {op} {right_repr}\"\n",
        "+    highlighter = config.get_terminal_writer()._highlight\n",
        "+\n",
        "+    explanation = None\n",
        "+    try:\n",
        "+        if op == \"==\":\n",
        "+            explanation = _compare_eq_any(left, right, highlighter, verbose)\n",
        "+        elif op == \"not in\":\n",
        "+            if istext(left) and istext(right):\n",
        "+                explanation = _notin_text(left, right, verbose)\n",
        "+        elif op == \"!=\":\n",
        "+            if isset(left) and isset(right):\n",
        "+                explanation = [\"Both sets are equal\"]\n",
        "+        elif op == \">=\":\n",
        "+            if isset(left) and isset(right):\n",
        "+                explanation = _compare_gte_set(left, right, highlighter, verbose)\n",
        "+        elif op == \"<=\":\n",
        "+            if isset(left) and isset(right):\n",
        "+                explanation = _compare_lte_set(left, right, highlighter, verbose)\n",
        "+        elif op == \">\":\n",
        "+            if isset(left) and isset(right):\n",
        "+                explanation = _compare_gt_set(left, right, highlighter, verbose)\n",
        "+        elif op == \"<\":\n",
        "+            if isset(left) and isset(right):\n",
        "+                explanation = _compare_lt_set(left, right, highlighter, verbose)\n",
        "+\n",
        "+    except outcomes.Exit:\n",
        "+        raise\n",
        "+    except Exception:\n",
        "+        repr_crash = _pytest._code.ExceptionInfo.from_current()._getreprcrash()\n",
        "+        explanation = [\n",
        "+            f\"(pytest_assertion plugin: representation of details failed: {repr_crash}.\",\n",
        "+            \" Probably an object has a faulty __repr__.)\",\n",
        "+        ]\n",
        "+\n",
        "+    if not explanation:\n",
        "+        return None\n",
        "+\n",
        "+    if explanation[0] != \"\":\n",
        "+        explanation = [\"\", *explanation]\n",
        "+    return [summary, *explanation]\n",
        "+\n",
        "+\n",
        "+def _compare_eq_any(\n",
        "+    left: Any, right: Any, highlighter: _HighlightFunc, verbose: int = 0\n",
        "+) -> list[str]:\n",
        "+    explanation = []\n",
        "+    if istext(left) and istext(right):\n",
        "+        explanation = _diff_text(left, right, highlighter, verbose)\n",
        "+    else:\n",
        "+        from _pytest.python_api import ApproxBase\n",
        "+\n",
        "+        if isinstance(left, ApproxBase) or isinstance(right, ApproxBase):\n",
        "+            # Although the common order should be obtained == expected, this ensures both ways\n",
        "+            approx_side = left if isinstance(left, ApproxBase) else right\n",
        "+            other_side = right if isinstance(left, ApproxBase) else left\n",
        "+\n",
        "+            explanation = approx_side._repr_compare(other_side)\n",
        "+        elif type(left) is type(right) and (\n",
        "+            isdatacls(left) or isattrs(left) or isnamedtuple(left)\n",
        "+        ):\n",
        "+            # Note: unlike dataclasses/attrs, namedtuples compare only the\n",
        "+            # field values, not the type or field names. But this branch\n",
        "+            # intentionally only handles the same-type case, which was often\n",
        "+            # used in older code bases before dataclasses/attrs were available.\n",
        "+            explanation = _compare_eq_cls(left, right, highlighter, verbose)\n",
        "+        elif issequence(left) and issequence(right):\n",
        "+            explanation = _compare_eq_sequence(left, right, highlighter, verbose)\n",
        "+        elif isset(left) and isset(right):\n",
        "+            explanation = _compare_eq_set(left, right, highlighter, verbose)\n",
        "+        elif isdict(left) and isdict(right):\n",
        "+            explanation = _compare_eq_dict(left, right, highlighter, verbose)\n",
        "+\n",
        "+        if isiterable(left) and isiterable(right):\n",
        "+            expl = _compare_eq_iterable(left, right, highlighter, verbose)\n",
        "+            explanation.extend(expl)\n",
        "+\n",
        "+    return explanation\n",
        "+\n",
        "+\n",
        "+def _diff_text(\n",
        "+    left: str, right: str, highlighter: _HighlightFunc, verbose: int = 0\n",
        "+) -> list[str]:\n",
        "+    \"\"\"Return the explanation for the diff between text.\n",
        "+\n",
        "+    Unless --verbose is used this will skip leading and trailing\n",
        "+    characters which are identical to keep the diff minimal.\n",
        "+    \"\"\"\n",
        "+    from difflib import ndiff\n",
        "+\n",
        "+    explanation: list[str] = []\n",
        "+\n",
        "+    if verbose < 1:\n",
        "+        i = 0  # just in case left or right has zero length\n",
        "+        for i in range(min(len(left), len(right))):\n",
        "+            if left[i] != right[i]:\n",
        "+                break\n",
        "+        if i > 42:\n",
        "+            i -= 10  # Provide some context\n",
        "+            explanation = [\n",
        "+                f\"Skipping {i} identical leading characters in diff, use -v to show\"\n",
        "+            ]\n",
        "+            left = left[i:]\n",
        "+            right = right[i:]\n",
        "+        if len(left) == len(right):\n",
        "+            for i in range(len(left)):\n",
        "+                if left[-i] != right[-i]:\n",
        "+                    break\n",
        "+            if i > 42:\n",
        "+                i -= 10  # Provide some context\n",
        "+                explanation += [\n",
        "+                    f\"Skipping {i} identical trailing \"\n",
        "+                    \"characters in diff, use -v to show\"\n",
        "+                ]\n",
        "+                left = left[:-i]\n",
        "+                right = right[:-i]\n",
        "+    keepends = True\n",
        "+    if left.isspace() or right.isspace():\n",
        "+        left = repr(str(left))\n",
        "+        right = repr(str(right))\n",
        "+        explanation += [\"Strings contain only whitespace, escaping them using repr()\"]\n",
        "+    # \"right\" is the expected base against which we compare \"left\",\n",
        "+    # see https://github.com/pytest-dev/pytest/issues/3333\n",
        "+    explanation.extend(\n",
        "+        highlighter(\n",
        "+            \"\\n\".join(\n",
        "+                line.strip(\"\\n\")\n",
        "+                for line in ndiff(right.splitlines(keepends), left.splitlines(keepends))\n",
        "+            ),\n",
        "+            lexer=\"diff\",\n",
        "+        ).splitlines()\n",
        "+    )\n",
        "+    return explanation\n",
        "+\n",
        "+\n",
        "+def _compare_eq_iterable(\n",
        "+    left: Iterable[Any],\n",
        "+    right: Iterable[Any],\n",
        "+    highlighter: _HighlightFunc,\n",
        "+    verbose: int = 0,\n",
        "+) -> list[str]:\n",
        "+    if verbose <= 0 and not running_on_ci():\n",
        "+        return [\"Use -v to get more diff\"]\n",
        "+    # dynamic import to speedup pytest\n",
        "+    import difflib\n",
        "+\n",
        "+    left_formatting = PrettyPrinter().pformat(left).splitlines()\n",
        "+    right_formatting = PrettyPrinter().pformat(right).splitlines()\n",
        "+\n",
        "+    explanation = [\"\", \"Full diff:\"]\n",
        "+    # \"right\" is the expected base against which we compare \"left\",\n",
        "+    # see https://github.com/pytest-dev/pytest/issues/3333\n",
        "+    explanation.extend(\n",
        "+        highlighter(\n",
        "+            \"\\n\".join(\n",
        "+                line.rstrip()\n",
        "+                for line in difflib.ndiff(right_formatting, left_formatting)\n",
        "+            ),\n",
        "+            lexer=\"diff\",\n",
        "+        ).splitlines()\n",
        "+    )\n",
        "+    return explanation\n",
        "+\n",
        "+\n",
        "+def _compare_eq_sequence(\n",
        "+    left: Sequence[Any],\n",
        "+    right: Sequence[Any],\n",
        "+    highlighter: _HighlightFunc,\n",
        "+    verbose: int = 0,\n",
        "+) -> list[str]:\n",
        "+    comparing_bytes = isinstance(left, bytes) and isinstance(right, bytes)\n",
        "+    explanation: list[str] = []\n",
        "+    len_left = len(left)\n",
        "+    len_right = len(right)\n",
        "+    for i in range(min(len_left, len_right)):\n",
        "+        if left[i] != right[i]:\n",
        "+            if comparing_bytes:\n",
        "+                # when comparing bytes, we want to see their ascii representation\n",
        "+                # instead of their numeric values (#5260)\n",
        "+                # using a slice gives us the ascii representation:\n",
        "+                # >>> s = b'foo'\n",
        "+                # >>> s[0]\n",
        "+                # 102\n",
        "+                # >>> s[0:1]\n",
        "+                # b'f'\n",
        "+                left_value = left[i : i + 1]\n",
        "+                right_value = right[i : i + 1]\n",
        "+            else:\n",
        "+                left_value = left[i]\n",
        "+                right_value = right[i]\n",
        "+\n",
        "+            explanation.append(\n",
        "+                f\"At index {i} diff:\"\n",
        "+                f\" {highlighter(repr(left_value))} != {highlighter(repr(right_value))}\"\n",
        "+            )\n",
        "+            break\n",
        "+\n",
        "+    if comparing_bytes:\n",
        "+        # when comparing bytes, it doesn't help to show the \"sides contain one or more\n",
        "+        # items\" longer explanation, so skip it\n",
        "+\n",
        "+        return explanation\n",
        "+\n",
        "+    len_diff = len_left - len_right\n",
        "+    if len_diff:\n",
        "+        if len_diff > 0:\n",
        "+            dir_with_more = \"Left\"\n",
        "+            extra = saferepr(left[len_right])\n",
        "+        else:\n",
        "+            len_diff = 0 - len_diff\n",
        "+            dir_with_more = \"Right\"\n",
        "+            extra = saferepr(right[len_left])\n",
        "+\n",
        "+        if len_diff == 1:\n",
        "+            explanation += [\n",
        "+                f\"{dir_with_more} contains one more item: {highlighter(extra)}\"\n",
        "+            ]\n",
        "+        else:\n",
        "+            explanation += [\n",
        "+                f\"{dir_with_more} contains {len_diff} more items, first extra item: {highlighter(extra)}\"\n",
        "+            ]\n",
        "+    return explanation\n",
        "+\n",
        "+\n",
        "+def _compare_eq_set(\n",
        "+    left: AbstractSet[Any],\n",
        "+    right: AbstractSet[Any],\n",
        "+    highlighter: _HighlightFunc,\n",
        "+    verbose: int = 0,\n",
        "+) -> list[str]:\n",
        "+    explanation = []\n",
        "+    explanation.extend(_set_one_sided_diff(\"left\", left, right, highlighter))\n",
        "+    explanation.extend(_set_one_sided_diff(\"right\", right, left, highlighter))\n",
        "+    return explanation\n",
        "+\n",
        "+\n",
        "+def _compare_gt_set(\n",
        "+    left: AbstractSet[Any],\n",
        "+    right: AbstractSet[Any],\n",
        "+    highlighter: _HighlightFunc,\n",
        "+    verbose: int = 0,\n",
        "+) -> list[str]:\n",
        "+    explanation = _compare_gte_set(left, right, highlighter)\n",
        "+    if not explanation:\n",
        "+        return [\"Both sets are equal\"]\n",
        "+    return explanation\n",
        "+\n",
        "+\n",
        "+def _compare_lt_set(\n",
        "+    left: AbstractSet[Any],\n",
        "+    right: AbstractSet[Any],\n",
        "+    highlighter: _HighlightFunc,\n",
        "+    verbose: int = 0,\n",
        "+) -> list[str]:\n",
        "+    explanation = _compare_lte_set(left, right, highlighter)\n",
        "+    if not explanation:\n",
        "+        return [\"Both sets are equal\"]\n",
        "+    return explanation\n",
        "+\n",
        "+\n",
        "+def _compare_gte_set(\n",
        "+    left: AbstractSet[Any],\n",
        "+    right: AbstractSet[Any],\n",
        "+    highlighter: _HighlightFunc,\n",
        "+    verbose: int = 0,\n",
        "+) -> list[str]:\n",
        "+    return _set_one_sided_diff(\"right\", right, left, highlighter)\n",
        "+\n",
        "+\n",
        "+def _compare_lte_set(\n",
        "+    left: AbstractSet[Any],\n",
        "+    right: AbstractSet[Any],\n",
        "+    highlighter: _HighlightFunc,\n",
        "+    verbose: int = 0,\n",
        "+) -> list[str]:\n",
        "+    return _set_one_sided_diff(\"left\", left, right, highlighter)\n",
        "+\n",
        "+\n",
        "+def _set_one_sided_diff(\n",
        "+    posn: str,\n",
        "+    set1: AbstractSet[Any],\n",
        "+    set2: AbstractSet[Any],\n",
        "+    highlighter: _HighlightFunc,\n",
        "+) -> list[str]:\n",
        "+    explanation = []\n",
        "+    diff = set1 - set2\n",
        "+    if diff:\n",
        "+        explanation.append(f\"Extra items in the {posn} set:\")\n",
        "+        for item in diff:\n",
        "+            explanation.append(highlighter(saferepr(item)))\n",
        "+    return explanation\n",
        "+\n",
        "+\n",
        "+def _compare_eq_dict(\n",
        "+    left: Mapping[Any, Any],\n",
        "+    right: Mapping[Any, Any],\n",
        "+    highlighter: _HighlightFunc,\n",
        "+    verbose: int = 0,\n",
        "+) -> list[str]:\n",
        "+    explanation: list[str] = []\n",
        "+    set_left = set(left)\n",
        "+    set_right = set(right)\n",
        "+    common = set_left.intersection(set_right)\n",
        "+    same = {k: left[k] for k in common if left[k] == right[k]}\n",
        "+    if same and verbose < 2:\n",
        "+        explanation += [f\"Omitting {len(same)} identical items, use -vv to show\"]\n",
        "+    elif same:\n",
        "+        explanation += [\"Common items:\"]\n",
        "+        explanation += highlighter(pprint.pformat(same)).splitlines()\n",
        "+    diff = {k for k in common if left[k] != right[k]}\n",
        "+    if diff:\n",
        "+        explanation += [\"Differing items:\"]\n",
        "+        for k in diff:\n",
        "+            explanation += [\n",
        "+                highlighter(saferepr({k: left[k]}))\n",
        "+                + \" != \"\n",
        "+                + highlighter(saferepr({k: right[k]}))\n",
        "+            ]\n",
        "+    extra_left = set_left - set_right\n",
        "+    len_extra_left = len(extra_left)\n",
        "+    if len_extra_left:\n",
        "+        explanation.append(\n",
        "+            f\"Left contains {len_extra_left} more item{'' if len_extra_left == 1 else 's'}:\"\n",
        "+        )\n",
        "+        explanation.extend(\n",
        "+            highlighter(pprint.pformat({k: left[k] for k in extra_left})).splitlines()\n",
        "+        )\n",
        "+    extra_right = set_right - set_left\n",
        "+    len_extra_right = len(extra_right)\n",
        "+    if len_extra_right:\n",
        "+        explanation.append(\n",
        "+            f\"Right contains {len_extra_right} more item{'' if len_extra_right == 1 else 's'}:\"\n",
        "+        )\n",
        "+        explanation.extend(\n",
        "+            highlighter(pprint.pformat({k: right[k] for k in extra_right})).splitlines()\n",
        "+        )\n",
        "+    return explanation\n",
        "+\n",
        "+\n",
        "+def _compare_eq_cls(\n",
        "+    left: Any, right: Any, highlighter: _HighlightFunc, verbose: int\n",
        "+) -> list[str]:\n",
        "+    if not has_default_eq(left):\n",
        "+        return []\n",
        "+    if isdatacls(left):\n",
        "+        import dataclasses\n",
        "+\n",
        "+        all_fields = dataclasses.fields(left)\n",
        "+        fields_to_check = [info.name for info in all_fields if info.compare]\n",
        "+    elif isattrs(left):\n",
        "+        all_fields = left.__attrs_attrs__\n",
        "+        fields_to_check = [field.name for field in all_fields if getattr(field, \"eq\")]\n",
        "+    elif isnamedtuple(left):\n",
        "+        fields_to_check = left._fields\n",
        "+    else:\n",
        "+        assert False\n",
        "+\n",
        "+    indent = \"  \"\n",
        "+    same = []\n",
        "+    diff = []\n",
        "+    for field in fields_to_check:\n",
        "+        if getattr(left, field) == getattr(right, field):\n",
        "+            same.append(field)\n",
        "+        else:\n",
        "+            diff.append(field)\n",
        "+\n",
        "+    explanation = []\n",
        "+    if same or diff:\n",
        "+        explanation += [\"\"]\n",
        "+    if same and verbose < 2:\n",
        "+        explanation.append(f\"Omitting {len(same)} identical items, use -vv to show\")\n",
        "+    elif same:\n",
        "+        explanation += [\"Matching attributes:\"]\n",
        "+        explanation += highlighter(pprint.pformat(same)).splitlines()\n",
        "+    if diff:\n",
        "+        explanation += [\"Differing attributes:\"]\n",
        "+        explanation += highlighter(pprint.pformat(diff)).splitlines()\n",
        "+        for field in diff:\n",
        "+            field_left = getattr(left, field)\n",
        "+            field_right = getattr(right, field)\n",
        "+            explanation += [\n",
        "+                \"\",\n",
        "+                f\"Drill down into differing attribute {field}:\",\n",
        "+                f\"{indent}{field}: {highlighter(repr(field_left))} != {highlighter(repr(field_right))}\",\n",
        "+            ]\n",
        "+            explanation += [\n",
        "+                indent + line\n",
        "+                for line in _compare_eq_any(\n",
        "+                    field_left, field_right, highlighter, verbose\n",
        "+                )\n",
        "+            ]\n",
        "+    return explanation\n",
        "+\n",
        "+\n",
        "+def _notin_text(term: str, text: str, verbose: int = 0) -> list[str]:\n",
        "+    index = text.find(term)\n",
        "+    head = text[:index]\n",
        "+    tail = text[index + len(term) :]\n",
        "+    correct_text = head + tail\n",
        "+    diff = _diff_text(text, correct_text, dummy_highlighter, verbose)\n",
        "+    newdiff = [f\"{saferepr(term, maxsize=42)} is contained here:\"]\n",
        "+    for line in diff:\n",
        "+        if line.startswith(\"Skipping\"):\n",
        "+            continue\n",
        "+        if line.startswith(\"- \"):\n",
        "+            continue\n",
        "+        if line.startswith(\"+ \"):\n",
        "+            newdiff.append(\"  \" + line[2:])\n",
        "+        else:\n",
        "+            newdiff.append(line)\n",
        "+    return newdiff\n",
        "+\n",
        "+\n",
        "+def running_on_ci() -> bool:\n",
        "+    \"\"\"Check if we're currently running on a CI system.\"\"\"\n",
        "+    env_vars = [\"CI\", \"BUILD_NUMBER\"]\n",
        "+    return any(var in os.environ for var in env_vars)\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/cacheprovider.py",
      "status": "added",
      "additions": 625,
      "deletions": 0,
      "patch": "@@ -0,0 +1,625 @@\n+# mypy: allow-untyped-defs\n+\"\"\"Implementation of the cache provider.\"\"\"\n+\n+# This plugin was not named \"cache\" to avoid conflicts with the external\n+# pytest-cache version.\n+from __future__ import annotations\n+\n+from collections.abc import Generator\n+from collections.abc import Iterable\n+import dataclasses\n+import errno\n+import json\n+import os\n+from pathlib import Path\n+import tempfile\n+from typing import final\n+\n+from .pathlib import resolve_from_str\n+from .pathlib import rm_rf\n+from .reports import CollectReport\n+from _pytest import nodes\n+from _pytest._io import TerminalWriter\n+from _pytest.config import Config\n+from _pytest.config import ExitCode\n+from _pytest.config import hookimpl\n+from _pytest.config.argparsing import Parser\n+from _pytest.deprecated import check_ispytest\n+from _pytest.fixtures import fixture\n+from _pytest.fixtures import FixtureRequest\n+from _pytest.main import Session\n+from _pytest.nodes import Directory\n+from _pytest.nodes import File\n+from _pytest.reports import TestReport\n+\n+\n+README_CONTENT = \"\"\"\\\n+# pytest cache directory #\n+\n+This directory contains data from the pytest's cache plugin,\n+which provides the `--lf` and `--ff` options, as well as the `cache` fixture.\n+\n+**Do not** commit this to version control.\n+\n+See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.\n+\"\"\"\n+\n+CACHEDIR_TAG_CONTENT = b\"\"\"\\\n+Signature: 8a477f597d28d172789f06886806bc55\n+# This file is a cache directory tag created by pytest.\n+# For information about cache directory tags, see:\n+#\thttps://bford.info/cachedir/spec.html\n+\"\"\"\n+\n+\n+@final\n+@dataclasses.dataclass\n+class Cache:\n+    \"\"\"Instance of the `cache` fixture.\"\"\"\n+\n+    _cachedir: Path = dataclasses.field(repr=False)\n+    _config: Config = dataclasses.field(repr=False)\n+\n+    # Sub-directory under cache-dir for directories created by `mkdir()`.\n+    _CACHE_PREFIX_DIRS = \"d\"\n+\n+    # Sub-directory under cache-dir for values created by `set()`.\n+    _CACHE_PREFIX_VALUES = \"v\"\n+\n+    def __init__(\n+        self, cachedir: Path, config: Config, *, _ispytest: bool = False\n+    ) -> None:\n+        check_ispytest(_ispytest)\n+        self._cachedir = cachedir\n+        self._config = config\n+\n+    @classmethod\n+    def for_config(cls, config: Config, *, _ispytest: bool = False) -> Cache:\n+        \"\"\"Create the Cache instance for a Config.\n+\n+        :meta private:\n+        \"\"\"\n+        check_ispytest(_ispytest)\n+        cachedir = cls.cache_dir_from_config(config, _ispytest=True)\n+        if config.getoption(\"cacheclear\") and cachedir.is_dir():\n+            cls.clear_cache(cachedir, _ispytest=True)\n+        return cls(cachedir, config, _ispytest=True)\n+\n+    @classmethod\n+    def clear_cache(cls, cachedir: Path, _ispytest: bool = False) -> None:\n+        \"\"\"Clear the sub-directories used to hold cached directories and values.\n+\n+        :meta private:\n+        \"\"\"\n+        check_ispytest(_ispytest)\n+        for prefix in (cls._CACHE_PREFIX_DIRS, cls._CACHE_PREFIX_VALUES):\n+            d = cachedir / prefix\n+            if d.is_dir():\n+                rm_rf(d)\n+\n+    @staticmethod\n+    def cache_dir_from_config(config: Config, *, _ispytest: bool = False) -> Path:\n+        \"\"\"Get the path to the cache directory for a Config.\n+\n+        :meta private:\n+        \"\"\"\n+        check_ispytest(_ispytest)\n+        return resolve_from_str(config.getini(\"cache_dir\"), config.rootpath)\n+\n+    def warn(self, fmt: str, *, _ispytest: bool = False, **args: object) -> None:\n+        \"\"\"Issue a cache warning.\n+\n+        :meta private:\n+        \"\"\"\n+        check_ispytest(_ispytest)\n+        import warnings\n+\n+        from _pytest.warning_types import PytestCacheWarning\n+\n+        warnings.warn(\n+            PytestCacheWarning(fmt.format(**args) if args else fmt),\n+            self._config.hook,\n+            stacklevel=3,\n+        )\n+\n+    def _mkdir(self, path: Path) -> None:\n+        self._ensure_cache_dir_and_supporting_files()\n+        path.mkdir(exist_ok=True, parents=True)\n+\n+    def mkdir(self, name: str) -> Path:\n+        \"\"\"Return a directory path object with the given name.\n+\n+        If the directory does not yet exist, it will be created. You can use\n+        it to manage files to e.g. store/retrieve database dumps across test\n+        sessions.\n+\n+        .. versionadded:: 7.0\n+\n+        :param name:\n+            Must be a string not containing a ``/`` separator.\n+            Make sure the name contains your plugin or application\n+            identifiers to prevent clashes with other cache users.\n+        \"\"\"\n+        path = Path(name)\n+        if len(path.parts) > 1:\n+            raise ValueError(\"name is not allowed to contain path separators\")\n+        res = self._cachedir.joinpath(self._CACHE_PREFIX_DIRS, path)\n+        self._mkdir(res)\n+        return res\n+\n+    def _getvaluepath(self, key: str) -> Path:\n+        return self._cachedir.joinpath(self._CACHE_PREFIX_VALUES, Path(key))\n+\n+    def get(self, key: str, default):\n+        \"\"\"Return the cached value for the given key.\n+\n+        If no value was yet cached or the value cannot be read, the specified\n+        default is returned.\n+\n+        :param key:\n+            Must be a ``/`` separated value. Usually the first\n+            name is the name of your plugin or your application.\n+        :param default:\n+            The value to return in case of a cache-miss or invalid cache value.\n+        \"\"\"\n+        path = self._getvaluepath(key)\n+        try:\n+            with path.open(\"r\", encoding=\"UTF-8\") as f:\n+                return json.load(f)\n+        except (ValueError, OSError):\n+            return default\n+\n+    def set(self, key: str, value: object) -> None:\n+        \"\"\"Save value for the given key.\n+\n+        :param key:\n+            Must be a ``/`` separated value. Usually the first\n+            name is the name of your plugin or your application.\n+        :param value:\n+            Must be of any combination of basic python types,\n+            including nested types like lists of dictionaries.\n+        \"\"\"\n+        path = self._getvaluepath(key)\n+        try:\n+            self._mkdir(path.parent)\n+        except OSError as exc:\n+            self.warn(\n+                f\"could not create cache path {path}: {exc}\",\n+                _ispytest=True,\n+            )\n+            return\n+        data = json.dumps(value, ensure_ascii=False, indent=2)\n+        try:\n+            f = path.open(\"w\", encoding=\"UTF-8\")\n+        except OSError as exc:\n+            self.warn(\n+                f\"cache could not write path {path}: {exc}\",\n+                _ispytest=True,\n+            )\n+        else:\n+            with f:\n+                f.write(data)\n+\n+    def _ensure_cache_dir_and_supporting_files(self) -> None:\n+        \"\"\"Create the cache dir and its supporting files.\"\"\"\n+        if self._cachedir.is_dir():\n+            return\n+\n+        self._cachedir.parent.mkdir(parents=True, exist_ok=True)\n+        with tempfile.TemporaryDirectory(\n+            prefix=\"pytest-cache-files-\",\n+            dir=self._cachedir.parent,\n+        ) as newpath:\n+            path = Path(newpath)\n+\n+            # Reset permissions to the default, see #12308.\n+            # Note: there's no way to get the current umask atomically, eek.\n+            umask = os.umask(0o022)\n+            os.umask(umask)\n+            path.chmod(0o777 - umask)\n+\n+            with open(path.joinpath(\"README.md\"), \"x\", encoding=\"UTF-8\") as f:\n+                f.write(README_CONTENT)\n+            with open(path.joinpath(\".gitignore\"), \"x\", encoding=\"UTF-8\") as f:\n+                f.write(\"# Created by pytest automatically.\\n*\\n\")\n+            with open(path.joinpath(\"CACHEDIR.TAG\"), \"xb\") as f:\n+                f.write(CACHEDIR_TAG_CONTENT)\n+\n+            try:\n+                path.rename(self._cachedir)\n+            except OSError as e:\n+                # If 2 concurrent pytests both race to the rename, the loser\n+                # gets \"Directory not empty\" from the rename. In this case,\n+                # everything is handled so just continue (while letting the\n+                # temporary directory be cleaned up).\n+                # On Windows, the error is a FileExistsError which translates to EEXIST.\n+                if e.errno not in (errno.ENOTEMPTY, errno.EEXIST):\n+                    raise\n+            else:\n+                # Create a directory in place of the one we just moved so that\n+                # `TemporaryDirectory`'s cleanup doesn't complain.\n+                #\n+                # TODO: pass ignore_cleanup_errors=True when we no longer support python < 3.10.\n+                # See https://github.com/python/cpython/issues/74168. Note that passing\n+                # delete=False would do the wrong thing in case of errors and isn't supported\n+                # until python 3.12.\n+                path.mkdir()\n+\n+\n+class LFPluginCollWrapper:\n+    def __init__(self, lfplugin: LFPlugin) -> None:\n+        self.lfplugin = lfplugin\n+        self._collected_at_least_one_failure = False\n+\n+    @hookimpl(wrapper=True)\n+    def pytest_make_collect_report(\n+        self, collector: nodes.Collector\n+    ) -> Generator[None, CollectReport, CollectReport]:\n+        res = yield\n+        if isinstance(collector, (Session, Directory)):\n+            # Sort any lf-paths to the beginning.\n+            lf_paths = self.lfplugin._last_failed_paths\n+\n+            # Use stable sort to prioritize last failed.\n+            def sort_key(node: nodes.Item | nodes.Collector) -> bool:\n+                return node.path in lf_paths\n+\n+            res.result = sorted(\n+                res.result,\n+                key=sort_key,\n+                reverse=True,\n+            )\n+\n+        elif isinstance(collector, File):\n+            if collector.path in self.lfplugin._last_failed_paths:\n+                result = res.result\n+                lastfailed = self.lfplugin.lastfailed\n+\n+                # Only filter with known failures.\n+                if not self._collected_at_least_one_failure:\n+                    if not any(x.nodeid in lastfailed for x in result):\n+                        return res\n+                    self.lfplugin.config.pluginmanager.register(\n+                        LFPluginCollSkipfiles(self.lfplugin), \"lfplugin-collskip\"\n+                    )\n+                    self._collected_at_least_one_failure = True\n+\n+                session = collector.session\n+                result[:] = [\n+                    x\n+                    for x in result\n+                    if x.nodeid in lastfailed\n+                    # Include any passed arguments (not trivial to filter).\n+                    or session.isinitpath(x.path)\n+                    # Keep all sub-collectors.\n+                    or isinstance(x, nodes.Collector)\n+                ]\n+\n+        return res\n+\n+\n+class LFPluginCollSkipfiles:\n+    def __init__(self, lfplugin: LFPlugin) -> None:\n+        self.lfplugin = lfplugin\n+\n+    @hookimpl\n+    def pytest_make_collect_report(\n+        self, collector: nodes.Collector\n+    ) -> CollectReport | None:\n+        if isinstance(collector, File):\n+            if collector.path not in self.lfplugin._last_failed_paths:\n+                self.lfplugin._skipped_files += 1\n+\n+                return CollectReport(\n+                    collector.nodeid, \"passed\", longrepr=None, result=[]\n+                )\n+        return None\n+\n+\n+class LFPlugin:\n+    \"\"\"Plugin which implements the --lf (run last-failing) option.\"\"\"\n+\n+    def __init__(self, config: Config) -> None:\n+        self.config = config\n+        active_keys = \"lf\", \"failedfirst\"\n+        self.active = any(config.getoption(key) for key in active_keys)\n+        assert config.cache\n+        self.lastfailed: dict[str, bool] = config.cache.get(\"cache/lastfailed\", {})\n+        self._previously_failed_count: int | None = None\n+        self._report_status: str | None = None\n+        self._skipped_files = 0  # count skipped files during collection due to --lf\n+\n+        if config.getoption(\"lf\"):\n+            self._last_failed_paths = self.get_last_failed_paths()\n+            config.pluginmanager.register(\n+                LFPluginCollWrapper(self), \"lfplugin-collwrapper\"\n+            )\n+\n+    def get_last_failed_paths(self) -> set[Path]:\n+        \"\"\"Return a set with all Paths of the previously failed nodeids and\n+        their parents.\"\"\"\n+        rootpath = self.config.rootpath\n+        result = set()\n+        for nodeid in self.lastfailed:\n+            path = rootpath / nodeid.split(\"::\")[0]\n+            result.add(path)\n+            result.update(path.parents)\n+        return {x for x in result if x.exists()}\n+\n+    def pytest_report_collectionfinish(self) -> str | None:\n+        if self.active and self.config.get_verbosity() >= 0:\n+            return f\"run-last-failure: {self._report_status}\"\n+        return None\n+\n+    def pytest_runtest_logreport(self, report: TestReport) -> None:\n+        if (report.when == \"call\" and report.passed) or report.skipped:\n+            self.lastfailed.pop(report.nodeid, None)\n+        elif report.failed:\n+            self.lastfailed[report.nodeid] = True\n+\n+    def pytest_collectreport(self, report: CollectReport) -> None:\n+        passed = report.outcome in (\"passed\", \"skipped\")\n+        if passed:\n+            if report.nodeid in self.lastfailed:\n+                self.lastfailed.pop(report.nodeid)\n+                self.lastfailed.update((item.nodeid, True) for item in report.result)\n+        else:\n+            self.lastfailed[report.nodeid] = True\n+\n+    @hookimpl(wrapper=True, tryfirst=True)\n+    def pytest_collection_modifyitems(\n+        self, config: Config, items: list[nodes.Item]\n+    ) -> Generator[None]:\n+        res = yield\n+\n+        if not self.active:\n+            return res\n+\n+        if self.lastfailed:\n+            previously_failed = []\n+            previously_passed = []\n+            for item in items:\n+                if item.nodeid in self.lastfailed:\n+                    previously_failed.append(item)\n+                else:\n+                    previously_passed.append(item)\n+            self._previously_failed_count = len(previously_failed)\n+\n+            if not previously_failed:\n+                # Running a subset of all tests with recorded failures\n+                # only outside of it.\n+                self._report_status = (\n+                    f\"{len(self.lastfailed)} known failures not in selected tests\"\n+                )\n+            else:\n+                if self.config.getoption(\"lf\"):\n+                    items[:] = previously_failed\n+                    config.hook.pytest_deselected(items=previously_passed)\n+                else:  # --failedfirst\n+                    items[:] = previously_failed + previously_passed\n+\n+                noun = \"failure\" if self._previously_failed_count == 1 else \"failures\"\n+                suffix = \" first\" if self.config.getoption(\"failedfirst\") else \"\"\n+                self._report_status = (\n+                    f\"rerun previous {self._previously_failed_count} {noun}{suffix}\"\n+                )\n+\n+            if self._skipped_files > 0:\n+                files_noun = \"file\" if self._skipped_files == 1 else \"files\"\n+                self._report_status += f\" (skipped {self._skipped_files} {files_noun})\"\n+        else:\n+            self._report_status = \"no previously failed tests, \"\n+            if self.config.getoption(\"last_failed_no_failures\") == \"none\":\n+                self._report_status += \"deselecting all items.\"\n+                config.hook.pytest_deselected(items=items[:])\n+                items[:] = []\n+            else:\n+                self._report_status += \"not deselecting items.\"\n+\n+        return res\n+\n+    def pytest_sessionfinish(self, session: Session) -> None:\n+        config = self.config\n+        if config.getoption(\"cacheshow\") or hasattr(config, \"workerinput\"):\n+            return\n+\n+        assert config.cache is not None\n+        saved_lastfailed = config.cache.get(\"cache/lastfailed\", {})\n+        if saved_lastfailed != self.lastfailed:\n+            config.cache.set(\"cache/lastfailed\", self.lastfailed)\n+\n+\n+class NFPlugin:\n+    \"\"\"Plugin which implements the --nf (run new-first) option.\"\"\"\n+\n+    def __init__(self, config: Config) -> None:\n+        self.config = config\n+        self.active = config.option.newfirst\n+        assert config.cache is not None\n+        self.cached_nodeids = set(config.cache.get(\"cache/nodeids\", []))\n+\n+    @hookimpl(wrapper=True, tryfirst=True)\n+    def pytest_collection_modifyitems(self, items: list[nodes.Item]) -> Generator[None]:\n+        res = yield\n+\n+        if self.active:\n+            new_items: dict[str, nodes.Item] = {}\n+            other_items: dict[str, nodes.Item] = {}\n+            for item in items:\n+                if item.nodeid not in self.cached_nodeids:\n+                    new_items[item.nodeid] = item\n+                else:\n+                    other_items[item.nodeid] = item\n+\n+            items[:] = self._get_increasing_order(\n+                new_items.values()\n+            ) + self._get_increasing_order(other_items.values())\n+            self.cached_nodeids.update(new_items)\n+        else:\n+            self.cached_nodeids.update(item.nodeid for item in items)\n+\n+        return res\n+\n+    def _get_increasing_order(self, items: Iterable[nodes.Item]) -> list[nodes.Item]:\n+        return sorted(items, key=lambda item: item.path.stat().st_mtime, reverse=True)\n+\n+    def pytest_sessionfinish(self) -> None:\n+        config = self.config\n+        if config.getoption(\"cacheshow\") or hasattr(config, \"workerinput\"):\n+            return\n+\n+        if config.getoption(\"collectonly\"):\n+            return\n+\n+        assert config.cache is not None\n+        config.cache.set(\"cache/nodeids\", sorted(self.cached_nodeids))\n+\n+\n+def pytest_addoption(parser: Parser) -> None:\n+    group = parser.getgroup(\"general\")\n+    group.addoption(\n+        \"--lf\",\n+        \"--last-failed\",\n+        action=\"store_true\",\n+        dest=\"lf\",\n+        help=\"Rerun only the tests that failed at the last run (or all if none failed)\",\n+    )\n+    group.addoption(\n+        \"--ff\",\n+        \"--failed-first\",\n+        action=\"store_true\",\n+        dest=\"failedfirst\",\n+        help=\"Run all tests, but run the last failures first. \"\n+        \"This may re-order tests and thus lead to \"\n+        \"repeated fixture setup/teardown.\",\n+    )\n+    group.addoption(\n+        \"--nf\",\n+        \"--new-first\",\n+        action=\"store_true\",\n+        dest=\"newfirst\",\n+        help=\"Run tests from new files first, then the rest of the tests \"\n+        \"sorted by file mtime\",\n+    )\n+    group.addoption(\n+        \"--cache-show\",\n+        action=\"append\",\n+        nargs=\"?\",\n+        dest=\"cacheshow\",\n+        help=(\n+            \"Show cache contents, don't perform collection or tests. \"\n+            \"Optional argument: glob (default: '*').\"\n+        ),\n+    )\n+    group.addoption(\n+        \"--cache-clear\",\n+        action=\"store_true\",\n+        dest=\"cacheclear\",\n+        help=\"Remove all cache contents at start of test run\",\n+    )\n+    cache_dir_default = \".pytest_cache\"\n+    if \"TOX_ENV_DIR\" in os.environ:\n+        cache_dir_default = os.path.join(os.environ[\"TOX_ENV_DIR\"], cache_dir_default)\n+    parser.addini(\"cache_dir\", default=cache_dir_default, help=\"Cache directory path\")\n+    group.addoption(\n+        \"--lfnf\",\n+        \"--last-failed-no-failures\",\n+        action=\"store\",\n+        dest=\"last_failed_no_failures\",\n+        choices=(\"all\", \"none\"),\n+        default=\"all\",\n+        help=\"With ``--lf``, determines whether to execute tests when there \"\n+        \"are no previously (known) failures or when no \"\n+        \"cached ``lastfailed`` data was found. \"\n+        \"``all`` (the default) runs the full test suite again. \"\n+        \"``none`` just emits a message about no known failures and exits successfully.\",\n+    )\n+\n+\n+def pytest_cmdline_main(config: Config) -> int | ExitCode | None:\n+    if config.option.cacheshow and not config.option.help:\n+        from _pytest.main import wrap_session\n+\n+        return wrap_session(config, cacheshow)\n+    return None\n+\n+\n+@hookimpl(tryfirst=True)\n+def pytest_configure(config: Config) -> None:\n+    config.cache = Cache.for_config(config, _ispytest=True)\n+    config.pluginmanager.register(LFPlugin(config), \"lfplugin\")\n+    config.pluginmanager.register(NFPlugin(config), \"nfplugin\")\n+\n+\n+@fixture\n+def cache(request: FixtureRequest) -> Cache:\n+    \"\"\"Return a cache object that can persist state between testing sessions.\n+\n+    cache.get(key, default)\n+    cache.set(key, value)\n+\n+    Keys must be ``/`` separated strings, where the first part is usually the\n+    name of your plugin or application to avoid clashes with other cache users.\n+\n+    Values can be any object handled by the json stdlib module.\n+    \"\"\"\n+    assert request.config.cache is not None\n+    return request.config.cache\n+\n+\n+def pytest_report_header(config: Config) -> str | None:\n+    \"\"\"Display cachedir with --cache-show and if non-default.\"\"\"\n+    if config.option.verbose > 0 or config.getini(\"cache_dir\") != \".pytest_cache\":\n+        assert config.cache is not None\n+        cachedir = config.cache._cachedir\n+        # TODO: evaluate generating upward relative paths\n+        # starting with .., ../.. if sensible\n+\n+        try:\n+            displaypath = cachedir.relative_to(config.rootpath)\n+        except ValueError:\n+            displaypath = cachedir\n+        return f\"cachedir: {displaypath}\"\n+    return None\n+\n+\n+def cacheshow(config: Config, session: Session) -> int:\n+    from pprint import pformat\n+\n+    assert config.cache is not None\n+\n+    tw = TerminalWriter()\n+    tw.line(\"cachedir: \" + str(config.cache._cachedir))\n+    if not config.cache._cachedir.is_dir():\n+        tw.line(\"cache is empty\")\n+        return 0\n+\n+    glob = config.option.cacheshow[0]\n+    if glob is None:\n+        glob = \"*\"\n+\n+    dummy = object()\n+    basedir = config.cache._cachedir\n+    vdir = basedir / Cache._CACHE_PREFIX_VALUES\n+    tw.sep(\"-\", f\"cache values for {glob!r}\")\n+    for valpath in sorted(x for x in vdir.rglob(glob) if x.is_file()):\n+        key = str(valpath.relative_to(vdir))\n+        val = config.cache.get(key, dummy)\n+        if val is dummy:\n+            tw.line(f\"{key} contains unreadable content, will be ignored\")\n+        else:\n+            tw.line(f\"{key} contains:\")\n+            for line in pformat(val).splitlines():\n+                tw.line(\"  \" + line)\n+\n+    ddir = basedir / Cache._CACHE_PREFIX_DIRS\n+    if ddir.is_dir():\n+        contents = sorted(ddir.rglob(glob))\n+        tw.sep(\"-\", f\"cache directories for {glob!r}\")\n+        for p in contents:\n+            # if p.is_dir():\n+            #    print(\"%s/\" % p.relative_to(basedir))\n+            if p.is_file():\n+                key = str(p.relative_to(basedir))\n+                tw.line(f\"{key} is a file of length {p.stat().st_size}\")\n+    return 0",
      "patch_lines": [
        "@@ -0,0 +1,625 @@\n",
        "+# mypy: allow-untyped-defs\n",
        "+\"\"\"Implementation of the cache provider.\"\"\"\n",
        "+\n",
        "+# This plugin was not named \"cache\" to avoid conflicts with the external\n",
        "+# pytest-cache version.\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+from collections.abc import Generator\n",
        "+from collections.abc import Iterable\n",
        "+import dataclasses\n",
        "+import errno\n",
        "+import json\n",
        "+import os\n",
        "+from pathlib import Path\n",
        "+import tempfile\n",
        "+from typing import final\n",
        "+\n",
        "+from .pathlib import resolve_from_str\n",
        "+from .pathlib import rm_rf\n",
        "+from .reports import CollectReport\n",
        "+from _pytest import nodes\n",
        "+from _pytest._io import TerminalWriter\n",
        "+from _pytest.config import Config\n",
        "+from _pytest.config import ExitCode\n",
        "+from _pytest.config import hookimpl\n",
        "+from _pytest.config.argparsing import Parser\n",
        "+from _pytest.deprecated import check_ispytest\n",
        "+from _pytest.fixtures import fixture\n",
        "+from _pytest.fixtures import FixtureRequest\n",
        "+from _pytest.main import Session\n",
        "+from _pytest.nodes import Directory\n",
        "+from _pytest.nodes import File\n",
        "+from _pytest.reports import TestReport\n",
        "+\n",
        "+\n",
        "+README_CONTENT = \"\"\"\\\n",
        "+# pytest cache directory #\n",
        "+\n",
        "+This directory contains data from the pytest's cache plugin,\n",
        "+which provides the `--lf` and `--ff` options, as well as the `cache` fixture.\n",
        "+\n",
        "+**Do not** commit this to version control.\n",
        "+\n",
        "+See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.\n",
        "+\"\"\"\n",
        "+\n",
        "+CACHEDIR_TAG_CONTENT = b\"\"\"\\\n",
        "+Signature: 8a477f597d28d172789f06886806bc55\n",
        "+# This file is a cache directory tag created by pytest.\n",
        "+# For information about cache directory tags, see:\n",
        "+#\thttps://bford.info/cachedir/spec.html\n",
        "+\"\"\"\n",
        "+\n",
        "+\n",
        "+@final\n",
        "+@dataclasses.dataclass\n",
        "+class Cache:\n",
        "+    \"\"\"Instance of the `cache` fixture.\"\"\"\n",
        "+\n",
        "+    _cachedir: Path = dataclasses.field(repr=False)\n",
        "+    _config: Config = dataclasses.field(repr=False)\n",
        "+\n",
        "+    # Sub-directory under cache-dir for directories created by `mkdir()`.\n",
        "+    _CACHE_PREFIX_DIRS = \"d\"\n",
        "+\n",
        "+    # Sub-directory under cache-dir for values created by `set()`.\n",
        "+    _CACHE_PREFIX_VALUES = \"v\"\n",
        "+\n",
        "+    def __init__(\n",
        "+        self, cachedir: Path, config: Config, *, _ispytest: bool = False\n",
        "+    ) -> None:\n",
        "+        check_ispytest(_ispytest)\n",
        "+        self._cachedir = cachedir\n",
        "+        self._config = config\n",
        "+\n",
        "+    @classmethod\n",
        "+    def for_config(cls, config: Config, *, _ispytest: bool = False) -> Cache:\n",
        "+        \"\"\"Create the Cache instance for a Config.\n",
        "+\n",
        "+        :meta private:\n",
        "+        \"\"\"\n",
        "+        check_ispytest(_ispytest)\n",
        "+        cachedir = cls.cache_dir_from_config(config, _ispytest=True)\n",
        "+        if config.getoption(\"cacheclear\") and cachedir.is_dir():\n",
        "+            cls.clear_cache(cachedir, _ispytest=True)\n",
        "+        return cls(cachedir, config, _ispytest=True)\n",
        "+\n",
        "+    @classmethod\n",
        "+    def clear_cache(cls, cachedir: Path, _ispytest: bool = False) -> None:\n",
        "+        \"\"\"Clear the sub-directories used to hold cached directories and values.\n",
        "+\n",
        "+        :meta private:\n",
        "+        \"\"\"\n",
        "+        check_ispytest(_ispytest)\n",
        "+        for prefix in (cls._CACHE_PREFIX_DIRS, cls._CACHE_PREFIX_VALUES):\n",
        "+            d = cachedir / prefix\n",
        "+            if d.is_dir():\n",
        "+                rm_rf(d)\n",
        "+\n",
        "+    @staticmethod\n",
        "+    def cache_dir_from_config(config: Config, *, _ispytest: bool = False) -> Path:\n",
        "+        \"\"\"Get the path to the cache directory for a Config.\n",
        "+\n",
        "+        :meta private:\n",
        "+        \"\"\"\n",
        "+        check_ispytest(_ispytest)\n",
        "+        return resolve_from_str(config.getini(\"cache_dir\"), config.rootpath)\n",
        "+\n",
        "+    def warn(self, fmt: str, *, _ispytest: bool = False, **args: object) -> None:\n",
        "+        \"\"\"Issue a cache warning.\n",
        "+\n",
        "+        :meta private:\n",
        "+        \"\"\"\n",
        "+        check_ispytest(_ispytest)\n",
        "+        import warnings\n",
        "+\n",
        "+        from _pytest.warning_types import PytestCacheWarning\n",
        "+\n",
        "+        warnings.warn(\n",
        "+            PytestCacheWarning(fmt.format(**args) if args else fmt),\n",
        "+            self._config.hook,\n",
        "+            stacklevel=3,\n",
        "+        )\n",
        "+\n",
        "+    def _mkdir(self, path: Path) -> None:\n",
        "+        self._ensure_cache_dir_and_supporting_files()\n",
        "+        path.mkdir(exist_ok=True, parents=True)\n",
        "+\n",
        "+    def mkdir(self, name: str) -> Path:\n",
        "+        \"\"\"Return a directory path object with the given name.\n",
        "+\n",
        "+        If the directory does not yet exist, it will be created. You can use\n",
        "+        it to manage files to e.g. store/retrieve database dumps across test\n",
        "+        sessions.\n",
        "+\n",
        "+        .. versionadded:: 7.0\n",
        "+\n",
        "+        :param name:\n",
        "+            Must be a string not containing a ``/`` separator.\n",
        "+            Make sure the name contains your plugin or application\n",
        "+            identifiers to prevent clashes with other cache users.\n",
        "+        \"\"\"\n",
        "+        path = Path(name)\n",
        "+        if len(path.parts) > 1:\n",
        "+            raise ValueError(\"name is not allowed to contain path separators\")\n",
        "+        res = self._cachedir.joinpath(self._CACHE_PREFIX_DIRS, path)\n",
        "+        self._mkdir(res)\n",
        "+        return res\n",
        "+\n",
        "+    def _getvaluepath(self, key: str) -> Path:\n",
        "+        return self._cachedir.joinpath(self._CACHE_PREFIX_VALUES, Path(key))\n",
        "+\n",
        "+    def get(self, key: str, default):\n",
        "+        \"\"\"Return the cached value for the given key.\n",
        "+\n",
        "+        If no value was yet cached or the value cannot be read, the specified\n",
        "+        default is returned.\n",
        "+\n",
        "+        :param key:\n",
        "+            Must be a ``/`` separated value. Usually the first\n",
        "+            name is the name of your plugin or your application.\n",
        "+        :param default:\n",
        "+            The value to return in case of a cache-miss or invalid cache value.\n",
        "+        \"\"\"\n",
        "+        path = self._getvaluepath(key)\n",
        "+        try:\n",
        "+            with path.open(\"r\", encoding=\"UTF-8\") as f:\n",
        "+                return json.load(f)\n",
        "+        except (ValueError, OSError):\n",
        "+            return default\n",
        "+\n",
        "+    def set(self, key: str, value: object) -> None:\n",
        "+        \"\"\"Save value for the given key.\n",
        "+\n",
        "+        :param key:\n",
        "+            Must be a ``/`` separated value. Usually the first\n",
        "+            name is the name of your plugin or your application.\n",
        "+        :param value:\n",
        "+            Must be of any combination of basic python types,\n",
        "+            including nested types like lists of dictionaries.\n",
        "+        \"\"\"\n",
        "+        path = self._getvaluepath(key)\n",
        "+        try:\n",
        "+            self._mkdir(path.parent)\n",
        "+        except OSError as exc:\n",
        "+            self.warn(\n",
        "+                f\"could not create cache path {path}: {exc}\",\n",
        "+                _ispytest=True,\n",
        "+            )\n",
        "+            return\n",
        "+        data = json.dumps(value, ensure_ascii=False, indent=2)\n",
        "+        try:\n",
        "+            f = path.open(\"w\", encoding=\"UTF-8\")\n",
        "+        except OSError as exc:\n",
        "+            self.warn(\n",
        "+                f\"cache could not write path {path}: {exc}\",\n",
        "+                _ispytest=True,\n",
        "+            )\n",
        "+        else:\n",
        "+            with f:\n",
        "+                f.write(data)\n",
        "+\n",
        "+    def _ensure_cache_dir_and_supporting_files(self) -> None:\n",
        "+        \"\"\"Create the cache dir and its supporting files.\"\"\"\n",
        "+        if self._cachedir.is_dir():\n",
        "+            return\n",
        "+\n",
        "+        self._cachedir.parent.mkdir(parents=True, exist_ok=True)\n",
        "+        with tempfile.TemporaryDirectory(\n",
        "+            prefix=\"pytest-cache-files-\",\n",
        "+            dir=self._cachedir.parent,\n",
        "+        ) as newpath:\n",
        "+            path = Path(newpath)\n",
        "+\n",
        "+            # Reset permissions to the default, see #12308.\n",
        "+            # Note: there's no way to get the current umask atomically, eek.\n",
        "+            umask = os.umask(0o022)\n",
        "+            os.umask(umask)\n",
        "+            path.chmod(0o777 - umask)\n",
        "+\n",
        "+            with open(path.joinpath(\"README.md\"), \"x\", encoding=\"UTF-8\") as f:\n",
        "+                f.write(README_CONTENT)\n",
        "+            with open(path.joinpath(\".gitignore\"), \"x\", encoding=\"UTF-8\") as f:\n",
        "+                f.write(\"# Created by pytest automatically.\\n*\\n\")\n",
        "+            with open(path.joinpath(\"CACHEDIR.TAG\"), \"xb\") as f:\n",
        "+                f.write(CACHEDIR_TAG_CONTENT)\n",
        "+\n",
        "+            try:\n",
        "+                path.rename(self._cachedir)\n",
        "+            except OSError as e:\n",
        "+                # If 2 concurrent pytests both race to the rename, the loser\n",
        "+                # gets \"Directory not empty\" from the rename. In this case,\n",
        "+                # everything is handled so just continue (while letting the\n",
        "+                # temporary directory be cleaned up).\n",
        "+                # On Windows, the error is a FileExistsError which translates to EEXIST.\n",
        "+                if e.errno not in (errno.ENOTEMPTY, errno.EEXIST):\n",
        "+                    raise\n",
        "+            else:\n",
        "+                # Create a directory in place of the one we just moved so that\n",
        "+                # `TemporaryDirectory`'s cleanup doesn't complain.\n",
        "+                #\n",
        "+                # TODO: pass ignore_cleanup_errors=True when we no longer support python < 3.10.\n",
        "+                # See https://github.com/python/cpython/issues/74168. Note that passing\n",
        "+                # delete=False would do the wrong thing in case of errors and isn't supported\n",
        "+                # until python 3.12.\n",
        "+                path.mkdir()\n",
        "+\n",
        "+\n",
        "+class LFPluginCollWrapper:\n",
        "+    def __init__(self, lfplugin: LFPlugin) -> None:\n",
        "+        self.lfplugin = lfplugin\n",
        "+        self._collected_at_least_one_failure = False\n",
        "+\n",
        "+    @hookimpl(wrapper=True)\n",
        "+    def pytest_make_collect_report(\n",
        "+        self, collector: nodes.Collector\n",
        "+    ) -> Generator[None, CollectReport, CollectReport]:\n",
        "+        res = yield\n",
        "+        if isinstance(collector, (Session, Directory)):\n",
        "+            # Sort any lf-paths to the beginning.\n",
        "+            lf_paths = self.lfplugin._last_failed_paths\n",
        "+\n",
        "+            # Use stable sort to prioritize last failed.\n",
        "+            def sort_key(node: nodes.Item | nodes.Collector) -> bool:\n",
        "+                return node.path in lf_paths\n",
        "+\n",
        "+            res.result = sorted(\n",
        "+                res.result,\n",
        "+                key=sort_key,\n",
        "+                reverse=True,\n",
        "+            )\n",
        "+\n",
        "+        elif isinstance(collector, File):\n",
        "+            if collector.path in self.lfplugin._last_failed_paths:\n",
        "+                result = res.result\n",
        "+                lastfailed = self.lfplugin.lastfailed\n",
        "+\n",
        "+                # Only filter with known failures.\n",
        "+                if not self._collected_at_least_one_failure:\n",
        "+                    if not any(x.nodeid in lastfailed for x in result):\n",
        "+                        return res\n",
        "+                    self.lfplugin.config.pluginmanager.register(\n",
        "+                        LFPluginCollSkipfiles(self.lfplugin), \"lfplugin-collskip\"\n",
        "+                    )\n",
        "+                    self._collected_at_least_one_failure = True\n",
        "+\n",
        "+                session = collector.session\n",
        "+                result[:] = [\n",
        "+                    x\n",
        "+                    for x in result\n",
        "+                    if x.nodeid in lastfailed\n",
        "+                    # Include any passed arguments (not trivial to filter).\n",
        "+                    or session.isinitpath(x.path)\n",
        "+                    # Keep all sub-collectors.\n",
        "+                    or isinstance(x, nodes.Collector)\n",
        "+                ]\n",
        "+\n",
        "+        return res\n",
        "+\n",
        "+\n",
        "+class LFPluginCollSkipfiles:\n",
        "+    def __init__(self, lfplugin: LFPlugin) -> None:\n",
        "+        self.lfplugin = lfplugin\n",
        "+\n",
        "+    @hookimpl\n",
        "+    def pytest_make_collect_report(\n",
        "+        self, collector: nodes.Collector\n",
        "+    ) -> CollectReport | None:\n",
        "+        if isinstance(collector, File):\n",
        "+            if collector.path not in self.lfplugin._last_failed_paths:\n",
        "+                self.lfplugin._skipped_files += 1\n",
        "+\n",
        "+                return CollectReport(\n",
        "+                    collector.nodeid, \"passed\", longrepr=None, result=[]\n",
        "+                )\n",
        "+        return None\n",
        "+\n",
        "+\n",
        "+class LFPlugin:\n",
        "+    \"\"\"Plugin which implements the --lf (run last-failing) option.\"\"\"\n",
        "+\n",
        "+    def __init__(self, config: Config) -> None:\n",
        "+        self.config = config\n",
        "+        active_keys = \"lf\", \"failedfirst\"\n",
        "+        self.active = any(config.getoption(key) for key in active_keys)\n",
        "+        assert config.cache\n",
        "+        self.lastfailed: dict[str, bool] = config.cache.get(\"cache/lastfailed\", {})\n",
        "+        self._previously_failed_count: int | None = None\n",
        "+        self._report_status: str | None = None\n",
        "+        self._skipped_files = 0  # count skipped files during collection due to --lf\n",
        "+\n",
        "+        if config.getoption(\"lf\"):\n",
        "+            self._last_failed_paths = self.get_last_failed_paths()\n",
        "+            config.pluginmanager.register(\n",
        "+                LFPluginCollWrapper(self), \"lfplugin-collwrapper\"\n",
        "+            )\n",
        "+\n",
        "+    def get_last_failed_paths(self) -> set[Path]:\n",
        "+        \"\"\"Return a set with all Paths of the previously failed nodeids and\n",
        "+        their parents.\"\"\"\n",
        "+        rootpath = self.config.rootpath\n",
        "+        result = set()\n",
        "+        for nodeid in self.lastfailed:\n",
        "+            path = rootpath / nodeid.split(\"::\")[0]\n",
        "+            result.add(path)\n",
        "+            result.update(path.parents)\n",
        "+        return {x for x in result if x.exists()}\n",
        "+\n",
        "+    def pytest_report_collectionfinish(self) -> str | None:\n",
        "+        if self.active and self.config.get_verbosity() >= 0:\n",
        "+            return f\"run-last-failure: {self._report_status}\"\n",
        "+        return None\n",
        "+\n",
        "+    def pytest_runtest_logreport(self, report: TestReport) -> None:\n",
        "+        if (report.when == \"call\" and report.passed) or report.skipped:\n",
        "+            self.lastfailed.pop(report.nodeid, None)\n",
        "+        elif report.failed:\n",
        "+            self.lastfailed[report.nodeid] = True\n",
        "+\n",
        "+    def pytest_collectreport(self, report: CollectReport) -> None:\n",
        "+        passed = report.outcome in (\"passed\", \"skipped\")\n",
        "+        if passed:\n",
        "+            if report.nodeid in self.lastfailed:\n",
        "+                self.lastfailed.pop(report.nodeid)\n",
        "+                self.lastfailed.update((item.nodeid, True) for item in report.result)\n",
        "+        else:\n",
        "+            self.lastfailed[report.nodeid] = True\n",
        "+\n",
        "+    @hookimpl(wrapper=True, tryfirst=True)\n",
        "+    def pytest_collection_modifyitems(\n",
        "+        self, config: Config, items: list[nodes.Item]\n",
        "+    ) -> Generator[None]:\n",
        "+        res = yield\n",
        "+\n",
        "+        if not self.active:\n",
        "+            return res\n",
        "+\n",
        "+        if self.lastfailed:\n",
        "+            previously_failed = []\n",
        "+            previously_passed = []\n",
        "+            for item in items:\n",
        "+                if item.nodeid in self.lastfailed:\n",
        "+                    previously_failed.append(item)\n",
        "+                else:\n",
        "+                    previously_passed.append(item)\n",
        "+            self._previously_failed_count = len(previously_failed)\n",
        "+\n",
        "+            if not previously_failed:\n",
        "+                # Running a subset of all tests with recorded failures\n",
        "+                # only outside of it.\n",
        "+                self._report_status = (\n",
        "+                    f\"{len(self.lastfailed)} known failures not in selected tests\"\n",
        "+                )\n",
        "+            else:\n",
        "+                if self.config.getoption(\"lf\"):\n",
        "+                    items[:] = previously_failed\n",
        "+                    config.hook.pytest_deselected(items=previously_passed)\n",
        "+                else:  # --failedfirst\n",
        "+                    items[:] = previously_failed + previously_passed\n",
        "+\n",
        "+                noun = \"failure\" if self._previously_failed_count == 1 else \"failures\"\n",
        "+                suffix = \" first\" if self.config.getoption(\"failedfirst\") else \"\"\n",
        "+                self._report_status = (\n",
        "+                    f\"rerun previous {self._previously_failed_count} {noun}{suffix}\"\n",
        "+                )\n",
        "+\n",
        "+            if self._skipped_files > 0:\n",
        "+                files_noun = \"file\" if self._skipped_files == 1 else \"files\"\n",
        "+                self._report_status += f\" (skipped {self._skipped_files} {files_noun})\"\n",
        "+        else:\n",
        "+            self._report_status = \"no previously failed tests, \"\n",
        "+            if self.config.getoption(\"last_failed_no_failures\") == \"none\":\n",
        "+                self._report_status += \"deselecting all items.\"\n",
        "+                config.hook.pytest_deselected(items=items[:])\n",
        "+                items[:] = []\n",
        "+            else:\n",
        "+                self._report_status += \"not deselecting items.\"\n",
        "+\n",
        "+        return res\n",
        "+\n",
        "+    def pytest_sessionfinish(self, session: Session) -> None:\n",
        "+        config = self.config\n",
        "+        if config.getoption(\"cacheshow\") or hasattr(config, \"workerinput\"):\n",
        "+            return\n",
        "+\n",
        "+        assert config.cache is not None\n",
        "+        saved_lastfailed = config.cache.get(\"cache/lastfailed\", {})\n",
        "+        if saved_lastfailed != self.lastfailed:\n",
        "+            config.cache.set(\"cache/lastfailed\", self.lastfailed)\n",
        "+\n",
        "+\n",
        "+class NFPlugin:\n",
        "+    \"\"\"Plugin which implements the --nf (run new-first) option.\"\"\"\n",
        "+\n",
        "+    def __init__(self, config: Config) -> None:\n",
        "+        self.config = config\n",
        "+        self.active = config.option.newfirst\n",
        "+        assert config.cache is not None\n",
        "+        self.cached_nodeids = set(config.cache.get(\"cache/nodeids\", []))\n",
        "+\n",
        "+    @hookimpl(wrapper=True, tryfirst=True)\n",
        "+    def pytest_collection_modifyitems(self, items: list[nodes.Item]) -> Generator[None]:\n",
        "+        res = yield\n",
        "+\n",
        "+        if self.active:\n",
        "+            new_items: dict[str, nodes.Item] = {}\n",
        "+            other_items: dict[str, nodes.Item] = {}\n",
        "+            for item in items:\n",
        "+                if item.nodeid not in self.cached_nodeids:\n",
        "+                    new_items[item.nodeid] = item\n",
        "+                else:\n",
        "+                    other_items[item.nodeid] = item\n",
        "+\n",
        "+            items[:] = self._get_increasing_order(\n",
        "+                new_items.values()\n",
        "+            ) + self._get_increasing_order(other_items.values())\n",
        "+            self.cached_nodeids.update(new_items)\n",
        "+        else:\n",
        "+            self.cached_nodeids.update(item.nodeid for item in items)\n",
        "+\n",
        "+        return res\n",
        "+\n",
        "+    def _get_increasing_order(self, items: Iterable[nodes.Item]) -> list[nodes.Item]:\n",
        "+        return sorted(items, key=lambda item: item.path.stat().st_mtime, reverse=True)\n",
        "+\n",
        "+    def pytest_sessionfinish(self) -> None:\n",
        "+        config = self.config\n",
        "+        if config.getoption(\"cacheshow\") or hasattr(config, \"workerinput\"):\n",
        "+            return\n",
        "+\n",
        "+        if config.getoption(\"collectonly\"):\n",
        "+            return\n",
        "+\n",
        "+        assert config.cache is not None\n",
        "+        config.cache.set(\"cache/nodeids\", sorted(self.cached_nodeids))\n",
        "+\n",
        "+\n",
        "+def pytest_addoption(parser: Parser) -> None:\n",
        "+    group = parser.getgroup(\"general\")\n",
        "+    group.addoption(\n",
        "+        \"--lf\",\n",
        "+        \"--last-failed\",\n",
        "+        action=\"store_true\",\n",
        "+        dest=\"lf\",\n",
        "+        help=\"Rerun only the tests that failed at the last run (or all if none failed)\",\n",
        "+    )\n",
        "+    group.addoption(\n",
        "+        \"--ff\",\n",
        "+        \"--failed-first\",\n",
        "+        action=\"store_true\",\n",
        "+        dest=\"failedfirst\",\n",
        "+        help=\"Run all tests, but run the last failures first. \"\n",
        "+        \"This may re-order tests and thus lead to \"\n",
        "+        \"repeated fixture setup/teardown.\",\n",
        "+    )\n",
        "+    group.addoption(\n",
        "+        \"--nf\",\n",
        "+        \"--new-first\",\n",
        "+        action=\"store_true\",\n",
        "+        dest=\"newfirst\",\n",
        "+        help=\"Run tests from new files first, then the rest of the tests \"\n",
        "+        \"sorted by file mtime\",\n",
        "+    )\n",
        "+    group.addoption(\n",
        "+        \"--cache-show\",\n",
        "+        action=\"append\",\n",
        "+        nargs=\"?\",\n",
        "+        dest=\"cacheshow\",\n",
        "+        help=(\n",
        "+            \"Show cache contents, don't perform collection or tests. \"\n",
        "+            \"Optional argument: glob (default: '*').\"\n",
        "+        ),\n",
        "+    )\n",
        "+    group.addoption(\n",
        "+        \"--cache-clear\",\n",
        "+        action=\"store_true\",\n",
        "+        dest=\"cacheclear\",\n",
        "+        help=\"Remove all cache contents at start of test run\",\n",
        "+    )\n",
        "+    cache_dir_default = \".pytest_cache\"\n",
        "+    if \"TOX_ENV_DIR\" in os.environ:\n",
        "+        cache_dir_default = os.path.join(os.environ[\"TOX_ENV_DIR\"], cache_dir_default)\n",
        "+    parser.addini(\"cache_dir\", default=cache_dir_default, help=\"Cache directory path\")\n",
        "+    group.addoption(\n",
        "+        \"--lfnf\",\n",
        "+        \"--last-failed-no-failures\",\n",
        "+        action=\"store\",\n",
        "+        dest=\"last_failed_no_failures\",\n",
        "+        choices=(\"all\", \"none\"),\n",
        "+        default=\"all\",\n",
        "+        help=\"With ``--lf``, determines whether to execute tests when there \"\n",
        "+        \"are no previously (known) failures or when no \"\n",
        "+        \"cached ``lastfailed`` data was found. \"\n",
        "+        \"``all`` (the default) runs the full test suite again. \"\n",
        "+        \"``none`` just emits a message about no known failures and exits successfully.\",\n",
        "+    )\n",
        "+\n",
        "+\n",
        "+def pytest_cmdline_main(config: Config) -> int | ExitCode | None:\n",
        "+    if config.option.cacheshow and not config.option.help:\n",
        "+        from _pytest.main import wrap_session\n",
        "+\n",
        "+        return wrap_session(config, cacheshow)\n",
        "+    return None\n",
        "+\n",
        "+\n",
        "+@hookimpl(tryfirst=True)\n",
        "+def pytest_configure(config: Config) -> None:\n",
        "+    config.cache = Cache.for_config(config, _ispytest=True)\n",
        "+    config.pluginmanager.register(LFPlugin(config), \"lfplugin\")\n",
        "+    config.pluginmanager.register(NFPlugin(config), \"nfplugin\")\n",
        "+\n",
        "+\n",
        "+@fixture\n",
        "+def cache(request: FixtureRequest) -> Cache:\n",
        "+    \"\"\"Return a cache object that can persist state between testing sessions.\n",
        "+\n",
        "+    cache.get(key, default)\n",
        "+    cache.set(key, value)\n",
        "+\n",
        "+    Keys must be ``/`` separated strings, where the first part is usually the\n",
        "+    name of your plugin or application to avoid clashes with other cache users.\n",
        "+\n",
        "+    Values can be any object handled by the json stdlib module.\n",
        "+    \"\"\"\n",
        "+    assert request.config.cache is not None\n",
        "+    return request.config.cache\n",
        "+\n",
        "+\n",
        "+def pytest_report_header(config: Config) -> str | None:\n",
        "+    \"\"\"Display cachedir with --cache-show and if non-default.\"\"\"\n",
        "+    if config.option.verbose > 0 or config.getini(\"cache_dir\") != \".pytest_cache\":\n",
        "+        assert config.cache is not None\n",
        "+        cachedir = config.cache._cachedir\n",
        "+        # TODO: evaluate generating upward relative paths\n",
        "+        # starting with .., ../.. if sensible\n",
        "+\n",
        "+        try:\n",
        "+            displaypath = cachedir.relative_to(config.rootpath)\n",
        "+        except ValueError:\n",
        "+            displaypath = cachedir\n",
        "+        return f\"cachedir: {displaypath}\"\n",
        "+    return None\n",
        "+\n",
        "+\n",
        "+def cacheshow(config: Config, session: Session) -> int:\n",
        "+    from pprint import pformat\n",
        "+\n",
        "+    assert config.cache is not None\n",
        "+\n",
        "+    tw = TerminalWriter()\n",
        "+    tw.line(\"cachedir: \" + str(config.cache._cachedir))\n",
        "+    if not config.cache._cachedir.is_dir():\n",
        "+        tw.line(\"cache is empty\")\n",
        "+        return 0\n",
        "+\n",
        "+    glob = config.option.cacheshow[0]\n",
        "+    if glob is None:\n",
        "+        glob = \"*\"\n",
        "+\n",
        "+    dummy = object()\n",
        "+    basedir = config.cache._cachedir\n",
        "+    vdir = basedir / Cache._CACHE_PREFIX_VALUES\n",
        "+    tw.sep(\"-\", f\"cache values for {glob!r}\")\n",
        "+    for valpath in sorted(x for x in vdir.rglob(glob) if x.is_file()):\n",
        "+        key = str(valpath.relative_to(vdir))\n",
        "+        val = config.cache.get(key, dummy)\n",
        "+        if val is dummy:\n",
        "+            tw.line(f\"{key} contains unreadable content, will be ignored\")\n",
        "+        else:\n",
        "+            tw.line(f\"{key} contains:\")\n",
        "+            for line in pformat(val).splitlines():\n",
        "+                tw.line(\"  \" + line)\n",
        "+\n",
        "+    ddir = basedir / Cache._CACHE_PREFIX_DIRS\n",
        "+    if ddir.is_dir():\n",
        "+        contents = sorted(ddir.rglob(glob))\n",
        "+        tw.sep(\"-\", f\"cache directories for {glob!r}\")\n",
        "+        for p in contents:\n",
        "+            # if p.is_dir():\n",
        "+            #    print(\"%s/\" % p.relative_to(basedir))\n",
        "+            if p.is_file():\n",
        "+                key = str(p.relative_to(basedir))\n",
        "+                tw.line(f\"{key} is a file of length {p.stat().st_size}\")\n",
        "+    return 0\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/capture.py",
      "status": "added",
      "additions": 1144,
      "deletions": 0,
      "patch": "@@ -0,0 +1,1144 @@\n+# mypy: allow-untyped-defs\n+\"\"\"Per-test stdout/stderr capturing mechanism.\"\"\"\n+\n+from __future__ import annotations\n+\n+import abc\n+import collections\n+from collections.abc import Generator\n+from collections.abc import Iterable\n+from collections.abc import Iterator\n+import contextlib\n+import io\n+from io import UnsupportedOperation\n+import os\n+import sys\n+from tempfile import TemporaryFile\n+from types import TracebackType\n+from typing import Any\n+from typing import AnyStr\n+from typing import BinaryIO\n+from typing import cast\n+from typing import Final\n+from typing import final\n+from typing import Generic\n+from typing import Literal\n+from typing import NamedTuple\n+from typing import TextIO\n+from typing import TYPE_CHECKING\n+\n+\n+if TYPE_CHECKING:\n+    from typing_extensions import Self\n+\n+from _pytest.config import Config\n+from _pytest.config import hookimpl\n+from _pytest.config.argparsing import Parser\n+from _pytest.deprecated import check_ispytest\n+from _pytest.fixtures import fixture\n+from _pytest.fixtures import SubRequest\n+from _pytest.nodes import Collector\n+from _pytest.nodes import File\n+from _pytest.nodes import Item\n+from _pytest.reports import CollectReport\n+\n+\n+_CaptureMethod = Literal[\"fd\", \"sys\", \"no\", \"tee-sys\"]\n+\n+\n+def pytest_addoption(parser: Parser) -> None:\n+    group = parser.getgroup(\"general\")\n+    group.addoption(\n+        \"--capture\",\n+        action=\"store\",\n+        default=\"fd\",\n+        metavar=\"method\",\n+        choices=[\"fd\", \"sys\", \"no\", \"tee-sys\"],\n+        help=\"Per-test capturing method: one of fd|sys|no|tee-sys\",\n+    )\n+    group._addoption(  # private to use reserved lower-case short option\n+        \"-s\",\n+        action=\"store_const\",\n+        const=\"no\",\n+        dest=\"capture\",\n+        help=\"Shortcut for --capture=no\",\n+    )\n+\n+\n+def _colorama_workaround() -> None:\n+    \"\"\"Ensure colorama is imported so that it attaches to the correct stdio\n+    handles on Windows.\n+\n+    colorama uses the terminal on import time. So if something does the\n+    first import of colorama while I/O capture is active, colorama will\n+    fail in various ways.\n+    \"\"\"\n+    if sys.platform.startswith(\"win32\"):\n+        try:\n+            import colorama  # noqa: F401\n+        except ImportError:\n+            pass\n+\n+\n+def _readline_workaround() -> None:\n+    \"\"\"Ensure readline is imported early so it attaches to the correct stdio handles.\n+\n+    This isn't a problem with the default GNU readline implementation, but in\n+    some configurations, Python uses libedit instead (on macOS, and for prebuilt\n+    binaries such as used by uv).\n+\n+    In theory this is only needed if readline.backend == \"libedit\", but the\n+    workaround consists of importing readline here, so we already worked around\n+    the issue by the time we could check if we need to.\n+    \"\"\"\n+    try:\n+        import readline  # noqa: F401\n+    except ImportError:\n+        pass\n+\n+\n+def _windowsconsoleio_workaround(stream: TextIO) -> None:\n+    \"\"\"Workaround for Windows Unicode console handling.\n+\n+    Python 3.6 implemented Unicode console handling for Windows. This works\n+    by reading/writing to the raw console handle using\n+    ``{Read,Write}ConsoleW``.\n+\n+    The problem is that we are going to ``dup2`` over the stdio file\n+    descriptors when doing ``FDCapture`` and this will ``CloseHandle`` the\n+    handles used by Python to write to the console. Though there is still some\n+    weirdness and the console handle seems to only be closed randomly and not\n+    on the first call to ``CloseHandle``, or maybe it gets reopened with the\n+    same handle value when we suspend capturing.\n+\n+    The workaround in this case will reopen stdio with a different fd which\n+    also means a different handle by replicating the logic in\n+    \"Py_lifecycle.c:initstdio/create_stdio\".\n+\n+    :param stream:\n+        In practice ``sys.stdout`` or ``sys.stderr``, but given\n+        here as parameter for unittesting purposes.\n+\n+    See https://github.com/pytest-dev/py/issues/103.\n+    \"\"\"\n+    if not sys.platform.startswith(\"win32\") or hasattr(sys, \"pypy_version_info\"):\n+        return\n+\n+    # Bail out if ``stream`` doesn't seem like a proper ``io`` stream (#2666).\n+    if not hasattr(stream, \"buffer\"):  # type: ignore[unreachable,unused-ignore]\n+        return\n+\n+    raw_stdout = stream.buffer.raw if hasattr(stream.buffer, \"raw\") else stream.buffer\n+\n+    if not isinstance(raw_stdout, io._WindowsConsoleIO):  # type: ignore[attr-defined,unused-ignore]\n+        return\n+\n+    def _reopen_stdio(f, mode):\n+        if not hasattr(stream.buffer, \"raw\") and mode[0] == \"w\":\n+            buffering = 0\n+        else:\n+            buffering = -1\n+\n+        return io.TextIOWrapper(\n+            open(os.dup(f.fileno()), mode, buffering),\n+            f.encoding,\n+            f.errors,\n+            f.newlines,\n+            f.line_buffering,\n+        )\n+\n+    sys.stdin = _reopen_stdio(sys.stdin, \"rb\")\n+    sys.stdout = _reopen_stdio(sys.stdout, \"wb\")\n+    sys.stderr = _reopen_stdio(sys.stderr, \"wb\")\n+\n+\n+@hookimpl(wrapper=True)\n+def pytest_load_initial_conftests(early_config: Config) -> Generator[None]:\n+    ns = early_config.known_args_namespace\n+    if ns.capture == \"fd\":\n+        _windowsconsoleio_workaround(sys.stdout)\n+    _colorama_workaround()\n+    _readline_workaround()\n+    pluginmanager = early_config.pluginmanager\n+    capman = CaptureManager(ns.capture)\n+    pluginmanager.register(capman, \"capturemanager\")\n+\n+    # Make sure that capturemanager is properly reset at final shutdown.\n+    early_config.add_cleanup(capman.stop_global_capturing)\n+\n+    # Finally trigger conftest loading but while capturing (issue #93).\n+    capman.start_global_capturing()\n+    try:\n+        try:\n+            yield\n+        finally:\n+            capman.suspend_global_capture()\n+    except BaseException:\n+        out, err = capman.read_global_capture()\n+        sys.stdout.write(out)\n+        sys.stderr.write(err)\n+        raise\n+\n+\n+# IO Helpers.\n+\n+\n+class EncodedFile(io.TextIOWrapper):\n+    __slots__ = ()\n+\n+    @property\n+    def name(self) -> str:\n+        # Ensure that file.name is a string. Workaround for a Python bug\n+        # fixed in >=3.7.4: https://bugs.python.org/issue36015\n+        return repr(self.buffer)\n+\n+    @property\n+    def mode(self) -> str:\n+        # TextIOWrapper doesn't expose a mode, but at least some of our\n+        # tests check it.\n+        assert hasattr(self.buffer, \"mode\")\n+        return cast(str, self.buffer.mode.replace(\"b\", \"\"))\n+\n+\n+class CaptureIO(io.TextIOWrapper):\n+    def __init__(self) -> None:\n+        super().__init__(io.BytesIO(), encoding=\"UTF-8\", newline=\"\", write_through=True)\n+\n+    def getvalue(self) -> str:\n+        assert isinstance(self.buffer, io.BytesIO)\n+        return self.buffer.getvalue().decode(\"UTF-8\")\n+\n+\n+class TeeCaptureIO(CaptureIO):\n+    def __init__(self, other: TextIO) -> None:\n+        self._other = other\n+        super().__init__()\n+\n+    def write(self, s: str) -> int:\n+        super().write(s)\n+        return self._other.write(s)\n+\n+\n+class DontReadFromInput(TextIO):\n+    @property\n+    def encoding(self) -> str:\n+        assert sys.__stdin__ is not None\n+        return sys.__stdin__.encoding\n+\n+    def read(self, size: int = -1) -> str:\n+        raise OSError(\n+            \"pytest: reading from stdin while output is captured!  Consider using `-s`.\"\n+        )\n+\n+    readline = read\n+\n+    def __next__(self) -> str:\n+        return self.readline()\n+\n+    def readlines(self, hint: int | None = -1) -> list[str]:\n+        raise OSError(\n+            \"pytest: reading from stdin while output is captured!  Consider using `-s`.\"\n+        )\n+\n+    def __iter__(self) -> Iterator[str]:\n+        return self\n+\n+    def fileno(self) -> int:\n+        raise UnsupportedOperation(\"redirected stdin is pseudofile, has no fileno()\")\n+\n+    def flush(self) -> None:\n+        raise UnsupportedOperation(\"redirected stdin is pseudofile, has no flush()\")\n+\n+    def isatty(self) -> bool:\n+        return False\n+\n+    def close(self) -> None:\n+        pass\n+\n+    def readable(self) -> bool:\n+        return False\n+\n+    def seek(self, offset: int, whence: int = 0) -> int:\n+        raise UnsupportedOperation(\"redirected stdin is pseudofile, has no seek(int)\")\n+\n+    def seekable(self) -> bool:\n+        return False\n+\n+    def tell(self) -> int:\n+        raise UnsupportedOperation(\"redirected stdin is pseudofile, has no tell()\")\n+\n+    def truncate(self, size: int | None = None) -> int:\n+        raise UnsupportedOperation(\"cannot truncate stdin\")\n+\n+    def write(self, data: str) -> int:\n+        raise UnsupportedOperation(\"cannot write to stdin\")\n+\n+    def writelines(self, lines: Iterable[str]) -> None:\n+        raise UnsupportedOperation(\"Cannot write to stdin\")\n+\n+    def writable(self) -> bool:\n+        return False\n+\n+    def __enter__(self) -> Self:\n+        return self\n+\n+    def __exit__(\n+        self,\n+        type: type[BaseException] | None,\n+        value: BaseException | None,\n+        traceback: TracebackType | None,\n+    ) -> None:\n+        pass\n+\n+    @property\n+    def buffer(self) -> BinaryIO:\n+        # The str/bytes doesn't actually matter in this type, so OK to fake.\n+        return self  # type: ignore[return-value]\n+\n+\n+# Capture classes.\n+\n+\n+class CaptureBase(abc.ABC, Generic[AnyStr]):\n+    EMPTY_BUFFER: AnyStr\n+\n+    @abc.abstractmethod\n+    def __init__(self, fd: int) -> None:\n+        raise NotImplementedError()\n+\n+    @abc.abstractmethod\n+    def start(self) -> None:\n+        raise NotImplementedError()\n+\n+    @abc.abstractmethod\n+    def done(self) -> None:\n+        raise NotImplementedError()\n+\n+    @abc.abstractmethod\n+    def suspend(self) -> None:\n+        raise NotImplementedError()\n+\n+    @abc.abstractmethod\n+    def resume(self) -> None:\n+        raise NotImplementedError()\n+\n+    @abc.abstractmethod\n+    def writeorg(self, data: AnyStr) -> None:\n+        raise NotImplementedError()\n+\n+    @abc.abstractmethod\n+    def snap(self) -> AnyStr:\n+        raise NotImplementedError()\n+\n+\n+patchsysdict = {0: \"stdin\", 1: \"stdout\", 2: \"stderr\"}\n+\n+\n+class NoCapture(CaptureBase[str]):\n+    EMPTY_BUFFER = \"\"\n+\n+    def __init__(self, fd: int) -> None:\n+        pass\n+\n+    def start(self) -> None:\n+        pass\n+\n+    def done(self) -> None:\n+        pass\n+\n+    def suspend(self) -> None:\n+        pass\n+\n+    def resume(self) -> None:\n+        pass\n+\n+    def snap(self) -> str:\n+        return \"\"\n+\n+    def writeorg(self, data: str) -> None:\n+        pass\n+\n+\n+class SysCaptureBase(CaptureBase[AnyStr]):\n+    def __init__(\n+        self, fd: int, tmpfile: TextIO | None = None, *, tee: bool = False\n+    ) -> None:\n+        name = patchsysdict[fd]\n+        self._old: TextIO = getattr(sys, name)\n+        self.name = name\n+        if tmpfile is None:\n+            if name == \"stdin\":\n+                tmpfile = DontReadFromInput()\n+            else:\n+                tmpfile = CaptureIO() if not tee else TeeCaptureIO(self._old)\n+        self.tmpfile = tmpfile\n+        self._state = \"initialized\"\n+\n+    def repr(self, class_name: str) -> str:\n+        return \"<{} {} _old={} _state={!r} tmpfile={!r}>\".format(\n+            class_name,\n+            self.name,\n+            (hasattr(self, \"_old\") and repr(self._old)) or \"<UNSET>\",\n+            self._state,\n+            self.tmpfile,\n+        )\n+\n+    def __repr__(self) -> str:\n+        return \"<{} {} _old={} _state={!r} tmpfile={!r}>\".format(\n+            self.__class__.__name__,\n+            self.name,\n+            (hasattr(self, \"_old\") and repr(self._old)) or \"<UNSET>\",\n+            self._state,\n+            self.tmpfile,\n+        )\n+\n+    def _assert_state(self, op: str, states: tuple[str, ...]) -> None:\n+        assert self._state in states, (\n+            \"cannot {} in state {!r}: expected one of {}\".format(\n+                op, self._state, \", \".join(states)\n+            )\n+        )\n+\n+    def start(self) -> None:\n+        self._assert_state(\"start\", (\"initialized\",))\n+        setattr(sys, self.name, self.tmpfile)\n+        self._state = \"started\"\n+\n+    def done(self) -> None:\n+        self._assert_state(\"done\", (\"initialized\", \"started\", \"suspended\", \"done\"))\n+        if self._state == \"done\":\n+            return\n+        setattr(sys, self.name, self._old)\n+        del self._old\n+        self.tmpfile.close()\n+        self._state = \"done\"\n+\n+    def suspend(self) -> None:\n+        self._assert_state(\"suspend\", (\"started\", \"suspended\"))\n+        setattr(sys, self.name, self._old)\n+        self._state = \"suspended\"\n+\n+    def resume(self) -> None:\n+        self._assert_state(\"resume\", (\"started\", \"suspended\"))\n+        if self._state == \"started\":\n+            return\n+        setattr(sys, self.name, self.tmpfile)\n+        self._state = \"started\"\n+\n+\n+class SysCaptureBinary(SysCaptureBase[bytes]):\n+    EMPTY_BUFFER = b\"\"\n+\n+    def snap(self) -> bytes:\n+        self._assert_state(\"snap\", (\"started\", \"suspended\"))\n+        self.tmpfile.seek(0)\n+        res = self.tmpfile.buffer.read()\n+        self.tmpfile.seek(0)\n+        self.tmpfile.truncate()\n+        return res\n+\n+    def writeorg(self, data: bytes) -> None:\n+        self._assert_state(\"writeorg\", (\"started\", \"suspended\"))\n+        self._old.flush()\n+        self._old.buffer.write(data)\n+        self._old.buffer.flush()\n+\n+\n+class SysCapture(SysCaptureBase[str]):\n+    EMPTY_BUFFER = \"\"\n+\n+    def snap(self) -> str:\n+        self._assert_state(\"snap\", (\"started\", \"suspended\"))\n+        assert isinstance(self.tmpfile, CaptureIO)\n+        res = self.tmpfile.getvalue()\n+        self.tmpfile.seek(0)\n+        self.tmpfile.truncate()\n+        return res\n+\n+    def writeorg(self, data: str) -> None:\n+        self._assert_state(\"writeorg\", (\"started\", \"suspended\"))\n+        self._old.write(data)\n+        self._old.flush()\n+\n+\n+class FDCaptureBase(CaptureBase[AnyStr]):\n+    def __init__(self, targetfd: int) -> None:\n+        self.targetfd = targetfd\n+\n+        try:\n+            os.fstat(targetfd)\n+        except OSError:\n+            # FD capturing is conceptually simple -- create a temporary file,\n+            # redirect the FD to it, redirect back when done. But when the\n+            # target FD is invalid it throws a wrench into this lovely scheme.\n+            #\n+            # Tests themselves shouldn't care if the FD is valid, FD capturing\n+            # should work regardless of external circumstances. So falling back\n+            # to just sys capturing is not a good option.\n+            #\n+            # Further complications are the need to support suspend() and the\n+            # possibility of FD reuse (e.g. the tmpfile getting the very same\n+            # target FD). The following approach is robust, I believe.\n+            self.targetfd_invalid: int | None = os.open(os.devnull, os.O_RDWR)\n+            os.dup2(self.targetfd_invalid, targetfd)\n+        else:\n+            self.targetfd_invalid = None\n+        self.targetfd_save = os.dup(targetfd)\n+\n+        if targetfd == 0:\n+            self.tmpfile = open(os.devnull, encoding=\"utf-8\")\n+            self.syscapture: CaptureBase[str] = SysCapture(targetfd)\n+        else:\n+            self.tmpfile = EncodedFile(\n+                TemporaryFile(buffering=0),\n+                encoding=\"utf-8\",\n+                errors=\"replace\",\n+                newline=\"\",\n+                write_through=True,\n+            )\n+            if targetfd in patchsysdict:\n+                self.syscapture = SysCapture(targetfd, self.tmpfile)\n+            else:\n+                self.syscapture = NoCapture(targetfd)\n+\n+        self._state = \"initialized\"\n+\n+    def __repr__(self) -> str:\n+        return (\n+            f\"<{self.__class__.__name__} {self.targetfd} oldfd={self.targetfd_save} \"\n+            f\"_state={self._state!r} tmpfile={self.tmpfile!r}>\"\n+        )\n+\n+    def _assert_state(self, op: str, states: tuple[str, ...]) -> None:\n+        assert self._state in states, (\n+            \"cannot {} in state {!r}: expected one of {}\".format(\n+                op, self._state, \", \".join(states)\n+            )\n+        )\n+\n+    def start(self) -> None:\n+        \"\"\"Start capturing on targetfd using memorized tmpfile.\"\"\"\n+        self._assert_state(\"start\", (\"initialized\",))\n+        os.dup2(self.tmpfile.fileno(), self.targetfd)\n+        self.syscapture.start()\n+        self._state = \"started\"\n+\n+    def done(self) -> None:\n+        \"\"\"Stop capturing, restore streams, return original capture file,\n+        seeked to position zero.\"\"\"\n+        self._assert_state(\"done\", (\"initialized\", \"started\", \"suspended\", \"done\"))\n+        if self._state == \"done\":\n+            return\n+        os.dup2(self.targetfd_save, self.targetfd)\n+        os.close(self.targetfd_save)\n+        if self.targetfd_invalid is not None:\n+            if self.targetfd_invalid != self.targetfd:\n+                os.close(self.targetfd)\n+            os.close(self.targetfd_invalid)\n+        self.syscapture.done()\n+        self.tmpfile.close()\n+        self._state = \"done\"\n+\n+    def suspend(self) -> None:\n+        self._assert_state(\"suspend\", (\"started\", \"suspended\"))\n+        if self._state == \"suspended\":\n+            return\n+        self.syscapture.suspend()\n+        os.dup2(self.targetfd_save, self.targetfd)\n+        self._state = \"suspended\"\n+\n+    def resume(self) -> None:\n+        self._assert_state(\"resume\", (\"started\", \"suspended\"))\n+        if self._state == \"started\":\n+            return\n+        self.syscapture.resume()\n+        os.dup2(self.tmpfile.fileno(), self.targetfd)\n+        self._state = \"started\"\n+\n+\n+class FDCaptureBinary(FDCaptureBase[bytes]):\n+    \"\"\"Capture IO to/from a given OS-level file descriptor.\n+\n+    snap() produces `bytes`.\n+    \"\"\"\n+\n+    EMPTY_BUFFER = b\"\"\n+\n+    def snap(self) -> bytes:\n+        self._assert_state(\"snap\", (\"started\", \"suspended\"))\n+        self.tmpfile.seek(0)\n+        res = self.tmpfile.buffer.read()\n+        self.tmpfile.seek(0)\n+        self.tmpfile.truncate()\n+        return res  # type: ignore[return-value]\n+\n+    def writeorg(self, data: bytes) -> None:\n+        \"\"\"Write to original file descriptor.\"\"\"\n+        self._assert_state(\"writeorg\", (\"started\", \"suspended\"))\n+        os.write(self.targetfd_save, data)\n+\n+\n+class FDCapture(FDCaptureBase[str]):\n+    \"\"\"Capture IO to/from a given OS-level file descriptor.\n+\n+    snap() produces text.\n+    \"\"\"\n+\n+    EMPTY_BUFFER = \"\"\n+\n+    def snap(self) -> str:\n+        self._assert_state(\"snap\", (\"started\", \"suspended\"))\n+        self.tmpfile.seek(0)\n+        res = self.tmpfile.read()\n+        self.tmpfile.seek(0)\n+        self.tmpfile.truncate()\n+        return res\n+\n+    def writeorg(self, data: str) -> None:\n+        \"\"\"Write to original file descriptor.\"\"\"\n+        self._assert_state(\"writeorg\", (\"started\", \"suspended\"))\n+        # XXX use encoding of original stream\n+        os.write(self.targetfd_save, data.encode(\"utf-8\"))\n+\n+\n+# MultiCapture\n+\n+\n+# Generic NamedTuple only supported since Python 3.11.\n+if sys.version_info >= (3, 11) or TYPE_CHECKING:\n+\n+    @final\n+    class CaptureResult(NamedTuple, Generic[AnyStr]):\n+        \"\"\"The result of :method:`caplog.readouterr() <pytest.CaptureFixture.readouterr>`.\"\"\"\n+\n+        out: AnyStr\n+        err: AnyStr\n+\n+else:\n+\n+    class CaptureResult(\n+        collections.namedtuple(\"CaptureResult\", [\"out\", \"err\"]),  # noqa: PYI024\n+        Generic[AnyStr],\n+    ):\n+        \"\"\"The result of :method:`caplog.readouterr() <pytest.CaptureFixture.readouterr>`.\"\"\"\n+\n+        __slots__ = ()\n+\n+\n+class MultiCapture(Generic[AnyStr]):\n+    _state = None\n+    _in_suspended = False\n+\n+    def __init__(\n+        self,\n+        in_: CaptureBase[AnyStr] | None,\n+        out: CaptureBase[AnyStr] | None,\n+        err: CaptureBase[AnyStr] | None,\n+    ) -> None:\n+        self.in_: CaptureBase[AnyStr] | None = in_\n+        self.out: CaptureBase[AnyStr] | None = out\n+        self.err: CaptureBase[AnyStr] | None = err\n+\n+    def __repr__(self) -> str:\n+        return (\n+            f\"<MultiCapture out={self.out!r} err={self.err!r} in_={self.in_!r} \"\n+            f\"_state={self._state!r} _in_suspended={self._in_suspended!r}>\"\n+        )\n+\n+    def start_capturing(self) -> None:\n+        self._state = \"started\"\n+        if self.in_:\n+            self.in_.start()\n+        if self.out:\n+            self.out.start()\n+        if self.err:\n+            self.err.start()\n+\n+    def pop_outerr_to_orig(self) -> tuple[AnyStr, AnyStr]:\n+        \"\"\"Pop current snapshot out/err capture and flush to orig streams.\"\"\"\n+        out, err = self.readouterr()\n+        if out:\n+            assert self.out is not None\n+            self.out.writeorg(out)\n+        if err:\n+            assert self.err is not None\n+            self.err.writeorg(err)\n+        return out, err\n+\n+    def suspend_capturing(self, in_: bool = False) -> None:\n+        self._state = \"suspended\"\n+        if self.out:\n+            self.out.suspend()\n+        if self.err:\n+            self.err.suspend()\n+        if in_ and self.in_:\n+            self.in_.suspend()\n+            self._in_suspended = True\n+\n+    def resume_capturing(self) -> None:\n+        self._state = \"started\"\n+        if self.out:\n+            self.out.resume()\n+        if self.err:\n+            self.err.resume()\n+        if self._in_suspended:\n+            assert self.in_ is not None\n+            self.in_.resume()\n+            self._in_suspended = False\n+\n+    def stop_capturing(self) -> None:\n+        \"\"\"Stop capturing and reset capturing streams.\"\"\"\n+        if self._state == \"stopped\":\n+            raise ValueError(\"was already stopped\")\n+        self._state = \"stopped\"\n+        if self.out:\n+            self.out.done()\n+        if self.err:\n+            self.err.done()\n+        if self.in_:\n+            self.in_.done()\n+\n+    def is_started(self) -> bool:\n+        \"\"\"Whether actively capturing -- not suspended or stopped.\"\"\"\n+        return self._state == \"started\"\n+\n+    def readouterr(self) -> CaptureResult[AnyStr]:\n+        out = self.out.snap() if self.out else \"\"\n+        err = self.err.snap() if self.err else \"\"\n+        # TODO: This type error is real, need to fix.\n+        return CaptureResult(out, err)  # type: ignore[arg-type]\n+\n+\n+def _get_multicapture(method: _CaptureMethod) -> MultiCapture[str]:\n+    if method == \"fd\":\n+        return MultiCapture(in_=FDCapture(0), out=FDCapture(1), err=FDCapture(2))\n+    elif method == \"sys\":\n+        return MultiCapture(in_=SysCapture(0), out=SysCapture(1), err=SysCapture(2))\n+    elif method == \"no\":\n+        return MultiCapture(in_=None, out=None, err=None)\n+    elif method == \"tee-sys\":\n+        return MultiCapture(\n+            in_=None, out=SysCapture(1, tee=True), err=SysCapture(2, tee=True)\n+        )\n+    raise ValueError(f\"unknown capturing method: {method!r}\")\n+\n+\n+# CaptureManager and CaptureFixture\n+\n+\n+class CaptureManager:\n+    \"\"\"The capture plugin.\n+\n+    Manages that the appropriate capture method is enabled/disabled during\n+    collection and each test phase (setup, call, teardown). After each of\n+    those points, the captured output is obtained and attached to the\n+    collection/runtest report.\n+\n+    There are two levels of capture:\n+\n+    * global: enabled by default and can be suppressed by the ``-s``\n+      option. This is always enabled/disabled during collection and each test\n+      phase.\n+\n+    * fixture: when a test function or one of its fixture depend on the\n+      ``capsys`` or ``capfd`` fixtures. In this case special handling is\n+      needed to ensure the fixtures take precedence over the global capture.\n+    \"\"\"\n+\n+    def __init__(self, method: _CaptureMethod) -> None:\n+        self._method: Final = method\n+        self._global_capturing: MultiCapture[str] | None = None\n+        self._capture_fixture: CaptureFixture[Any] | None = None\n+\n+    def __repr__(self) -> str:\n+        return (\n+            f\"<CaptureManager _method={self._method!r} _global_capturing={self._global_capturing!r} \"\n+            f\"_capture_fixture={self._capture_fixture!r}>\"\n+        )\n+\n+    def is_capturing(self) -> str | bool:\n+        if self.is_globally_capturing():\n+            return \"global\"\n+        if self._capture_fixture:\n+            return f\"fixture {self._capture_fixture.request.fixturename}\"\n+        return False\n+\n+    # Global capturing control\n+\n+    def is_globally_capturing(self) -> bool:\n+        return self._method != \"no\"\n+\n+    def start_global_capturing(self) -> None:\n+        assert self._global_capturing is None\n+        self._global_capturing = _get_multicapture(self._method)\n+        self._global_capturing.start_capturing()\n+\n+    def stop_global_capturing(self) -> None:\n+        if self._global_capturing is not None:\n+            self._global_capturing.pop_outerr_to_orig()\n+            self._global_capturing.stop_capturing()\n+            self._global_capturing = None\n+\n+    def resume_global_capture(self) -> None:\n+        # During teardown of the python process, and on rare occasions, capture\n+        # attributes can be `None` while trying to resume global capture.\n+        if self._global_capturing is not None:\n+            self._global_capturing.resume_capturing()\n+\n+    def suspend_global_capture(self, in_: bool = False) -> None:\n+        if self._global_capturing is not None:\n+            self._global_capturing.suspend_capturing(in_=in_)\n+\n+    def suspend(self, in_: bool = False) -> None:\n+        # Need to undo local capsys-et-al if it exists before disabling global capture.\n+        self.suspend_fixture()\n+        self.suspend_global_capture(in_)\n+\n+    def resume(self) -> None:\n+        self.resume_global_capture()\n+        self.resume_fixture()\n+\n+    def read_global_capture(self) -> CaptureResult[str]:\n+        assert self._global_capturing is not None\n+        return self._global_capturing.readouterr()\n+\n+    # Fixture Control\n+\n+    def set_fixture(self, capture_fixture: CaptureFixture[Any]) -> None:\n+        if self._capture_fixture:\n+            current_fixture = self._capture_fixture.request.fixturename\n+            requested_fixture = capture_fixture.request.fixturename\n+            capture_fixture.request.raiseerror(\n+                f\"cannot use {requested_fixture} and {current_fixture} at the same time\"\n+            )\n+        self._capture_fixture = capture_fixture\n+\n+    def unset_fixture(self) -> None:\n+        self._capture_fixture = None\n+\n+    def activate_fixture(self) -> None:\n+        \"\"\"If the current item is using ``capsys`` or ``capfd``, activate\n+        them so they take precedence over the global capture.\"\"\"\n+        if self._capture_fixture:\n+            self._capture_fixture._start()\n+\n+    def deactivate_fixture(self) -> None:\n+        \"\"\"Deactivate the ``capsys`` or ``capfd`` fixture of this item, if any.\"\"\"\n+        if self._capture_fixture:\n+            self._capture_fixture.close()\n+\n+    def suspend_fixture(self) -> None:\n+        if self._capture_fixture:\n+            self._capture_fixture._suspend()\n+\n+    def resume_fixture(self) -> None:\n+        if self._capture_fixture:\n+            self._capture_fixture._resume()\n+\n+    # Helper context managers\n+\n+    @contextlib.contextmanager\n+    def global_and_fixture_disabled(self) -> Generator[None]:\n+        \"\"\"Context manager to temporarily disable global and current fixture capturing.\"\"\"\n+        do_fixture = self._capture_fixture and self._capture_fixture._is_started()\n+        if do_fixture:\n+            self.suspend_fixture()\n+        do_global = self._global_capturing and self._global_capturing.is_started()\n+        if do_global:\n+            self.suspend_global_capture()\n+        try:\n+            yield\n+        finally:\n+            if do_global:\n+                self.resume_global_capture()\n+            if do_fixture:\n+                self.resume_fixture()\n+\n+    @contextlib.contextmanager\n+    def item_capture(self, when: str, item: Item) -> Generator[None]:\n+        self.resume_global_capture()\n+        self.activate_fixture()\n+        try:\n+            yield\n+        finally:\n+            self.deactivate_fixture()\n+            self.suspend_global_capture(in_=False)\n+\n+            out, err = self.read_global_capture()\n+            item.add_report_section(when, \"stdout\", out)\n+            item.add_report_section(when, \"stderr\", err)\n+\n+    # Hooks\n+\n+    @hookimpl(wrapper=True)\n+    def pytest_make_collect_report(\n+        self, collector: Collector\n+    ) -> Generator[None, CollectReport, CollectReport]:\n+        if isinstance(collector, File):\n+            self.resume_global_capture()\n+            try:\n+                rep = yield\n+            finally:\n+                self.suspend_global_capture()\n+            out, err = self.read_global_capture()\n+            if out:\n+                rep.sections.append((\"Captured stdout\", out))\n+            if err:\n+                rep.sections.append((\"Captured stderr\", err))\n+        else:\n+            rep = yield\n+        return rep\n+\n+    @hookimpl(wrapper=True)\n+    def pytest_runtest_setup(self, item: Item) -> Generator[None]:\n+        with self.item_capture(\"setup\", item):\n+            return (yield)\n+\n+    @hookimpl(wrapper=True)\n+    def pytest_runtest_call(self, item: Item) -> Generator[None]:\n+        with self.item_capture(\"call\", item):\n+            return (yield)\n+\n+    @hookimpl(wrapper=True)\n+    def pytest_runtest_teardown(self, item: Item) -> Generator[None]:\n+        with self.item_capture(\"teardown\", item):\n+            return (yield)\n+\n+    @hookimpl(tryfirst=True)\n+    def pytest_keyboard_interrupt(self) -> None:\n+        self.stop_global_capturing()\n+\n+    @hookimpl(tryfirst=True)\n+    def pytest_internalerror(self) -> None:\n+        self.stop_global_capturing()\n+\n+\n+class CaptureFixture(Generic[AnyStr]):\n+    \"\"\"Object returned by the :fixture:`capsys`, :fixture:`capsysbinary`,\n+    :fixture:`capfd` and :fixture:`capfdbinary` fixtures.\"\"\"\n+\n+    def __init__(\n+        self,\n+        captureclass: type[CaptureBase[AnyStr]],\n+        request: SubRequest,\n+        *,\n+        config: dict[str, Any] | None = None,\n+        _ispytest: bool = False,\n+    ) -> None:\n+        check_ispytest(_ispytest)\n+        self.captureclass: type[CaptureBase[AnyStr]] = captureclass\n+        self.request = request\n+        self._config = config if config else {}\n+        self._capture: MultiCapture[AnyStr] | None = None\n+        self._captured_out: AnyStr = self.captureclass.EMPTY_BUFFER\n+        self._captured_err: AnyStr = self.captureclass.EMPTY_BUFFER\n+\n+    def _start(self) -> None:\n+        if self._capture is None:\n+            self._capture = MultiCapture(\n+                in_=None,\n+                out=self.captureclass(1, **self._config),\n+                err=self.captureclass(2, **self._config),\n+            )\n+            self._capture.start_capturing()\n+\n+    def close(self) -> None:\n+        if self._capture is not None:\n+            out, err = self._capture.pop_outerr_to_orig()\n+            self._captured_out += out\n+            self._captured_err += err\n+            self._capture.stop_capturing()\n+            self._capture = None\n+\n+    def readouterr(self) -> CaptureResult[AnyStr]:\n+        \"\"\"Read and return the captured output so far, resetting the internal\n+        buffer.\n+\n+        :returns:\n+            The captured content as a namedtuple with ``out`` and ``err``\n+            string attributes.\n+        \"\"\"\n+        captured_out, captured_err = self._captured_out, self._captured_err\n+        if self._capture is not None:\n+            out, err = self._capture.readouterr()\n+            captured_out += out\n+            captured_err += err\n+        self._captured_out = self.captureclass.EMPTY_BUFFER\n+        self._captured_err = self.captureclass.EMPTY_BUFFER\n+        return CaptureResult(captured_out, captured_err)\n+\n+    def _suspend(self) -> None:\n+        \"\"\"Suspend this fixture's own capturing temporarily.\"\"\"\n+        if self._capture is not None:\n+            self._capture.suspend_capturing()\n+\n+    def _resume(self) -> None:\n+        \"\"\"Resume this fixture's own capturing temporarily.\"\"\"\n+        if self._capture is not None:\n+            self._capture.resume_capturing()\n+\n+    def _is_started(self) -> bool:\n+        \"\"\"Whether actively capturing -- not disabled or closed.\"\"\"\n+        if self._capture is not None:\n+            return self._capture.is_started()\n+        return False\n+\n+    @contextlib.contextmanager\n+    def disabled(self) -> Generator[None]:\n+        \"\"\"Temporarily disable capturing while inside the ``with`` block.\"\"\"\n+        capmanager: CaptureManager = self.request.config.pluginmanager.getplugin(\n+            \"capturemanager\"\n+        )\n+        with capmanager.global_and_fixture_disabled():\n+            yield\n+\n+\n+# The fixtures.\n+\n+\n+@fixture\n+def capsys(request: SubRequest) -> Generator[CaptureFixture[str]]:\n+    r\"\"\"Enable text capturing of writes to ``sys.stdout`` and ``sys.stderr``.\n+\n+    The captured output is made available via ``capsys.readouterr()`` method\n+    calls, which return a ``(out, err)`` namedtuple.\n+    ``out`` and ``err`` will be ``text`` objects.\n+\n+    Returns an instance of :class:`CaptureFixture[str] <pytest.CaptureFixture>`.\n+\n+    Example:\n+\n+    .. code-block:: python\n+\n+        def test_output(capsys):\n+            print(\"hello\")\n+            captured = capsys.readouterr()\n+            assert captured.out == \"hello\\n\"\n+    \"\"\"\n+    capman: CaptureManager = request.config.pluginmanager.getplugin(\"capturemanager\")\n+    capture_fixture = CaptureFixture(SysCapture, request, _ispytest=True)\n+    capman.set_fixture(capture_fixture)\n+    capture_fixture._start()\n+    yield capture_fixture\n+    capture_fixture.close()\n+    capman.unset_fixture()\n+\n+\n+@fixture\n+def capteesys(request: SubRequest) -> Generator[CaptureFixture[str]]:\n+    r\"\"\"Enable simultaneous text capturing and pass-through of writes\n+    to ``sys.stdout`` and ``sys.stderr`` as defined by ``--capture=``.\n+\n+\n+    The captured output is made available via ``capteesys.readouterr()`` method\n+    calls, which return a ``(out, err)`` namedtuple.\n+    ``out`` and ``err`` will be ``text`` objects.\n+\n+    The output is also passed-through, allowing it to be \"live-printed\",\n+    reported, or both as defined by ``--capture=``.\n+\n+    Returns an instance of :class:`CaptureFixture[str] <pytest.CaptureFixture>`.\n+\n+    Example:\n+\n+    .. code-block:: python\n+\n+        def test_output(capteesys):\n+            print(\"hello\")\n+            captured = capteesys.readouterr()\n+            assert captured.out == \"hello\\n\"\n+    \"\"\"\n+    capman: CaptureManager = request.config.pluginmanager.getplugin(\"capturemanager\")\n+    capture_fixture = CaptureFixture(\n+        SysCapture, request, config=dict(tee=True), _ispytest=True\n+    )\n+    capman.set_fixture(capture_fixture)\n+    capture_fixture._start()\n+    yield capture_fixture\n+    capture_fixture.close()\n+    capman.unset_fixture()\n+\n+\n+@fixture\n+def capsysbinary(request: SubRequest) -> Generator[CaptureFixture[bytes]]:\n+    r\"\"\"Enable bytes capturing of writes to ``sys.stdout`` and ``sys.stderr``.\n+\n+    The captured output is made available via ``capsysbinary.readouterr()``\n+    method calls, which return a ``(out, err)`` namedtuple.\n+    ``out`` and ``err`` will be ``bytes`` objects.\n+\n+    Returns an instance of :class:`CaptureFixture[bytes] <pytest.CaptureFixture>`.\n+\n+    Example:\n+\n+    .. code-block:: python\n+\n+        def test_output(capsysbinary):\n+            print(\"hello\")\n+            captured = capsysbinary.readouterr()\n+            assert captured.out == b\"hello\\n\"\n+    \"\"\"\n+    capman: CaptureManager = request.config.pluginmanager.getplugin(\"capturemanager\")\n+    capture_fixture = CaptureFixture(SysCaptureBinary, request, _ispytest=True)\n+    capman.set_fixture(capture_fixture)\n+    capture_fixture._start()\n+    yield capture_fixture\n+    capture_fixture.close()\n+    capman.unset_fixture()\n+\n+\n+@fixture\n+def capfd(request: SubRequest) -> Generator[CaptureFixture[str]]:\n+    r\"\"\"Enable text capturing of writes to file descriptors ``1`` and ``2``.\n+\n+    The captured output is made available via ``capfd.readouterr()`` method\n+    calls, which return a ``(out, err)`` namedtuple.\n+    ``out`` and ``err`` will be ``text`` objects.\n+\n+    Returns an instance of :class:`CaptureFixture[str] <pytest.CaptureFixture>`.\n+\n+    Example:\n+\n+    .. code-block:: python\n+\n+        def test_system_echo(capfd):\n+            os.system('echo \"hello\"')\n+            captured = capfd.readouterr()\n+            assert captured.out == \"hello\\n\"\n+    \"\"\"\n+    capman: CaptureManager = request.config.pluginmanager.getplugin(\"capturemanager\")\n+    capture_fixture = CaptureFixture(FDCapture, request, _ispytest=True)\n+    capman.set_fixture(capture_fixture)\n+    capture_fixture._start()\n+    yield capture_fixture\n+    capture_fixture.close()\n+    capman.unset_fixture()\n+\n+\n+@fixture\n+def capfdbinary(request: SubRequest) -> Generator[CaptureFixture[bytes]]:\n+    r\"\"\"Enable bytes capturing of writes to file descriptors ``1`` and ``2``.\n+\n+    The captured output is made available via ``capfd.readouterr()`` method\n+    calls, which return a ``(out, err)`` namedtuple.\n+    ``out`` and ``err`` will be ``byte`` objects.\n+\n+    Returns an instance of :class:`CaptureFixture[bytes] <pytest.CaptureFixture>`.\n+\n+    Example:\n+\n+    .. code-block:: python\n+\n+        def test_system_echo(capfdbinary):\n+            os.system('echo \"hello\"')\n+            captured = capfdbinary.readouterr()\n+            assert captured.out == b\"hello\\n\"\n+\n+    \"\"\"\n+    capman: CaptureManager = request.config.pluginmanager.getplugin(\"capturemanager\")\n+    capture_fixture = CaptureFixture(FDCaptureBinary, request, _ispytest=True)\n+    capman.set_fixture(capture_fixture)\n+    capture_fixture._start()\n+    yield capture_fixture\n+    capture_fixture.close()\n+    capman.unset_fixture()",
      "patch_lines": [
        "@@ -0,0 +1,1144 @@\n",
        "+# mypy: allow-untyped-defs\n",
        "+\"\"\"Per-test stdout/stderr capturing mechanism.\"\"\"\n",
        "+\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+import abc\n",
        "+import collections\n",
        "+from collections.abc import Generator\n",
        "+from collections.abc import Iterable\n",
        "+from collections.abc import Iterator\n",
        "+import contextlib\n",
        "+import io\n",
        "+from io import UnsupportedOperation\n",
        "+import os\n",
        "+import sys\n",
        "+from tempfile import TemporaryFile\n",
        "+from types import TracebackType\n",
        "+from typing import Any\n",
        "+from typing import AnyStr\n",
        "+from typing import BinaryIO\n",
        "+from typing import cast\n",
        "+from typing import Final\n",
        "+from typing import final\n",
        "+from typing import Generic\n",
        "+from typing import Literal\n",
        "+from typing import NamedTuple\n",
        "+from typing import TextIO\n",
        "+from typing import TYPE_CHECKING\n",
        "+\n",
        "+\n",
        "+if TYPE_CHECKING:\n",
        "+    from typing_extensions import Self\n",
        "+\n",
        "+from _pytest.config import Config\n",
        "+from _pytest.config import hookimpl\n",
        "+from _pytest.config.argparsing import Parser\n",
        "+from _pytest.deprecated import check_ispytest\n",
        "+from _pytest.fixtures import fixture\n",
        "+from _pytest.fixtures import SubRequest\n",
        "+from _pytest.nodes import Collector\n",
        "+from _pytest.nodes import File\n",
        "+from _pytest.nodes import Item\n",
        "+from _pytest.reports import CollectReport\n",
        "+\n",
        "+\n",
        "+_CaptureMethod = Literal[\"fd\", \"sys\", \"no\", \"tee-sys\"]\n",
        "+\n",
        "+\n",
        "+def pytest_addoption(parser: Parser) -> None:\n",
        "+    group = parser.getgroup(\"general\")\n",
        "+    group.addoption(\n",
        "+        \"--capture\",\n",
        "+        action=\"store\",\n",
        "+        default=\"fd\",\n",
        "+        metavar=\"method\",\n",
        "+        choices=[\"fd\", \"sys\", \"no\", \"tee-sys\"],\n",
        "+        help=\"Per-test capturing method: one of fd|sys|no|tee-sys\",\n",
        "+    )\n",
        "+    group._addoption(  # private to use reserved lower-case short option\n",
        "+        \"-s\",\n",
        "+        action=\"store_const\",\n",
        "+        const=\"no\",\n",
        "+        dest=\"capture\",\n",
        "+        help=\"Shortcut for --capture=no\",\n",
        "+    )\n",
        "+\n",
        "+\n",
        "+def _colorama_workaround() -> None:\n",
        "+    \"\"\"Ensure colorama is imported so that it attaches to the correct stdio\n",
        "+    handles on Windows.\n",
        "+\n",
        "+    colorama uses the terminal on import time. So if something does the\n",
        "+    first import of colorama while I/O capture is active, colorama will\n",
        "+    fail in various ways.\n",
        "+    \"\"\"\n",
        "+    if sys.platform.startswith(\"win32\"):\n",
        "+        try:\n",
        "+            import colorama  # noqa: F401\n",
        "+        except ImportError:\n",
        "+            pass\n",
        "+\n",
        "+\n",
        "+def _readline_workaround() -> None:\n",
        "+    \"\"\"Ensure readline is imported early so it attaches to the correct stdio handles.\n",
        "+\n",
        "+    This isn't a problem with the default GNU readline implementation, but in\n",
        "+    some configurations, Python uses libedit instead (on macOS, and for prebuilt\n",
        "+    binaries such as used by uv).\n",
        "+\n",
        "+    In theory this is only needed if readline.backend == \"libedit\", but the\n",
        "+    workaround consists of importing readline here, so we already worked around\n",
        "+    the issue by the time we could check if we need to.\n",
        "+    \"\"\"\n",
        "+    try:\n",
        "+        import readline  # noqa: F401\n",
        "+    except ImportError:\n",
        "+        pass\n",
        "+\n",
        "+\n",
        "+def _windowsconsoleio_workaround(stream: TextIO) -> None:\n",
        "+    \"\"\"Workaround for Windows Unicode console handling.\n",
        "+\n",
        "+    Python 3.6 implemented Unicode console handling for Windows. This works\n",
        "+    by reading/writing to the raw console handle using\n",
        "+    ``{Read,Write}ConsoleW``.\n",
        "+\n",
        "+    The problem is that we are going to ``dup2`` over the stdio file\n",
        "+    descriptors when doing ``FDCapture`` and this will ``CloseHandle`` the\n",
        "+    handles used by Python to write to the console. Though there is still some\n",
        "+    weirdness and the console handle seems to only be closed randomly and not\n",
        "+    on the first call to ``CloseHandle``, or maybe it gets reopened with the\n",
        "+    same handle value when we suspend capturing.\n",
        "+\n",
        "+    The workaround in this case will reopen stdio with a different fd which\n",
        "+    also means a different handle by replicating the logic in\n",
        "+    \"Py_lifecycle.c:initstdio/create_stdio\".\n",
        "+\n",
        "+    :param stream:\n",
        "+        In practice ``sys.stdout`` or ``sys.stderr``, but given\n",
        "+        here as parameter for unittesting purposes.\n",
        "+\n",
        "+    See https://github.com/pytest-dev/py/issues/103.\n",
        "+    \"\"\"\n",
        "+    if not sys.platform.startswith(\"win32\") or hasattr(sys, \"pypy_version_info\"):\n",
        "+        return\n",
        "+\n",
        "+    # Bail out if ``stream`` doesn't seem like a proper ``io`` stream (#2666).\n",
        "+    if not hasattr(stream, \"buffer\"):  # type: ignore[unreachable,unused-ignore]\n",
        "+        return\n",
        "+\n",
        "+    raw_stdout = stream.buffer.raw if hasattr(stream.buffer, \"raw\") else stream.buffer\n",
        "+\n",
        "+    if not isinstance(raw_stdout, io._WindowsConsoleIO):  # type: ignore[attr-defined,unused-ignore]\n",
        "+        return\n",
        "+\n",
        "+    def _reopen_stdio(f, mode):\n",
        "+        if not hasattr(stream.buffer, \"raw\") and mode[0] == \"w\":\n",
        "+            buffering = 0\n",
        "+        else:\n",
        "+            buffering = -1\n",
        "+\n",
        "+        return io.TextIOWrapper(\n",
        "+            open(os.dup(f.fileno()), mode, buffering),\n",
        "+            f.encoding,\n",
        "+            f.errors,\n",
        "+            f.newlines,\n",
        "+            f.line_buffering,\n",
        "+        )\n",
        "+\n",
        "+    sys.stdin = _reopen_stdio(sys.stdin, \"rb\")\n",
        "+    sys.stdout = _reopen_stdio(sys.stdout, \"wb\")\n",
        "+    sys.stderr = _reopen_stdio(sys.stderr, \"wb\")\n",
        "+\n",
        "+\n",
        "+@hookimpl(wrapper=True)\n",
        "+def pytest_load_initial_conftests(early_config: Config) -> Generator[None]:\n",
        "+    ns = early_config.known_args_namespace\n",
        "+    if ns.capture == \"fd\":\n",
        "+        _windowsconsoleio_workaround(sys.stdout)\n",
        "+    _colorama_workaround()\n",
        "+    _readline_workaround()\n",
        "+    pluginmanager = early_config.pluginmanager\n",
        "+    capman = CaptureManager(ns.capture)\n",
        "+    pluginmanager.register(capman, \"capturemanager\")\n",
        "+\n",
        "+    # Make sure that capturemanager is properly reset at final shutdown.\n",
        "+    early_config.add_cleanup(capman.stop_global_capturing)\n",
        "+\n",
        "+    # Finally trigger conftest loading but while capturing (issue #93).\n",
        "+    capman.start_global_capturing()\n",
        "+    try:\n",
        "+        try:\n",
        "+            yield\n",
        "+        finally:\n",
        "+            capman.suspend_global_capture()\n",
        "+    except BaseException:\n",
        "+        out, err = capman.read_global_capture()\n",
        "+        sys.stdout.write(out)\n",
        "+        sys.stderr.write(err)\n",
        "+        raise\n",
        "+\n",
        "+\n",
        "+# IO Helpers.\n",
        "+\n",
        "+\n",
        "+class EncodedFile(io.TextIOWrapper):\n",
        "+    __slots__ = ()\n",
        "+\n",
        "+    @property\n",
        "+    def name(self) -> str:\n",
        "+        # Ensure that file.name is a string. Workaround for a Python bug\n",
        "+        # fixed in >=3.7.4: https://bugs.python.org/issue36015\n",
        "+        return repr(self.buffer)\n",
        "+\n",
        "+    @property\n",
        "+    def mode(self) -> str:\n",
        "+        # TextIOWrapper doesn't expose a mode, but at least some of our\n",
        "+        # tests check it.\n",
        "+        assert hasattr(self.buffer, \"mode\")\n",
        "+        return cast(str, self.buffer.mode.replace(\"b\", \"\"))\n",
        "+\n",
        "+\n",
        "+class CaptureIO(io.TextIOWrapper):\n",
        "+    def __init__(self) -> None:\n",
        "+        super().__init__(io.BytesIO(), encoding=\"UTF-8\", newline=\"\", write_through=True)\n",
        "+\n",
        "+    def getvalue(self) -> str:\n",
        "+        assert isinstance(self.buffer, io.BytesIO)\n",
        "+        return self.buffer.getvalue().decode(\"UTF-8\")\n",
        "+\n",
        "+\n",
        "+class TeeCaptureIO(CaptureIO):\n",
        "+    def __init__(self, other: TextIO) -> None:\n",
        "+        self._other = other\n",
        "+        super().__init__()\n",
        "+\n",
        "+    def write(self, s: str) -> int:\n",
        "+        super().write(s)\n",
        "+        return self._other.write(s)\n",
        "+\n",
        "+\n",
        "+class DontReadFromInput(TextIO):\n",
        "+    @property\n",
        "+    def encoding(self) -> str:\n",
        "+        assert sys.__stdin__ is not None\n",
        "+        return sys.__stdin__.encoding\n",
        "+\n",
        "+    def read(self, size: int = -1) -> str:\n",
        "+        raise OSError(\n",
        "+            \"pytest: reading from stdin while output is captured!  Consider using `-s`.\"\n",
        "+        )\n",
        "+\n",
        "+    readline = read\n",
        "+\n",
        "+    def __next__(self) -> str:\n",
        "+        return self.readline()\n",
        "+\n",
        "+    def readlines(self, hint: int | None = -1) -> list[str]:\n",
        "+        raise OSError(\n",
        "+            \"pytest: reading from stdin while output is captured!  Consider using `-s`.\"\n",
        "+        )\n",
        "+\n",
        "+    def __iter__(self) -> Iterator[str]:\n",
        "+        return self\n",
        "+\n",
        "+    def fileno(self) -> int:\n",
        "+        raise UnsupportedOperation(\"redirected stdin is pseudofile, has no fileno()\")\n",
        "+\n",
        "+    def flush(self) -> None:\n",
        "+        raise UnsupportedOperation(\"redirected stdin is pseudofile, has no flush()\")\n",
        "+\n",
        "+    def isatty(self) -> bool:\n",
        "+        return False\n",
        "+\n",
        "+    def close(self) -> None:\n",
        "+        pass\n",
        "+\n",
        "+    def readable(self) -> bool:\n",
        "+        return False\n",
        "+\n",
        "+    def seek(self, offset: int, whence: int = 0) -> int:\n",
        "+        raise UnsupportedOperation(\"redirected stdin is pseudofile, has no seek(int)\")\n",
        "+\n",
        "+    def seekable(self) -> bool:\n",
        "+        return False\n",
        "+\n",
        "+    def tell(self) -> int:\n",
        "+        raise UnsupportedOperation(\"redirected stdin is pseudofile, has no tell()\")\n",
        "+\n",
        "+    def truncate(self, size: int | None = None) -> int:\n",
        "+        raise UnsupportedOperation(\"cannot truncate stdin\")\n",
        "+\n",
        "+    def write(self, data: str) -> int:\n",
        "+        raise UnsupportedOperation(\"cannot write to stdin\")\n",
        "+\n",
        "+    def writelines(self, lines: Iterable[str]) -> None:\n",
        "+        raise UnsupportedOperation(\"Cannot write to stdin\")\n",
        "+\n",
        "+    def writable(self) -> bool:\n",
        "+        return False\n",
        "+\n",
        "+    def __enter__(self) -> Self:\n",
        "+        return self\n",
        "+\n",
        "+    def __exit__(\n",
        "+        self,\n",
        "+        type: type[BaseException] | None,\n",
        "+        value: BaseException | None,\n",
        "+        traceback: TracebackType | None,\n",
        "+    ) -> None:\n",
        "+        pass\n",
        "+\n",
        "+    @property\n",
        "+    def buffer(self) -> BinaryIO:\n",
        "+        # The str/bytes doesn't actually matter in this type, so OK to fake.\n",
        "+        return self  # type: ignore[return-value]\n",
        "+\n",
        "+\n",
        "+# Capture classes.\n",
        "+\n",
        "+\n",
        "+class CaptureBase(abc.ABC, Generic[AnyStr]):\n",
        "+    EMPTY_BUFFER: AnyStr\n",
        "+\n",
        "+    @abc.abstractmethod\n",
        "+    def __init__(self, fd: int) -> None:\n",
        "+        raise NotImplementedError()\n",
        "+\n",
        "+    @abc.abstractmethod\n",
        "+    def start(self) -> None:\n",
        "+        raise NotImplementedError()\n",
        "+\n",
        "+    @abc.abstractmethod\n",
        "+    def done(self) -> None:\n",
        "+        raise NotImplementedError()\n",
        "+\n",
        "+    @abc.abstractmethod\n",
        "+    def suspend(self) -> None:\n",
        "+        raise NotImplementedError()\n",
        "+\n",
        "+    @abc.abstractmethod\n",
        "+    def resume(self) -> None:\n",
        "+        raise NotImplementedError()\n",
        "+\n",
        "+    @abc.abstractmethod\n",
        "+    def writeorg(self, data: AnyStr) -> None:\n",
        "+        raise NotImplementedError()\n",
        "+\n",
        "+    @abc.abstractmethod\n",
        "+    def snap(self) -> AnyStr:\n",
        "+        raise NotImplementedError()\n",
        "+\n",
        "+\n",
        "+patchsysdict = {0: \"stdin\", 1: \"stdout\", 2: \"stderr\"}\n",
        "+\n",
        "+\n",
        "+class NoCapture(CaptureBase[str]):\n",
        "+    EMPTY_BUFFER = \"\"\n",
        "+\n",
        "+    def __init__(self, fd: int) -> None:\n",
        "+        pass\n",
        "+\n",
        "+    def start(self) -> None:\n",
        "+        pass\n",
        "+\n",
        "+    def done(self) -> None:\n",
        "+        pass\n",
        "+\n",
        "+    def suspend(self) -> None:\n",
        "+        pass\n",
        "+\n",
        "+    def resume(self) -> None:\n",
        "+        pass\n",
        "+\n",
        "+    def snap(self) -> str:\n",
        "+        return \"\"\n",
        "+\n",
        "+    def writeorg(self, data: str) -> None:\n",
        "+        pass\n",
        "+\n",
        "+\n",
        "+class SysCaptureBase(CaptureBase[AnyStr]):\n",
        "+    def __init__(\n",
        "+        self, fd: int, tmpfile: TextIO | None = None, *, tee: bool = False\n",
        "+    ) -> None:\n",
        "+        name = patchsysdict[fd]\n",
        "+        self._old: TextIO = getattr(sys, name)\n",
        "+        self.name = name\n",
        "+        if tmpfile is None:\n",
        "+            if name == \"stdin\":\n",
        "+                tmpfile = DontReadFromInput()\n",
        "+            else:\n",
        "+                tmpfile = CaptureIO() if not tee else TeeCaptureIO(self._old)\n",
        "+        self.tmpfile = tmpfile\n",
        "+        self._state = \"initialized\"\n",
        "+\n",
        "+    def repr(self, class_name: str) -> str:\n",
        "+        return \"<{} {} _old={} _state={!r} tmpfile={!r}>\".format(\n",
        "+            class_name,\n",
        "+            self.name,\n",
        "+            (hasattr(self, \"_old\") and repr(self._old)) or \"<UNSET>\",\n",
        "+            self._state,\n",
        "+            self.tmpfile,\n",
        "+        )\n",
        "+\n",
        "+    def __repr__(self) -> str:\n",
        "+        return \"<{} {} _old={} _state={!r} tmpfile={!r}>\".format(\n",
        "+            self.__class__.__name__,\n",
        "+            self.name,\n",
        "+            (hasattr(self, \"_old\") and repr(self._old)) or \"<UNSET>\",\n",
        "+            self._state,\n",
        "+            self.tmpfile,\n",
        "+        )\n",
        "+\n",
        "+    def _assert_state(self, op: str, states: tuple[str, ...]) -> None:\n",
        "+        assert self._state in states, (\n",
        "+            \"cannot {} in state {!r}: expected one of {}\".format(\n",
        "+                op, self._state, \", \".join(states)\n",
        "+            )\n",
        "+        )\n",
        "+\n",
        "+    def start(self) -> None:\n",
        "+        self._assert_state(\"start\", (\"initialized\",))\n",
        "+        setattr(sys, self.name, self.tmpfile)\n",
        "+        self._state = \"started\"\n",
        "+\n",
        "+    def done(self) -> None:\n",
        "+        self._assert_state(\"done\", (\"initialized\", \"started\", \"suspended\", \"done\"))\n",
        "+        if self._state == \"done\":\n",
        "+            return\n",
        "+        setattr(sys, self.name, self._old)\n",
        "+        del self._old\n",
        "+        self.tmpfile.close()\n",
        "+        self._state = \"done\"\n",
        "+\n",
        "+    def suspend(self) -> None:\n",
        "+        self._assert_state(\"suspend\", (\"started\", \"suspended\"))\n",
        "+        setattr(sys, self.name, self._old)\n",
        "+        self._state = \"suspended\"\n",
        "+\n",
        "+    def resume(self) -> None:\n",
        "+        self._assert_state(\"resume\", (\"started\", \"suspended\"))\n",
        "+        if self._state == \"started\":\n",
        "+            return\n",
        "+        setattr(sys, self.name, self.tmpfile)\n",
        "+        self._state = \"started\"\n",
        "+\n",
        "+\n",
        "+class SysCaptureBinary(SysCaptureBase[bytes]):\n",
        "+    EMPTY_BUFFER = b\"\"\n",
        "+\n",
        "+    def snap(self) -> bytes:\n",
        "+        self._assert_state(\"snap\", (\"started\", \"suspended\"))\n",
        "+        self.tmpfile.seek(0)\n",
        "+        res = self.tmpfile.buffer.read()\n",
        "+        self.tmpfile.seek(0)\n",
        "+        self.tmpfile.truncate()\n",
        "+        return res\n",
        "+\n",
        "+    def writeorg(self, data: bytes) -> None:\n",
        "+        self._assert_state(\"writeorg\", (\"started\", \"suspended\"))\n",
        "+        self._old.flush()\n",
        "+        self._old.buffer.write(data)\n",
        "+        self._old.buffer.flush()\n",
        "+\n",
        "+\n",
        "+class SysCapture(SysCaptureBase[str]):\n",
        "+    EMPTY_BUFFER = \"\"\n",
        "+\n",
        "+    def snap(self) -> str:\n",
        "+        self._assert_state(\"snap\", (\"started\", \"suspended\"))\n",
        "+        assert isinstance(self.tmpfile, CaptureIO)\n",
        "+        res = self.tmpfile.getvalue()\n",
        "+        self.tmpfile.seek(0)\n",
        "+        self.tmpfile.truncate()\n",
        "+        return res\n",
        "+\n",
        "+    def writeorg(self, data: str) -> None:\n",
        "+        self._assert_state(\"writeorg\", (\"started\", \"suspended\"))\n",
        "+        self._old.write(data)\n",
        "+        self._old.flush()\n",
        "+\n",
        "+\n",
        "+class FDCaptureBase(CaptureBase[AnyStr]):\n",
        "+    def __init__(self, targetfd: int) -> None:\n",
        "+        self.targetfd = targetfd\n",
        "+\n",
        "+        try:\n",
        "+            os.fstat(targetfd)\n",
        "+        except OSError:\n",
        "+            # FD capturing is conceptually simple -- create a temporary file,\n",
        "+            # redirect the FD to it, redirect back when done. But when the\n",
        "+            # target FD is invalid it throws a wrench into this lovely scheme.\n",
        "+            #\n",
        "+            # Tests themselves shouldn't care if the FD is valid, FD capturing\n",
        "+            # should work regardless of external circumstances. So falling back\n",
        "+            # to just sys capturing is not a good option.\n",
        "+            #\n",
        "+            # Further complications are the need to support suspend() and the\n",
        "+            # possibility of FD reuse (e.g. the tmpfile getting the very same\n",
        "+            # target FD). The following approach is robust, I believe.\n",
        "+            self.targetfd_invalid: int | None = os.open(os.devnull, os.O_RDWR)\n",
        "+            os.dup2(self.targetfd_invalid, targetfd)\n",
        "+        else:\n",
        "+            self.targetfd_invalid = None\n",
        "+        self.targetfd_save = os.dup(targetfd)\n",
        "+\n",
        "+        if targetfd == 0:\n",
        "+            self.tmpfile = open(os.devnull, encoding=\"utf-8\")\n",
        "+            self.syscapture: CaptureBase[str] = SysCapture(targetfd)\n",
        "+        else:\n",
        "+            self.tmpfile = EncodedFile(\n",
        "+                TemporaryFile(buffering=0),\n",
        "+                encoding=\"utf-8\",\n",
        "+                errors=\"replace\",\n",
        "+                newline=\"\",\n",
        "+                write_through=True,\n",
        "+            )\n",
        "+            if targetfd in patchsysdict:\n",
        "+                self.syscapture = SysCapture(targetfd, self.tmpfile)\n",
        "+            else:\n",
        "+                self.syscapture = NoCapture(targetfd)\n",
        "+\n",
        "+        self._state = \"initialized\"\n",
        "+\n",
        "+    def __repr__(self) -> str:\n",
        "+        return (\n",
        "+            f\"<{self.__class__.__name__} {self.targetfd} oldfd={self.targetfd_save} \"\n",
        "+            f\"_state={self._state!r} tmpfile={self.tmpfile!r}>\"\n",
        "+        )\n",
        "+\n",
        "+    def _assert_state(self, op: str, states: tuple[str, ...]) -> None:\n",
        "+        assert self._state in states, (\n",
        "+            \"cannot {} in state {!r}: expected one of {}\".format(\n",
        "+                op, self._state, \", \".join(states)\n",
        "+            )\n",
        "+        )\n",
        "+\n",
        "+    def start(self) -> None:\n",
        "+        \"\"\"Start capturing on targetfd using memorized tmpfile.\"\"\"\n",
        "+        self._assert_state(\"start\", (\"initialized\",))\n",
        "+        os.dup2(self.tmpfile.fileno(), self.targetfd)\n",
        "+        self.syscapture.start()\n",
        "+        self._state = \"started\"\n",
        "+\n",
        "+    def done(self) -> None:\n",
        "+        \"\"\"Stop capturing, restore streams, return original capture file,\n",
        "+        seeked to position zero.\"\"\"\n",
        "+        self._assert_state(\"done\", (\"initialized\", \"started\", \"suspended\", \"done\"))\n",
        "+        if self._state == \"done\":\n",
        "+            return\n",
        "+        os.dup2(self.targetfd_save, self.targetfd)\n",
        "+        os.close(self.targetfd_save)\n",
        "+        if self.targetfd_invalid is not None:\n",
        "+            if self.targetfd_invalid != self.targetfd:\n",
        "+                os.close(self.targetfd)\n",
        "+            os.close(self.targetfd_invalid)\n",
        "+        self.syscapture.done()\n",
        "+        self.tmpfile.close()\n",
        "+        self._state = \"done\"\n",
        "+\n",
        "+    def suspend(self) -> None:\n",
        "+        self._assert_state(\"suspend\", (\"started\", \"suspended\"))\n",
        "+        if self._state == \"suspended\":\n",
        "+            return\n",
        "+        self.syscapture.suspend()\n",
        "+        os.dup2(self.targetfd_save, self.targetfd)\n",
        "+        self._state = \"suspended\"\n",
        "+\n",
        "+    def resume(self) -> None:\n",
        "+        self._assert_state(\"resume\", (\"started\", \"suspended\"))\n",
        "+        if self._state == \"started\":\n",
        "+            return\n",
        "+        self.syscapture.resume()\n",
        "+        os.dup2(self.tmpfile.fileno(), self.targetfd)\n",
        "+        self._state = \"started\"\n",
        "+\n",
        "+\n",
        "+class FDCaptureBinary(FDCaptureBase[bytes]):\n",
        "+    \"\"\"Capture IO to/from a given OS-level file descriptor.\n",
        "+\n",
        "+    snap() produces `bytes`.\n",
        "+    \"\"\"\n",
        "+\n",
        "+    EMPTY_BUFFER = b\"\"\n",
        "+\n",
        "+    def snap(self) -> bytes:\n",
        "+        self._assert_state(\"snap\", (\"started\", \"suspended\"))\n",
        "+        self.tmpfile.seek(0)\n",
        "+        res = self.tmpfile.buffer.read()\n",
        "+        self.tmpfile.seek(0)\n",
        "+        self.tmpfile.truncate()\n",
        "+        return res  # type: ignore[return-value]\n",
        "+\n",
        "+    def writeorg(self, data: bytes) -> None:\n",
        "+        \"\"\"Write to original file descriptor.\"\"\"\n",
        "+        self._assert_state(\"writeorg\", (\"started\", \"suspended\"))\n",
        "+        os.write(self.targetfd_save, data)\n",
        "+\n",
        "+\n",
        "+class FDCapture(FDCaptureBase[str]):\n",
        "+    \"\"\"Capture IO to/from a given OS-level file descriptor.\n",
        "+\n",
        "+    snap() produces text.\n",
        "+    \"\"\"\n",
        "+\n",
        "+    EMPTY_BUFFER = \"\"\n",
        "+\n",
        "+    def snap(self) -> str:\n",
        "+        self._assert_state(\"snap\", (\"started\", \"suspended\"))\n",
        "+        self.tmpfile.seek(0)\n",
        "+        res = self.tmpfile.read()\n",
        "+        self.tmpfile.seek(0)\n",
        "+        self.tmpfile.truncate()\n",
        "+        return res\n",
        "+\n",
        "+    def writeorg(self, data: str) -> None:\n",
        "+        \"\"\"Write to original file descriptor.\"\"\"\n",
        "+        self._assert_state(\"writeorg\", (\"started\", \"suspended\"))\n",
        "+        # XXX use encoding of original stream\n",
        "+        os.write(self.targetfd_save, data.encode(\"utf-8\"))\n",
        "+\n",
        "+\n",
        "+# MultiCapture\n",
        "+\n",
        "+\n",
        "+# Generic NamedTuple only supported since Python 3.11.\n",
        "+if sys.version_info >= (3, 11) or TYPE_CHECKING:\n",
        "+\n",
        "+    @final\n",
        "+    class CaptureResult(NamedTuple, Generic[AnyStr]):\n",
        "+        \"\"\"The result of :method:`caplog.readouterr() <pytest.CaptureFixture.readouterr>`.\"\"\"\n",
        "+\n",
        "+        out: AnyStr\n",
        "+        err: AnyStr\n",
        "+\n",
        "+else:\n",
        "+\n",
        "+    class CaptureResult(\n",
        "+        collections.namedtuple(\"CaptureResult\", [\"out\", \"err\"]),  # noqa: PYI024\n",
        "+        Generic[AnyStr],\n",
        "+    ):\n",
        "+        \"\"\"The result of :method:`caplog.readouterr() <pytest.CaptureFixture.readouterr>`.\"\"\"\n",
        "+\n",
        "+        __slots__ = ()\n",
        "+\n",
        "+\n",
        "+class MultiCapture(Generic[AnyStr]):\n",
        "+    _state = None\n",
        "+    _in_suspended = False\n",
        "+\n",
        "+    def __init__(\n",
        "+        self,\n",
        "+        in_: CaptureBase[AnyStr] | None,\n",
        "+        out: CaptureBase[AnyStr] | None,\n",
        "+        err: CaptureBase[AnyStr] | None,\n",
        "+    ) -> None:\n",
        "+        self.in_: CaptureBase[AnyStr] | None = in_\n",
        "+        self.out: CaptureBase[AnyStr] | None = out\n",
        "+        self.err: CaptureBase[AnyStr] | None = err\n",
        "+\n",
        "+    def __repr__(self) -> str:\n",
        "+        return (\n",
        "+            f\"<MultiCapture out={self.out!r} err={self.err!r} in_={self.in_!r} \"\n",
        "+            f\"_state={self._state!r} _in_suspended={self._in_suspended!r}>\"\n",
        "+        )\n",
        "+\n",
        "+    def start_capturing(self) -> None:\n",
        "+        self._state = \"started\"\n",
        "+        if self.in_:\n",
        "+            self.in_.start()\n",
        "+        if self.out:\n",
        "+            self.out.start()\n",
        "+        if self.err:\n",
        "+            self.err.start()\n",
        "+\n",
        "+    def pop_outerr_to_orig(self) -> tuple[AnyStr, AnyStr]:\n",
        "+        \"\"\"Pop current snapshot out/err capture and flush to orig streams.\"\"\"\n",
        "+        out, err = self.readouterr()\n",
        "+        if out:\n",
        "+            assert self.out is not None\n",
        "+            self.out.writeorg(out)\n",
        "+        if err:\n",
        "+            assert self.err is not None\n",
        "+            self.err.writeorg(err)\n",
        "+        return out, err\n",
        "+\n",
        "+    def suspend_capturing(self, in_: bool = False) -> None:\n",
        "+        self._state = \"suspended\"\n",
        "+        if self.out:\n",
        "+            self.out.suspend()\n",
        "+        if self.err:\n",
        "+            self.err.suspend()\n",
        "+        if in_ and self.in_:\n",
        "+            self.in_.suspend()\n",
        "+            self._in_suspended = True\n",
        "+\n",
        "+    def resume_capturing(self) -> None:\n",
        "+        self._state = \"started\"\n",
        "+        if self.out:\n",
        "+            self.out.resume()\n",
        "+        if self.err:\n",
        "+            self.err.resume()\n",
        "+        if self._in_suspended:\n",
        "+            assert self.in_ is not None\n",
        "+            self.in_.resume()\n",
        "+            self._in_suspended = False\n",
        "+\n",
        "+    def stop_capturing(self) -> None:\n",
        "+        \"\"\"Stop capturing and reset capturing streams.\"\"\"\n",
        "+        if self._state == \"stopped\":\n",
        "+            raise ValueError(\"was already stopped\")\n",
        "+        self._state = \"stopped\"\n",
        "+        if self.out:\n",
        "+            self.out.done()\n",
        "+        if self.err:\n",
        "+            self.err.done()\n",
        "+        if self.in_:\n",
        "+            self.in_.done()\n",
        "+\n",
        "+    def is_started(self) -> bool:\n",
        "+        \"\"\"Whether actively capturing -- not suspended or stopped.\"\"\"\n",
        "+        return self._state == \"started\"\n",
        "+\n",
        "+    def readouterr(self) -> CaptureResult[AnyStr]:\n",
        "+        out = self.out.snap() if self.out else \"\"\n",
        "+        err = self.err.snap() if self.err else \"\"\n",
        "+        # TODO: This type error is real, need to fix.\n",
        "+        return CaptureResult(out, err)  # type: ignore[arg-type]\n",
        "+\n",
        "+\n",
        "+def _get_multicapture(method: _CaptureMethod) -> MultiCapture[str]:\n",
        "+    if method == \"fd\":\n",
        "+        return MultiCapture(in_=FDCapture(0), out=FDCapture(1), err=FDCapture(2))\n",
        "+    elif method == \"sys\":\n",
        "+        return MultiCapture(in_=SysCapture(0), out=SysCapture(1), err=SysCapture(2))\n",
        "+    elif method == \"no\":\n",
        "+        return MultiCapture(in_=None, out=None, err=None)\n",
        "+    elif method == \"tee-sys\":\n",
        "+        return MultiCapture(\n",
        "+            in_=None, out=SysCapture(1, tee=True), err=SysCapture(2, tee=True)\n",
        "+        )\n",
        "+    raise ValueError(f\"unknown capturing method: {method!r}\")\n",
        "+\n",
        "+\n",
        "+# CaptureManager and CaptureFixture\n",
        "+\n",
        "+\n",
        "+class CaptureManager:\n",
        "+    \"\"\"The capture plugin.\n",
        "+\n",
        "+    Manages that the appropriate capture method is enabled/disabled during\n",
        "+    collection and each test phase (setup, call, teardown). After each of\n",
        "+    those points, the captured output is obtained and attached to the\n",
        "+    collection/runtest report.\n",
        "+\n",
        "+    There are two levels of capture:\n",
        "+\n",
        "+    * global: enabled by default and can be suppressed by the ``-s``\n",
        "+      option. This is always enabled/disabled during collection and each test\n",
        "+      phase.\n",
        "+\n",
        "+    * fixture: when a test function or one of its fixture depend on the\n",
        "+      ``capsys`` or ``capfd`` fixtures. In this case special handling is\n",
        "+      needed to ensure the fixtures take precedence over the global capture.\n",
        "+    \"\"\"\n",
        "+\n",
        "+    def __init__(self, method: _CaptureMethod) -> None:\n",
        "+        self._method: Final = method\n",
        "+        self._global_capturing: MultiCapture[str] | None = None\n",
        "+        self._capture_fixture: CaptureFixture[Any] | None = None\n",
        "+\n",
        "+    def __repr__(self) -> str:\n",
        "+        return (\n",
        "+            f\"<CaptureManager _method={self._method!r} _global_capturing={self._global_capturing!r} \"\n",
        "+            f\"_capture_fixture={self._capture_fixture!r}>\"\n",
        "+        )\n",
        "+\n",
        "+    def is_capturing(self) -> str | bool:\n",
        "+        if self.is_globally_capturing():\n",
        "+            return \"global\"\n",
        "+        if self._capture_fixture:\n",
        "+            return f\"fixture {self._capture_fixture.request.fixturename}\"\n",
        "+        return False\n",
        "+\n",
        "+    # Global capturing control\n",
        "+\n",
        "+    def is_globally_capturing(self) -> bool:\n",
        "+        return self._method != \"no\"\n",
        "+\n",
        "+    def start_global_capturing(self) -> None:\n",
        "+        assert self._global_capturing is None\n",
        "+        self._global_capturing = _get_multicapture(self._method)\n",
        "+        self._global_capturing.start_capturing()\n",
        "+\n",
        "+    def stop_global_capturing(self) -> None:\n",
        "+        if self._global_capturing is not None:\n",
        "+            self._global_capturing.pop_outerr_to_orig()\n",
        "+            self._global_capturing.stop_capturing()\n",
        "+            self._global_capturing = None\n",
        "+\n",
        "+    def resume_global_capture(self) -> None:\n",
        "+        # During teardown of the python process, and on rare occasions, capture\n",
        "+        # attributes can be `None` while trying to resume global capture.\n",
        "+        if self._global_capturing is not None:\n",
        "+            self._global_capturing.resume_capturing()\n",
        "+\n",
        "+    def suspend_global_capture(self, in_: bool = False) -> None:\n",
        "+        if self._global_capturing is not None:\n",
        "+            self._global_capturing.suspend_capturing(in_=in_)\n",
        "+\n",
        "+    def suspend(self, in_: bool = False) -> None:\n",
        "+        # Need to undo local capsys-et-al if it exists before disabling global capture.\n",
        "+        self.suspend_fixture()\n",
        "+        self.suspend_global_capture(in_)\n",
        "+\n",
        "+    def resume(self) -> None:\n",
        "+        self.resume_global_capture()\n",
        "+        self.resume_fixture()\n",
        "+\n",
        "+    def read_global_capture(self) -> CaptureResult[str]:\n",
        "+        assert self._global_capturing is not None\n",
        "+        return self._global_capturing.readouterr()\n",
        "+\n",
        "+    # Fixture Control\n",
        "+\n",
        "+    def set_fixture(self, capture_fixture: CaptureFixture[Any]) -> None:\n",
        "+        if self._capture_fixture:\n",
        "+            current_fixture = self._capture_fixture.request.fixturename\n",
        "+            requested_fixture = capture_fixture.request.fixturename\n",
        "+            capture_fixture.request.raiseerror(\n",
        "+                f\"cannot use {requested_fixture} and {current_fixture} at the same time\"\n",
        "+            )\n",
        "+        self._capture_fixture = capture_fixture\n",
        "+\n",
        "+    def unset_fixture(self) -> None:\n",
        "+        self._capture_fixture = None\n",
        "+\n",
        "+    def activate_fixture(self) -> None:\n",
        "+        \"\"\"If the current item is using ``capsys`` or ``capfd``, activate\n",
        "+        them so they take precedence over the global capture.\"\"\"\n",
        "+        if self._capture_fixture:\n",
        "+            self._capture_fixture._start()\n",
        "+\n",
        "+    def deactivate_fixture(self) -> None:\n",
        "+        \"\"\"Deactivate the ``capsys`` or ``capfd`` fixture of this item, if any.\"\"\"\n",
        "+        if self._capture_fixture:\n",
        "+            self._capture_fixture.close()\n",
        "+\n",
        "+    def suspend_fixture(self) -> None:\n",
        "+        if self._capture_fixture:\n",
        "+            self._capture_fixture._suspend()\n",
        "+\n",
        "+    def resume_fixture(self) -> None:\n",
        "+        if self._capture_fixture:\n",
        "+            self._capture_fixture._resume()\n",
        "+\n",
        "+    # Helper context managers\n",
        "+\n",
        "+    @contextlib.contextmanager\n",
        "+    def global_and_fixture_disabled(self) -> Generator[None]:\n",
        "+        \"\"\"Context manager to temporarily disable global and current fixture capturing.\"\"\"\n",
        "+        do_fixture = self._capture_fixture and self._capture_fixture._is_started()\n",
        "+        if do_fixture:\n",
        "+            self.suspend_fixture()\n",
        "+        do_global = self._global_capturing and self._global_capturing.is_started()\n",
        "+        if do_global:\n",
        "+            self.suspend_global_capture()\n",
        "+        try:\n",
        "+            yield\n",
        "+        finally:\n",
        "+            if do_global:\n",
        "+                self.resume_global_capture()\n",
        "+            if do_fixture:\n",
        "+                self.resume_fixture()\n",
        "+\n",
        "+    @contextlib.contextmanager\n",
        "+    def item_capture(self, when: str, item: Item) -> Generator[None]:\n",
        "+        self.resume_global_capture()\n",
        "+        self.activate_fixture()\n",
        "+        try:\n",
        "+            yield\n",
        "+        finally:\n",
        "+            self.deactivate_fixture()\n",
        "+            self.suspend_global_capture(in_=False)\n",
        "+\n",
        "+            out, err = self.read_global_capture()\n",
        "+            item.add_report_section(when, \"stdout\", out)\n",
        "+            item.add_report_section(when, \"stderr\", err)\n",
        "+\n",
        "+    # Hooks\n",
        "+\n",
        "+    @hookimpl(wrapper=True)\n",
        "+    def pytest_make_collect_report(\n",
        "+        self, collector: Collector\n",
        "+    ) -> Generator[None, CollectReport, CollectReport]:\n",
        "+        if isinstance(collector, File):\n",
        "+            self.resume_global_capture()\n",
        "+            try:\n",
        "+                rep = yield\n",
        "+            finally:\n",
        "+                self.suspend_global_capture()\n",
        "+            out, err = self.read_global_capture()\n",
        "+            if out:\n",
        "+                rep.sections.append((\"Captured stdout\", out))\n",
        "+            if err:\n",
        "+                rep.sections.append((\"Captured stderr\", err))\n",
        "+        else:\n",
        "+            rep = yield\n",
        "+        return rep\n",
        "+\n",
        "+    @hookimpl(wrapper=True)\n",
        "+    def pytest_runtest_setup(self, item: Item) -> Generator[None]:\n",
        "+        with self.item_capture(\"setup\", item):\n",
        "+            return (yield)\n",
        "+\n",
        "+    @hookimpl(wrapper=True)\n",
        "+    def pytest_runtest_call(self, item: Item) -> Generator[None]:\n",
        "+        with self.item_capture(\"call\", item):\n",
        "+            return (yield)\n",
        "+\n",
        "+    @hookimpl(wrapper=True)\n",
        "+    def pytest_runtest_teardown(self, item: Item) -> Generator[None]:\n",
        "+        with self.item_capture(\"teardown\", item):\n",
        "+            return (yield)\n",
        "+\n",
        "+    @hookimpl(tryfirst=True)\n",
        "+    def pytest_keyboard_interrupt(self) -> None:\n",
        "+        self.stop_global_capturing()\n",
        "+\n",
        "+    @hookimpl(tryfirst=True)\n",
        "+    def pytest_internalerror(self) -> None:\n",
        "+        self.stop_global_capturing()\n",
        "+\n",
        "+\n",
        "+class CaptureFixture(Generic[AnyStr]):\n",
        "+    \"\"\"Object returned by the :fixture:`capsys`, :fixture:`capsysbinary`,\n",
        "+    :fixture:`capfd` and :fixture:`capfdbinary` fixtures.\"\"\"\n",
        "+\n",
        "+    def __init__(\n",
        "+        self,\n",
        "+        captureclass: type[CaptureBase[AnyStr]],\n",
        "+        request: SubRequest,\n",
        "+        *,\n",
        "+        config: dict[str, Any] | None = None,\n",
        "+        _ispytest: bool = False,\n",
        "+    ) -> None:\n",
        "+        check_ispytest(_ispytest)\n",
        "+        self.captureclass: type[CaptureBase[AnyStr]] = captureclass\n",
        "+        self.request = request\n",
        "+        self._config = config if config else {}\n",
        "+        self._capture: MultiCapture[AnyStr] | None = None\n",
        "+        self._captured_out: AnyStr = self.captureclass.EMPTY_BUFFER\n",
        "+        self._captured_err: AnyStr = self.captureclass.EMPTY_BUFFER\n",
        "+\n",
        "+    def _start(self) -> None:\n",
        "+        if self._capture is None:\n",
        "+            self._capture = MultiCapture(\n",
        "+                in_=None,\n",
        "+                out=self.captureclass(1, **self._config),\n",
        "+                err=self.captureclass(2, **self._config),\n",
        "+            )\n",
        "+            self._capture.start_capturing()\n",
        "+\n",
        "+    def close(self) -> None:\n",
        "+        if self._capture is not None:\n",
        "+            out, err = self._capture.pop_outerr_to_orig()\n",
        "+            self._captured_out += out\n",
        "+            self._captured_err += err\n",
        "+            self._capture.stop_capturing()\n",
        "+            self._capture = None\n",
        "+\n",
        "+    def readouterr(self) -> CaptureResult[AnyStr]:\n",
        "+        \"\"\"Read and return the captured output so far, resetting the internal\n",
        "+        buffer.\n",
        "+\n",
        "+        :returns:\n",
        "+            The captured content as a namedtuple with ``out`` and ``err``\n",
        "+            string attributes.\n",
        "+        \"\"\"\n",
        "+        captured_out, captured_err = self._captured_out, self._captured_err\n",
        "+        if self._capture is not None:\n",
        "+            out, err = self._capture.readouterr()\n",
        "+            captured_out += out\n",
        "+            captured_err += err\n",
        "+        self._captured_out = self.captureclass.EMPTY_BUFFER\n",
        "+        self._captured_err = self.captureclass.EMPTY_BUFFER\n",
        "+        return CaptureResult(captured_out, captured_err)\n",
        "+\n",
        "+    def _suspend(self) -> None:\n",
        "+        \"\"\"Suspend this fixture's own capturing temporarily.\"\"\"\n",
        "+        if self._capture is not None:\n",
        "+            self._capture.suspend_capturing()\n",
        "+\n",
        "+    def _resume(self) -> None:\n",
        "+        \"\"\"Resume this fixture's own capturing temporarily.\"\"\"\n",
        "+        if self._capture is not None:\n",
        "+            self._capture.resume_capturing()\n",
        "+\n",
        "+    def _is_started(self) -> bool:\n",
        "+        \"\"\"Whether actively capturing -- not disabled or closed.\"\"\"\n",
        "+        if self._capture is not None:\n",
        "+            return self._capture.is_started()\n",
        "+        return False\n",
        "+\n",
        "+    @contextlib.contextmanager\n",
        "+    def disabled(self) -> Generator[None]:\n",
        "+        \"\"\"Temporarily disable capturing while inside the ``with`` block.\"\"\"\n",
        "+        capmanager: CaptureManager = self.request.config.pluginmanager.getplugin(\n",
        "+            \"capturemanager\"\n",
        "+        )\n",
        "+        with capmanager.global_and_fixture_disabled():\n",
        "+            yield\n",
        "+\n",
        "+\n",
        "+# The fixtures.\n",
        "+\n",
        "+\n",
        "+@fixture\n",
        "+def capsys(request: SubRequest) -> Generator[CaptureFixture[str]]:\n",
        "+    r\"\"\"Enable text capturing of writes to ``sys.stdout`` and ``sys.stderr``.\n",
        "+\n",
        "+    The captured output is made available via ``capsys.readouterr()`` method\n",
        "+    calls, which return a ``(out, err)`` namedtuple.\n",
        "+    ``out`` and ``err`` will be ``text`` objects.\n",
        "+\n",
        "+    Returns an instance of :class:`CaptureFixture[str] <pytest.CaptureFixture>`.\n",
        "+\n",
        "+    Example:\n",
        "+\n",
        "+    .. code-block:: python\n",
        "+\n",
        "+        def test_output(capsys):\n",
        "+            print(\"hello\")\n",
        "+            captured = capsys.readouterr()\n",
        "+            assert captured.out == \"hello\\n\"\n",
        "+    \"\"\"\n",
        "+    capman: CaptureManager = request.config.pluginmanager.getplugin(\"capturemanager\")\n",
        "+    capture_fixture = CaptureFixture(SysCapture, request, _ispytest=True)\n",
        "+    capman.set_fixture(capture_fixture)\n",
        "+    capture_fixture._start()\n",
        "+    yield capture_fixture\n",
        "+    capture_fixture.close()\n",
        "+    capman.unset_fixture()\n",
        "+\n",
        "+\n",
        "+@fixture\n",
        "+def capteesys(request: SubRequest) -> Generator[CaptureFixture[str]]:\n",
        "+    r\"\"\"Enable simultaneous text capturing and pass-through of writes\n",
        "+    to ``sys.stdout`` and ``sys.stderr`` as defined by ``--capture=``.\n",
        "+\n",
        "+\n",
        "+    The captured output is made available via ``capteesys.readouterr()`` method\n",
        "+    calls, which return a ``(out, err)`` namedtuple.\n",
        "+    ``out`` and ``err`` will be ``text`` objects.\n",
        "+\n",
        "+    The output is also passed-through, allowing it to be \"live-printed\",\n",
        "+    reported, or both as defined by ``--capture=``.\n",
        "+\n",
        "+    Returns an instance of :class:`CaptureFixture[str] <pytest.CaptureFixture>`.\n",
        "+\n",
        "+    Example:\n",
        "+\n",
        "+    .. code-block:: python\n",
        "+\n",
        "+        def test_output(capteesys):\n",
        "+            print(\"hello\")\n",
        "+            captured = capteesys.readouterr()\n",
        "+            assert captured.out == \"hello\\n\"\n",
        "+    \"\"\"\n",
        "+    capman: CaptureManager = request.config.pluginmanager.getplugin(\"capturemanager\")\n",
        "+    capture_fixture = CaptureFixture(\n",
        "+        SysCapture, request, config=dict(tee=True), _ispytest=True\n",
        "+    )\n",
        "+    capman.set_fixture(capture_fixture)\n",
        "+    capture_fixture._start()\n",
        "+    yield capture_fixture\n",
        "+    capture_fixture.close()\n",
        "+    capman.unset_fixture()\n",
        "+\n",
        "+\n",
        "+@fixture\n",
        "+def capsysbinary(request: SubRequest) -> Generator[CaptureFixture[bytes]]:\n",
        "+    r\"\"\"Enable bytes capturing of writes to ``sys.stdout`` and ``sys.stderr``.\n",
        "+\n",
        "+    The captured output is made available via ``capsysbinary.readouterr()``\n",
        "+    method calls, which return a ``(out, err)`` namedtuple.\n",
        "+    ``out`` and ``err`` will be ``bytes`` objects.\n",
        "+\n",
        "+    Returns an instance of :class:`CaptureFixture[bytes] <pytest.CaptureFixture>`.\n",
        "+\n",
        "+    Example:\n",
        "+\n",
        "+    .. code-block:: python\n",
        "+\n",
        "+        def test_output(capsysbinary):\n",
        "+            print(\"hello\")\n",
        "+            captured = capsysbinary.readouterr()\n",
        "+            assert captured.out == b\"hello\\n\"\n",
        "+    \"\"\"\n",
        "+    capman: CaptureManager = request.config.pluginmanager.getplugin(\"capturemanager\")\n",
        "+    capture_fixture = CaptureFixture(SysCaptureBinary, request, _ispytest=True)\n",
        "+    capman.set_fixture(capture_fixture)\n",
        "+    capture_fixture._start()\n",
        "+    yield capture_fixture\n",
        "+    capture_fixture.close()\n",
        "+    capman.unset_fixture()\n",
        "+\n",
        "+\n",
        "+@fixture\n",
        "+def capfd(request: SubRequest) -> Generator[CaptureFixture[str]]:\n",
        "+    r\"\"\"Enable text capturing of writes to file descriptors ``1`` and ``2``.\n",
        "+\n",
        "+    The captured output is made available via ``capfd.readouterr()`` method\n",
        "+    calls, which return a ``(out, err)`` namedtuple.\n",
        "+    ``out`` and ``err`` will be ``text`` objects.\n",
        "+\n",
        "+    Returns an instance of :class:`CaptureFixture[str] <pytest.CaptureFixture>`.\n",
        "+\n",
        "+    Example:\n",
        "+\n",
        "+    .. code-block:: python\n",
        "+\n",
        "+        def test_system_echo(capfd):\n",
        "+            os.system('echo \"hello\"')\n",
        "+            captured = capfd.readouterr()\n",
        "+            assert captured.out == \"hello\\n\"\n",
        "+    \"\"\"\n",
        "+    capman: CaptureManager = request.config.pluginmanager.getplugin(\"capturemanager\")\n",
        "+    capture_fixture = CaptureFixture(FDCapture, request, _ispytest=True)\n",
        "+    capman.set_fixture(capture_fixture)\n",
        "+    capture_fixture._start()\n",
        "+    yield capture_fixture\n",
        "+    capture_fixture.close()\n",
        "+    capman.unset_fixture()\n",
        "+\n",
        "+\n",
        "+@fixture\n",
        "+def capfdbinary(request: SubRequest) -> Generator[CaptureFixture[bytes]]:\n",
        "+    r\"\"\"Enable bytes capturing of writes to file descriptors ``1`` and ``2``.\n",
        "+\n",
        "+    The captured output is made available via ``capfd.readouterr()`` method\n",
        "+    calls, which return a ``(out, err)`` namedtuple.\n",
        "+    ``out`` and ``err`` will be ``byte`` objects.\n",
        "+\n",
        "+    Returns an instance of :class:`CaptureFixture[bytes] <pytest.CaptureFixture>`.\n",
        "+\n",
        "+    Example:\n",
        "+\n",
        "+    .. code-block:: python\n",
        "+\n",
        "+        def test_system_echo(capfdbinary):\n",
        "+            os.system('echo \"hello\"')\n",
        "+            captured = capfdbinary.readouterr()\n",
        "+            assert captured.out == b\"hello\\n\"\n",
        "+\n",
        "+    \"\"\"\n",
        "+    capman: CaptureManager = request.config.pluginmanager.getplugin(\"capturemanager\")\n",
        "+    capture_fixture = CaptureFixture(FDCaptureBinary, request, _ispytest=True)\n",
        "+    capman.set_fixture(capture_fixture)\n",
        "+    capture_fixture._start()\n",
        "+    yield capture_fixture\n",
        "+    capture_fixture.close()\n",
        "+    capman.unset_fixture()\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/compat.py",
      "status": "added",
      "additions": 322,
      "deletions": 0,
      "patch": "@@ -0,0 +1,322 @@\n+# mypy: allow-untyped-defs\n+\"\"\"Python version compatibility code.\"\"\"\n+\n+from __future__ import annotations\n+\n+from collections.abc import Callable\n+import enum\n+import functools\n+import inspect\n+from inspect import Parameter\n+from inspect import signature\n+import os\n+from pathlib import Path\n+import sys\n+from typing import Any\n+from typing import Final\n+from typing import NoReturn\n+\n+import py\n+\n+\n+#: constant to prepare valuing pylib path replacements/lazy proxies later on\n+#  intended for removal in pytest 8.0 or 9.0\n+\n+# fmt: off\n+# intentional space to create a fake difference for the verification\n+LEGACY_PATH = py.path. local\n+# fmt: on\n+\n+\n+def legacy_path(path: str | os.PathLike[str]) -> LEGACY_PATH:\n+    \"\"\"Internal wrapper to prepare lazy proxies for legacy_path instances\"\"\"\n+    return LEGACY_PATH(path)\n+\n+\n+# fmt: off\n+# Singleton type for NOTSET, as described in:\n+# https://www.python.org/dev/peps/pep-0484/#support-for-singleton-types-in-unions\n+class NotSetType(enum.Enum):\n+    token = 0\n+NOTSET: Final = NotSetType.token\n+# fmt: on\n+\n+\n+def iscoroutinefunction(func: object) -> bool:\n+    \"\"\"Return True if func is a coroutine function (a function defined with async\n+    def syntax, and doesn't contain yield), or a function decorated with\n+    @asyncio.coroutine.\n+\n+    Note: copied and modified from Python 3.5's builtin coroutines.py to avoid\n+    importing asyncio directly, which in turns also initializes the \"logging\"\n+    module as a side-effect (see issue #8).\n+    \"\"\"\n+    return inspect.iscoroutinefunction(func) or getattr(func, \"_is_coroutine\", False)\n+\n+\n+def is_async_function(func: object) -> bool:\n+    \"\"\"Return True if the given function seems to be an async function or\n+    an async generator.\"\"\"\n+    return iscoroutinefunction(func) or inspect.isasyncgenfunction(func)\n+\n+\n+def getlocation(function, curdir: str | os.PathLike[str] | None = None) -> str:\n+    function = get_real_func(function)\n+    fn = Path(inspect.getfile(function))\n+    lineno = function.__code__.co_firstlineno\n+    if curdir is not None:\n+        try:\n+            relfn = fn.relative_to(curdir)\n+        except ValueError:\n+            pass\n+        else:\n+            return f\"{relfn}:{lineno + 1}\"\n+    return f\"{fn}:{lineno + 1}\"\n+\n+\n+def num_mock_patch_args(function) -> int:\n+    \"\"\"Return number of arguments used up by mock arguments (if any).\"\"\"\n+    patchings = getattr(function, \"patchings\", None)\n+    if not patchings:\n+        return 0\n+\n+    mock_sentinel = getattr(sys.modules.get(\"mock\"), \"DEFAULT\", object())\n+    ut_mock_sentinel = getattr(sys.modules.get(\"unittest.mock\"), \"DEFAULT\", object())\n+\n+    return len(\n+        [\n+            p\n+            for p in patchings\n+            if not p.attribute_name\n+            and (p.new is mock_sentinel or p.new is ut_mock_sentinel)\n+        ]\n+    )\n+\n+\n+def getfuncargnames(\n+    function: Callable[..., object],\n+    *,\n+    name: str = \"\",\n+    cls: type | None = None,\n+) -> tuple[str, ...]:\n+    \"\"\"Return the names of a function's mandatory arguments.\n+\n+    Should return the names of all function arguments that:\n+    * Aren't bound to an instance or type as in instance or class methods.\n+    * Don't have default values.\n+    * Aren't bound with functools.partial.\n+    * Aren't replaced with mocks.\n+\n+    The cls arguments indicate that the function should be treated as a bound\n+    method even though it's not unless the function is a static method.\n+\n+    The name parameter should be the original name in which the function was collected.\n+    \"\"\"\n+    # TODO(RonnyPfannschmidt): This function should be refactored when we\n+    # revisit fixtures. The fixture mechanism should ask the node for\n+    # the fixture names, and not try to obtain directly from the\n+    # function object well after collection has occurred.\n+\n+    # The parameters attribute of a Signature object contains an\n+    # ordered mapping of parameter names to Parameter instances.  This\n+    # creates a tuple of the names of the parameters that don't have\n+    # defaults.\n+    try:\n+        parameters = signature(function).parameters.values()\n+    except (ValueError, TypeError) as e:\n+        from _pytest.outcomes import fail\n+\n+        fail(\n+            f\"Could not determine arguments of {function!r}: {e}\",\n+            pytrace=False,\n+        )\n+\n+    arg_names = tuple(\n+        p.name\n+        for p in parameters\n+        if (\n+            p.kind is Parameter.POSITIONAL_OR_KEYWORD\n+            or p.kind is Parameter.KEYWORD_ONLY\n+        )\n+        and p.default is Parameter.empty\n+    )\n+    if not name:\n+        name = function.__name__\n+\n+    # If this function should be treated as a bound method even though\n+    # it's passed as an unbound method or function, and its first parameter\n+    # wasn't defined as positional only, remove the first parameter name.\n+    if not any(p.kind is Parameter.POSITIONAL_ONLY for p in parameters) and (\n+        # Not using `getattr` because we don't want to resolve the staticmethod.\n+        # Not using `cls.__dict__` because we want to check the entire MRO.\n+        cls\n+        and not isinstance(\n+            inspect.getattr_static(cls, name, default=None), staticmethod\n+        )\n+    ):\n+        arg_names = arg_names[1:]\n+    # Remove any names that will be replaced with mocks.\n+    if hasattr(function, \"__wrapped__\"):\n+        arg_names = arg_names[num_mock_patch_args(function) :]\n+    return arg_names\n+\n+\n+def get_default_arg_names(function: Callable[..., Any]) -> tuple[str, ...]:\n+    # Note: this code intentionally mirrors the code at the beginning of\n+    # getfuncargnames, to get the arguments which were excluded from its result\n+    # because they had default values.\n+    return tuple(\n+        p.name\n+        for p in signature(function).parameters.values()\n+        if p.kind in (Parameter.POSITIONAL_OR_KEYWORD, Parameter.KEYWORD_ONLY)\n+        and p.default is not Parameter.empty\n+    )\n+\n+\n+_non_printable_ascii_translate_table = {\n+    i: f\"\\\\x{i:02x}\" for i in range(128) if i not in range(32, 127)\n+}\n+_non_printable_ascii_translate_table.update(\n+    {ord(\"\\t\"): \"\\\\t\", ord(\"\\r\"): \"\\\\r\", ord(\"\\n\"): \"\\\\n\"}\n+)\n+\n+\n+def ascii_escaped(val: bytes | str) -> str:\n+    r\"\"\"If val is pure ASCII, return it as an str, otherwise, escape\n+    bytes objects into a sequence of escaped bytes:\n+\n+    b'\\xc3\\xb4\\xc5\\xd6' -> r'\\xc3\\xb4\\xc5\\xd6'\n+\n+    and escapes strings into a sequence of escaped unicode ids, e.g.:\n+\n+    r'4\\nV\\U00043efa\\x0eMXWB\\x1e\\u3028\\u15fd\\xcd\\U0007d944'\n+\n+    Note:\n+       The obvious \"v.decode('unicode-escape')\" will return\n+       valid UTF-8 unicode if it finds them in bytes, but we\n+       want to return escaped bytes for any byte, even if they match\n+       a UTF-8 string.\n+    \"\"\"\n+    if isinstance(val, bytes):\n+        ret = val.decode(\"ascii\", \"backslashreplace\")\n+    else:\n+        ret = val.encode(\"unicode_escape\").decode(\"ascii\")\n+    return ret.translate(_non_printable_ascii_translate_table)\n+\n+\n+def get_real_func(obj):\n+    \"\"\"Get the real function object of the (possibly) wrapped object by\n+    :func:`functools.wraps`, or :func:`functools.partial`.\"\"\"\n+    obj = inspect.unwrap(obj)\n+\n+    if isinstance(obj, functools.partial):\n+        obj = obj.func\n+    return obj\n+\n+\n+def getimfunc(func):\n+    try:\n+        return func.__func__\n+    except AttributeError:\n+        return func\n+\n+\n+def safe_getattr(object: Any, name: str, default: Any) -> Any:\n+    \"\"\"Like getattr but return default upon any Exception or any OutcomeException.\n+\n+    Attribute access can potentially fail for 'evil' Python objects.\n+    See issue #214.\n+    It catches OutcomeException because of #2490 (issue #580), new outcomes\n+    are derived from BaseException instead of Exception (for more details\n+    check #2707).\n+    \"\"\"\n+    from _pytest.outcomes import TEST_OUTCOME\n+\n+    try:\n+        return getattr(object, name, default)\n+    except TEST_OUTCOME:\n+        return default\n+\n+\n+def safe_isclass(obj: object) -> bool:\n+    \"\"\"Ignore any exception via isinstance on Python 3.\"\"\"\n+    try:\n+        return inspect.isclass(obj)\n+    except Exception:\n+        return False\n+\n+\n+def get_user_id() -> int | None:\n+    \"\"\"Return the current process's real user id or None if it could not be\n+    determined.\n+\n+    :return: The user id or None if it could not be determined.\n+    \"\"\"\n+    # mypy follows the version and platform checking expectation of PEP 484:\n+    # https://mypy.readthedocs.io/en/stable/common_issues.html?highlight=platform#python-version-and-system-platform-checks\n+    # Containment checks are too complex for mypy v1.5.0 and cause failure.\n+    if sys.platform == \"win32\" or sys.platform == \"emscripten\":\n+        # win32 does not have a getuid() function.\n+        # Emscripten has a return 0 stub.\n+        return None\n+    else:\n+        # On other platforms, a return value of -1 is assumed to indicate that\n+        # the current process's real user id could not be determined.\n+        ERROR = -1\n+        uid = os.getuid()\n+        return uid if uid != ERROR else None\n+\n+\n+# Perform exhaustiveness checking.\n+#\n+# Consider this example:\n+#\n+#     MyUnion = Union[int, str]\n+#\n+#     def handle(x: MyUnion) -> int {\n+#         if isinstance(x, int):\n+#             return 1\n+#         elif isinstance(x, str):\n+#             return 2\n+#         else:\n+#             raise Exception('unreachable')\n+#\n+# Now suppose we add a new variant:\n+#\n+#     MyUnion = Union[int, str, bytes]\n+#\n+# After doing this, we must remember ourselves to go and update the handle\n+# function to handle the new variant.\n+#\n+# With `assert_never` we can do better:\n+#\n+#     // raise Exception('unreachable')\n+#     return assert_never(x)\n+#\n+# Now, if we forget to handle the new variant, the type-checker will emit a\n+# compile-time error, instead of the runtime error we would have gotten\n+# previously.\n+#\n+# This also work for Enums (if you use `is` to compare) and Literals.\n+def assert_never(value: NoReturn) -> NoReturn:\n+    assert False, f\"Unhandled value: {value} ({type(value).__name__})\"\n+\n+\n+class CallableBool:\n+    \"\"\"\n+    A bool-like object that can also be called, returning its true/false value.\n+\n+    Used for backwards compatibility in cases where something was supposed to be a method\n+    but was implemented as a simple attribute by mistake (see `TerminalReporter.isatty`).\n+\n+    Do not use in new code.\n+    \"\"\"\n+\n+    def __init__(self, value: bool) -> None:\n+        self._value = value\n+\n+    def __bool__(self) -> bool:\n+        return self._value\n+\n+    def __call__(self) -> bool:\n+        return self._value",
      "patch_lines": [
        "@@ -0,0 +1,322 @@\n",
        "+# mypy: allow-untyped-defs\n",
        "+\"\"\"Python version compatibility code.\"\"\"\n",
        "+\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+from collections.abc import Callable\n",
        "+import enum\n",
        "+import functools\n",
        "+import inspect\n",
        "+from inspect import Parameter\n",
        "+from inspect import signature\n",
        "+import os\n",
        "+from pathlib import Path\n",
        "+import sys\n",
        "+from typing import Any\n",
        "+from typing import Final\n",
        "+from typing import NoReturn\n",
        "+\n",
        "+import py\n",
        "+\n",
        "+\n",
        "+#: constant to prepare valuing pylib path replacements/lazy proxies later on\n",
        "+#  intended for removal in pytest 8.0 or 9.0\n",
        "+\n",
        "+# fmt: off\n",
        "+# intentional space to create a fake difference for the verification\n",
        "+LEGACY_PATH = py.path. local\n",
        "+# fmt: on\n",
        "+\n",
        "+\n",
        "+def legacy_path(path: str | os.PathLike[str]) -> LEGACY_PATH:\n",
        "+    \"\"\"Internal wrapper to prepare lazy proxies for legacy_path instances\"\"\"\n",
        "+    return LEGACY_PATH(path)\n",
        "+\n",
        "+\n",
        "+# fmt: off\n",
        "+# Singleton type for NOTSET, as described in:\n",
        "+# https://www.python.org/dev/peps/pep-0484/#support-for-singleton-types-in-unions\n",
        "+class NotSetType(enum.Enum):\n",
        "+    token = 0\n",
        "+NOTSET: Final = NotSetType.token\n",
        "+# fmt: on\n",
        "+\n",
        "+\n",
        "+def iscoroutinefunction(func: object) -> bool:\n",
        "+    \"\"\"Return True if func is a coroutine function (a function defined with async\n",
        "+    def syntax, and doesn't contain yield), or a function decorated with\n",
        "+    @asyncio.coroutine.\n",
        "+\n",
        "+    Note: copied and modified from Python 3.5's builtin coroutines.py to avoid\n",
        "+    importing asyncio directly, which in turns also initializes the \"logging\"\n",
        "+    module as a side-effect (see issue #8).\n",
        "+    \"\"\"\n",
        "+    return inspect.iscoroutinefunction(func) or getattr(func, \"_is_coroutine\", False)\n",
        "+\n",
        "+\n",
        "+def is_async_function(func: object) -> bool:\n",
        "+    \"\"\"Return True if the given function seems to be an async function or\n",
        "+    an async generator.\"\"\"\n",
        "+    return iscoroutinefunction(func) or inspect.isasyncgenfunction(func)\n",
        "+\n",
        "+\n",
        "+def getlocation(function, curdir: str | os.PathLike[str] | None = None) -> str:\n",
        "+    function = get_real_func(function)\n",
        "+    fn = Path(inspect.getfile(function))\n",
        "+    lineno = function.__code__.co_firstlineno\n",
        "+    if curdir is not None:\n",
        "+        try:\n",
        "+            relfn = fn.relative_to(curdir)\n",
        "+        except ValueError:\n",
        "+            pass\n",
        "+        else:\n",
        "+            return f\"{relfn}:{lineno + 1}\"\n",
        "+    return f\"{fn}:{lineno + 1}\"\n",
        "+\n",
        "+\n",
        "+def num_mock_patch_args(function) -> int:\n",
        "+    \"\"\"Return number of arguments used up by mock arguments (if any).\"\"\"\n",
        "+    patchings = getattr(function, \"patchings\", None)\n",
        "+    if not patchings:\n",
        "+        return 0\n",
        "+\n",
        "+    mock_sentinel = getattr(sys.modules.get(\"mock\"), \"DEFAULT\", object())\n",
        "+    ut_mock_sentinel = getattr(sys.modules.get(\"unittest.mock\"), \"DEFAULT\", object())\n",
        "+\n",
        "+    return len(\n",
        "+        [\n",
        "+            p\n",
        "+            for p in patchings\n",
        "+            if not p.attribute_name\n",
        "+            and (p.new is mock_sentinel or p.new is ut_mock_sentinel)\n",
        "+        ]\n",
        "+    )\n",
        "+\n",
        "+\n",
        "+def getfuncargnames(\n",
        "+    function: Callable[..., object],\n",
        "+    *,\n",
        "+    name: str = \"\",\n",
        "+    cls: type | None = None,\n",
        "+) -> tuple[str, ...]:\n",
        "+    \"\"\"Return the names of a function's mandatory arguments.\n",
        "+\n",
        "+    Should return the names of all function arguments that:\n",
        "+    * Aren't bound to an instance or type as in instance or class methods.\n",
        "+    * Don't have default values.\n",
        "+    * Aren't bound with functools.partial.\n",
        "+    * Aren't replaced with mocks.\n",
        "+\n",
        "+    The cls arguments indicate that the function should be treated as a bound\n",
        "+    method even though it's not unless the function is a static method.\n",
        "+\n",
        "+    The name parameter should be the original name in which the function was collected.\n",
        "+    \"\"\"\n",
        "+    # TODO(RonnyPfannschmidt): This function should be refactored when we\n",
        "+    # revisit fixtures. The fixture mechanism should ask the node for\n",
        "+    # the fixture names, and not try to obtain directly from the\n",
        "+    # function object well after collection has occurred.\n",
        "+\n",
        "+    # The parameters attribute of a Signature object contains an\n",
        "+    # ordered mapping of parameter names to Parameter instances.  This\n",
        "+    # creates a tuple of the names of the parameters that don't have\n",
        "+    # defaults.\n",
        "+    try:\n",
        "+        parameters = signature(function).parameters.values()\n",
        "+    except (ValueError, TypeError) as e:\n",
        "+        from _pytest.outcomes import fail\n",
        "+\n",
        "+        fail(\n",
        "+            f\"Could not determine arguments of {function!r}: {e}\",\n",
        "+            pytrace=False,\n",
        "+        )\n",
        "+\n",
        "+    arg_names = tuple(\n",
        "+        p.name\n",
        "+        for p in parameters\n",
        "+        if (\n",
        "+            p.kind is Parameter.POSITIONAL_OR_KEYWORD\n",
        "+            or p.kind is Parameter.KEYWORD_ONLY\n",
        "+        )\n",
        "+        and p.default is Parameter.empty\n",
        "+    )\n",
        "+    if not name:\n",
        "+        name = function.__name__\n",
        "+\n",
        "+    # If this function should be treated as a bound method even though\n",
        "+    # it's passed as an unbound method or function, and its first parameter\n",
        "+    # wasn't defined as positional only, remove the first parameter name.\n",
        "+    if not any(p.kind is Parameter.POSITIONAL_ONLY for p in parameters) and (\n",
        "+        # Not using `getattr` because we don't want to resolve the staticmethod.\n",
        "+        # Not using `cls.__dict__` because we want to check the entire MRO.\n",
        "+        cls\n",
        "+        and not isinstance(\n",
        "+            inspect.getattr_static(cls, name, default=None), staticmethod\n",
        "+        )\n",
        "+    ):\n",
        "+        arg_names = arg_names[1:]\n",
        "+    # Remove any names that will be replaced with mocks.\n",
        "+    if hasattr(function, \"__wrapped__\"):\n",
        "+        arg_names = arg_names[num_mock_patch_args(function) :]\n",
        "+    return arg_names\n",
        "+\n",
        "+\n",
        "+def get_default_arg_names(function: Callable[..., Any]) -> tuple[str, ...]:\n",
        "+    # Note: this code intentionally mirrors the code at the beginning of\n",
        "+    # getfuncargnames, to get the arguments which were excluded from its result\n",
        "+    # because they had default values.\n",
        "+    return tuple(\n",
        "+        p.name\n",
        "+        for p in signature(function).parameters.values()\n",
        "+        if p.kind in (Parameter.POSITIONAL_OR_KEYWORD, Parameter.KEYWORD_ONLY)\n",
        "+        and p.default is not Parameter.empty\n",
        "+    )\n",
        "+\n",
        "+\n",
        "+_non_printable_ascii_translate_table = {\n",
        "+    i: f\"\\\\x{i:02x}\" for i in range(128) if i not in range(32, 127)\n",
        "+}\n",
        "+_non_printable_ascii_translate_table.update(\n",
        "+    {ord(\"\\t\"): \"\\\\t\", ord(\"\\r\"): \"\\\\r\", ord(\"\\n\"): \"\\\\n\"}\n",
        "+)\n",
        "+\n",
        "+\n",
        "+def ascii_escaped(val: bytes | str) -> str:\n",
        "+    r\"\"\"If val is pure ASCII, return it as an str, otherwise, escape\n",
        "+    bytes objects into a sequence of escaped bytes:\n",
        "+\n",
        "+    b'\\xc3\\xb4\\xc5\\xd6' -> r'\\xc3\\xb4\\xc5\\xd6'\n",
        "+\n",
        "+    and escapes strings into a sequence of escaped unicode ids, e.g.:\n",
        "+\n",
        "+    r'4\\nV\\U00043efa\\x0eMXWB\\x1e\\u3028\\u15fd\\xcd\\U0007d944'\n",
        "+\n",
        "+    Note:\n",
        "+       The obvious \"v.decode('unicode-escape')\" will return\n",
        "+       valid UTF-8 unicode if it finds them in bytes, but we\n",
        "+       want to return escaped bytes for any byte, even if they match\n",
        "+       a UTF-8 string.\n",
        "+    \"\"\"\n",
        "+    if isinstance(val, bytes):\n",
        "+        ret = val.decode(\"ascii\", \"backslashreplace\")\n",
        "+    else:\n",
        "+        ret = val.encode(\"unicode_escape\").decode(\"ascii\")\n",
        "+    return ret.translate(_non_printable_ascii_translate_table)\n",
        "+\n",
        "+\n",
        "+def get_real_func(obj):\n",
        "+    \"\"\"Get the real function object of the (possibly) wrapped object by\n",
        "+    :func:`functools.wraps`, or :func:`functools.partial`.\"\"\"\n",
        "+    obj = inspect.unwrap(obj)\n",
        "+\n",
        "+    if isinstance(obj, functools.partial):\n",
        "+        obj = obj.func\n",
        "+    return obj\n",
        "+\n",
        "+\n",
        "+def getimfunc(func):\n",
        "+    try:\n",
        "+        return func.__func__\n",
        "+    except AttributeError:\n",
        "+        return func\n",
        "+\n",
        "+\n",
        "+def safe_getattr(object: Any, name: str, default: Any) -> Any:\n",
        "+    \"\"\"Like getattr but return default upon any Exception or any OutcomeException.\n",
        "+\n",
        "+    Attribute access can potentially fail for 'evil' Python objects.\n",
        "+    See issue #214.\n",
        "+    It catches OutcomeException because of #2490 (issue #580), new outcomes\n",
        "+    are derived from BaseException instead of Exception (for more details\n",
        "+    check #2707).\n",
        "+    \"\"\"\n",
        "+    from _pytest.outcomes import TEST_OUTCOME\n",
        "+\n",
        "+    try:\n",
        "+        return getattr(object, name, default)\n",
        "+    except TEST_OUTCOME:\n",
        "+        return default\n",
        "+\n",
        "+\n",
        "+def safe_isclass(obj: object) -> bool:\n",
        "+    \"\"\"Ignore any exception via isinstance on Python 3.\"\"\"\n",
        "+    try:\n",
        "+        return inspect.isclass(obj)\n",
        "+    except Exception:\n",
        "+        return False\n",
        "+\n",
        "+\n",
        "+def get_user_id() -> int | None:\n",
        "+    \"\"\"Return the current process's real user id or None if it could not be\n",
        "+    determined.\n",
        "+\n",
        "+    :return: The user id or None if it could not be determined.\n",
        "+    \"\"\"\n",
        "+    # mypy follows the version and platform checking expectation of PEP 484:\n",
        "+    # https://mypy.readthedocs.io/en/stable/common_issues.html?highlight=platform#python-version-and-system-platform-checks\n",
        "+    # Containment checks are too complex for mypy v1.5.0 and cause failure.\n",
        "+    if sys.platform == \"win32\" or sys.platform == \"emscripten\":\n",
        "+        # win32 does not have a getuid() function.\n",
        "+        # Emscripten has a return 0 stub.\n",
        "+        return None\n",
        "+    else:\n",
        "+        # On other platforms, a return value of -1 is assumed to indicate that\n",
        "+        # the current process's real user id could not be determined.\n",
        "+        ERROR = -1\n",
        "+        uid = os.getuid()\n",
        "+        return uid if uid != ERROR else None\n",
        "+\n",
        "+\n",
        "+# Perform exhaustiveness checking.\n",
        "+#\n",
        "+# Consider this example:\n",
        "+#\n",
        "+#     MyUnion = Union[int, str]\n",
        "+#\n",
        "+#     def handle(x: MyUnion) -> int {\n",
        "+#         if isinstance(x, int):\n",
        "+#             return 1\n",
        "+#         elif isinstance(x, str):\n",
        "+#             return 2\n",
        "+#         else:\n",
        "+#             raise Exception('unreachable')\n",
        "+#\n",
        "+# Now suppose we add a new variant:\n",
        "+#\n",
        "+#     MyUnion = Union[int, str, bytes]\n",
        "+#\n",
        "+# After doing this, we must remember ourselves to go and update the handle\n",
        "+# function to handle the new variant.\n",
        "+#\n",
        "+# With `assert_never` we can do better:\n",
        "+#\n",
        "+#     // raise Exception('unreachable')\n",
        "+#     return assert_never(x)\n",
        "+#\n",
        "+# Now, if we forget to handle the new variant, the type-checker will emit a\n",
        "+# compile-time error, instead of the runtime error we would have gotten\n",
        "+# previously.\n",
        "+#\n",
        "+# This also work for Enums (if you use `is` to compare) and Literals.\n",
        "+def assert_never(value: NoReturn) -> NoReturn:\n",
        "+    assert False, f\"Unhandled value: {value} ({type(value).__name__})\"\n",
        "+\n",
        "+\n",
        "+class CallableBool:\n",
        "+    \"\"\"\n",
        "+    A bool-like object that can also be called, returning its true/false value.\n",
        "+\n",
        "+    Used for backwards compatibility in cases where something was supposed to be a method\n",
        "+    but was implemented as a simple attribute by mistake (see `TerminalReporter.isatty`).\n",
        "+\n",
        "+    Do not use in new code.\n",
        "+    \"\"\"\n",
        "+\n",
        "+    def __init__(self, value: bool) -> None:\n",
        "+        self._value = value\n",
        "+\n",
        "+    def __bool__(self) -> bool:\n",
        "+        return self._value\n",
        "+\n",
        "+    def __call__(self) -> bool:\n",
        "+        return self._value\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/config/__init__.py",
      "status": "added",
      "additions": 2029,
      "deletions": 0,
      "patch": "@@ -0,0 +1,2029 @@\n+# mypy: allow-untyped-defs\n+\"\"\"Command line options, ini-file and conftest.py processing.\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import collections.abc\n+from collections.abc import Callable\n+from collections.abc import Generator\n+from collections.abc import Iterable\n+from collections.abc import Iterator\n+from collections.abc import Sequence\n+import contextlib\n+import copy\n+import dataclasses\n+import enum\n+from functools import lru_cache\n+import glob\n+import importlib.metadata\n+import inspect\n+import os\n+import pathlib\n+import re\n+import shlex\n+import sys\n+from textwrap import dedent\n+import types\n+from types import FunctionType\n+from typing import Any\n+from typing import cast\n+from typing import Final\n+from typing import final\n+from typing import IO\n+from typing import TextIO\n+from typing import TYPE_CHECKING\n+import warnings\n+\n+import pluggy\n+from pluggy import HookimplMarker\n+from pluggy import HookimplOpts\n+from pluggy import HookspecMarker\n+from pluggy import HookspecOpts\n+from pluggy import PluginManager\n+\n+from .compat import PathAwareHookProxy\n+from .exceptions import PrintHelp as PrintHelp\n+from .exceptions import UsageError as UsageError\n+from .findpaths import determine_setup\n+from _pytest import __version__\n+import _pytest._code\n+from _pytest._code import ExceptionInfo\n+from _pytest._code import filter_traceback\n+from _pytest._code.code import TracebackStyle\n+from _pytest._io import TerminalWriter\n+from _pytest.config.argparsing import Argument\n+from _pytest.config.argparsing import Parser\n+import _pytest.deprecated\n+import _pytest.hookspec\n+from _pytest.outcomes import fail\n+from _pytest.outcomes import Skipped\n+from _pytest.pathlib import absolutepath\n+from _pytest.pathlib import bestrelpath\n+from _pytest.pathlib import import_path\n+from _pytest.pathlib import ImportMode\n+from _pytest.pathlib import resolve_package_path\n+from _pytest.pathlib import safe_exists\n+from _pytest.stash import Stash\n+from _pytest.warning_types import PytestConfigWarning\n+from _pytest.warning_types import warn_explicit_for\n+\n+\n+if TYPE_CHECKING:\n+    from _pytest.assertion.rewrite import AssertionRewritingHook\n+    from _pytest.cacheprovider import Cache\n+    from _pytest.terminal import TerminalReporter\n+\n+_PluggyPlugin = object\n+\"\"\"A type to represent plugin objects.\n+\n+Plugins can be any namespace, so we can't narrow it down much, but we use an\n+alias to make the intent clear.\n+\n+Ideally this type would be provided by pluggy itself.\n+\"\"\"\n+\n+\n+hookimpl = HookimplMarker(\"pytest\")\n+hookspec = HookspecMarker(\"pytest\")\n+\n+\n+@final\n+class ExitCode(enum.IntEnum):\n+    \"\"\"Encodes the valid exit codes by pytest.\n+\n+    Currently users and plugins may supply other exit codes as well.\n+\n+    .. versionadded:: 5.0\n+    \"\"\"\n+\n+    #: Tests passed.\n+    OK = 0\n+    #: Tests failed.\n+    TESTS_FAILED = 1\n+    #: pytest was interrupted.\n+    INTERRUPTED = 2\n+    #: An internal error got in the way.\n+    INTERNAL_ERROR = 3\n+    #: pytest was misused.\n+    USAGE_ERROR = 4\n+    #: pytest couldn't find tests.\n+    NO_TESTS_COLLECTED = 5\n+\n+\n+class ConftestImportFailure(Exception):\n+    def __init__(\n+        self,\n+        path: pathlib.Path,\n+        *,\n+        cause: Exception,\n+    ) -> None:\n+        self.path = path\n+        self.cause = cause\n+\n+    def __str__(self) -> str:\n+        return f\"{type(self.cause).__name__}: {self.cause} (from {self.path})\"\n+\n+\n+def filter_traceback_for_conftest_import_failure(\n+    entry: _pytest._code.TracebackEntry,\n+) -> bool:\n+    \"\"\"Filter tracebacks entries which point to pytest internals or importlib.\n+\n+    Make a special case for importlib because we use it to import test modules and conftest files\n+    in _pytest.pathlib.import_path.\n+    \"\"\"\n+    return filter_traceback(entry) and \"importlib\" not in str(entry.path).split(os.sep)\n+\n+\n+def main(\n+    args: list[str] | os.PathLike[str] | None = None,\n+    plugins: Sequence[str | _PluggyPlugin] | None = None,\n+) -> int | ExitCode:\n+    \"\"\"Perform an in-process test run.\n+\n+    :param args:\n+        List of command line arguments. If `None` or not given, defaults to reading\n+        arguments directly from the process command line (:data:`sys.argv`).\n+    :param plugins: List of plugin objects to be auto-registered during initialization.\n+\n+    :returns: An exit code.\n+    \"\"\"\n+    old_pytest_version = os.environ.get(\"PYTEST_VERSION\")\n+    try:\n+        os.environ[\"PYTEST_VERSION\"] = __version__\n+        try:\n+            config = _prepareconfig(args, plugins)\n+        except ConftestImportFailure as e:\n+            exc_info = ExceptionInfo.from_exception(e.cause)\n+            tw = TerminalWriter(sys.stderr)\n+            tw.line(f\"ImportError while loading conftest '{e.path}'.\", red=True)\n+            exc_info.traceback = exc_info.traceback.filter(\n+                filter_traceback_for_conftest_import_failure\n+            )\n+            exc_repr = (\n+                exc_info.getrepr(style=\"short\", chain=False)\n+                if exc_info.traceback\n+                else exc_info.exconly()\n+            )\n+            formatted_tb = str(exc_repr)\n+            for line in formatted_tb.splitlines():\n+                tw.line(line.rstrip(), red=True)\n+            return ExitCode.USAGE_ERROR\n+        else:\n+            try:\n+                ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)\n+                try:\n+                    return ExitCode(ret)\n+                except ValueError:\n+                    return ret\n+            finally:\n+                config._ensure_unconfigure()\n+    except UsageError as e:\n+        tw = TerminalWriter(sys.stderr)\n+        for msg in e.args:\n+            tw.line(f\"ERROR: {msg}\\n\", red=True)\n+        return ExitCode.USAGE_ERROR\n+    finally:\n+        if old_pytest_version is None:\n+            os.environ.pop(\"PYTEST_VERSION\", None)\n+        else:\n+            os.environ[\"PYTEST_VERSION\"] = old_pytest_version\n+\n+\n+def console_main() -> int:\n+    \"\"\"The CLI entry point of pytest.\n+\n+    This function is not meant for programmable use; use `main()` instead.\n+    \"\"\"\n+    # https://docs.python.org/3/library/signal.html#note-on-sigpipe\n+    try:\n+        code = main()\n+        sys.stdout.flush()\n+        return code\n+    except BrokenPipeError:\n+        # Python flushes standard streams on exit; redirect remaining output\n+        # to devnull to avoid another BrokenPipeError at shutdown\n+        devnull = os.open(os.devnull, os.O_WRONLY)\n+        os.dup2(devnull, sys.stdout.fileno())\n+        return 1  # Python exits with error code 1 on EPIPE\n+\n+\n+class cmdline:  # compatibility namespace\n+    main = staticmethod(main)\n+\n+\n+def filename_arg(path: str, optname: str) -> str:\n+    \"\"\"Argparse type validator for filename arguments.\n+\n+    :path: Path of filename.\n+    :optname: Name of the option.\n+    \"\"\"\n+    if os.path.isdir(path):\n+        raise UsageError(f\"{optname} must be a filename, given: {path}\")\n+    return path\n+\n+\n+def directory_arg(path: str, optname: str) -> str:\n+    \"\"\"Argparse type validator for directory arguments.\n+\n+    :path: Path of directory.\n+    :optname: Name of the option.\n+    \"\"\"\n+    if not os.path.isdir(path):\n+        raise UsageError(f\"{optname} must be a directory, given: {path}\")\n+    return path\n+\n+\n+# Plugins that cannot be disabled via \"-p no:X\" currently.\n+essential_plugins = (\n+    \"mark\",\n+    \"main\",\n+    \"runner\",\n+    \"fixtures\",\n+    \"helpconfig\",  # Provides -p.\n+)\n+\n+default_plugins = (\n+    *essential_plugins,\n+    \"python\",\n+    \"terminal\",\n+    \"debugging\",\n+    \"unittest\",\n+    \"capture\",\n+    \"skipping\",\n+    \"legacypath\",\n+    \"tmpdir\",\n+    \"monkeypatch\",\n+    \"recwarn\",\n+    \"pastebin\",\n+    \"assertion\",\n+    \"junitxml\",\n+    \"doctest\",\n+    \"cacheprovider\",\n+    \"freeze_support\",\n+    \"setuponly\",\n+    \"setupplan\",\n+    \"stepwise\",\n+    \"unraisableexception\",\n+    \"threadexception\",\n+    \"warnings\",\n+    \"logging\",\n+    \"reports\",\n+    \"faulthandler\",\n+)\n+\n+builtin_plugins = {\n+    *default_plugins,\n+    \"pytester\",\n+    \"pytester_assertions\",\n+}\n+\n+\n+def get_config(\n+    args: list[str] | None = None,\n+    plugins: Sequence[str | _PluggyPlugin] | None = None,\n+) -> Config:\n+    # subsequent calls to main will create a fresh instance\n+    pluginmanager = PytestPluginManager()\n+    config = Config(\n+        pluginmanager,\n+        invocation_params=Config.InvocationParams(\n+            args=args or (),\n+            plugins=plugins,\n+            dir=pathlib.Path.cwd(),\n+        ),\n+    )\n+\n+    if args is not None:\n+        # Handle any \"-p no:plugin\" args.\n+        pluginmanager.consider_preparse(args, exclude_only=True)\n+\n+    for spec in default_plugins:\n+        pluginmanager.import_plugin(spec)\n+\n+    return config\n+\n+\n+def get_plugin_manager() -> PytestPluginManager:\n+    \"\"\"Obtain a new instance of the\n+    :py:class:`pytest.PytestPluginManager`, with default plugins\n+    already loaded.\n+\n+    This function can be used by integration with other tools, like hooking\n+    into pytest to run tests into an IDE.\n+    \"\"\"\n+    return get_config().pluginmanager\n+\n+\n+def _prepareconfig(\n+    args: list[str] | os.PathLike[str] | None = None,\n+    plugins: Sequence[str | _PluggyPlugin] | None = None,\n+) -> Config:\n+    if args is None:\n+        args = sys.argv[1:]\n+    elif isinstance(args, os.PathLike):\n+        args = [os.fspath(args)]\n+    elif not isinstance(args, list):\n+        msg = (  # type:ignore[unreachable]\n+            \"`args` parameter expected to be a list of strings, got: {!r} (type: {})\"\n+        )\n+        raise TypeError(msg.format(args, type(args)))\n+\n+    config = get_config(args, plugins)\n+    pluginmanager = config.pluginmanager\n+    try:\n+        if plugins:\n+            for plugin in plugins:\n+                if isinstance(plugin, str):\n+                    pluginmanager.consider_pluginarg(plugin)\n+                else:\n+                    pluginmanager.register(plugin)\n+        config = pluginmanager.hook.pytest_cmdline_parse(\n+            pluginmanager=pluginmanager, args=args\n+        )\n+        return config\n+    except BaseException:\n+        config._ensure_unconfigure()\n+        raise\n+\n+\n+def _get_directory(path: pathlib.Path) -> pathlib.Path:\n+    \"\"\"Get the directory of a path - itself if already a directory.\"\"\"\n+    if path.is_file():\n+        return path.parent\n+    else:\n+        return path\n+\n+\n+def _get_legacy_hook_marks(\n+    method: Any,\n+    hook_type: str,\n+    opt_names: tuple[str, ...],\n+) -> dict[str, bool]:\n+    if TYPE_CHECKING:\n+        # abuse typeguard from importlib to avoid massive method type union that's lacking an alias\n+        assert inspect.isroutine(method)\n+    known_marks: set[str] = {m.name for m in getattr(method, \"pytestmark\", [])}\n+    must_warn: list[str] = []\n+    opts: dict[str, bool] = {}\n+    for opt_name in opt_names:\n+        opt_attr = getattr(method, opt_name, AttributeError)\n+        if opt_attr is not AttributeError:\n+            must_warn.append(f\"{opt_name}={opt_attr}\")\n+            opts[opt_name] = True\n+        elif opt_name in known_marks:\n+            must_warn.append(f\"{opt_name}=True\")\n+            opts[opt_name] = True\n+        else:\n+            opts[opt_name] = False\n+    if must_warn:\n+        hook_opts = \", \".join(must_warn)\n+        message = _pytest.deprecated.HOOK_LEGACY_MARKING.format(\n+            type=hook_type,\n+            fullname=method.__qualname__,\n+            hook_opts=hook_opts,\n+        )\n+        warn_explicit_for(cast(FunctionType, method), message)\n+    return opts\n+\n+\n+@final\n+class PytestPluginManager(PluginManager):\n+    \"\"\"A :py:class:`pluggy.PluginManager <pluggy.PluginManager>` with\n+    additional pytest-specific functionality:\n+\n+    * Loading plugins from the command line, ``PYTEST_PLUGINS`` env variable and\n+      ``pytest_plugins`` global variables found in plugins being loaded.\n+    * ``conftest.py`` loading during start-up.\n+    \"\"\"\n+\n+    def __init__(self) -> None:\n+        from _pytest.assertion import DummyRewriteHook\n+        from _pytest.assertion import RewriteHook\n+\n+        super().__init__(\"pytest\")\n+\n+        # -- State related to local conftest plugins.\n+        # All loaded conftest modules.\n+        self._conftest_plugins: set[types.ModuleType] = set()\n+        # All conftest modules applicable for a directory.\n+        # This includes the directory's own conftest modules as well\n+        # as those of its parent directories.\n+        self._dirpath2confmods: dict[pathlib.Path, list[types.ModuleType]] = {}\n+        # Cutoff directory above which conftests are no longer discovered.\n+        self._confcutdir: pathlib.Path | None = None\n+        # If set, conftest loading is skipped.\n+        self._noconftest = False\n+\n+        # _getconftestmodules()'s call to _get_directory() causes a stat\n+        # storm when it's called potentially thousands of times in a test\n+        # session (#9478), often with the same path, so cache it.\n+        self._get_directory = lru_cache(256)(_get_directory)\n+\n+        # plugins that were explicitly skipped with pytest.skip\n+        # list of (module name, skip reason)\n+        # previously we would issue a warning when a plugin was skipped, but\n+        # since we refactored warnings as first citizens of Config, they are\n+        # just stored here to be used later.\n+        self.skipped_plugins: list[tuple[str, str]] = []\n+\n+        self.add_hookspecs(_pytest.hookspec)\n+        self.register(self)\n+        if os.environ.get(\"PYTEST_DEBUG\"):\n+            err: IO[str] = sys.stderr\n+            encoding: str = getattr(err, \"encoding\", \"utf8\")\n+            try:\n+                err = open(\n+                    os.dup(err.fileno()),\n+                    mode=err.mode,\n+                    buffering=1,\n+                    encoding=encoding,\n+                )\n+            except Exception:\n+                pass\n+            self.trace.root.setwriter(err.write)\n+            self.enable_tracing()\n+\n+        # Config._consider_importhook will set a real object if required.\n+        self.rewrite_hook: RewriteHook = DummyRewriteHook()\n+        # Used to know when we are importing conftests after the pytest_configure stage.\n+        self._configured = False\n+\n+    def parse_hookimpl_opts(\n+        self, plugin: _PluggyPlugin, name: str\n+    ) -> HookimplOpts | None:\n+        \"\"\":meta private:\"\"\"\n+        # pytest hooks are always prefixed with \"pytest_\",\n+        # so we avoid accessing possibly non-readable attributes\n+        # (see issue #1073).\n+        if not name.startswith(\"pytest_\"):\n+            return None\n+        # Ignore names which cannot be hooks.\n+        if name == \"pytest_plugins\":\n+            return None\n+\n+        opts = super().parse_hookimpl_opts(plugin, name)\n+        if opts is not None:\n+            return opts\n+\n+        method = getattr(plugin, name)\n+        # Consider only actual functions for hooks (#3775).\n+        if not inspect.isroutine(method):\n+            return None\n+        # Collect unmarked hooks as long as they have the `pytest_' prefix.\n+        legacy = _get_legacy_hook_marks(\n+            method, \"impl\", (\"tryfirst\", \"trylast\", \"optionalhook\", \"hookwrapper\")\n+        )\n+        return cast(HookimplOpts, legacy)\n+\n+    def parse_hookspec_opts(self, module_or_class, name: str) -> HookspecOpts | None:\n+        \"\"\":meta private:\"\"\"\n+        opts = super().parse_hookspec_opts(module_or_class, name)\n+        if opts is None:\n+            method = getattr(module_or_class, name)\n+            if name.startswith(\"pytest_\"):\n+                legacy = _get_legacy_hook_marks(\n+                    method, \"spec\", (\"firstresult\", \"historic\")\n+                )\n+                opts = cast(HookspecOpts, legacy)\n+        return opts\n+\n+    def register(self, plugin: _PluggyPlugin, name: str | None = None) -> str | None:\n+        if name in _pytest.deprecated.DEPRECATED_EXTERNAL_PLUGINS:\n+            warnings.warn(\n+                PytestConfigWarning(\n+                    \"{} plugin has been merged into the core, \"\n+                    \"please remove it from your requirements.\".format(\n+                        name.replace(\"_\", \"-\")\n+                    )\n+                )\n+            )\n+            return None\n+        plugin_name = super().register(plugin, name)\n+        if plugin_name is not None:\n+            self.hook.pytest_plugin_registered.call_historic(\n+                kwargs=dict(\n+                    plugin=plugin,\n+                    plugin_name=plugin_name,\n+                    manager=self,\n+                )\n+            )\n+\n+            if isinstance(plugin, types.ModuleType):\n+                self.consider_module(plugin)\n+        return plugin_name\n+\n+    def getplugin(self, name: str):\n+        # Support deprecated naming because plugins (xdist e.g.) use it.\n+        plugin: _PluggyPlugin | None = self.get_plugin(name)\n+        return plugin\n+\n+    def hasplugin(self, name: str) -> bool:\n+        \"\"\"Return whether a plugin with the given name is registered.\"\"\"\n+        return bool(self.get_plugin(name))\n+\n+    def pytest_configure(self, config: Config) -> None:\n+        \"\"\":meta private:\"\"\"\n+        # XXX now that the pluginmanager exposes hookimpl(tryfirst...)\n+        # we should remove tryfirst/trylast as markers.\n+        config.addinivalue_line(\n+            \"markers\",\n+            \"tryfirst: mark a hook implementation function such that the \"\n+            \"plugin machinery will try to call it first/as early as possible. \"\n+            \"DEPRECATED, use @pytest.hookimpl(tryfirst=True) instead.\",\n+        )\n+        config.addinivalue_line(\n+            \"markers\",\n+            \"trylast: mark a hook implementation function such that the \"\n+            \"plugin machinery will try to call it last/as late as possible. \"\n+            \"DEPRECATED, use @pytest.hookimpl(trylast=True) instead.\",\n+        )\n+        self._configured = True\n+\n+    #\n+    # Internal API for local conftest plugin handling.\n+    #\n+    def _set_initial_conftests(\n+        self,\n+        args: Sequence[str | pathlib.Path],\n+        pyargs: bool,\n+        noconftest: bool,\n+        rootpath: pathlib.Path,\n+        confcutdir: pathlib.Path | None,\n+        invocation_dir: pathlib.Path,\n+        importmode: ImportMode | str,\n+        *,\n+        consider_namespace_packages: bool,\n+    ) -> None:\n+        \"\"\"Load initial conftest files given a preparsed \"namespace\".\n+\n+        As conftest files may add their own command line options which have\n+        arguments ('--my-opt somepath') we might get some false positives.\n+        All builtin and 3rd party plugins will have been loaded, however, so\n+        common options will not confuse our logic here.\n+        \"\"\"\n+        self._confcutdir = (\n+            absolutepath(invocation_dir / confcutdir) if confcutdir else None\n+        )\n+        self._noconftest = noconftest\n+        self._using_pyargs = pyargs\n+        foundanchor = False\n+        for initial_path in args:\n+            path = str(initial_path)\n+            # remove node-id syntax\n+            i = path.find(\"::\")\n+            if i != -1:\n+                path = path[:i]\n+            anchor = absolutepath(invocation_dir / path)\n+\n+            # Ensure we do not break if what appears to be an anchor\n+            # is in fact a very long option (#10169, #11394).\n+            if safe_exists(anchor):\n+                self._try_load_conftest(\n+                    anchor,\n+                    importmode,\n+                    rootpath,\n+                    consider_namespace_packages=consider_namespace_packages,\n+                )\n+                foundanchor = True\n+        if not foundanchor:\n+            self._try_load_conftest(\n+                invocation_dir,\n+                importmode,\n+                rootpath,\n+                consider_namespace_packages=consider_namespace_packages,\n+            )\n+\n+    def _is_in_confcutdir(self, path: pathlib.Path) -> bool:\n+        \"\"\"Whether to consider the given path to load conftests from.\"\"\"\n+        if self._confcutdir is None:\n+            return True\n+        # The semantics here are literally:\n+        #   Do not load a conftest if it is found upwards from confcut dir.\n+        # But this is *not* the same as:\n+        #   Load only conftests from confcutdir or below.\n+        # At first glance they might seem the same thing, however we do support use cases where\n+        # we want to load conftests that are not found in confcutdir or below, but are found\n+        # in completely different directory hierarchies like packages installed\n+        # in out-of-source trees.\n+        # (see #9767 for a regression where the logic was inverted).\n+        return path not in self._confcutdir.parents\n+\n+    def _try_load_conftest(\n+        self,\n+        anchor: pathlib.Path,\n+        importmode: str | ImportMode,\n+        rootpath: pathlib.Path,\n+        *,\n+        consider_namespace_packages: bool,\n+    ) -> None:\n+        self._loadconftestmodules(\n+            anchor,\n+            importmode,\n+            rootpath,\n+            consider_namespace_packages=consider_namespace_packages,\n+        )\n+        # let's also consider test* subdirs\n+        if anchor.is_dir():\n+            for x in anchor.glob(\"test*\"):\n+                if x.is_dir():\n+                    self._loadconftestmodules(\n+                        x,\n+                        importmode,\n+                        rootpath,\n+                        consider_namespace_packages=consider_namespace_packages,\n+                    )\n+\n+    def _loadconftestmodules(\n+        self,\n+        path: pathlib.Path,\n+        importmode: str | ImportMode,\n+        rootpath: pathlib.Path,\n+        *,\n+        consider_namespace_packages: bool,\n+    ) -> None:\n+        if self._noconftest:\n+            return\n+\n+        directory = self._get_directory(path)\n+\n+        # Optimization: avoid repeated searches in the same directory.\n+        # Assumes always called with same importmode and rootpath.\n+        if directory in self._dirpath2confmods:\n+            return\n+\n+        clist = []\n+        for parent in reversed((directory, *directory.parents)):\n+            if self._is_in_confcutdir(parent):\n+                conftestpath = parent / \"conftest.py\"\n+                if conftestpath.is_file():\n+                    mod = self._importconftest(\n+                        conftestpath,\n+                        importmode,\n+                        rootpath,\n+                        consider_namespace_packages=consider_namespace_packages,\n+                    )\n+                    clist.append(mod)\n+        self._dirpath2confmods[directory] = clist\n+\n+    def _getconftestmodules(self, path: pathlib.Path) -> Sequence[types.ModuleType]:\n+        directory = self._get_directory(path)\n+        return self._dirpath2confmods.get(directory, ())\n+\n+    def _rget_with_confmod(\n+        self,\n+        name: str,\n+        path: pathlib.Path,\n+    ) -> tuple[types.ModuleType, Any]:\n+        modules = self._getconftestmodules(path)\n+        for mod in reversed(modules):\n+            try:\n+                return mod, getattr(mod, name)\n+            except AttributeError:\n+                continue\n+        raise KeyError(name)\n+\n+    def _importconftest(\n+        self,\n+        conftestpath: pathlib.Path,\n+        importmode: str | ImportMode,\n+        rootpath: pathlib.Path,\n+        *,\n+        consider_namespace_packages: bool,\n+    ) -> types.ModuleType:\n+        conftestpath_plugin_name = str(conftestpath)\n+        existing = self.get_plugin(conftestpath_plugin_name)\n+        if existing is not None:\n+            return cast(types.ModuleType, existing)\n+\n+        # conftest.py files there are not in a Python package all have module\n+        # name \"conftest\", and thus conflict with each other. Clear the existing\n+        # before loading the new one, otherwise the existing one will be\n+        # returned from the module cache.\n+        pkgpath = resolve_package_path(conftestpath)\n+        if pkgpath is None:\n+            try:\n+                del sys.modules[conftestpath.stem]\n+            except KeyError:\n+                pass\n+\n+        try:\n+            mod = import_path(\n+                conftestpath,\n+                mode=importmode,\n+                root=rootpath,\n+                consider_namespace_packages=consider_namespace_packages,\n+            )\n+        except Exception as e:\n+            assert e.__traceback__ is not None\n+            raise ConftestImportFailure(conftestpath, cause=e) from e\n+\n+        self._check_non_top_pytest_plugins(mod, conftestpath)\n+\n+        self._conftest_plugins.add(mod)\n+        dirpath = conftestpath.parent\n+        if dirpath in self._dirpath2confmods:\n+            for path, mods in self._dirpath2confmods.items():\n+                if dirpath in path.parents or path == dirpath:\n+                    if mod in mods:\n+                        raise AssertionError(\n+                            f\"While trying to load conftest path {conftestpath!s}, \"\n+                            f\"found that the module {mod} is already loaded with path {mod.__file__}. \"\n+                            \"This is not supposed to happen. Please report this issue to pytest.\"\n+                        )\n+                    mods.append(mod)\n+        self.trace(f\"loading conftestmodule {mod!r}\")\n+        self.consider_conftest(mod, registration_name=conftestpath_plugin_name)\n+        return mod\n+\n+    def _check_non_top_pytest_plugins(\n+        self,\n+        mod: types.ModuleType,\n+        conftestpath: pathlib.Path,\n+    ) -> None:\n+        if (\n+            hasattr(mod, \"pytest_plugins\")\n+            and self._configured\n+            and not self._using_pyargs\n+        ):\n+            msg = (\n+                \"Defining 'pytest_plugins' in a non-top-level conftest is no longer supported:\\n\"\n+                \"It affects the entire test suite instead of just below the conftest as expected.\\n\"\n+                \"  {}\\n\"\n+                \"Please move it to a top level conftest file at the rootdir:\\n\"\n+                \"  {}\\n\"\n+                \"For more information, visit:\\n\"\n+                \"  https://docs.pytest.org/en/stable/deprecations.html#pytest-plugins-in-non-top-level-conftest-files\"\n+            )\n+            fail(msg.format(conftestpath, self._confcutdir), pytrace=False)\n+\n+    #\n+    # API for bootstrapping plugin loading\n+    #\n+    #\n+\n+    def consider_preparse(\n+        self, args: Sequence[str], *, exclude_only: bool = False\n+    ) -> None:\n+        \"\"\":meta private:\"\"\"\n+        i = 0\n+        n = len(args)\n+        while i < n:\n+            opt = args[i]\n+            i += 1\n+            if isinstance(opt, str):\n+                if opt == \"-p\":\n+                    try:\n+                        parg = args[i]\n+                    except IndexError:\n+                        return\n+                    i += 1\n+                elif opt.startswith(\"-p\"):\n+                    parg = opt[2:]\n+                else:\n+                    continue\n+                parg = parg.strip()\n+                if exclude_only and not parg.startswith(\"no:\"):\n+                    continue\n+                self.consider_pluginarg(parg)\n+\n+    def consider_pluginarg(self, arg: str) -> None:\n+        \"\"\":meta private:\"\"\"\n+        if arg.startswith(\"no:\"):\n+            name = arg[3:]\n+            if name in essential_plugins:\n+                raise UsageError(f\"plugin {name} cannot be disabled\")\n+\n+            # PR #4304: remove stepwise if cacheprovider is blocked.\n+            if name == \"cacheprovider\":\n+                self.set_blocked(\"stepwise\")\n+                self.set_blocked(\"pytest_stepwise\")\n+\n+            self.set_blocked(name)\n+            if not name.startswith(\"pytest_\"):\n+                self.set_blocked(\"pytest_\" + name)\n+        else:\n+            name = arg\n+            # Unblock the plugin.\n+            self.unblock(name)\n+            if not name.startswith(\"pytest_\"):\n+                self.unblock(\"pytest_\" + name)\n+            self.import_plugin(arg, consider_entry_points=True)\n+\n+    def consider_conftest(\n+        self, conftestmodule: types.ModuleType, registration_name: str\n+    ) -> None:\n+        \"\"\":meta private:\"\"\"\n+        self.register(conftestmodule, name=registration_name)\n+\n+    def consider_env(self) -> None:\n+        \"\"\":meta private:\"\"\"\n+        self._import_plugin_specs(os.environ.get(\"PYTEST_PLUGINS\"))\n+\n+    def consider_module(self, mod: types.ModuleType) -> None:\n+        \"\"\":meta private:\"\"\"\n+        self._import_plugin_specs(getattr(mod, \"pytest_plugins\", []))\n+\n+    def _import_plugin_specs(\n+        self, spec: None | types.ModuleType | str | Sequence[str]\n+    ) -> None:\n+        plugins = _get_plugin_specs_as_list(spec)\n+        for import_spec in plugins:\n+            self.import_plugin(import_spec)\n+\n+    def import_plugin(self, modname: str, consider_entry_points: bool = False) -> None:\n+        \"\"\"Import a plugin with ``modname``.\n+\n+        If ``consider_entry_points`` is True, entry point names are also\n+        considered to find a plugin.\n+        \"\"\"\n+        # Most often modname refers to builtin modules, e.g. \"pytester\",\n+        # \"terminal\" or \"capture\".  Those plugins are registered under their\n+        # basename for historic purposes but must be imported with the\n+        # _pytest prefix.\n+        assert isinstance(modname, str), (\n+            f\"module name as text required, got {modname!r}\"\n+        )\n+        if self.is_blocked(modname) or self.get_plugin(modname) is not None:\n+            return\n+\n+        importspec = \"_pytest.\" + modname if modname in builtin_plugins else modname\n+        self.rewrite_hook.mark_rewrite(importspec)\n+\n+        if consider_entry_points:\n+            loaded = self.load_setuptools_entrypoints(\"pytest11\", name=modname)\n+            if loaded:\n+                return\n+\n+        try:\n+            __import__(importspec)\n+        except ImportError as e:\n+            raise ImportError(\n+                f'Error importing plugin \"{modname}\": {e.args[0]}'\n+            ).with_traceback(e.__traceback__) from e\n+\n+        except Skipped as e:\n+            self.skipped_plugins.append((modname, e.msg or \"\"))\n+        else:\n+            mod = sys.modules[importspec]\n+            self.register(mod, modname)\n+\n+\n+def _get_plugin_specs_as_list(\n+    specs: None | types.ModuleType | str | Sequence[str],\n+) -> list[str]:\n+    \"\"\"Parse a plugins specification into a list of plugin names.\"\"\"\n+    # None means empty.\n+    if specs is None:\n+        return []\n+    # Workaround for #3899 - a submodule which happens to be called \"pytest_plugins\".\n+    if isinstance(specs, types.ModuleType):\n+        return []\n+    # Comma-separated list.\n+    if isinstance(specs, str):\n+        return specs.split(\",\") if specs else []\n+    # Direct specification.\n+    if isinstance(specs, collections.abc.Sequence):\n+        return list(specs)\n+    raise UsageError(\n+        f\"Plugins may be specified as a sequence or a ','-separated string of plugin names. Got: {specs!r}\"\n+    )\n+\n+\n+class Notset:\n+    def __repr__(self):\n+        return \"<NOTSET>\"\n+\n+\n+notset = Notset()\n+\n+\n+def _iter_rewritable_modules(package_files: Iterable[str]) -> Iterator[str]:\n+    \"\"\"Given an iterable of file names in a source distribution, return the \"names\" that should\n+    be marked for assertion rewrite.\n+\n+    For example the package \"pytest_mock/__init__.py\" should be added as \"pytest_mock\" in\n+    the assertion rewrite mechanism.\n+\n+    This function has to deal with dist-info based distributions and egg based distributions\n+    (which are still very much in use for \"editable\" installs).\n+\n+    Here are the file names as seen in a dist-info based distribution:\n+\n+        pytest_mock/__init__.py\n+        pytest_mock/_version.py\n+        pytest_mock/plugin.py\n+        pytest_mock.egg-info/PKG-INFO\n+\n+    Here are the file names as seen in an egg based distribution:\n+\n+        src/pytest_mock/__init__.py\n+        src/pytest_mock/_version.py\n+        src/pytest_mock/plugin.py\n+        src/pytest_mock.egg-info/PKG-INFO\n+        LICENSE\n+        setup.py\n+\n+    We have to take in account those two distribution flavors in order to determine which\n+    names should be considered for assertion rewriting.\n+\n+    More information:\n+        https://github.com/pytest-dev/pytest-mock/issues/167\n+    \"\"\"\n+    package_files = list(package_files)\n+    seen_some = False\n+    for fn in package_files:\n+        is_simple_module = \"/\" not in fn and fn.endswith(\".py\")\n+        is_package = fn.count(\"/\") == 1 and fn.endswith(\"__init__.py\")\n+        if is_simple_module:\n+            module_name, _ = os.path.splitext(fn)\n+            # we ignore \"setup.py\" at the root of the distribution\n+            # as well as editable installation finder modules made by setuptools\n+            if module_name != \"setup\" and not module_name.startswith(\"__editable__\"):\n+                seen_some = True\n+                yield module_name\n+        elif is_package:\n+            package_name = os.path.dirname(fn)\n+            seen_some = True\n+            yield package_name\n+\n+    if not seen_some:\n+        # At this point we did not find any packages or modules suitable for assertion\n+        # rewriting, so we try again by stripping the first path component (to account for\n+        # \"src\" based source trees for example).\n+        # This approach lets us have the common case continue to be fast, as egg-distributions\n+        # are rarer.\n+        new_package_files = []\n+        for fn in package_files:\n+            parts = fn.split(\"/\")\n+            new_fn = \"/\".join(parts[1:])\n+            if new_fn:\n+                new_package_files.append(new_fn)\n+        if new_package_files:\n+            yield from _iter_rewritable_modules(new_package_files)\n+\n+\n+@final\n+class Config:\n+    \"\"\"Access to configuration values, pluginmanager and plugin hooks.\n+\n+    :param PytestPluginManager pluginmanager:\n+        A pytest PluginManager.\n+\n+    :param InvocationParams invocation_params:\n+        Object containing parameters regarding the :func:`pytest.main`\n+        invocation.\n+    \"\"\"\n+\n+    @final\n+    @dataclasses.dataclass(frozen=True)\n+    class InvocationParams:\n+        \"\"\"Holds parameters passed during :func:`pytest.main`.\n+\n+        The object attributes are read-only.\n+\n+        .. versionadded:: 5.1\n+\n+        .. note::\n+\n+            Note that the environment variable ``PYTEST_ADDOPTS`` and the ``addopts``\n+            ini option are handled by pytest, not being included in the ``args`` attribute.\n+\n+            Plugins accessing ``InvocationParams`` must be aware of that.\n+        \"\"\"\n+\n+        args: tuple[str, ...]\n+        \"\"\"The command-line arguments as passed to :func:`pytest.main`.\"\"\"\n+        plugins: Sequence[str | _PluggyPlugin] | None\n+        \"\"\"Extra plugins, might be `None`.\"\"\"\n+        dir: pathlib.Path\n+        \"\"\"The directory from which :func:`pytest.main` was invoked. :type: pathlib.Path\"\"\"\n+\n+        def __init__(\n+            self,\n+            *,\n+            args: Iterable[str],\n+            plugins: Sequence[str | _PluggyPlugin] | None,\n+            dir: pathlib.Path,\n+        ) -> None:\n+            object.__setattr__(self, \"args\", tuple(args))\n+            object.__setattr__(self, \"plugins\", plugins)\n+            object.__setattr__(self, \"dir\", dir)\n+\n+    class ArgsSource(enum.Enum):\n+        \"\"\"Indicates the source of the test arguments.\n+\n+        .. versionadded:: 7.2\n+        \"\"\"\n+\n+        #: Command line arguments.\n+        ARGS = enum.auto()\n+        #: Invocation directory.\n+        INVOCATION_DIR = enum.auto()\n+        INCOVATION_DIR = INVOCATION_DIR  # backwards compatibility alias\n+        #: 'testpaths' configuration value.\n+        TESTPATHS = enum.auto()\n+\n+    # Set by cacheprovider plugin.\n+    cache: Cache\n+\n+    def __init__(\n+        self,\n+        pluginmanager: PytestPluginManager,\n+        *,\n+        invocation_params: InvocationParams | None = None,\n+    ) -> None:\n+        from .argparsing import FILE_OR_DIR\n+        from .argparsing import Parser\n+\n+        if invocation_params is None:\n+            invocation_params = self.InvocationParams(\n+                args=(), plugins=None, dir=pathlib.Path.cwd()\n+            )\n+\n+        self.option = argparse.Namespace()\n+        \"\"\"Access to command line option as attributes.\n+\n+        :type: argparse.Namespace\n+        \"\"\"\n+\n+        self.invocation_params = invocation_params\n+        \"\"\"The parameters with which pytest was invoked.\n+\n+        :type: InvocationParams\n+        \"\"\"\n+\n+        _a = FILE_OR_DIR\n+        self._parser = Parser(\n+            usage=f\"%(prog)s [options] [{_a}] [{_a}] [...]\",\n+            processopt=self._processopt,\n+            _ispytest=True,\n+        )\n+        self.pluginmanager = pluginmanager\n+        \"\"\"The plugin manager handles plugin registration and hook invocation.\n+\n+        :type: PytestPluginManager\n+        \"\"\"\n+\n+        self.stash = Stash()\n+        \"\"\"A place where plugins can store information on the config for their\n+        own use.\n+\n+        :type: Stash\n+        \"\"\"\n+        # Deprecated alias. Was never public. Can be removed in a few releases.\n+        self._store = self.stash\n+\n+        self.trace = self.pluginmanager.trace.root.get(\"config\")\n+        self.hook: pluggy.HookRelay = PathAwareHookProxy(self.pluginmanager.hook)  # type: ignore[assignment]\n+        self._inicache: dict[str, Any] = {}\n+        self._override_ini: Sequence[str] = ()\n+        self._opt2dest: dict[str, str] = {}\n+        self._cleanup_stack = contextlib.ExitStack()\n+        self.pluginmanager.register(self, \"pytestconfig\")\n+        self._configured = False\n+        self.hook.pytest_addoption.call_historic(\n+            kwargs=dict(parser=self._parser, pluginmanager=self.pluginmanager)\n+        )\n+        self.args_source = Config.ArgsSource.ARGS\n+        self.args: list[str] = []\n+\n+    @property\n+    def rootpath(self) -> pathlib.Path:\n+        \"\"\"The path to the :ref:`rootdir <rootdir>`.\n+\n+        :type: pathlib.Path\n+\n+        .. versionadded:: 6.1\n+        \"\"\"\n+        return self._rootpath\n+\n+    @property\n+    def inipath(self) -> pathlib.Path | None:\n+        \"\"\"The path to the :ref:`configfile <configfiles>`.\n+\n+        .. versionadded:: 6.1\n+        \"\"\"\n+        return self._inipath\n+\n+    def add_cleanup(self, func: Callable[[], None]) -> None:\n+        \"\"\"Add a function to be called when the config object gets out of\n+        use (usually coinciding with pytest_unconfigure).\n+        \"\"\"\n+        self._cleanup_stack.callback(func)\n+\n+    def _do_configure(self) -> None:\n+        assert not self._configured\n+        self._configured = True\n+        self.hook.pytest_configure.call_historic(kwargs=dict(config=self))\n+\n+    def _ensure_unconfigure(self) -> None:\n+        try:\n+            if self._configured:\n+                self._configured = False\n+                try:\n+                    self.hook.pytest_unconfigure(config=self)\n+                finally:\n+                    self.hook.pytest_configure._call_history = []\n+        finally:\n+            try:\n+                self._cleanup_stack.close()\n+            finally:\n+                self._cleanup_stack = contextlib.ExitStack()\n+\n+    def get_terminal_writer(self) -> TerminalWriter:\n+        terminalreporter: TerminalReporter | None = self.pluginmanager.get_plugin(\n+            \"terminalreporter\"\n+        )\n+        assert terminalreporter is not None\n+        return terminalreporter._tw\n+\n+    def pytest_cmdline_parse(\n+        self, pluginmanager: PytestPluginManager, args: list[str]\n+    ) -> Config:\n+        try:\n+            self.parse(args)\n+        except UsageError:\n+            # Handle --version and --help here in a minimal fashion.\n+            # This gets done via helpconfig normally, but its\n+            # pytest_cmdline_main is not called in case of errors.\n+            if getattr(self.option, \"version\", False) or \"--version\" in args:\n+                from _pytest.helpconfig import showversion\n+\n+                showversion(self)\n+            elif (\n+                getattr(self.option, \"help\", False) or \"--help\" in args or \"-h\" in args\n+            ):\n+                self._parser._getparser().print_help()\n+                sys.stdout.write(\n+                    \"\\nNOTE: displaying only minimal help due to UsageError.\\n\\n\"\n+                )\n+\n+            raise\n+\n+        return self\n+\n+    def notify_exception(\n+        self,\n+        excinfo: ExceptionInfo[BaseException],\n+        option: argparse.Namespace | None = None,\n+    ) -> None:\n+        if option and getattr(option, \"fulltrace\", False):\n+            style: TracebackStyle = \"long\"\n+        else:\n+            style = \"native\"\n+        excrepr = excinfo.getrepr(\n+            funcargs=True, showlocals=getattr(option, \"showlocals\", False), style=style\n+        )\n+        res = self.hook.pytest_internalerror(excrepr=excrepr, excinfo=excinfo)\n+        if not any(res):\n+            for line in str(excrepr).split(\"\\n\"):\n+                sys.stderr.write(f\"INTERNALERROR> {line}\\n\")\n+                sys.stderr.flush()\n+\n+    def cwd_relative_nodeid(self, nodeid: str) -> str:\n+        # nodeid's are relative to the rootpath, compute relative to cwd.\n+        if self.invocation_params.dir != self.rootpath:\n+            base_path_part, *nodeid_part = nodeid.split(\"::\")\n+            # Only process path part\n+            fullpath = self.rootpath / base_path_part\n+            relative_path = bestrelpath(self.invocation_params.dir, fullpath)\n+\n+            nodeid = \"::\".join([relative_path, *nodeid_part])\n+        return nodeid\n+\n+    @classmethod\n+    def fromdictargs(cls, option_dict, args) -> Config:\n+        \"\"\"Constructor usable for subprocesses.\"\"\"\n+        config = get_config(args)\n+        config.option.__dict__.update(option_dict)\n+        config.parse(args, addopts=False)\n+        for x in config.option.plugins:\n+            config.pluginmanager.consider_pluginarg(x)\n+        return config\n+\n+    def _processopt(self, opt: Argument) -> None:\n+        for name in opt._short_opts + opt._long_opts:\n+            self._opt2dest[name] = opt.dest\n+\n+        if hasattr(opt, \"default\"):\n+            if not hasattr(self.option, opt.dest):\n+                setattr(self.option, opt.dest, opt.default)\n+\n+    @hookimpl(trylast=True)\n+    def pytest_load_initial_conftests(self, early_config: Config) -> None:\n+        # We haven't fully parsed the command line arguments yet, so\n+        # early_config.args it not set yet. But we need it for\n+        # discovering the initial conftests. So \"pre-run\" the logic here.\n+        # It will be done for real in `parse()`.\n+        args, args_source = early_config._decide_args(\n+            args=early_config.known_args_namespace.file_or_dir,\n+            pyargs=early_config.known_args_namespace.pyargs,\n+            testpaths=early_config.getini(\"testpaths\"),\n+            invocation_dir=early_config.invocation_params.dir,\n+            rootpath=early_config.rootpath,\n+            warn=False,\n+        )\n+        self.pluginmanager._set_initial_conftests(\n+            args=args,\n+            pyargs=early_config.known_args_namespace.pyargs,\n+            noconftest=early_config.known_args_namespace.noconftest,\n+            rootpath=early_config.rootpath,\n+            confcutdir=early_config.known_args_namespace.confcutdir,\n+            invocation_dir=early_config.invocation_params.dir,\n+            importmode=early_config.known_args_namespace.importmode,\n+            consider_namespace_packages=early_config.getini(\n+                \"consider_namespace_packages\"\n+            ),\n+        )\n+\n+    def _initini(self, args: Sequence[str]) -> None:\n+        ns, unknown_args = self._parser.parse_known_and_unknown_args(\n+            args, namespace=copy.copy(self.option)\n+        )\n+        rootpath, inipath, inicfg = determine_setup(\n+            inifile=ns.inifilename,\n+            args=ns.file_or_dir + unknown_args,\n+            rootdir_cmd_arg=ns.rootdir or None,\n+            invocation_dir=self.invocation_params.dir,\n+        )\n+        self._rootpath = rootpath\n+        self._inipath = inipath\n+        self.inicfg = inicfg\n+        self._parser.extra_info[\"rootdir\"] = str(self.rootpath)\n+        self._parser.extra_info[\"inifile\"] = str(self.inipath)\n+        self._parser.addini(\"addopts\", \"Extra command line options\", \"args\")\n+        self._parser.addini(\"minversion\", \"Minimally required pytest version\")\n+        self._parser.addini(\n+            \"pythonpath\", type=\"paths\", help=\"Add paths to sys.path\", default=[]\n+        )\n+        self._parser.addini(\n+            \"required_plugins\",\n+            \"Plugins that must be present for pytest to run\",\n+            type=\"args\",\n+            default=[],\n+        )\n+        self._override_ini = ns.override_ini or ()\n+\n+    def _consider_importhook(self, args: Sequence[str]) -> None:\n+        \"\"\"Install the PEP 302 import hook if using assertion rewriting.\n+\n+        Needs to parse the --assert=<mode> option from the commandline\n+        and find all the installed plugins to mark them for rewriting\n+        by the importhook.\n+        \"\"\"\n+        ns, unknown_args = self._parser.parse_known_and_unknown_args(args)\n+        mode = getattr(ns, \"assertmode\", \"plain\")\n+\n+        disable_autoload = getattr(ns, \"disable_plugin_autoload\", False) or bool(\n+            os.environ.get(\"PYTEST_DISABLE_PLUGIN_AUTOLOAD\")\n+        )\n+        if mode == \"rewrite\":\n+            import _pytest.assertion\n+\n+            try:\n+                hook = _pytest.assertion.install_importhook(self)\n+            except SystemError:\n+                mode = \"plain\"\n+            else:\n+                self._mark_plugins_for_rewrite(hook, disable_autoload)\n+        self._warn_about_missing_assertion(mode)\n+\n+    def _mark_plugins_for_rewrite(\n+        self, hook: AssertionRewritingHook, disable_autoload: bool\n+    ) -> None:\n+        \"\"\"Given an importhook, mark for rewrite any top-level\n+        modules or packages in the distribution package for\n+        all pytest plugins.\"\"\"\n+        self.pluginmanager.rewrite_hook = hook\n+\n+        if disable_autoload:\n+            # We don't autoload from distribution package entry points,\n+            # no need to continue.\n+            return\n+\n+        package_files = (\n+            str(file)\n+            for dist in importlib.metadata.distributions()\n+            if any(ep.group == \"pytest11\" for ep in dist.entry_points)\n+            for file in dist.files or []\n+        )\n+\n+        for name in _iter_rewritable_modules(package_files):\n+            hook.mark_rewrite(name)\n+\n+    def _configure_python_path(self) -> None:\n+        # `pythonpath = a b` will set `sys.path` to `[a, b, x, y, z, ...]`\n+        for path in reversed(self.getini(\"pythonpath\")):\n+            sys.path.insert(0, str(path))\n+        self.add_cleanup(self._unconfigure_python_path)\n+\n+    def _unconfigure_python_path(self) -> None:\n+        for path in self.getini(\"pythonpath\"):\n+            path_str = str(path)\n+            if path_str in sys.path:\n+                sys.path.remove(path_str)\n+\n+    def _validate_args(self, args: list[str], via: str) -> list[str]:\n+        \"\"\"Validate known args.\"\"\"\n+        self._parser._config_source_hint = via  # type: ignore\n+        try:\n+            self._parser.parse_known_and_unknown_args(\n+                args, namespace=copy.copy(self.option)\n+            )\n+        finally:\n+            del self._parser._config_source_hint  # type: ignore\n+\n+        return args\n+\n+    def _decide_args(\n+        self,\n+        *,\n+        args: list[str],\n+        pyargs: bool,\n+        testpaths: list[str],\n+        invocation_dir: pathlib.Path,\n+        rootpath: pathlib.Path,\n+        warn: bool,\n+    ) -> tuple[list[str], ArgsSource]:\n+        \"\"\"Decide the args (initial paths/nodeids) to use given the relevant inputs.\n+\n+        :param warn: Whether can issue warnings.\n+\n+        :returns: The args and the args source. Guaranteed to be non-empty.\n+        \"\"\"\n+        if args:\n+            source = Config.ArgsSource.ARGS\n+            result = args\n+        else:\n+            if invocation_dir == rootpath:\n+                source = Config.ArgsSource.TESTPATHS\n+                if pyargs:\n+                    result = testpaths\n+                else:\n+                    result = []\n+                    for path in testpaths:\n+                        result.extend(sorted(glob.iglob(path, recursive=True)))\n+                    if testpaths and not result:\n+                        if warn:\n+                            warning_text = (\n+                                \"No files were found in testpaths; \"\n+                                \"consider removing or adjusting your testpaths configuration. \"\n+                                \"Searching recursively from the current directory instead.\"\n+                            )\n+                            self.issue_config_time_warning(\n+                                PytestConfigWarning(warning_text), stacklevel=3\n+                            )\n+            else:\n+                result = []\n+            if not result:\n+                source = Config.ArgsSource.INVOCATION_DIR\n+                result = [str(invocation_dir)]\n+        return result, source\n+\n+    def _preparse(self, args: list[str], addopts: bool = True) -> None:\n+        if addopts:\n+            env_addopts = os.environ.get(\"PYTEST_ADDOPTS\", \"\")\n+            if len(env_addopts):\n+                args[:] = (\n+                    self._validate_args(shlex.split(env_addopts), \"via PYTEST_ADDOPTS\")\n+                    + args\n+                )\n+        self._initini(args)\n+        if addopts:\n+            args[:] = (\n+                self._validate_args(self.getini(\"addopts\"), \"via addopts config\") + args\n+            )\n+\n+        self.known_args_namespace = self._parser.parse_known_args(\n+            args, namespace=copy.copy(self.option)\n+        )\n+        self._checkversion()\n+        self._consider_importhook(args)\n+        self._configure_python_path()\n+        self.pluginmanager.consider_preparse(args, exclude_only=False)\n+        if (\n+            not os.environ.get(\"PYTEST_DISABLE_PLUGIN_AUTOLOAD\")\n+            and not self.known_args_namespace.disable_plugin_autoload\n+        ):\n+            # Autoloading from distribution package entry point has\n+            # not been disabled.\n+            self.pluginmanager.load_setuptools_entrypoints(\"pytest11\")\n+        # Otherwise only plugins explicitly specified in PYTEST_PLUGINS\n+        # are going to be loaded.\n+        self.pluginmanager.consider_env()\n+\n+        self.known_args_namespace = self._parser.parse_known_args(\n+            args, namespace=copy.copy(self.known_args_namespace)\n+        )\n+\n+        self._validate_plugins()\n+        self._warn_about_skipped_plugins()\n+\n+        if self.known_args_namespace.confcutdir is None:\n+            if self.inipath is not None:\n+                confcutdir = str(self.inipath.parent)\n+            else:\n+                confcutdir = str(self.rootpath)\n+            self.known_args_namespace.confcutdir = confcutdir\n+        try:\n+            self.hook.pytest_load_initial_conftests(\n+                early_config=self, args=args, parser=self._parser\n+            )\n+        except ConftestImportFailure as e:\n+            if self.known_args_namespace.help or self.known_args_namespace.version:\n+                # we don't want to prevent --help/--version to work\n+                # so just let it pass and print a warning at the end\n+                self.issue_config_time_warning(\n+                    PytestConfigWarning(f\"could not load initial conftests: {e.path}\"),\n+                    stacklevel=2,\n+                )\n+            else:\n+                raise\n+\n+    @hookimpl(wrapper=True)\n+    def pytest_collection(self) -> Generator[None, object, object]:\n+        # Validate invalid ini keys after collection is done so we take in account\n+        # options added by late-loading conftest files.\n+        try:\n+            return (yield)\n+        finally:\n+            self._validate_config_options()\n+\n+    def _checkversion(self) -> None:\n+        import pytest\n+\n+        minver = self.inicfg.get(\"minversion\", None)\n+        if minver:\n+            # Imported lazily to improve start-up time.\n+            from packaging.version import Version\n+\n+            if not isinstance(minver, str):\n+                raise pytest.UsageError(\n+                    f\"{self.inipath}: 'minversion' must be a single value\"\n+                )\n+\n+            if Version(minver) > Version(pytest.__version__):\n+                raise pytest.UsageError(\n+                    f\"{self.inipath}: 'minversion' requires pytest-{minver}, actual pytest-{pytest.__version__}'\"\n+                )\n+\n+    def _validate_config_options(self) -> None:\n+        for key in sorted(self._get_unknown_ini_keys()):\n+            self._warn_or_fail_if_strict(f\"Unknown config option: {key}\\n\")\n+\n+    def _validate_plugins(self) -> None:\n+        required_plugins = sorted(self.getini(\"required_plugins\"))\n+        if not required_plugins:\n+            return\n+\n+        # Imported lazily to improve start-up time.\n+        from packaging.requirements import InvalidRequirement\n+        from packaging.requirements import Requirement\n+        from packaging.version import Version\n+\n+        plugin_info = self.pluginmanager.list_plugin_distinfo()\n+        plugin_dist_info = {dist.project_name: dist.version for _, dist in plugin_info}\n+\n+        missing_plugins = []\n+        for required_plugin in required_plugins:\n+            try:\n+                req = Requirement(required_plugin)\n+            except InvalidRequirement:\n+                missing_plugins.append(required_plugin)\n+                continue\n+\n+            if req.name not in plugin_dist_info:\n+                missing_plugins.append(required_plugin)\n+            elif not req.specifier.contains(\n+                Version(plugin_dist_info[req.name]), prereleases=True\n+            ):\n+                missing_plugins.append(required_plugin)\n+\n+        if missing_plugins:\n+            raise UsageError(\n+                \"Missing required plugins: {}\".format(\", \".join(missing_plugins)),\n+            )\n+\n+    def _warn_or_fail_if_strict(self, message: str) -> None:\n+        if self.known_args_namespace.strict_config:\n+            raise UsageError(message)\n+\n+        self.issue_config_time_warning(PytestConfigWarning(message), stacklevel=3)\n+\n+    def _get_unknown_ini_keys(self) -> list[str]:\n+        parser_inicfg = self._parser._inidict\n+        return [name for name in self.inicfg if name not in parser_inicfg]\n+\n+    def parse(self, args: list[str], addopts: bool = True) -> None:\n+        # Parse given cmdline arguments into this config object.\n+        assert self.args == [], (\n+            \"can only parse cmdline args at most once per Config object\"\n+        )\n+        self.hook.pytest_addhooks.call_historic(\n+            kwargs=dict(pluginmanager=self.pluginmanager)\n+        )\n+        self._preparse(args, addopts=addopts)\n+        self._parser.after_preparse = True  # type: ignore\n+        try:\n+            args = self._parser.parse_setoption(\n+                args, self.option, namespace=self.option\n+            )\n+            self.args, self.args_source = self._decide_args(\n+                args=args,\n+                pyargs=self.known_args_namespace.pyargs,\n+                testpaths=self.getini(\"testpaths\"),\n+                invocation_dir=self.invocation_params.dir,\n+                rootpath=self.rootpath,\n+                warn=True,\n+            )\n+        except PrintHelp:\n+            pass\n+\n+    def issue_config_time_warning(self, warning: Warning, stacklevel: int) -> None:\n+        \"\"\"Issue and handle a warning during the \"configure\" stage.\n+\n+        During ``pytest_configure`` we can't capture warnings using the ``catch_warnings_for_item``\n+        function because it is not possible to have hook wrappers around ``pytest_configure``.\n+\n+        This function is mainly intended for plugins that need to issue warnings during\n+        ``pytest_configure`` (or similar stages).\n+\n+        :param warning: The warning instance.\n+        :param stacklevel: stacklevel forwarded to warnings.warn.\n+        \"\"\"\n+        if self.pluginmanager.is_blocked(\"warnings\"):\n+            return\n+\n+        cmdline_filters = self.known_args_namespace.pythonwarnings or []\n+        config_filters = self.getini(\"filterwarnings\")\n+\n+        with warnings.catch_warnings(record=True) as records:\n+            warnings.simplefilter(\"always\", type(warning))\n+            apply_warning_filters(config_filters, cmdline_filters)\n+            warnings.warn(warning, stacklevel=stacklevel)\n+\n+        if records:\n+            frame = sys._getframe(stacklevel - 1)\n+            location = frame.f_code.co_filename, frame.f_lineno, frame.f_code.co_name\n+            self.hook.pytest_warning_recorded.call_historic(\n+                kwargs=dict(\n+                    warning_message=records[0],\n+                    when=\"config\",\n+                    nodeid=\"\",\n+                    location=location,\n+                )\n+            )\n+\n+    def addinivalue_line(self, name: str, line: str) -> None:\n+        \"\"\"Add a line to an ini-file option. The option must have been\n+        declared but might not yet be set in which case the line becomes\n+        the first line in its value.\"\"\"\n+        x = self.getini(name)\n+        assert isinstance(x, list)\n+        x.append(line)  # modifies the cached list inline\n+\n+    def getini(self, name: str) -> Any:\n+        \"\"\"Return configuration value from an :ref:`ini file <configfiles>`.\n+\n+        If a configuration value is not defined in an\n+        :ref:`ini file <configfiles>`, then the ``default`` value provided while\n+        registering the configuration through\n+        :func:`parser.addini <pytest.Parser.addini>` will be returned.\n+        Please note that you can even provide ``None`` as a valid\n+        default value.\n+\n+        If ``default`` is not provided while registering using\n+        :func:`parser.addini <pytest.Parser.addini>`, then a default value\n+        based on the ``type`` parameter passed to\n+        :func:`parser.addini <pytest.Parser.addini>` will be returned.\n+        The default values based on ``type`` are:\n+        ``paths``, ``pathlist``, ``args`` and ``linelist`` : empty list ``[]``\n+        ``bool`` : ``False``\n+        ``string`` : empty string ``\"\"``\n+        ``int`` : ``0``\n+        ``float`` : ``0.0``\n+\n+        If neither the ``default`` nor the ``type`` parameter is passed\n+        while registering the configuration through\n+        :func:`parser.addini <pytest.Parser.addini>`, then the configuration\n+        is treated as a string and a default empty string '' is returned.\n+\n+        If the specified name hasn't been registered through a prior\n+        :func:`parser.addini <pytest.Parser.addini>` call (usually from a\n+        plugin), a ValueError is raised.\n+        \"\"\"\n+        try:\n+            return self._inicache[name]\n+        except KeyError:\n+            self._inicache[name] = val = self._getini(name)\n+            return val\n+\n+    # Meant for easy monkeypatching by legacypath plugin.\n+    # Can be inlined back (with no cover removed) once legacypath is gone.\n+    def _getini_unknown_type(self, name: str, type: str, value: object):\n+        msg = (\n+            f\"Option {name} has unknown configuration type {type} with value {value!r}\"\n+        )\n+        raise ValueError(msg)  # pragma: no cover\n+\n+    def _getini(self, name: str):\n+        try:\n+            description, type, default = self._parser._inidict[name]\n+        except KeyError as e:\n+            raise ValueError(f\"unknown configuration value: {name!r}\") from e\n+        override_value = self._get_override_ini_value(name)\n+        if override_value is None:\n+            try:\n+                value = self.inicfg[name]\n+            except KeyError:\n+                return default\n+        else:\n+            value = override_value\n+        # Coerce the values based on types.\n+        #\n+        # Note: some coercions are only required if we are reading from .ini files, because\n+        # the file format doesn't contain type information, but when reading from toml we will\n+        # get either str or list of str values (see _parse_ini_config_from_pyproject_toml).\n+        # For example:\n+        #\n+        #   ini:\n+        #     a_line_list = \"tests acceptance\"\n+        #   in this case, we need to split the string to obtain a list of strings.\n+        #\n+        #   toml:\n+        #     a_line_list = [\"tests\", \"acceptance\"]\n+        #   in this case, we already have a list ready to use.\n+        #\n+        if type == \"paths\":\n+            dp = (\n+                self.inipath.parent\n+                if self.inipath is not None\n+                else self.invocation_params.dir\n+            )\n+            input_values = shlex.split(value) if isinstance(value, str) else value\n+            return [dp / x for x in input_values]\n+        elif type == \"args\":\n+            return shlex.split(value) if isinstance(value, str) else value\n+        elif type == \"linelist\":\n+            if isinstance(value, str):\n+                return [t for t in map(lambda x: x.strip(), value.split(\"\\n\")) if t]\n+            else:\n+                return value\n+        elif type == \"bool\":\n+            return _strtobool(str(value).strip())\n+        elif type == \"string\":\n+            return value\n+        elif type == \"int\":\n+            if not isinstance(value, str):\n+                raise TypeError(\n+                    f\"Expected an int string for option {name} of type integer, but got: {value!r}\"\n+                ) from None\n+            return int(value)\n+        elif type == \"float\":\n+            if not isinstance(value, str):\n+                raise TypeError(\n+                    f\"Expected a float string for option {name} of type float, but got: {value!r}\"\n+                ) from None\n+            return float(value)\n+        elif type is None:\n+            return value\n+        else:\n+            return self._getini_unknown_type(name, type, value)\n+\n+    def _getconftest_pathlist(\n+        self, name: str, path: pathlib.Path\n+    ) -> list[pathlib.Path] | None:\n+        try:\n+            mod, relroots = self.pluginmanager._rget_with_confmod(name, path)\n+        except KeyError:\n+            return None\n+        assert mod.__file__ is not None\n+        modpath = pathlib.Path(mod.__file__).parent\n+        values: list[pathlib.Path] = []\n+        for relroot in relroots:\n+            if isinstance(relroot, os.PathLike):\n+                relroot = pathlib.Path(relroot)\n+            else:\n+                relroot = relroot.replace(\"/\", os.sep)\n+                relroot = absolutepath(modpath / relroot)\n+            values.append(relroot)\n+        return values\n+\n+    def _get_override_ini_value(self, name: str) -> str | None:\n+        value = None\n+        # override_ini is a list of \"ini=value\" options.\n+        # Always use the last item if multiple values are set for same ini-name,\n+        # e.g. -o foo=bar1 -o foo=bar2 will set foo to bar2.\n+        for ini_config in self._override_ini:\n+            try:\n+                key, user_ini_value = ini_config.split(\"=\", 1)\n+            except ValueError as e:\n+                raise UsageError(\n+                    f\"-o/--override-ini expects option=value style (got: {ini_config!r}).\"\n+                ) from e\n+            else:\n+                if key == name:\n+                    value = user_ini_value\n+        return value\n+\n+    def getoption(self, name: str, default: Any = notset, skip: bool = False):\n+        \"\"\"Return command line option value.\n+\n+        :param name: Name of the option. You may also specify\n+            the literal ``--OPT`` option instead of the \"dest\" option name.\n+        :param default: Fallback value if no option of that name is **declared** via :hook:`pytest_addoption`.\n+            Note this parameter will be ignored when the option is **declared** even if the option's value is ``None``.\n+        :param skip: If ``True``, raise :func:`pytest.skip` if option is undeclared or has a ``None`` value.\n+            Note that even if ``True``, if a default was specified it will be returned instead of a skip.\n+        \"\"\"\n+        name = self._opt2dest.get(name, name)\n+        try:\n+            val = getattr(self.option, name)\n+            if val is None and skip:\n+                raise AttributeError(name)\n+            return val\n+        except AttributeError as e:\n+            if default is not notset:\n+                return default\n+            if skip:\n+                import pytest\n+\n+                pytest.skip(f\"no {name!r} option found\")\n+            raise ValueError(f\"no option named {name!r}\") from e\n+\n+    def getvalue(self, name: str, path=None):\n+        \"\"\"Deprecated, use getoption() instead.\"\"\"\n+        return self.getoption(name)\n+\n+    def getvalueorskip(self, name: str, path=None):\n+        \"\"\"Deprecated, use getoption(skip=True) instead.\"\"\"\n+        return self.getoption(name, skip=True)\n+\n+    #: Verbosity type for failed assertions (see :confval:`verbosity_assertions`).\n+    VERBOSITY_ASSERTIONS: Final = \"assertions\"\n+    #: Verbosity type for test case execution (see :confval:`verbosity_test_cases`).\n+    VERBOSITY_TEST_CASES: Final = \"test_cases\"\n+    _VERBOSITY_INI_DEFAULT: Final = \"auto\"\n+\n+    def get_verbosity(self, verbosity_type: str | None = None) -> int:\n+        r\"\"\"Retrieve the verbosity level for a fine-grained verbosity type.\n+\n+        :param verbosity_type: Verbosity type to get level for. If a level is\n+            configured for the given type, that value will be returned. If the\n+            given type is not a known verbosity type, the global verbosity\n+            level will be returned. If the given type is None (default), the\n+            global verbosity level will be returned.\n+\n+        To configure a level for a fine-grained verbosity type, the\n+        configuration file should have a setting for the configuration name\n+        and a numeric value for the verbosity level. A special value of \"auto\"\n+        can be used to explicitly use the global verbosity level.\n+\n+        Example:\n+\n+        .. code-block:: ini\n+\n+            # content of pytest.ini\n+            [pytest]\n+            verbosity_assertions = 2\n+\n+        .. code-block:: console\n+\n+            pytest -v\n+\n+        .. code-block:: python\n+\n+            print(config.get_verbosity())  # 1\n+            print(config.get_verbosity(Config.VERBOSITY_ASSERTIONS))  # 2\n+        \"\"\"\n+        global_level = self.getoption(\"verbose\", default=0)\n+        assert isinstance(global_level, int)\n+        if verbosity_type is None:\n+            return global_level\n+\n+        ini_name = Config._verbosity_ini_name(verbosity_type)\n+        if ini_name not in self._parser._inidict:\n+            return global_level\n+\n+        level = self.getini(ini_name)\n+        if level == Config._VERBOSITY_INI_DEFAULT:\n+            return global_level\n+\n+        return int(level)\n+\n+    @staticmethod\n+    def _verbosity_ini_name(verbosity_type: str) -> str:\n+        return f\"verbosity_{verbosity_type}\"\n+\n+    @staticmethod\n+    def _add_verbosity_ini(parser: Parser, verbosity_type: str, help: str) -> None:\n+        \"\"\"Add a output verbosity configuration option for the given output type.\n+\n+        :param parser: Parser for command line arguments and ini-file values.\n+        :param verbosity_type: Fine-grained verbosity category.\n+        :param help: Description of the output this type controls.\n+\n+        The value should be retrieved via a call to\n+        :py:func:`config.get_verbosity(type) <pytest.Config.get_verbosity>`.\n+        \"\"\"\n+        parser.addini(\n+            Config._verbosity_ini_name(verbosity_type),\n+            help=help,\n+            type=\"string\",\n+            default=Config._VERBOSITY_INI_DEFAULT,\n+        )\n+\n+    def _warn_about_missing_assertion(self, mode: str) -> None:\n+        if not _assertion_supported():\n+            if mode == \"plain\":\n+                warning_text = (\n+                    \"ASSERTIONS ARE NOT EXECUTED\"\n+                    \" and FAILING TESTS WILL PASS.  Are you\"\n+                    \" using python -O?\"\n+                )\n+            else:\n+                warning_text = (\n+                    \"assertions not in test modules or\"\n+                    \" plugins will be ignored\"\n+                    \" because assert statements are not executed \"\n+                    \"by the underlying Python interpreter \"\n+                    \"(are you using python -O?)\\n\"\n+                )\n+            self.issue_config_time_warning(\n+                PytestConfigWarning(warning_text),\n+                stacklevel=3,\n+            )\n+\n+    def _warn_about_skipped_plugins(self) -> None:\n+        for module_name, msg in self.pluginmanager.skipped_plugins:\n+            self.issue_config_time_warning(\n+                PytestConfigWarning(f\"skipped plugin {module_name!r}: {msg}\"),\n+                stacklevel=2,\n+            )\n+\n+\n+def _assertion_supported() -> bool:\n+    try:\n+        assert False\n+    except AssertionError:\n+        return True\n+    else:\n+        return False  # type: ignore[unreachable]\n+\n+\n+def create_terminal_writer(\n+    config: Config, file: TextIO | None = None\n+) -> TerminalWriter:\n+    \"\"\"Create a TerminalWriter instance configured according to the options\n+    in the config object.\n+\n+    Every code which requires a TerminalWriter object and has access to a\n+    config object should use this function.\n+    \"\"\"\n+    tw = TerminalWriter(file=file)\n+\n+    if config.option.color == \"yes\":\n+        tw.hasmarkup = True\n+    elif config.option.color == \"no\":\n+        tw.hasmarkup = False\n+\n+    if config.option.code_highlight == \"yes\":\n+        tw.code_highlight = True\n+    elif config.option.code_highlight == \"no\":\n+        tw.code_highlight = False\n+\n+    return tw\n+\n+\n+def _strtobool(val: str) -> bool:\n+    \"\"\"Convert a string representation of truth to True or False.\n+\n+    True values are 'y', 'yes', 't', 'true', 'on', and '1'; false values\n+    are 'n', 'no', 'f', 'false', 'off', and '0'.  Raises ValueError if\n+    'val' is anything else.\n+\n+    .. note:: Copied from distutils.util.\n+    \"\"\"\n+    val = val.lower()\n+    if val in (\"y\", \"yes\", \"t\", \"true\", \"on\", \"1\"):\n+        return True\n+    elif val in (\"n\", \"no\", \"f\", \"false\", \"off\", \"0\"):\n+        return False\n+    else:\n+        raise ValueError(f\"invalid truth value {val!r}\")\n+\n+\n+@lru_cache(maxsize=50)\n+def parse_warning_filter(\n+    arg: str, *, escape: bool\n+) -> tuple[warnings._ActionKind, str, type[Warning], str, int]:\n+    \"\"\"Parse a warnings filter string.\n+\n+    This is copied from warnings._setoption with the following changes:\n+\n+    * Does not apply the filter.\n+    * Escaping is optional.\n+    * Raises UsageError so we get nice error messages on failure.\n+    \"\"\"\n+    __tracebackhide__ = True\n+    error_template = dedent(\n+        f\"\"\"\\\n+        while parsing the following warning configuration:\n+\n+          {arg}\n+\n+        This error occurred:\n+\n+        {{error}}\n+        \"\"\"\n+    )\n+\n+    parts = arg.split(\":\")\n+    if len(parts) > 5:\n+        doc_url = (\n+            \"https://docs.python.org/3/library/warnings.html#describing-warning-filters\"\n+        )\n+        error = dedent(\n+            f\"\"\"\\\n+            Too many fields ({len(parts)}), expected at most 5 separated by colons:\n+\n+              action:message:category:module:line\n+\n+            For more information please consult: {doc_url}\n+            \"\"\"\n+        )\n+        raise UsageError(error_template.format(error=error))\n+\n+    while len(parts) < 5:\n+        parts.append(\"\")\n+    action_, message, category_, module, lineno_ = (s.strip() for s in parts)\n+    try:\n+        action: warnings._ActionKind = warnings._getaction(action_)  # type: ignore[attr-defined]\n+    except warnings._OptionError as e:\n+        raise UsageError(error_template.format(error=str(e))) from None\n+    try:\n+        category: type[Warning] = _resolve_warning_category(category_)\n+    except Exception:\n+        exc_info = ExceptionInfo.from_current()\n+        exception_text = exc_info.getrepr(style=\"native\")\n+        raise UsageError(error_template.format(error=exception_text)) from None\n+    if message and escape:\n+        message = re.escape(message)\n+    if module and escape:\n+        module = re.escape(module) + r\"\\Z\"\n+    if lineno_:\n+        try:\n+            lineno = int(lineno_)\n+            if lineno < 0:\n+                raise ValueError(\"number is negative\")\n+        except ValueError as e:\n+            raise UsageError(\n+                error_template.format(error=f\"invalid lineno {lineno_!r}: {e}\")\n+            ) from None\n+    else:\n+        lineno = 0\n+    try:\n+        re.compile(message)\n+        re.compile(module)\n+    except re.error as e:\n+        raise UsageError(\n+            error_template.format(error=f\"Invalid regex {e.pattern!r}: {e}\")\n+        ) from None\n+    return action, message, category, module, lineno\n+\n+\n+def _resolve_warning_category(category: str) -> type[Warning]:\n+    \"\"\"\n+    Copied from warnings._getcategory, but changed so it lets exceptions (specially ImportErrors)\n+    propagate so we can get access to their tracebacks (#9218).\n+    \"\"\"\n+    __tracebackhide__ = True\n+    if not category:\n+        return Warning\n+\n+    if \".\" not in category:\n+        import builtins as m\n+\n+        klass = category\n+    else:\n+        module, _, klass = category.rpartition(\".\")\n+        m = __import__(module, None, None, [klass])\n+    cat = getattr(m, klass)\n+    if not issubclass(cat, Warning):\n+        raise UsageError(f\"{cat} is not a Warning subclass\")\n+    return cast(type[Warning], cat)\n+\n+\n+def apply_warning_filters(\n+    config_filters: Iterable[str], cmdline_filters: Iterable[str]\n+) -> None:\n+    \"\"\"Applies pytest-configured filters to the warnings module\"\"\"\n+    # Filters should have this precedence: cmdline options, config.\n+    # Filters should be applied in the inverse order of precedence.\n+    for arg in config_filters:\n+        warnings.filterwarnings(*parse_warning_filter(arg, escape=False))\n+\n+    for arg in cmdline_filters:\n+        warnings.filterwarnings(*parse_warning_filter(arg, escape=True))",
      "patch_lines": [
        "@@ -0,0 +1,2029 @@\n",
        "+# mypy: allow-untyped-defs\n",
        "+\"\"\"Command line options, ini-file and conftest.py processing.\"\"\"\n",
        "+\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+import argparse\n",
        "+import collections.abc\n",
        "+from collections.abc import Callable\n",
        "+from collections.abc import Generator\n",
        "+from collections.abc import Iterable\n",
        "+from collections.abc import Iterator\n",
        "+from collections.abc import Sequence\n",
        "+import contextlib\n",
        "+import copy\n",
        "+import dataclasses\n",
        "+import enum\n",
        "+from functools import lru_cache\n",
        "+import glob\n",
        "+import importlib.metadata\n",
        "+import inspect\n",
        "+import os\n",
        "+import pathlib\n",
        "+import re\n",
        "+import shlex\n",
        "+import sys\n",
        "+from textwrap import dedent\n",
        "+import types\n",
        "+from types import FunctionType\n",
        "+from typing import Any\n",
        "+from typing import cast\n",
        "+from typing import Final\n",
        "+from typing import final\n",
        "+from typing import IO\n",
        "+from typing import TextIO\n",
        "+from typing import TYPE_CHECKING\n",
        "+import warnings\n",
        "+\n",
        "+import pluggy\n",
        "+from pluggy import HookimplMarker\n",
        "+from pluggy import HookimplOpts\n",
        "+from pluggy import HookspecMarker\n",
        "+from pluggy import HookspecOpts\n",
        "+from pluggy import PluginManager\n",
        "+\n",
        "+from .compat import PathAwareHookProxy\n",
        "+from .exceptions import PrintHelp as PrintHelp\n",
        "+from .exceptions import UsageError as UsageError\n",
        "+from .findpaths import determine_setup\n",
        "+from _pytest import __version__\n",
        "+import _pytest._code\n",
        "+from _pytest._code import ExceptionInfo\n",
        "+from _pytest._code import filter_traceback\n",
        "+from _pytest._code.code import TracebackStyle\n",
        "+from _pytest._io import TerminalWriter\n",
        "+from _pytest.config.argparsing import Argument\n",
        "+from _pytest.config.argparsing import Parser\n",
        "+import _pytest.deprecated\n",
        "+import _pytest.hookspec\n",
        "+from _pytest.outcomes import fail\n",
        "+from _pytest.outcomes import Skipped\n",
        "+from _pytest.pathlib import absolutepath\n",
        "+from _pytest.pathlib import bestrelpath\n",
        "+from _pytest.pathlib import import_path\n",
        "+from _pytest.pathlib import ImportMode\n",
        "+from _pytest.pathlib import resolve_package_path\n",
        "+from _pytest.pathlib import safe_exists\n",
        "+from _pytest.stash import Stash\n",
        "+from _pytest.warning_types import PytestConfigWarning\n",
        "+from _pytest.warning_types import warn_explicit_for\n",
        "+\n",
        "+\n",
        "+if TYPE_CHECKING:\n",
        "+    from _pytest.assertion.rewrite import AssertionRewritingHook\n",
        "+    from _pytest.cacheprovider import Cache\n",
        "+    from _pytest.terminal import TerminalReporter\n",
        "+\n",
        "+_PluggyPlugin = object\n",
        "+\"\"\"A type to represent plugin objects.\n",
        "+\n",
        "+Plugins can be any namespace, so we can't narrow it down much, but we use an\n",
        "+alias to make the intent clear.\n",
        "+\n",
        "+Ideally this type would be provided by pluggy itself.\n",
        "+\"\"\"\n",
        "+\n",
        "+\n",
        "+hookimpl = HookimplMarker(\"pytest\")\n",
        "+hookspec = HookspecMarker(\"pytest\")\n",
        "+\n",
        "+\n",
        "+@final\n",
        "+class ExitCode(enum.IntEnum):\n",
        "+    \"\"\"Encodes the valid exit codes by pytest.\n",
        "+\n",
        "+    Currently users and plugins may supply other exit codes as well.\n",
        "+\n",
        "+    .. versionadded:: 5.0\n",
        "+    \"\"\"\n",
        "+\n",
        "+    #: Tests passed.\n",
        "+    OK = 0\n",
        "+    #: Tests failed.\n",
        "+    TESTS_FAILED = 1\n",
        "+    #: pytest was interrupted.\n",
        "+    INTERRUPTED = 2\n",
        "+    #: An internal error got in the way.\n",
        "+    INTERNAL_ERROR = 3\n",
        "+    #: pytest was misused.\n",
        "+    USAGE_ERROR = 4\n",
        "+    #: pytest couldn't find tests.\n",
        "+    NO_TESTS_COLLECTED = 5\n",
        "+\n",
        "+\n",
        "+class ConftestImportFailure(Exception):\n",
        "+    def __init__(\n",
        "+        self,\n",
        "+        path: pathlib.Path,\n",
        "+        *,\n",
        "+        cause: Exception,\n",
        "+    ) -> None:\n",
        "+        self.path = path\n",
        "+        self.cause = cause\n",
        "+\n",
        "+    def __str__(self) -> str:\n",
        "+        return f\"{type(self.cause).__name__}: {self.cause} (from {self.path})\"\n",
        "+\n",
        "+\n",
        "+def filter_traceback_for_conftest_import_failure(\n",
        "+    entry: _pytest._code.TracebackEntry,\n",
        "+) -> bool:\n",
        "+    \"\"\"Filter tracebacks entries which point to pytest internals or importlib.\n",
        "+\n",
        "+    Make a special case for importlib because we use it to import test modules and conftest files\n",
        "+    in _pytest.pathlib.import_path.\n",
        "+    \"\"\"\n",
        "+    return filter_traceback(entry) and \"importlib\" not in str(entry.path).split(os.sep)\n",
        "+\n",
        "+\n",
        "+def main(\n",
        "+    args: list[str] | os.PathLike[str] | None = None,\n",
        "+    plugins: Sequence[str | _PluggyPlugin] | None = None,\n",
        "+) -> int | ExitCode:\n",
        "+    \"\"\"Perform an in-process test run.\n",
        "+\n",
        "+    :param args:\n",
        "+        List of command line arguments. If `None` or not given, defaults to reading\n",
        "+        arguments directly from the process command line (:data:`sys.argv`).\n",
        "+    :param plugins: List of plugin objects to be auto-registered during initialization.\n",
        "+\n",
        "+    :returns: An exit code.\n",
        "+    \"\"\"\n",
        "+    old_pytest_version = os.environ.get(\"PYTEST_VERSION\")\n",
        "+    try:\n",
        "+        os.environ[\"PYTEST_VERSION\"] = __version__\n",
        "+        try:\n",
        "+            config = _prepareconfig(args, plugins)\n",
        "+        except ConftestImportFailure as e:\n",
        "+            exc_info = ExceptionInfo.from_exception(e.cause)\n",
        "+            tw = TerminalWriter(sys.stderr)\n",
        "+            tw.line(f\"ImportError while loading conftest '{e.path}'.\", red=True)\n",
        "+            exc_info.traceback = exc_info.traceback.filter(\n",
        "+                filter_traceback_for_conftest_import_failure\n",
        "+            )\n",
        "+            exc_repr = (\n",
        "+                exc_info.getrepr(style=\"short\", chain=False)\n",
        "+                if exc_info.traceback\n",
        "+                else exc_info.exconly()\n",
        "+            )\n",
        "+            formatted_tb = str(exc_repr)\n",
        "+            for line in formatted_tb.splitlines():\n",
        "+                tw.line(line.rstrip(), red=True)\n",
        "+            return ExitCode.USAGE_ERROR\n",
        "+        else:\n",
        "+            try:\n",
        "+                ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)\n",
        "+                try:\n",
        "+                    return ExitCode(ret)\n",
        "+                except ValueError:\n",
        "+                    return ret\n",
        "+            finally:\n",
        "+                config._ensure_unconfigure()\n",
        "+    except UsageError as e:\n",
        "+        tw = TerminalWriter(sys.stderr)\n",
        "+        for msg in e.args:\n",
        "+            tw.line(f\"ERROR: {msg}\\n\", red=True)\n",
        "+        return ExitCode.USAGE_ERROR\n",
        "+    finally:\n",
        "+        if old_pytest_version is None:\n",
        "+            os.environ.pop(\"PYTEST_VERSION\", None)\n",
        "+        else:\n",
        "+            os.environ[\"PYTEST_VERSION\"] = old_pytest_version\n",
        "+\n",
        "+\n",
        "+def console_main() -> int:\n",
        "+    \"\"\"The CLI entry point of pytest.\n",
        "+\n",
        "+    This function is not meant for programmable use; use `main()` instead.\n",
        "+    \"\"\"\n",
        "+    # https://docs.python.org/3/library/signal.html#note-on-sigpipe\n",
        "+    try:\n",
        "+        code = main()\n",
        "+        sys.stdout.flush()\n",
        "+        return code\n",
        "+    except BrokenPipeError:\n",
        "+        # Python flushes standard streams on exit; redirect remaining output\n",
        "+        # to devnull to avoid another BrokenPipeError at shutdown\n",
        "+        devnull = os.open(os.devnull, os.O_WRONLY)\n",
        "+        os.dup2(devnull, sys.stdout.fileno())\n",
        "+        return 1  # Python exits with error code 1 on EPIPE\n",
        "+\n",
        "+\n",
        "+class cmdline:  # compatibility namespace\n",
        "+    main = staticmethod(main)\n",
        "+\n",
        "+\n",
        "+def filename_arg(path: str, optname: str) -> str:\n",
        "+    \"\"\"Argparse type validator for filename arguments.\n",
        "+\n",
        "+    :path: Path of filename.\n",
        "+    :optname: Name of the option.\n",
        "+    \"\"\"\n",
        "+    if os.path.isdir(path):\n",
        "+        raise UsageError(f\"{optname} must be a filename, given: {path}\")\n",
        "+    return path\n",
        "+\n",
        "+\n",
        "+def directory_arg(path: str, optname: str) -> str:\n",
        "+    \"\"\"Argparse type validator for directory arguments.\n",
        "+\n",
        "+    :path: Path of directory.\n",
        "+    :optname: Name of the option.\n",
        "+    \"\"\"\n",
        "+    if not os.path.isdir(path):\n",
        "+        raise UsageError(f\"{optname} must be a directory, given: {path}\")\n",
        "+    return path\n",
        "+\n",
        "+\n",
        "+# Plugins that cannot be disabled via \"-p no:X\" currently.\n",
        "+essential_plugins = (\n",
        "+    \"mark\",\n",
        "+    \"main\",\n",
        "+    \"runner\",\n",
        "+    \"fixtures\",\n",
        "+    \"helpconfig\",  # Provides -p.\n",
        "+)\n",
        "+\n",
        "+default_plugins = (\n",
        "+    *essential_plugins,\n",
        "+    \"python\",\n",
        "+    \"terminal\",\n",
        "+    \"debugging\",\n",
        "+    \"unittest\",\n",
        "+    \"capture\",\n",
        "+    \"skipping\",\n",
        "+    \"legacypath\",\n",
        "+    \"tmpdir\",\n",
        "+    \"monkeypatch\",\n",
        "+    \"recwarn\",\n",
        "+    \"pastebin\",\n",
        "+    \"assertion\",\n",
        "+    \"junitxml\",\n",
        "+    \"doctest\",\n",
        "+    \"cacheprovider\",\n",
        "+    \"freeze_support\",\n",
        "+    \"setuponly\",\n",
        "+    \"setupplan\",\n",
        "+    \"stepwise\",\n",
        "+    \"unraisableexception\",\n",
        "+    \"threadexception\",\n",
        "+    \"warnings\",\n",
        "+    \"logging\",\n",
        "+    \"reports\",\n",
        "+    \"faulthandler\",\n",
        "+)\n",
        "+\n",
        "+builtin_plugins = {\n",
        "+    *default_plugins,\n",
        "+    \"pytester\",\n",
        "+    \"pytester_assertions\",\n",
        "+}\n",
        "+\n",
        "+\n",
        "+def get_config(\n",
        "+    args: list[str] | None = None,\n",
        "+    plugins: Sequence[str | _PluggyPlugin] | None = None,\n",
        "+) -> Config:\n",
        "+    # subsequent calls to main will create a fresh instance\n",
        "+    pluginmanager = PytestPluginManager()\n",
        "+    config = Config(\n",
        "+        pluginmanager,\n",
        "+        invocation_params=Config.InvocationParams(\n",
        "+            args=args or (),\n",
        "+            plugins=plugins,\n",
        "+            dir=pathlib.Path.cwd(),\n",
        "+        ),\n",
        "+    )\n",
        "+\n",
        "+    if args is not None:\n",
        "+        # Handle any \"-p no:plugin\" args.\n",
        "+        pluginmanager.consider_preparse(args, exclude_only=True)\n",
        "+\n",
        "+    for spec in default_plugins:\n",
        "+        pluginmanager.import_plugin(spec)\n",
        "+\n",
        "+    return config\n",
        "+\n",
        "+\n",
        "+def get_plugin_manager() -> PytestPluginManager:\n",
        "+    \"\"\"Obtain a new instance of the\n",
        "+    :py:class:`pytest.PytestPluginManager`, with default plugins\n",
        "+    already loaded.\n",
        "+\n",
        "+    This function can be used by integration with other tools, like hooking\n",
        "+    into pytest to run tests into an IDE.\n",
        "+    \"\"\"\n",
        "+    return get_config().pluginmanager\n",
        "+\n",
        "+\n",
        "+def _prepareconfig(\n",
        "+    args: list[str] | os.PathLike[str] | None = None,\n",
        "+    plugins: Sequence[str | _PluggyPlugin] | None = None,\n",
        "+) -> Config:\n",
        "+    if args is None:\n",
        "+        args = sys.argv[1:]\n",
        "+    elif isinstance(args, os.PathLike):\n",
        "+        args = [os.fspath(args)]\n",
        "+    elif not isinstance(args, list):\n",
        "+        msg = (  # type:ignore[unreachable]\n",
        "+            \"`args` parameter expected to be a list of strings, got: {!r} (type: {})\"\n",
        "+        )\n",
        "+        raise TypeError(msg.format(args, type(args)))\n",
        "+\n",
        "+    config = get_config(args, plugins)\n",
        "+    pluginmanager = config.pluginmanager\n",
        "+    try:\n",
        "+        if plugins:\n",
        "+            for plugin in plugins:\n",
        "+                if isinstance(plugin, str):\n",
        "+                    pluginmanager.consider_pluginarg(plugin)\n",
        "+                else:\n",
        "+                    pluginmanager.register(plugin)\n",
        "+        config = pluginmanager.hook.pytest_cmdline_parse(\n",
        "+            pluginmanager=pluginmanager, args=args\n",
        "+        )\n",
        "+        return config\n",
        "+    except BaseException:\n",
        "+        config._ensure_unconfigure()\n",
        "+        raise\n",
        "+\n",
        "+\n",
        "+def _get_directory(path: pathlib.Path) -> pathlib.Path:\n",
        "+    \"\"\"Get the directory of a path - itself if already a directory.\"\"\"\n",
        "+    if path.is_file():\n",
        "+        return path.parent\n",
        "+    else:\n",
        "+        return path\n",
        "+\n",
        "+\n",
        "+def _get_legacy_hook_marks(\n",
        "+    method: Any,\n",
        "+    hook_type: str,\n",
        "+    opt_names: tuple[str, ...],\n",
        "+) -> dict[str, bool]:\n",
        "+    if TYPE_CHECKING:\n",
        "+        # abuse typeguard from importlib to avoid massive method type union that's lacking an alias\n",
        "+        assert inspect.isroutine(method)\n",
        "+    known_marks: set[str] = {m.name for m in getattr(method, \"pytestmark\", [])}\n",
        "+    must_warn: list[str] = []\n",
        "+    opts: dict[str, bool] = {}\n",
        "+    for opt_name in opt_names:\n",
        "+        opt_attr = getattr(method, opt_name, AttributeError)\n",
        "+        if opt_attr is not AttributeError:\n",
        "+            must_warn.append(f\"{opt_name}={opt_attr}\")\n",
        "+            opts[opt_name] = True\n",
        "+        elif opt_name in known_marks:\n",
        "+            must_warn.append(f\"{opt_name}=True\")\n",
        "+            opts[opt_name] = True\n",
        "+        else:\n",
        "+            opts[opt_name] = False\n",
        "+    if must_warn:\n",
        "+        hook_opts = \", \".join(must_warn)\n",
        "+        message = _pytest.deprecated.HOOK_LEGACY_MARKING.format(\n",
        "+            type=hook_type,\n",
        "+            fullname=method.__qualname__,\n",
        "+            hook_opts=hook_opts,\n",
        "+        )\n",
        "+        warn_explicit_for(cast(FunctionType, method), message)\n",
        "+    return opts\n",
        "+\n",
        "+\n",
        "+@final\n",
        "+class PytestPluginManager(PluginManager):\n",
        "+    \"\"\"A :py:class:`pluggy.PluginManager <pluggy.PluginManager>` with\n",
        "+    additional pytest-specific functionality:\n",
        "+\n",
        "+    * Loading plugins from the command line, ``PYTEST_PLUGINS`` env variable and\n",
        "+      ``pytest_plugins`` global variables found in plugins being loaded.\n",
        "+    * ``conftest.py`` loading during start-up.\n",
        "+    \"\"\"\n",
        "+\n",
        "+    def __init__(self) -> None:\n",
        "+        from _pytest.assertion import DummyRewriteHook\n",
        "+        from _pytest.assertion import RewriteHook\n",
        "+\n",
        "+        super().__init__(\"pytest\")\n",
        "+\n",
        "+        # -- State related to local conftest plugins.\n",
        "+        # All loaded conftest modules.\n",
        "+        self._conftest_plugins: set[types.ModuleType] = set()\n",
        "+        # All conftest modules applicable for a directory.\n",
        "+        # This includes the directory's own conftest modules as well\n",
        "+        # as those of its parent directories.\n",
        "+        self._dirpath2confmods: dict[pathlib.Path, list[types.ModuleType]] = {}\n",
        "+        # Cutoff directory above which conftests are no longer discovered.\n",
        "+        self._confcutdir: pathlib.Path | None = None\n",
        "+        # If set, conftest loading is skipped.\n",
        "+        self._noconftest = False\n",
        "+\n",
        "+        # _getconftestmodules()'s call to _get_directory() causes a stat\n",
        "+        # storm when it's called potentially thousands of times in a test\n",
        "+        # session (#9478), often with the same path, so cache it.\n",
        "+        self._get_directory = lru_cache(256)(_get_directory)\n",
        "+\n",
        "+        # plugins that were explicitly skipped with pytest.skip\n",
        "+        # list of (module name, skip reason)\n",
        "+        # previously we would issue a warning when a plugin was skipped, but\n",
        "+        # since we refactored warnings as first citizens of Config, they are\n",
        "+        # just stored here to be used later.\n",
        "+        self.skipped_plugins: list[tuple[str, str]] = []\n",
        "+\n",
        "+        self.add_hookspecs(_pytest.hookspec)\n",
        "+        self.register(self)\n",
        "+        if os.environ.get(\"PYTEST_DEBUG\"):\n",
        "+            err: IO[str] = sys.stderr\n",
        "+            encoding: str = getattr(err, \"encoding\", \"utf8\")\n",
        "+            try:\n",
        "+                err = open(\n",
        "+                    os.dup(err.fileno()),\n",
        "+                    mode=err.mode,\n",
        "+                    buffering=1,\n",
        "+                    encoding=encoding,\n",
        "+                )\n",
        "+            except Exception:\n",
        "+                pass\n",
        "+            self.trace.root.setwriter(err.write)\n",
        "+            self.enable_tracing()\n",
        "+\n",
        "+        # Config._consider_importhook will set a real object if required.\n",
        "+        self.rewrite_hook: RewriteHook = DummyRewriteHook()\n",
        "+        # Used to know when we are importing conftests after the pytest_configure stage.\n",
        "+        self._configured = False\n",
        "+\n",
        "+    def parse_hookimpl_opts(\n",
        "+        self, plugin: _PluggyPlugin, name: str\n",
        "+    ) -> HookimplOpts | None:\n",
        "+        \"\"\":meta private:\"\"\"\n",
        "+        # pytest hooks are always prefixed with \"pytest_\",\n",
        "+        # so we avoid accessing possibly non-readable attributes\n",
        "+        # (see issue #1073).\n",
        "+        if not name.startswith(\"pytest_\"):\n",
        "+            return None\n",
        "+        # Ignore names which cannot be hooks.\n",
        "+        if name == \"pytest_plugins\":\n",
        "+            return None\n",
        "+\n",
        "+        opts = super().parse_hookimpl_opts(plugin, name)\n",
        "+        if opts is not None:\n",
        "+            return opts\n",
        "+\n",
        "+        method = getattr(plugin, name)\n",
        "+        # Consider only actual functions for hooks (#3775).\n",
        "+        if not inspect.isroutine(method):\n",
        "+            return None\n",
        "+        # Collect unmarked hooks as long as they have the `pytest_' prefix.\n",
        "+        legacy = _get_legacy_hook_marks(\n",
        "+            method, \"impl\", (\"tryfirst\", \"trylast\", \"optionalhook\", \"hookwrapper\")\n",
        "+        )\n",
        "+        return cast(HookimplOpts, legacy)\n",
        "+\n",
        "+    def parse_hookspec_opts(self, module_or_class, name: str) -> HookspecOpts | None:\n",
        "+        \"\"\":meta private:\"\"\"\n",
        "+        opts = super().parse_hookspec_opts(module_or_class, name)\n",
        "+        if opts is None:\n",
        "+            method = getattr(module_or_class, name)\n",
        "+            if name.startswith(\"pytest_\"):\n",
        "+                legacy = _get_legacy_hook_marks(\n",
        "+                    method, \"spec\", (\"firstresult\", \"historic\")\n",
        "+                )\n",
        "+                opts = cast(HookspecOpts, legacy)\n",
        "+        return opts\n",
        "+\n",
        "+    def register(self, plugin: _PluggyPlugin, name: str | None = None) -> str | None:\n",
        "+        if name in _pytest.deprecated.DEPRECATED_EXTERNAL_PLUGINS:\n",
        "+            warnings.warn(\n",
        "+                PytestConfigWarning(\n",
        "+                    \"{} plugin has been merged into the core, \"\n",
        "+                    \"please remove it from your requirements.\".format(\n",
        "+                        name.replace(\"_\", \"-\")\n",
        "+                    )\n",
        "+                )\n",
        "+            )\n",
        "+            return None\n",
        "+        plugin_name = super().register(plugin, name)\n",
        "+        if plugin_name is not None:\n",
        "+            self.hook.pytest_plugin_registered.call_historic(\n",
        "+                kwargs=dict(\n",
        "+                    plugin=plugin,\n",
        "+                    plugin_name=plugin_name,\n",
        "+                    manager=self,\n",
        "+                )\n",
        "+            )\n",
        "+\n",
        "+            if isinstance(plugin, types.ModuleType):\n",
        "+                self.consider_module(plugin)\n",
        "+        return plugin_name\n",
        "+\n",
        "+    def getplugin(self, name: str):\n",
        "+        # Support deprecated naming because plugins (xdist e.g.) use it.\n",
        "+        plugin: _PluggyPlugin | None = self.get_plugin(name)\n",
        "+        return plugin\n",
        "+\n",
        "+    def hasplugin(self, name: str) -> bool:\n",
        "+        \"\"\"Return whether a plugin with the given name is registered.\"\"\"\n",
        "+        return bool(self.get_plugin(name))\n",
        "+\n",
        "+    def pytest_configure(self, config: Config) -> None:\n",
        "+        \"\"\":meta private:\"\"\"\n",
        "+        # XXX now that the pluginmanager exposes hookimpl(tryfirst...)\n",
        "+        # we should remove tryfirst/trylast as markers.\n",
        "+        config.addinivalue_line(\n",
        "+            \"markers\",\n",
        "+            \"tryfirst: mark a hook implementation function such that the \"\n",
        "+            \"plugin machinery will try to call it first/as early as possible. \"\n",
        "+            \"DEPRECATED, use @pytest.hookimpl(tryfirst=True) instead.\",\n",
        "+        )\n",
        "+        config.addinivalue_line(\n",
        "+            \"markers\",\n",
        "+            \"trylast: mark a hook implementation function such that the \"\n",
        "+            \"plugin machinery will try to call it last/as late as possible. \"\n",
        "+            \"DEPRECATED, use @pytest.hookimpl(trylast=True) instead.\",\n",
        "+        )\n",
        "+        self._configured = True\n",
        "+\n",
        "+    #\n",
        "+    # Internal API for local conftest plugin handling.\n",
        "+    #\n",
        "+    def _set_initial_conftests(\n",
        "+        self,\n",
        "+        args: Sequence[str | pathlib.Path],\n",
        "+        pyargs: bool,\n",
        "+        noconftest: bool,\n",
        "+        rootpath: pathlib.Path,\n",
        "+        confcutdir: pathlib.Path | None,\n",
        "+        invocation_dir: pathlib.Path,\n",
        "+        importmode: ImportMode | str,\n",
        "+        *,\n",
        "+        consider_namespace_packages: bool,\n",
        "+    ) -> None:\n",
        "+        \"\"\"Load initial conftest files given a preparsed \"namespace\".\n",
        "+\n",
        "+        As conftest files may add their own command line options which have\n",
        "+        arguments ('--my-opt somepath') we might get some false positives.\n",
        "+        All builtin and 3rd party plugins will have been loaded, however, so\n",
        "+        common options will not confuse our logic here.\n",
        "+        \"\"\"\n",
        "+        self._confcutdir = (\n",
        "+            absolutepath(invocation_dir / confcutdir) if confcutdir else None\n",
        "+        )\n",
        "+        self._noconftest = noconftest\n",
        "+        self._using_pyargs = pyargs\n",
        "+        foundanchor = False\n",
        "+        for initial_path in args:\n",
        "+            path = str(initial_path)\n",
        "+            # remove node-id syntax\n",
        "+            i = path.find(\"::\")\n",
        "+            if i != -1:\n",
        "+                path = path[:i]\n",
        "+            anchor = absolutepath(invocation_dir / path)\n",
        "+\n",
        "+            # Ensure we do not break if what appears to be an anchor\n",
        "+            # is in fact a very long option (#10169, #11394).\n",
        "+            if safe_exists(anchor):\n",
        "+                self._try_load_conftest(\n",
        "+                    anchor,\n",
        "+                    importmode,\n",
        "+                    rootpath,\n",
        "+                    consider_namespace_packages=consider_namespace_packages,\n",
        "+                )\n",
        "+                foundanchor = True\n",
        "+        if not foundanchor:\n",
        "+            self._try_load_conftest(\n",
        "+                invocation_dir,\n",
        "+                importmode,\n",
        "+                rootpath,\n",
        "+                consider_namespace_packages=consider_namespace_packages,\n",
        "+            )\n",
        "+\n",
        "+    def _is_in_confcutdir(self, path: pathlib.Path) -> bool:\n",
        "+        \"\"\"Whether to consider the given path to load conftests from.\"\"\"\n",
        "+        if self._confcutdir is None:\n",
        "+            return True\n",
        "+        # The semantics here are literally:\n",
        "+        #   Do not load a conftest if it is found upwards from confcut dir.\n",
        "+        # But this is *not* the same as:\n",
        "+        #   Load only conftests from confcutdir or below.\n",
        "+        # At first glance they might seem the same thing, however we do support use cases where\n",
        "+        # we want to load conftests that are not found in confcutdir or below, but are found\n",
        "+        # in completely different directory hierarchies like packages installed\n",
        "+        # in out-of-source trees.\n",
        "+        # (see #9767 for a regression where the logic was inverted).\n",
        "+        return path not in self._confcutdir.parents\n",
        "+\n",
        "+    def _try_load_conftest(\n",
        "+        self,\n",
        "+        anchor: pathlib.Path,\n",
        "+        importmode: str | ImportMode,\n",
        "+        rootpath: pathlib.Path,\n",
        "+        *,\n",
        "+        consider_namespace_packages: bool,\n",
        "+    ) -> None:\n",
        "+        self._loadconftestmodules(\n",
        "+            anchor,\n",
        "+            importmode,\n",
        "+            rootpath,\n",
        "+            consider_namespace_packages=consider_namespace_packages,\n",
        "+        )\n",
        "+        # let's also consider test* subdirs\n",
        "+        if anchor.is_dir():\n",
        "+            for x in anchor.glob(\"test*\"):\n",
        "+                if x.is_dir():\n",
        "+                    self._loadconftestmodules(\n",
        "+                        x,\n",
        "+                        importmode,\n",
        "+                        rootpath,\n",
        "+                        consider_namespace_packages=consider_namespace_packages,\n",
        "+                    )\n",
        "+\n",
        "+    def _loadconftestmodules(\n",
        "+        self,\n",
        "+        path: pathlib.Path,\n",
        "+        importmode: str | ImportMode,\n",
        "+        rootpath: pathlib.Path,\n",
        "+        *,\n",
        "+        consider_namespace_packages: bool,\n",
        "+    ) -> None:\n",
        "+        if self._noconftest:\n",
        "+            return\n",
        "+\n",
        "+        directory = self._get_directory(path)\n",
        "+\n",
        "+        # Optimization: avoid repeated searches in the same directory.\n",
        "+        # Assumes always called with same importmode and rootpath.\n",
        "+        if directory in self._dirpath2confmods:\n",
        "+            return\n",
        "+\n",
        "+        clist = []\n",
        "+        for parent in reversed((directory, *directory.parents)):\n",
        "+            if self._is_in_confcutdir(parent):\n",
        "+                conftestpath = parent / \"conftest.py\"\n",
        "+                if conftestpath.is_file():\n",
        "+                    mod = self._importconftest(\n",
        "+                        conftestpath,\n",
        "+                        importmode,\n",
        "+                        rootpath,\n",
        "+                        consider_namespace_packages=consider_namespace_packages,\n",
        "+                    )\n",
        "+                    clist.append(mod)\n",
        "+        self._dirpath2confmods[directory] = clist\n",
        "+\n",
        "+    def _getconftestmodules(self, path: pathlib.Path) -> Sequence[types.ModuleType]:\n",
        "+        directory = self._get_directory(path)\n",
        "+        return self._dirpath2confmods.get(directory, ())\n",
        "+\n",
        "+    def _rget_with_confmod(\n",
        "+        self,\n",
        "+        name: str,\n",
        "+        path: pathlib.Path,\n",
        "+    ) -> tuple[types.ModuleType, Any]:\n",
        "+        modules = self._getconftestmodules(path)\n",
        "+        for mod in reversed(modules):\n",
        "+            try:\n",
        "+                return mod, getattr(mod, name)\n",
        "+            except AttributeError:\n",
        "+                continue\n",
        "+        raise KeyError(name)\n",
        "+\n",
        "+    def _importconftest(\n",
        "+        self,\n",
        "+        conftestpath: pathlib.Path,\n",
        "+        importmode: str | ImportMode,\n",
        "+        rootpath: pathlib.Path,\n",
        "+        *,\n",
        "+        consider_namespace_packages: bool,\n",
        "+    ) -> types.ModuleType:\n",
        "+        conftestpath_plugin_name = str(conftestpath)\n",
        "+        existing = self.get_plugin(conftestpath_plugin_name)\n",
        "+        if existing is not None:\n",
        "+            return cast(types.ModuleType, existing)\n",
        "+\n",
        "+        # conftest.py files there are not in a Python package all have module\n",
        "+        # name \"conftest\", and thus conflict with each other. Clear the existing\n",
        "+        # before loading the new one, otherwise the existing one will be\n",
        "+        # returned from the module cache.\n",
        "+        pkgpath = resolve_package_path(conftestpath)\n",
        "+        if pkgpath is None:\n",
        "+            try:\n",
        "+                del sys.modules[conftestpath.stem]\n",
        "+            except KeyError:\n",
        "+                pass\n",
        "+\n",
        "+        try:\n",
        "+            mod = import_path(\n",
        "+                conftestpath,\n",
        "+                mode=importmode,\n",
        "+                root=rootpath,\n",
        "+                consider_namespace_packages=consider_namespace_packages,\n",
        "+            )\n",
        "+        except Exception as e:\n",
        "+            assert e.__traceback__ is not None\n",
        "+            raise ConftestImportFailure(conftestpath, cause=e) from e\n",
        "+\n",
        "+        self._check_non_top_pytest_plugins(mod, conftestpath)\n",
        "+\n",
        "+        self._conftest_plugins.add(mod)\n",
        "+        dirpath = conftestpath.parent\n",
        "+        if dirpath in self._dirpath2confmods:\n",
        "+            for path, mods in self._dirpath2confmods.items():\n",
        "+                if dirpath in path.parents or path == dirpath:\n",
        "+                    if mod in mods:\n",
        "+                        raise AssertionError(\n",
        "+                            f\"While trying to load conftest path {conftestpath!s}, \"\n",
        "+                            f\"found that the module {mod} is already loaded with path {mod.__file__}. \"\n",
        "+                            \"This is not supposed to happen. Please report this issue to pytest.\"\n",
        "+                        )\n",
        "+                    mods.append(mod)\n",
        "+        self.trace(f\"loading conftestmodule {mod!r}\")\n",
        "+        self.consider_conftest(mod, registration_name=conftestpath_plugin_name)\n",
        "+        return mod\n",
        "+\n",
        "+    def _check_non_top_pytest_plugins(\n",
        "+        self,\n",
        "+        mod: types.ModuleType,\n",
        "+        conftestpath: pathlib.Path,\n",
        "+    ) -> None:\n",
        "+        if (\n",
        "+            hasattr(mod, \"pytest_plugins\")\n",
        "+            and self._configured\n",
        "+            and not self._using_pyargs\n",
        "+        ):\n",
        "+            msg = (\n",
        "+                \"Defining 'pytest_plugins' in a non-top-level conftest is no longer supported:\\n\"\n",
        "+                \"It affects the entire test suite instead of just below the conftest as expected.\\n\"\n",
        "+                \"  {}\\n\"\n",
        "+                \"Please move it to a top level conftest file at the rootdir:\\n\"\n",
        "+                \"  {}\\n\"\n",
        "+                \"For more information, visit:\\n\"\n",
        "+                \"  https://docs.pytest.org/en/stable/deprecations.html#pytest-plugins-in-non-top-level-conftest-files\"\n",
        "+            )\n",
        "+            fail(msg.format(conftestpath, self._confcutdir), pytrace=False)\n",
        "+\n",
        "+    #\n",
        "+    # API for bootstrapping plugin loading\n",
        "+    #\n",
        "+    #\n",
        "+\n",
        "+    def consider_preparse(\n",
        "+        self, args: Sequence[str], *, exclude_only: bool = False\n",
        "+    ) -> None:\n",
        "+        \"\"\":meta private:\"\"\"\n",
        "+        i = 0\n",
        "+        n = len(args)\n",
        "+        while i < n:\n",
        "+            opt = args[i]\n",
        "+            i += 1\n",
        "+            if isinstance(opt, str):\n",
        "+                if opt == \"-p\":\n",
        "+                    try:\n",
        "+                        parg = args[i]\n",
        "+                    except IndexError:\n",
        "+                        return\n",
        "+                    i += 1\n",
        "+                elif opt.startswith(\"-p\"):\n",
        "+                    parg = opt[2:]\n",
        "+                else:\n",
        "+                    continue\n",
        "+                parg = parg.strip()\n",
        "+                if exclude_only and not parg.startswith(\"no:\"):\n",
        "+                    continue\n",
        "+                self.consider_pluginarg(parg)\n",
        "+\n",
        "+    def consider_pluginarg(self, arg: str) -> None:\n",
        "+        \"\"\":meta private:\"\"\"\n",
        "+        if arg.startswith(\"no:\"):\n",
        "+            name = arg[3:]\n",
        "+            if name in essential_plugins:\n",
        "+                raise UsageError(f\"plugin {name} cannot be disabled\")\n",
        "+\n",
        "+            # PR #4304: remove stepwise if cacheprovider is blocked.\n",
        "+            if name == \"cacheprovider\":\n",
        "+                self.set_blocked(\"stepwise\")\n",
        "+                self.set_blocked(\"pytest_stepwise\")\n",
        "+\n",
        "+            self.set_blocked(name)\n",
        "+            if not name.startswith(\"pytest_\"):\n",
        "+                self.set_blocked(\"pytest_\" + name)\n",
        "+        else:\n",
        "+            name = arg\n",
        "+            # Unblock the plugin.\n",
        "+            self.unblock(name)\n",
        "+            if not name.startswith(\"pytest_\"):\n",
        "+                self.unblock(\"pytest_\" + name)\n",
        "+            self.import_plugin(arg, consider_entry_points=True)\n",
        "+\n",
        "+    def consider_conftest(\n",
        "+        self, conftestmodule: types.ModuleType, registration_name: str\n",
        "+    ) -> None:\n",
        "+        \"\"\":meta private:\"\"\"\n",
        "+        self.register(conftestmodule, name=registration_name)\n",
        "+\n",
        "+    def consider_env(self) -> None:\n",
        "+        \"\"\":meta private:\"\"\"\n",
        "+        self._import_plugin_specs(os.environ.get(\"PYTEST_PLUGINS\"))\n",
        "+\n",
        "+    def consider_module(self, mod: types.ModuleType) -> None:\n",
        "+        \"\"\":meta private:\"\"\"\n",
        "+        self._import_plugin_specs(getattr(mod, \"pytest_plugins\", []))\n",
        "+\n",
        "+    def _import_plugin_specs(\n",
        "+        self, spec: None | types.ModuleType | str | Sequence[str]\n",
        "+    ) -> None:\n",
        "+        plugins = _get_plugin_specs_as_list(spec)\n",
        "+        for import_spec in plugins:\n",
        "+            self.import_plugin(import_spec)\n",
        "+\n",
        "+    def import_plugin(self, modname: str, consider_entry_points: bool = False) -> None:\n",
        "+        \"\"\"Import a plugin with ``modname``.\n",
        "+\n",
        "+        If ``consider_entry_points`` is True, entry point names are also\n",
        "+        considered to find a plugin.\n",
        "+        \"\"\"\n",
        "+        # Most often modname refers to builtin modules, e.g. \"pytester\",\n",
        "+        # \"terminal\" or \"capture\".  Those plugins are registered under their\n",
        "+        # basename for historic purposes but must be imported with the\n",
        "+        # _pytest prefix.\n",
        "+        assert isinstance(modname, str), (\n",
        "+            f\"module name as text required, got {modname!r}\"\n",
        "+        )\n",
        "+        if self.is_blocked(modname) or self.get_plugin(modname) is not None:\n",
        "+            return\n",
        "+\n",
        "+        importspec = \"_pytest.\" + modname if modname in builtin_plugins else modname\n",
        "+        self.rewrite_hook.mark_rewrite(importspec)\n",
        "+\n",
        "+        if consider_entry_points:\n",
        "+            loaded = self.load_setuptools_entrypoints(\"pytest11\", name=modname)\n",
        "+            if loaded:\n",
        "+                return\n",
        "+\n",
        "+        try:\n",
        "+            __import__(importspec)\n",
        "+        except ImportError as e:\n",
        "+            raise ImportError(\n",
        "+                f'Error importing plugin \"{modname}\": {e.args[0]}'\n",
        "+            ).with_traceback(e.__traceback__) from e\n",
        "+\n",
        "+        except Skipped as e:\n",
        "+            self.skipped_plugins.append((modname, e.msg or \"\"))\n",
        "+        else:\n",
        "+            mod = sys.modules[importspec]\n",
        "+            self.register(mod, modname)\n",
        "+\n",
        "+\n",
        "+def _get_plugin_specs_as_list(\n",
        "+    specs: None | types.ModuleType | str | Sequence[str],\n",
        "+) -> list[str]:\n",
        "+    \"\"\"Parse a plugins specification into a list of plugin names.\"\"\"\n",
        "+    # None means empty.\n",
        "+    if specs is None:\n",
        "+        return []\n",
        "+    # Workaround for #3899 - a submodule which happens to be called \"pytest_plugins\".\n",
        "+    if isinstance(specs, types.ModuleType):\n",
        "+        return []\n",
        "+    # Comma-separated list.\n",
        "+    if isinstance(specs, str):\n",
        "+        return specs.split(\",\") if specs else []\n",
        "+    # Direct specification.\n",
        "+    if isinstance(specs, collections.abc.Sequence):\n",
        "+        return list(specs)\n",
        "+    raise UsageError(\n",
        "+        f\"Plugins may be specified as a sequence or a ','-separated string of plugin names. Got: {specs!r}\"\n",
        "+    )\n",
        "+\n",
        "+\n",
        "+class Notset:\n",
        "+    def __repr__(self):\n",
        "+        return \"<NOTSET>\"\n",
        "+\n",
        "+\n",
        "+notset = Notset()\n",
        "+\n",
        "+\n",
        "+def _iter_rewritable_modules(package_files: Iterable[str]) -> Iterator[str]:\n",
        "+    \"\"\"Given an iterable of file names in a source distribution, return the \"names\" that should\n",
        "+    be marked for assertion rewrite.\n",
        "+\n",
        "+    For example the package \"pytest_mock/__init__.py\" should be added as \"pytest_mock\" in\n",
        "+    the assertion rewrite mechanism.\n",
        "+\n",
        "+    This function has to deal with dist-info based distributions and egg based distributions\n",
        "+    (which are still very much in use for \"editable\" installs).\n",
        "+\n",
        "+    Here are the file names as seen in a dist-info based distribution:\n",
        "+\n",
        "+        pytest_mock/__init__.py\n",
        "+        pytest_mock/_version.py\n",
        "+        pytest_mock/plugin.py\n",
        "+        pytest_mock.egg-info/PKG-INFO\n",
        "+\n",
        "+    Here are the file names as seen in an egg based distribution:\n",
        "+\n",
        "+        src/pytest_mock/__init__.py\n",
        "+        src/pytest_mock/_version.py\n",
        "+        src/pytest_mock/plugin.py\n",
        "+        src/pytest_mock.egg-info/PKG-INFO\n",
        "+        LICENSE\n",
        "+        setup.py\n",
        "+\n",
        "+    We have to take in account those two distribution flavors in order to determine which\n",
        "+    names should be considered for assertion rewriting.\n",
        "+\n",
        "+    More information:\n",
        "+        https://github.com/pytest-dev/pytest-mock/issues/167\n",
        "+    \"\"\"\n",
        "+    package_files = list(package_files)\n",
        "+    seen_some = False\n",
        "+    for fn in package_files:\n",
        "+        is_simple_module = \"/\" not in fn and fn.endswith(\".py\")\n",
        "+        is_package = fn.count(\"/\") == 1 and fn.endswith(\"__init__.py\")\n",
        "+        if is_simple_module:\n",
        "+            module_name, _ = os.path.splitext(fn)\n",
        "+            # we ignore \"setup.py\" at the root of the distribution\n",
        "+            # as well as editable installation finder modules made by setuptools\n",
        "+            if module_name != \"setup\" and not module_name.startswith(\"__editable__\"):\n",
        "+                seen_some = True\n",
        "+                yield module_name\n",
        "+        elif is_package:\n",
        "+            package_name = os.path.dirname(fn)\n",
        "+            seen_some = True\n",
        "+            yield package_name\n",
        "+\n",
        "+    if not seen_some:\n",
        "+        # At this point we did not find any packages or modules suitable for assertion\n",
        "+        # rewriting, so we try again by stripping the first path component (to account for\n",
        "+        # \"src\" based source trees for example).\n",
        "+        # This approach lets us have the common case continue to be fast, as egg-distributions\n",
        "+        # are rarer.\n",
        "+        new_package_files = []\n",
        "+        for fn in package_files:\n",
        "+            parts = fn.split(\"/\")\n",
        "+            new_fn = \"/\".join(parts[1:])\n",
        "+            if new_fn:\n",
        "+                new_package_files.append(new_fn)\n",
        "+        if new_package_files:\n",
        "+            yield from _iter_rewritable_modules(new_package_files)\n",
        "+\n",
        "+\n",
        "+@final\n",
        "+class Config:\n",
        "+    \"\"\"Access to configuration values, pluginmanager and plugin hooks.\n",
        "+\n",
        "+    :param PytestPluginManager pluginmanager:\n",
        "+        A pytest PluginManager.\n",
        "+\n",
        "+    :param InvocationParams invocation_params:\n",
        "+        Object containing parameters regarding the :func:`pytest.main`\n",
        "+        invocation.\n",
        "+    \"\"\"\n",
        "+\n",
        "+    @final\n",
        "+    @dataclasses.dataclass(frozen=True)\n",
        "+    class InvocationParams:\n",
        "+        \"\"\"Holds parameters passed during :func:`pytest.main`.\n",
        "+\n",
        "+        The object attributes are read-only.\n",
        "+\n",
        "+        .. versionadded:: 5.1\n",
        "+\n",
        "+        .. note::\n",
        "+\n",
        "+            Note that the environment variable ``PYTEST_ADDOPTS`` and the ``addopts``\n",
        "+            ini option are handled by pytest, not being included in the ``args`` attribute.\n",
        "+\n",
        "+            Plugins accessing ``InvocationParams`` must be aware of that.\n",
        "+        \"\"\"\n",
        "+\n",
        "+        args: tuple[str, ...]\n",
        "+        \"\"\"The command-line arguments as passed to :func:`pytest.main`.\"\"\"\n",
        "+        plugins: Sequence[str | _PluggyPlugin] | None\n",
        "+        \"\"\"Extra plugins, might be `None`.\"\"\"\n",
        "+        dir: pathlib.Path\n",
        "+        \"\"\"The directory from which :func:`pytest.main` was invoked. :type: pathlib.Path\"\"\"\n",
        "+\n",
        "+        def __init__(\n",
        "+            self,\n",
        "+            *,\n",
        "+            args: Iterable[str],\n",
        "+            plugins: Sequence[str | _PluggyPlugin] | None,\n",
        "+            dir: pathlib.Path,\n",
        "+        ) -> None:\n",
        "+            object.__setattr__(self, \"args\", tuple(args))\n",
        "+            object.__setattr__(self, \"plugins\", plugins)\n",
        "+            object.__setattr__(self, \"dir\", dir)\n",
        "+\n",
        "+    class ArgsSource(enum.Enum):\n",
        "+        \"\"\"Indicates the source of the test arguments.\n",
        "+\n",
        "+        .. versionadded:: 7.2\n",
        "+        \"\"\"\n",
        "+\n",
        "+        #: Command line arguments.\n",
        "+        ARGS = enum.auto()\n",
        "+        #: Invocation directory.\n",
        "+        INVOCATION_DIR = enum.auto()\n",
        "+        INCOVATION_DIR = INVOCATION_DIR  # backwards compatibility alias\n",
        "+        #: 'testpaths' configuration value.\n",
        "+        TESTPATHS = enum.auto()\n",
        "+\n",
        "+    # Set by cacheprovider plugin.\n",
        "+    cache: Cache\n",
        "+\n",
        "+    def __init__(\n",
        "+        self,\n",
        "+        pluginmanager: PytestPluginManager,\n",
        "+        *,\n",
        "+        invocation_params: InvocationParams | None = None,\n",
        "+    ) -> None:\n",
        "+        from .argparsing import FILE_OR_DIR\n",
        "+        from .argparsing import Parser\n",
        "+\n",
        "+        if invocation_params is None:\n",
        "+            invocation_params = self.InvocationParams(\n",
        "+                args=(), plugins=None, dir=pathlib.Path.cwd()\n",
        "+            )\n",
        "+\n",
        "+        self.option = argparse.Namespace()\n",
        "+        \"\"\"Access to command line option as attributes.\n",
        "+\n",
        "+        :type: argparse.Namespace\n",
        "+        \"\"\"\n",
        "+\n",
        "+        self.invocation_params = invocation_params\n",
        "+        \"\"\"The parameters with which pytest was invoked.\n",
        "+\n",
        "+        :type: InvocationParams\n",
        "+        \"\"\"\n",
        "+\n",
        "+        _a = FILE_OR_DIR\n",
        "+        self._parser = Parser(\n",
        "+            usage=f\"%(prog)s [options] [{_a}] [{_a}] [...]\",\n",
        "+            processopt=self._processopt,\n",
        "+            _ispytest=True,\n",
        "+        )\n",
        "+        self.pluginmanager = pluginmanager\n",
        "+        \"\"\"The plugin manager handles plugin registration and hook invocation.\n",
        "+\n",
        "+        :type: PytestPluginManager\n",
        "+        \"\"\"\n",
        "+\n",
        "+        self.stash = Stash()\n",
        "+        \"\"\"A place where plugins can store information on the config for their\n",
        "+        own use.\n",
        "+\n",
        "+        :type: Stash\n",
        "+        \"\"\"\n",
        "+        # Deprecated alias. Was never public. Can be removed in a few releases.\n",
        "+        self._store = self.stash\n",
        "+\n",
        "+        self.trace = self.pluginmanager.trace.root.get(\"config\")\n",
        "+        self.hook: pluggy.HookRelay = PathAwareHookProxy(self.pluginmanager.hook)  # type: ignore[assignment]\n",
        "+        self._inicache: dict[str, Any] = {}\n",
        "+        self._override_ini: Sequence[str] = ()\n",
        "+        self._opt2dest: dict[str, str] = {}\n",
        "+        self._cleanup_stack = contextlib.ExitStack()\n",
        "+        self.pluginmanager.register(self, \"pytestconfig\")\n",
        "+        self._configured = False\n",
        "+        self.hook.pytest_addoption.call_historic(\n",
        "+            kwargs=dict(parser=self._parser, pluginmanager=self.pluginmanager)\n",
        "+        )\n",
        "+        self.args_source = Config.ArgsSource.ARGS\n",
        "+        self.args: list[str] = []\n",
        "+\n",
        "+    @property\n",
        "+    def rootpath(self) -> pathlib.Path:\n",
        "+        \"\"\"The path to the :ref:`rootdir <rootdir>`.\n",
        "+\n",
        "+        :type: pathlib.Path\n",
        "+\n",
        "+        .. versionadded:: 6.1\n",
        "+        \"\"\"\n",
        "+        return self._rootpath\n",
        "+\n",
        "+    @property\n",
        "+    def inipath(self) -> pathlib.Path | None:\n",
        "+        \"\"\"The path to the :ref:`configfile <configfiles>`.\n",
        "+\n",
        "+        .. versionadded:: 6.1\n",
        "+        \"\"\"\n",
        "+        return self._inipath\n",
        "+\n",
        "+    def add_cleanup(self, func: Callable[[], None]) -> None:\n",
        "+        \"\"\"Add a function to be called when the config object gets out of\n",
        "+        use (usually coinciding with pytest_unconfigure).\n",
        "+        \"\"\"\n",
        "+        self._cleanup_stack.callback(func)\n",
        "+\n",
        "+    def _do_configure(self) -> None:\n",
        "+        assert not self._configured\n",
        "+        self._configured = True\n",
        "+        self.hook.pytest_configure.call_historic(kwargs=dict(config=self))\n",
        "+\n",
        "+    def _ensure_unconfigure(self) -> None:\n",
        "+        try:\n",
        "+            if self._configured:\n",
        "+                self._configured = False\n",
        "+                try:\n",
        "+                    self.hook.pytest_unconfigure(config=self)\n",
        "+                finally:\n",
        "+                    self.hook.pytest_configure._call_history = []\n",
        "+        finally:\n",
        "+            try:\n",
        "+                self._cleanup_stack.close()\n",
        "+            finally:\n",
        "+                self._cleanup_stack = contextlib.ExitStack()\n",
        "+\n",
        "+    def get_terminal_writer(self) -> TerminalWriter:\n",
        "+        terminalreporter: TerminalReporter | None = self.pluginmanager.get_plugin(\n",
        "+            \"terminalreporter\"\n",
        "+        )\n",
        "+        assert terminalreporter is not None\n",
        "+        return terminalreporter._tw\n",
        "+\n",
        "+    def pytest_cmdline_parse(\n",
        "+        self, pluginmanager: PytestPluginManager, args: list[str]\n",
        "+    ) -> Config:\n",
        "+        try:\n",
        "+            self.parse(args)\n",
        "+        except UsageError:\n",
        "+            # Handle --version and --help here in a minimal fashion.\n",
        "+            # This gets done via helpconfig normally, but its\n",
        "+            # pytest_cmdline_main is not called in case of errors.\n",
        "+            if getattr(self.option, \"version\", False) or \"--version\" in args:\n",
        "+                from _pytest.helpconfig import showversion\n",
        "+\n",
        "+                showversion(self)\n",
        "+            elif (\n",
        "+                getattr(self.option, \"help\", False) or \"--help\" in args or \"-h\" in args\n",
        "+            ):\n",
        "+                self._parser._getparser().print_help()\n",
        "+                sys.stdout.write(\n",
        "+                    \"\\nNOTE: displaying only minimal help due to UsageError.\\n\\n\"\n",
        "+                )\n",
        "+\n",
        "+            raise\n",
        "+\n",
        "+        return self\n",
        "+\n",
        "+    def notify_exception(\n",
        "+        self,\n",
        "+        excinfo: ExceptionInfo[BaseException],\n",
        "+        option: argparse.Namespace | None = None,\n",
        "+    ) -> None:\n",
        "+        if option and getattr(option, \"fulltrace\", False):\n",
        "+            style: TracebackStyle = \"long\"\n",
        "+        else:\n",
        "+            style = \"native\"\n",
        "+        excrepr = excinfo.getrepr(\n",
        "+            funcargs=True, showlocals=getattr(option, \"showlocals\", False), style=style\n",
        "+        )\n",
        "+        res = self.hook.pytest_internalerror(excrepr=excrepr, excinfo=excinfo)\n",
        "+        if not any(res):\n",
        "+            for line in str(excrepr).split(\"\\n\"):\n",
        "+                sys.stderr.write(f\"INTERNALERROR> {line}\\n\")\n",
        "+                sys.stderr.flush()\n",
        "+\n",
        "+    def cwd_relative_nodeid(self, nodeid: str) -> str:\n",
        "+        # nodeid's are relative to the rootpath, compute relative to cwd.\n",
        "+        if self.invocation_params.dir != self.rootpath:\n",
        "+            base_path_part, *nodeid_part = nodeid.split(\"::\")\n",
        "+            # Only process path part\n",
        "+            fullpath = self.rootpath / base_path_part\n",
        "+            relative_path = bestrelpath(self.invocation_params.dir, fullpath)\n",
        "+\n",
        "+            nodeid = \"::\".join([relative_path, *nodeid_part])\n",
        "+        return nodeid\n",
        "+\n",
        "+    @classmethod\n",
        "+    def fromdictargs(cls, option_dict, args) -> Config:\n",
        "+        \"\"\"Constructor usable for subprocesses.\"\"\"\n",
        "+        config = get_config(args)\n",
        "+        config.option.__dict__.update(option_dict)\n",
        "+        config.parse(args, addopts=False)\n",
        "+        for x in config.option.plugins:\n",
        "+            config.pluginmanager.consider_pluginarg(x)\n",
        "+        return config\n",
        "+\n",
        "+    def _processopt(self, opt: Argument) -> None:\n",
        "+        for name in opt._short_opts + opt._long_opts:\n",
        "+            self._opt2dest[name] = opt.dest\n",
        "+\n",
        "+        if hasattr(opt, \"default\"):\n",
        "+            if not hasattr(self.option, opt.dest):\n",
        "+                setattr(self.option, opt.dest, opt.default)\n",
        "+\n",
        "+    @hookimpl(trylast=True)\n",
        "+    def pytest_load_initial_conftests(self, early_config: Config) -> None:\n",
        "+        # We haven't fully parsed the command line arguments yet, so\n",
        "+        # early_config.args it not set yet. But we need it for\n",
        "+        # discovering the initial conftests. So \"pre-run\" the logic here.\n",
        "+        # It will be done for real in `parse()`.\n",
        "+        args, args_source = early_config._decide_args(\n",
        "+            args=early_config.known_args_namespace.file_or_dir,\n",
        "+            pyargs=early_config.known_args_namespace.pyargs,\n",
        "+            testpaths=early_config.getini(\"testpaths\"),\n",
        "+            invocation_dir=early_config.invocation_params.dir,\n",
        "+            rootpath=early_config.rootpath,\n",
        "+            warn=False,\n",
        "+        )\n",
        "+        self.pluginmanager._set_initial_conftests(\n",
        "+            args=args,\n",
        "+            pyargs=early_config.known_args_namespace.pyargs,\n",
        "+            noconftest=early_config.known_args_namespace.noconftest,\n",
        "+            rootpath=early_config.rootpath,\n",
        "+            confcutdir=early_config.known_args_namespace.confcutdir,\n",
        "+            invocation_dir=early_config.invocation_params.dir,\n",
        "+            importmode=early_config.known_args_namespace.importmode,\n",
        "+            consider_namespace_packages=early_config.getini(\n",
        "+                \"consider_namespace_packages\"\n",
        "+            ),\n",
        "+        )\n",
        "+\n",
        "+    def _initini(self, args: Sequence[str]) -> None:\n",
        "+        ns, unknown_args = self._parser.parse_known_and_unknown_args(\n",
        "+            args, namespace=copy.copy(self.option)\n",
        "+        )\n",
        "+        rootpath, inipath, inicfg = determine_setup(\n",
        "+            inifile=ns.inifilename,\n",
        "+            args=ns.file_or_dir + unknown_args,\n",
        "+            rootdir_cmd_arg=ns.rootdir or None,\n",
        "+            invocation_dir=self.invocation_params.dir,\n",
        "+        )\n",
        "+        self._rootpath = rootpath\n",
        "+        self._inipath = inipath\n",
        "+        self.inicfg = inicfg\n",
        "+        self._parser.extra_info[\"rootdir\"] = str(self.rootpath)\n",
        "+        self._parser.extra_info[\"inifile\"] = str(self.inipath)\n",
        "+        self._parser.addini(\"addopts\", \"Extra command line options\", \"args\")\n",
        "+        self._parser.addini(\"minversion\", \"Minimally required pytest version\")\n",
        "+        self._parser.addini(\n",
        "+            \"pythonpath\", type=\"paths\", help=\"Add paths to sys.path\", default=[]\n",
        "+        )\n",
        "+        self._parser.addini(\n",
        "+            \"required_plugins\",\n",
        "+            \"Plugins that must be present for pytest to run\",\n",
        "+            type=\"args\",\n",
        "+            default=[],\n",
        "+        )\n",
        "+        self._override_ini = ns.override_ini or ()\n",
        "+\n",
        "+    def _consider_importhook(self, args: Sequence[str]) -> None:\n",
        "+        \"\"\"Install the PEP 302 import hook if using assertion rewriting.\n",
        "+\n",
        "+        Needs to parse the --assert=<mode> option from the commandline\n",
        "+        and find all the installed plugins to mark them for rewriting\n",
        "+        by the importhook.\n",
        "+        \"\"\"\n",
        "+        ns, unknown_args = self._parser.parse_known_and_unknown_args(args)\n",
        "+        mode = getattr(ns, \"assertmode\", \"plain\")\n",
        "+\n",
        "+        disable_autoload = getattr(ns, \"disable_plugin_autoload\", False) or bool(\n",
        "+            os.environ.get(\"PYTEST_DISABLE_PLUGIN_AUTOLOAD\")\n",
        "+        )\n",
        "+        if mode == \"rewrite\":\n",
        "+            import _pytest.assertion\n",
        "+\n",
        "+            try:\n",
        "+                hook = _pytest.assertion.install_importhook(self)\n",
        "+            except SystemError:\n",
        "+                mode = \"plain\"\n",
        "+            else:\n",
        "+                self._mark_plugins_for_rewrite(hook, disable_autoload)\n",
        "+        self._warn_about_missing_assertion(mode)\n",
        "+\n",
        "+    def _mark_plugins_for_rewrite(\n",
        "+        self, hook: AssertionRewritingHook, disable_autoload: bool\n",
        "+    ) -> None:\n",
        "+        \"\"\"Given an importhook, mark for rewrite any top-level\n",
        "+        modules or packages in the distribution package for\n",
        "+        all pytest plugins.\"\"\"\n",
        "+        self.pluginmanager.rewrite_hook = hook\n",
        "+\n",
        "+        if disable_autoload:\n",
        "+            # We don't autoload from distribution package entry points,\n",
        "+            # no need to continue.\n",
        "+            return\n",
        "+\n",
        "+        package_files = (\n",
        "+            str(file)\n",
        "+            for dist in importlib.metadata.distributions()\n",
        "+            if any(ep.group == \"pytest11\" for ep in dist.entry_points)\n",
        "+            for file in dist.files or []\n",
        "+        )\n",
        "+\n",
        "+        for name in _iter_rewritable_modules(package_files):\n",
        "+            hook.mark_rewrite(name)\n",
        "+\n",
        "+    def _configure_python_path(self) -> None:\n",
        "+        # `pythonpath = a b` will set `sys.path` to `[a, b, x, y, z, ...]`\n",
        "+        for path in reversed(self.getini(\"pythonpath\")):\n",
        "+            sys.path.insert(0, str(path))\n",
        "+        self.add_cleanup(self._unconfigure_python_path)\n",
        "+\n",
        "+    def _unconfigure_python_path(self) -> None:\n",
        "+        for path in self.getini(\"pythonpath\"):\n",
        "+            path_str = str(path)\n",
        "+            if path_str in sys.path:\n",
        "+                sys.path.remove(path_str)\n",
        "+\n",
        "+    def _validate_args(self, args: list[str], via: str) -> list[str]:\n",
        "+        \"\"\"Validate known args.\"\"\"\n",
        "+        self._parser._config_source_hint = via  # type: ignore\n",
        "+        try:\n",
        "+            self._parser.parse_known_and_unknown_args(\n",
        "+                args, namespace=copy.copy(self.option)\n",
        "+            )\n",
        "+        finally:\n",
        "+            del self._parser._config_source_hint  # type: ignore\n",
        "+\n",
        "+        return args\n",
        "+\n",
        "+    def _decide_args(\n",
        "+        self,\n",
        "+        *,\n",
        "+        args: list[str],\n",
        "+        pyargs: bool,\n",
        "+        testpaths: list[str],\n",
        "+        invocation_dir: pathlib.Path,\n",
        "+        rootpath: pathlib.Path,\n",
        "+        warn: bool,\n",
        "+    ) -> tuple[list[str], ArgsSource]:\n",
        "+        \"\"\"Decide the args (initial paths/nodeids) to use given the relevant inputs.\n",
        "+\n",
        "+        :param warn: Whether can issue warnings.\n",
        "+\n",
        "+        :returns: The args and the args source. Guaranteed to be non-empty.\n",
        "+        \"\"\"\n",
        "+        if args:\n",
        "+            source = Config.ArgsSource.ARGS\n",
        "+            result = args\n",
        "+        else:\n",
        "+            if invocation_dir == rootpath:\n",
        "+                source = Config.ArgsSource.TESTPATHS\n",
        "+                if pyargs:\n",
        "+                    result = testpaths\n",
        "+                else:\n",
        "+                    result = []\n",
        "+                    for path in testpaths:\n",
        "+                        result.extend(sorted(glob.iglob(path, recursive=True)))\n",
        "+                    if testpaths and not result:\n",
        "+                        if warn:\n",
        "+                            warning_text = (\n",
        "+                                \"No files were found in testpaths; \"\n",
        "+                                \"consider removing or adjusting your testpaths configuration. \"\n",
        "+                                \"Searching recursively from the current directory instead.\"\n",
        "+                            )\n",
        "+                            self.issue_config_time_warning(\n",
        "+                                PytestConfigWarning(warning_text), stacklevel=3\n",
        "+                            )\n",
        "+            else:\n",
        "+                result = []\n",
        "+            if not result:\n",
        "+                source = Config.ArgsSource.INVOCATION_DIR\n",
        "+                result = [str(invocation_dir)]\n",
        "+        return result, source\n",
        "+\n",
        "+    def _preparse(self, args: list[str], addopts: bool = True) -> None:\n",
        "+        if addopts:\n",
        "+            env_addopts = os.environ.get(\"PYTEST_ADDOPTS\", \"\")\n",
        "+            if len(env_addopts):\n",
        "+                args[:] = (\n",
        "+                    self._validate_args(shlex.split(env_addopts), \"via PYTEST_ADDOPTS\")\n",
        "+                    + args\n",
        "+                )\n",
        "+        self._initini(args)\n",
        "+        if addopts:\n",
        "+            args[:] = (\n",
        "+                self._validate_args(self.getini(\"addopts\"), \"via addopts config\") + args\n",
        "+            )\n",
        "+\n",
        "+        self.known_args_namespace = self._parser.parse_known_args(\n",
        "+            args, namespace=copy.copy(self.option)\n",
        "+        )\n",
        "+        self._checkversion()\n",
        "+        self._consider_importhook(args)\n",
        "+        self._configure_python_path()\n",
        "+        self.pluginmanager.consider_preparse(args, exclude_only=False)\n",
        "+        if (\n",
        "+            not os.environ.get(\"PYTEST_DISABLE_PLUGIN_AUTOLOAD\")\n",
        "+            and not self.known_args_namespace.disable_plugin_autoload\n",
        "+        ):\n",
        "+            # Autoloading from distribution package entry point has\n",
        "+            # not been disabled.\n",
        "+            self.pluginmanager.load_setuptools_entrypoints(\"pytest11\")\n",
        "+        # Otherwise only plugins explicitly specified in PYTEST_PLUGINS\n",
        "+        # are going to be loaded.\n",
        "+        self.pluginmanager.consider_env()\n",
        "+\n",
        "+        self.known_args_namespace = self._parser.parse_known_args(\n",
        "+            args, namespace=copy.copy(self.known_args_namespace)\n",
        "+        )\n",
        "+\n",
        "+        self._validate_plugins()\n",
        "+        self._warn_about_skipped_plugins()\n",
        "+\n",
        "+        if self.known_args_namespace.confcutdir is None:\n",
        "+            if self.inipath is not None:\n",
        "+                confcutdir = str(self.inipath.parent)\n",
        "+            else:\n",
        "+                confcutdir = str(self.rootpath)\n",
        "+            self.known_args_namespace.confcutdir = confcutdir\n",
        "+        try:\n",
        "+            self.hook.pytest_load_initial_conftests(\n",
        "+                early_config=self, args=args, parser=self._parser\n",
        "+            )\n",
        "+        except ConftestImportFailure as e:\n",
        "+            if self.known_args_namespace.help or self.known_args_namespace.version:\n",
        "+                # we don't want to prevent --help/--version to work\n",
        "+                # so just let it pass and print a warning at the end\n",
        "+                self.issue_config_time_warning(\n",
        "+                    PytestConfigWarning(f\"could not load initial conftests: {e.path}\"),\n",
        "+                    stacklevel=2,\n",
        "+                )\n",
        "+            else:\n",
        "+                raise\n",
        "+\n",
        "+    @hookimpl(wrapper=True)\n",
        "+    def pytest_collection(self) -> Generator[None, object, object]:\n",
        "+        # Validate invalid ini keys after collection is done so we take in account\n",
        "+        # options added by late-loading conftest files.\n",
        "+        try:\n",
        "+            return (yield)\n",
        "+        finally:\n",
        "+            self._validate_config_options()\n",
        "+\n",
        "+    def _checkversion(self) -> None:\n",
        "+        import pytest\n",
        "+\n",
        "+        minver = self.inicfg.get(\"minversion\", None)\n",
        "+        if minver:\n",
        "+            # Imported lazily to improve start-up time.\n",
        "+            from packaging.version import Version\n",
        "+\n",
        "+            if not isinstance(minver, str):\n",
        "+                raise pytest.UsageError(\n",
        "+                    f\"{self.inipath}: 'minversion' must be a single value\"\n",
        "+                )\n",
        "+\n",
        "+            if Version(minver) > Version(pytest.__version__):\n",
        "+                raise pytest.UsageError(\n",
        "+                    f\"{self.inipath}: 'minversion' requires pytest-{minver}, actual pytest-{pytest.__version__}'\"\n",
        "+                )\n",
        "+\n",
        "+    def _validate_config_options(self) -> None:\n",
        "+        for key in sorted(self._get_unknown_ini_keys()):\n",
        "+            self._warn_or_fail_if_strict(f\"Unknown config option: {key}\\n\")\n",
        "+\n",
        "+    def _validate_plugins(self) -> None:\n",
        "+        required_plugins = sorted(self.getini(\"required_plugins\"))\n",
        "+        if not required_plugins:\n",
        "+            return\n",
        "+\n",
        "+        # Imported lazily to improve start-up time.\n",
        "+        from packaging.requirements import InvalidRequirement\n",
        "+        from packaging.requirements import Requirement\n",
        "+        from packaging.version import Version\n",
        "+\n",
        "+        plugin_info = self.pluginmanager.list_plugin_distinfo()\n",
        "+        plugin_dist_info = {dist.project_name: dist.version for _, dist in plugin_info}\n",
        "+\n",
        "+        missing_plugins = []\n",
        "+        for required_plugin in required_plugins:\n",
        "+            try:\n",
        "+                req = Requirement(required_plugin)\n",
        "+            except InvalidRequirement:\n",
        "+                missing_plugins.append(required_plugin)\n",
        "+                continue\n",
        "+\n",
        "+            if req.name not in plugin_dist_info:\n",
        "+                missing_plugins.append(required_plugin)\n",
        "+            elif not req.specifier.contains(\n",
        "+                Version(plugin_dist_info[req.name]), prereleases=True\n",
        "+            ):\n",
        "+                missing_plugins.append(required_plugin)\n",
        "+\n",
        "+        if missing_plugins:\n",
        "+            raise UsageError(\n",
        "+                \"Missing required plugins: {}\".format(\", \".join(missing_plugins)),\n",
        "+            )\n",
        "+\n",
        "+    def _warn_or_fail_if_strict(self, message: str) -> None:\n",
        "+        if self.known_args_namespace.strict_config:\n",
        "+            raise UsageError(message)\n",
        "+\n",
        "+        self.issue_config_time_warning(PytestConfigWarning(message), stacklevel=3)\n",
        "+\n",
        "+    def _get_unknown_ini_keys(self) -> list[str]:\n",
        "+        parser_inicfg = self._parser._inidict\n",
        "+        return [name for name in self.inicfg if name not in parser_inicfg]\n",
        "+\n",
        "+    def parse(self, args: list[str], addopts: bool = True) -> None:\n",
        "+        # Parse given cmdline arguments into this config object.\n",
        "+        assert self.args == [], (\n",
        "+            \"can only parse cmdline args at most once per Config object\"\n",
        "+        )\n",
        "+        self.hook.pytest_addhooks.call_historic(\n",
        "+            kwargs=dict(pluginmanager=self.pluginmanager)\n",
        "+        )\n",
        "+        self._preparse(args, addopts=addopts)\n",
        "+        self._parser.after_preparse = True  # type: ignore\n",
        "+        try:\n",
        "+            args = self._parser.parse_setoption(\n",
        "+                args, self.option, namespace=self.option\n",
        "+            )\n",
        "+            self.args, self.args_source = self._decide_args(\n",
        "+                args=args,\n",
        "+                pyargs=self.known_args_namespace.pyargs,\n",
        "+                testpaths=self.getini(\"testpaths\"),\n",
        "+                invocation_dir=self.invocation_params.dir,\n",
        "+                rootpath=self.rootpath,\n",
        "+                warn=True,\n",
        "+            )\n",
        "+        except PrintHelp:\n",
        "+            pass\n",
        "+\n",
        "+    def issue_config_time_warning(self, warning: Warning, stacklevel: int) -> None:\n",
        "+        \"\"\"Issue and handle a warning during the \"configure\" stage.\n",
        "+\n",
        "+        During ``pytest_configure`` we can't capture warnings using the ``catch_warnings_for_item``\n",
        "+        function because it is not possible to have hook wrappers around ``pytest_configure``.\n",
        "+\n",
        "+        This function is mainly intended for plugins that need to issue warnings during\n",
        "+        ``pytest_configure`` (or similar stages).\n",
        "+\n",
        "+        :param warning: The warning instance.\n",
        "+        :param stacklevel: stacklevel forwarded to warnings.warn.\n",
        "+        \"\"\"\n",
        "+        if self.pluginmanager.is_blocked(\"warnings\"):\n",
        "+            return\n",
        "+\n",
        "+        cmdline_filters = self.known_args_namespace.pythonwarnings or []\n",
        "+        config_filters = self.getini(\"filterwarnings\")\n",
        "+\n",
        "+        with warnings.catch_warnings(record=True) as records:\n",
        "+            warnings.simplefilter(\"always\", type(warning))\n",
        "+            apply_warning_filters(config_filters, cmdline_filters)\n",
        "+            warnings.warn(warning, stacklevel=stacklevel)\n",
        "+\n",
        "+        if records:\n",
        "+            frame = sys._getframe(stacklevel - 1)\n",
        "+            location = frame.f_code.co_filename, frame.f_lineno, frame.f_code.co_name\n",
        "+            self.hook.pytest_warning_recorded.call_historic(\n",
        "+                kwargs=dict(\n",
        "+                    warning_message=records[0],\n",
        "+                    when=\"config\",\n",
        "+                    nodeid=\"\",\n",
        "+                    location=location,\n",
        "+                )\n",
        "+            )\n",
        "+\n",
        "+    def addinivalue_line(self, name: str, line: str) -> None:\n",
        "+        \"\"\"Add a line to an ini-file option. The option must have been\n",
        "+        declared but might not yet be set in which case the line becomes\n",
        "+        the first line in its value.\"\"\"\n",
        "+        x = self.getini(name)\n",
        "+        assert isinstance(x, list)\n",
        "+        x.append(line)  # modifies the cached list inline\n",
        "+\n",
        "+    def getini(self, name: str) -> Any:\n",
        "+        \"\"\"Return configuration value from an :ref:`ini file <configfiles>`.\n",
        "+\n",
        "+        If a configuration value is not defined in an\n",
        "+        :ref:`ini file <configfiles>`, then the ``default`` value provided while\n",
        "+        registering the configuration through\n",
        "+        :func:`parser.addini <pytest.Parser.addini>` will be returned.\n",
        "+        Please note that you can even provide ``None`` as a valid\n",
        "+        default value.\n",
        "+\n",
        "+        If ``default`` is not provided while registering using\n",
        "+        :func:`parser.addini <pytest.Parser.addini>`, then a default value\n",
        "+        based on the ``type`` parameter passed to\n",
        "+        :func:`parser.addini <pytest.Parser.addini>` will be returned.\n",
        "+        The default values based on ``type`` are:\n",
        "+        ``paths``, ``pathlist``, ``args`` and ``linelist`` : empty list ``[]``\n",
        "+        ``bool`` : ``False``\n",
        "+        ``string`` : empty string ``\"\"``\n",
        "+        ``int`` : ``0``\n",
        "+        ``float`` : ``0.0``\n",
        "+\n",
        "+        If neither the ``default`` nor the ``type`` parameter is passed\n",
        "+        while registering the configuration through\n",
        "+        :func:`parser.addini <pytest.Parser.addini>`, then the configuration\n",
        "+        is treated as a string and a default empty string '' is returned.\n",
        "+\n",
        "+        If the specified name hasn't been registered through a prior\n",
        "+        :func:`parser.addini <pytest.Parser.addini>` call (usually from a\n",
        "+        plugin), a ValueError is raised.\n",
        "+        \"\"\"\n",
        "+        try:\n",
        "+            return self._inicache[name]\n",
        "+        except KeyError:\n",
        "+            self._inicache[name] = val = self._getini(name)\n",
        "+            return val\n",
        "+\n",
        "+    # Meant for easy monkeypatching by legacypath plugin.\n",
        "+    # Can be inlined back (with no cover removed) once legacypath is gone.\n",
        "+    def _getini_unknown_type(self, name: str, type: str, value: object):\n",
        "+        msg = (\n",
        "+            f\"Option {name} has unknown configuration type {type} with value {value!r}\"\n",
        "+        )\n",
        "+        raise ValueError(msg)  # pragma: no cover\n",
        "+\n",
        "+    def _getini(self, name: str):\n",
        "+        try:\n",
        "+            description, type, default = self._parser._inidict[name]\n",
        "+        except KeyError as e:\n",
        "+            raise ValueError(f\"unknown configuration value: {name!r}\") from e\n",
        "+        override_value = self._get_override_ini_value(name)\n",
        "+        if override_value is None:\n",
        "+            try:\n",
        "+                value = self.inicfg[name]\n",
        "+            except KeyError:\n",
        "+                return default\n",
        "+        else:\n",
        "+            value = override_value\n",
        "+        # Coerce the values based on types.\n",
        "+        #\n",
        "+        # Note: some coercions are only required if we are reading from .ini files, because\n",
        "+        # the file format doesn't contain type information, but when reading from toml we will\n",
        "+        # get either str or list of str values (see _parse_ini_config_from_pyproject_toml).\n",
        "+        # For example:\n",
        "+        #\n",
        "+        #   ini:\n",
        "+        #     a_line_list = \"tests acceptance\"\n",
        "+        #   in this case, we need to split the string to obtain a list of strings.\n",
        "+        #\n",
        "+        #   toml:\n",
        "+        #     a_line_list = [\"tests\", \"acceptance\"]\n",
        "+        #   in this case, we already have a list ready to use.\n",
        "+        #\n",
        "+        if type == \"paths\":\n",
        "+            dp = (\n",
        "+                self.inipath.parent\n",
        "+                if self.inipath is not None\n",
        "+                else self.invocation_params.dir\n",
        "+            )\n",
        "+            input_values = shlex.split(value) if isinstance(value, str) else value\n",
        "+            return [dp / x for x in input_values]\n",
        "+        elif type == \"args\":\n",
        "+            return shlex.split(value) if isinstance(value, str) else value\n",
        "+        elif type == \"linelist\":\n",
        "+            if isinstance(value, str):\n",
        "+                return [t for t in map(lambda x: x.strip(), value.split(\"\\n\")) if t]\n",
        "+            else:\n",
        "+                return value\n",
        "+        elif type == \"bool\":\n",
        "+            return _strtobool(str(value).strip())\n",
        "+        elif type == \"string\":\n",
        "+            return value\n",
        "+        elif type == \"int\":\n",
        "+            if not isinstance(value, str):\n",
        "+                raise TypeError(\n",
        "+                    f\"Expected an int string for option {name} of type integer, but got: {value!r}\"\n",
        "+                ) from None\n",
        "+            return int(value)\n",
        "+        elif type == \"float\":\n",
        "+            if not isinstance(value, str):\n",
        "+                raise TypeError(\n",
        "+                    f\"Expected a float string for option {name} of type float, but got: {value!r}\"\n",
        "+                ) from None\n",
        "+            return float(value)\n",
        "+        elif type is None:\n",
        "+            return value\n",
        "+        else:\n",
        "+            return self._getini_unknown_type(name, type, value)\n",
        "+\n",
        "+    def _getconftest_pathlist(\n",
        "+        self, name: str, path: pathlib.Path\n",
        "+    ) -> list[pathlib.Path] | None:\n",
        "+        try:\n",
        "+            mod, relroots = self.pluginmanager._rget_with_confmod(name, path)\n",
        "+        except KeyError:\n",
        "+            return None\n",
        "+        assert mod.__file__ is not None\n",
        "+        modpath = pathlib.Path(mod.__file__).parent\n",
        "+        values: list[pathlib.Path] = []\n",
        "+        for relroot in relroots:\n",
        "+            if isinstance(relroot, os.PathLike):\n",
        "+                relroot = pathlib.Path(relroot)\n",
        "+            else:\n",
        "+                relroot = relroot.replace(\"/\", os.sep)\n",
        "+                relroot = absolutepath(modpath / relroot)\n",
        "+            values.append(relroot)\n",
        "+        return values\n",
        "+\n",
        "+    def _get_override_ini_value(self, name: str) -> str | None:\n",
        "+        value = None\n",
        "+        # override_ini is a list of \"ini=value\" options.\n",
        "+        # Always use the last item if multiple values are set for same ini-name,\n",
        "+        # e.g. -o foo=bar1 -o foo=bar2 will set foo to bar2.\n",
        "+        for ini_config in self._override_ini:\n",
        "+            try:\n",
        "+                key, user_ini_value = ini_config.split(\"=\", 1)\n",
        "+            except ValueError as e:\n",
        "+                raise UsageError(\n",
        "+                    f\"-o/--override-ini expects option=value style (got: {ini_config!r}).\"\n",
        "+                ) from e\n",
        "+            else:\n",
        "+                if key == name:\n",
        "+                    value = user_ini_value\n",
        "+        return value\n",
        "+\n",
        "+    def getoption(self, name: str, default: Any = notset, skip: bool = False):\n",
        "+        \"\"\"Return command line option value.\n",
        "+\n",
        "+        :param name: Name of the option. You may also specify\n",
        "+            the literal ``--OPT`` option instead of the \"dest\" option name.\n",
        "+        :param default: Fallback value if no option of that name is **declared** via :hook:`pytest_addoption`.\n",
        "+            Note this parameter will be ignored when the option is **declared** even if the option's value is ``None``.\n",
        "+        :param skip: If ``True``, raise :func:`pytest.skip` if option is undeclared or has a ``None`` value.\n",
        "+            Note that even if ``True``, if a default was specified it will be returned instead of a skip.\n",
        "+        \"\"\"\n",
        "+        name = self._opt2dest.get(name, name)\n",
        "+        try:\n",
        "+            val = getattr(self.option, name)\n",
        "+            if val is None and skip:\n",
        "+                raise AttributeError(name)\n",
        "+            return val\n",
        "+        except AttributeError as e:\n",
        "+            if default is not notset:\n",
        "+                return default\n",
        "+            if skip:\n",
        "+                import pytest\n",
        "+\n",
        "+                pytest.skip(f\"no {name!r} option found\")\n",
        "+            raise ValueError(f\"no option named {name!r}\") from e\n",
        "+\n",
        "+    def getvalue(self, name: str, path=None):\n",
        "+        \"\"\"Deprecated, use getoption() instead.\"\"\"\n",
        "+        return self.getoption(name)\n",
        "+\n",
        "+    def getvalueorskip(self, name: str, path=None):\n",
        "+        \"\"\"Deprecated, use getoption(skip=True) instead.\"\"\"\n",
        "+        return self.getoption(name, skip=True)\n",
        "+\n",
        "+    #: Verbosity type for failed assertions (see :confval:`verbosity_assertions`).\n",
        "+    VERBOSITY_ASSERTIONS: Final = \"assertions\"\n",
        "+    #: Verbosity type for test case execution (see :confval:`verbosity_test_cases`).\n",
        "+    VERBOSITY_TEST_CASES: Final = \"test_cases\"\n",
        "+    _VERBOSITY_INI_DEFAULT: Final = \"auto\"\n",
        "+\n",
        "+    def get_verbosity(self, verbosity_type: str | None = None) -> int:\n",
        "+        r\"\"\"Retrieve the verbosity level for a fine-grained verbosity type.\n",
        "+\n",
        "+        :param verbosity_type: Verbosity type to get level for. If a level is\n",
        "+            configured for the given type, that value will be returned. If the\n",
        "+            given type is not a known verbosity type, the global verbosity\n",
        "+            level will be returned. If the given type is None (default), the\n",
        "+            global verbosity level will be returned.\n",
        "+\n",
        "+        To configure a level for a fine-grained verbosity type, the\n",
        "+        configuration file should have a setting for the configuration name\n",
        "+        and a numeric value for the verbosity level. A special value of \"auto\"\n",
        "+        can be used to explicitly use the global verbosity level.\n",
        "+\n",
        "+        Example:\n",
        "+\n",
        "+        .. code-block:: ini\n",
        "+\n",
        "+            # content of pytest.ini\n",
        "+            [pytest]\n",
        "+            verbosity_assertions = 2\n",
        "+\n",
        "+        .. code-block:: console\n",
        "+\n",
        "+            pytest -v\n",
        "+\n",
        "+        .. code-block:: python\n",
        "+\n",
        "+            print(config.get_verbosity())  # 1\n",
        "+            print(config.get_verbosity(Config.VERBOSITY_ASSERTIONS))  # 2\n",
        "+        \"\"\"\n",
        "+        global_level = self.getoption(\"verbose\", default=0)\n",
        "+        assert isinstance(global_level, int)\n",
        "+        if verbosity_type is None:\n",
        "+            return global_level\n",
        "+\n",
        "+        ini_name = Config._verbosity_ini_name(verbosity_type)\n",
        "+        if ini_name not in self._parser._inidict:\n",
        "+            return global_level\n",
        "+\n",
        "+        level = self.getini(ini_name)\n",
        "+        if level == Config._VERBOSITY_INI_DEFAULT:\n",
        "+            return global_level\n",
        "+\n",
        "+        return int(level)\n",
        "+\n",
        "+    @staticmethod\n",
        "+    def _verbosity_ini_name(verbosity_type: str) -> str:\n",
        "+        return f\"verbosity_{verbosity_type}\"\n",
        "+\n",
        "+    @staticmethod\n",
        "+    def _add_verbosity_ini(parser: Parser, verbosity_type: str, help: str) -> None:\n",
        "+        \"\"\"Add a output verbosity configuration option for the given output type.\n",
        "+\n",
        "+        :param parser: Parser for command line arguments and ini-file values.\n",
        "+        :param verbosity_type: Fine-grained verbosity category.\n",
        "+        :param help: Description of the output this type controls.\n",
        "+\n",
        "+        The value should be retrieved via a call to\n",
        "+        :py:func:`config.get_verbosity(type) <pytest.Config.get_verbosity>`.\n",
        "+        \"\"\"\n",
        "+        parser.addini(\n",
        "+            Config._verbosity_ini_name(verbosity_type),\n",
        "+            help=help,\n",
        "+            type=\"string\",\n",
        "+            default=Config._VERBOSITY_INI_DEFAULT,\n",
        "+        )\n",
        "+\n",
        "+    def _warn_about_missing_assertion(self, mode: str) -> None:\n",
        "+        if not _assertion_supported():\n",
        "+            if mode == \"plain\":\n",
        "+                warning_text = (\n",
        "+                    \"ASSERTIONS ARE NOT EXECUTED\"\n",
        "+                    \" and FAILING TESTS WILL PASS.  Are you\"\n",
        "+                    \" using python -O?\"\n",
        "+                )\n",
        "+            else:\n",
        "+                warning_text = (\n",
        "+                    \"assertions not in test modules or\"\n",
        "+                    \" plugins will be ignored\"\n",
        "+                    \" because assert statements are not executed \"\n",
        "+                    \"by the underlying Python interpreter \"\n",
        "+                    \"(are you using python -O?)\\n\"\n",
        "+                )\n",
        "+            self.issue_config_time_warning(\n",
        "+                PytestConfigWarning(warning_text),\n",
        "+                stacklevel=3,\n",
        "+            )\n",
        "+\n",
        "+    def _warn_about_skipped_plugins(self) -> None:\n",
        "+        for module_name, msg in self.pluginmanager.skipped_plugins:\n",
        "+            self.issue_config_time_warning(\n",
        "+                PytestConfigWarning(f\"skipped plugin {module_name!r}: {msg}\"),\n",
        "+                stacklevel=2,\n",
        "+            )\n",
        "+\n",
        "+\n",
        "+def _assertion_supported() -> bool:\n",
        "+    try:\n",
        "+        assert False\n",
        "+    except AssertionError:\n",
        "+        return True\n",
        "+    else:\n",
        "+        return False  # type: ignore[unreachable]\n",
        "+\n",
        "+\n",
        "+def create_terminal_writer(\n",
        "+    config: Config, file: TextIO | None = None\n",
        "+) -> TerminalWriter:\n",
        "+    \"\"\"Create a TerminalWriter instance configured according to the options\n",
        "+    in the config object.\n",
        "+\n",
        "+    Every code which requires a TerminalWriter object and has access to a\n",
        "+    config object should use this function.\n",
        "+    \"\"\"\n",
        "+    tw = TerminalWriter(file=file)\n",
        "+\n",
        "+    if config.option.color == \"yes\":\n",
        "+        tw.hasmarkup = True\n",
        "+    elif config.option.color == \"no\":\n",
        "+        tw.hasmarkup = False\n",
        "+\n",
        "+    if config.option.code_highlight == \"yes\":\n",
        "+        tw.code_highlight = True\n",
        "+    elif config.option.code_highlight == \"no\":\n",
        "+        tw.code_highlight = False\n",
        "+\n",
        "+    return tw\n",
        "+\n",
        "+\n",
        "+def _strtobool(val: str) -> bool:\n",
        "+    \"\"\"Convert a string representation of truth to True or False.\n",
        "+\n",
        "+    True values are 'y', 'yes', 't', 'true', 'on', and '1'; false values\n",
        "+    are 'n', 'no', 'f', 'false', 'off', and '0'.  Raises ValueError if\n",
        "+    'val' is anything else.\n",
        "+\n",
        "+    .. note:: Copied from distutils.util.\n",
        "+    \"\"\"\n",
        "+    val = val.lower()\n",
        "+    if val in (\"y\", \"yes\", \"t\", \"true\", \"on\", \"1\"):\n",
        "+        return True\n",
        "+    elif val in (\"n\", \"no\", \"f\", \"false\", \"off\", \"0\"):\n",
        "+        return False\n",
        "+    else:\n",
        "+        raise ValueError(f\"invalid truth value {val!r}\")\n",
        "+\n",
        "+\n",
        "+@lru_cache(maxsize=50)\n",
        "+def parse_warning_filter(\n",
        "+    arg: str, *, escape: bool\n",
        "+) -> tuple[warnings._ActionKind, str, type[Warning], str, int]:\n",
        "+    \"\"\"Parse a warnings filter string.\n",
        "+\n",
        "+    This is copied from warnings._setoption with the following changes:\n",
        "+\n",
        "+    * Does not apply the filter.\n",
        "+    * Escaping is optional.\n",
        "+    * Raises UsageError so we get nice error messages on failure.\n",
        "+    \"\"\"\n",
        "+    __tracebackhide__ = True\n",
        "+    error_template = dedent(\n",
        "+        f\"\"\"\\\n",
        "+        while parsing the following warning configuration:\n",
        "+\n",
        "+          {arg}\n",
        "+\n",
        "+        This error occurred:\n",
        "+\n",
        "+        {{error}}\n",
        "+        \"\"\"\n",
        "+    )\n",
        "+\n",
        "+    parts = arg.split(\":\")\n",
        "+    if len(parts) > 5:\n",
        "+        doc_url = (\n",
        "+            \"https://docs.python.org/3/library/warnings.html#describing-warning-filters\"\n",
        "+        )\n",
        "+        error = dedent(\n",
        "+            f\"\"\"\\\n",
        "+            Too many fields ({len(parts)}), expected at most 5 separated by colons:\n",
        "+\n",
        "+              action:message:category:module:line\n",
        "+\n",
        "+            For more information please consult: {doc_url}\n",
        "+            \"\"\"\n",
        "+        )\n",
        "+        raise UsageError(error_template.format(error=error))\n",
        "+\n",
        "+    while len(parts) < 5:\n",
        "+        parts.append(\"\")\n",
        "+    action_, message, category_, module, lineno_ = (s.strip() for s in parts)\n",
        "+    try:\n",
        "+        action: warnings._ActionKind = warnings._getaction(action_)  # type: ignore[attr-defined]\n",
        "+    except warnings._OptionError as e:\n",
        "+        raise UsageError(error_template.format(error=str(e))) from None\n",
        "+    try:\n",
        "+        category: type[Warning] = _resolve_warning_category(category_)\n",
        "+    except Exception:\n",
        "+        exc_info = ExceptionInfo.from_current()\n",
        "+        exception_text = exc_info.getrepr(style=\"native\")\n",
        "+        raise UsageError(error_template.format(error=exception_text)) from None\n",
        "+    if message and escape:\n",
        "+        message = re.escape(message)\n",
        "+    if module and escape:\n",
        "+        module = re.escape(module) + r\"\\Z\"\n",
        "+    if lineno_:\n",
        "+        try:\n",
        "+            lineno = int(lineno_)\n",
        "+            if lineno < 0:\n",
        "+                raise ValueError(\"number is negative\")\n",
        "+        except ValueError as e:\n",
        "+            raise UsageError(\n",
        "+                error_template.format(error=f\"invalid lineno {lineno_!r}: {e}\")\n",
        "+            ) from None\n",
        "+    else:\n",
        "+        lineno = 0\n",
        "+    try:\n",
        "+        re.compile(message)\n",
        "+        re.compile(module)\n",
        "+    except re.error as e:\n",
        "+        raise UsageError(\n",
        "+            error_template.format(error=f\"Invalid regex {e.pattern!r}: {e}\")\n",
        "+        ) from None\n",
        "+    return action, message, category, module, lineno\n",
        "+\n",
        "+\n",
        "+def _resolve_warning_category(category: str) -> type[Warning]:\n",
        "+    \"\"\"\n",
        "+    Copied from warnings._getcategory, but changed so it lets exceptions (specially ImportErrors)\n",
        "+    propagate so we can get access to their tracebacks (#9218).\n",
        "+    \"\"\"\n",
        "+    __tracebackhide__ = True\n",
        "+    if not category:\n",
        "+        return Warning\n",
        "+\n",
        "+    if \".\" not in category:\n",
        "+        import builtins as m\n",
        "+\n",
        "+        klass = category\n",
        "+    else:\n",
        "+        module, _, klass = category.rpartition(\".\")\n",
        "+        m = __import__(module, None, None, [klass])\n",
        "+    cat = getattr(m, klass)\n",
        "+    if not issubclass(cat, Warning):\n",
        "+        raise UsageError(f\"{cat} is not a Warning subclass\")\n",
        "+    return cast(type[Warning], cat)\n",
        "+\n",
        "+\n",
        "+def apply_warning_filters(\n",
        "+    config_filters: Iterable[str], cmdline_filters: Iterable[str]\n",
        "+) -> None:\n",
        "+    \"\"\"Applies pytest-configured filters to the warnings module\"\"\"\n",
        "+    # Filters should have this precedence: cmdline options, config.\n",
        "+    # Filters should be applied in the inverse order of precedence.\n",
        "+    for arg in config_filters:\n",
        "+        warnings.filterwarnings(*parse_warning_filter(arg, escape=False))\n",
        "+\n",
        "+    for arg in cmdline_filters:\n",
        "+        warnings.filterwarnings(*parse_warning_filter(arg, escape=True))\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/config/argparsing.py",
      "status": "added",
      "additions": 533,
      "deletions": 0,
      "patch": "@@ -0,0 +1,533 @@\n+# mypy: allow-untyped-defs\n+from __future__ import annotations\n+\n+import argparse\n+from collections.abc import Callable\n+from collections.abc import Mapping\n+from collections.abc import Sequence\n+import os\n+from typing import Any\n+from typing import cast\n+from typing import final\n+from typing import Literal\n+from typing import NoReturn\n+\n+import _pytest._io\n+from _pytest.config.exceptions import UsageError\n+from _pytest.deprecated import check_ispytest\n+\n+\n+FILE_OR_DIR = \"file_or_dir\"\n+\n+\n+class NotSet:\n+    def __repr__(self) -> str:\n+        return \"<notset>\"\n+\n+\n+NOT_SET = NotSet()\n+\n+\n+@final\n+class Parser:\n+    \"\"\"Parser for command line arguments and ini-file values.\n+\n+    :ivar extra_info: Dict of generic param -> value to display in case\n+        there's an error processing the command line arguments.\n+    \"\"\"\n+\n+    prog: str | None = None\n+\n+    def __init__(\n+        self,\n+        usage: str | None = None,\n+        processopt: Callable[[Argument], None] | None = None,\n+        *,\n+        _ispytest: bool = False,\n+    ) -> None:\n+        check_ispytest(_ispytest)\n+        self._anonymous = OptionGroup(\"Custom options\", parser=self, _ispytest=True)\n+        self._groups: list[OptionGroup] = []\n+        self._processopt = processopt\n+        self._usage = usage\n+        self._inidict: dict[str, tuple[str, str | None, Any]] = {}\n+        self._ininames: list[str] = []\n+        self.extra_info: dict[str, Any] = {}\n+\n+    def processoption(self, option: Argument) -> None:\n+        if self._processopt:\n+            if option.dest:\n+                self._processopt(option)\n+\n+    def getgroup(\n+        self, name: str, description: str = \"\", after: str | None = None\n+    ) -> OptionGroup:\n+        \"\"\"Get (or create) a named option Group.\n+\n+        :param name: Name of the option group.\n+        :param description: Long description for --help output.\n+        :param after: Name of another group, used for ordering --help output.\n+        :returns: The option group.\n+\n+        The returned group object has an ``addoption`` method with the same\n+        signature as :func:`parser.addoption <pytest.Parser.addoption>` but\n+        will be shown in the respective group in the output of\n+        ``pytest --help``.\n+        \"\"\"\n+        for group in self._groups:\n+            if group.name == name:\n+                return group\n+        group = OptionGroup(name, description, parser=self, _ispytest=True)\n+        i = 0\n+        for i, grp in enumerate(self._groups):\n+            if grp.name == after:\n+                break\n+        self._groups.insert(i + 1, group)\n+        return group\n+\n+    def addoption(self, *opts: str, **attrs: Any) -> None:\n+        \"\"\"Register a command line option.\n+\n+        :param opts:\n+            Option names, can be short or long options.\n+        :param attrs:\n+            Same attributes as the argparse library's :meth:`add_argument()\n+            <argparse.ArgumentParser.add_argument>` function accepts.\n+\n+        After command line parsing, options are available on the pytest config\n+        object via ``config.option.NAME`` where ``NAME`` is usually set\n+        by passing a ``dest`` attribute, for example\n+        ``addoption(\"--long\", dest=\"NAME\", ...)``.\n+        \"\"\"\n+        self._anonymous.addoption(*opts, **attrs)\n+\n+    def parse(\n+        self,\n+        args: Sequence[str | os.PathLike[str]],\n+        namespace: argparse.Namespace | None = None,\n+    ) -> argparse.Namespace:\n+        from _pytest._argcomplete import try_argcomplete\n+\n+        self.optparser = self._getparser()\n+        try_argcomplete(self.optparser)\n+        strargs = [os.fspath(x) for x in args]\n+        return self.optparser.parse_args(strargs, namespace=namespace)\n+\n+    def _getparser(self) -> MyOptionParser:\n+        from _pytest._argcomplete import filescompleter\n+\n+        optparser = MyOptionParser(self, self.extra_info, prog=self.prog)\n+        groups = [*self._groups, self._anonymous]\n+        for group in groups:\n+            if group.options:\n+                desc = group.description or group.name\n+                arggroup = optparser.add_argument_group(desc)\n+                for option in group.options:\n+                    n = option.names()\n+                    a = option.attrs()\n+                    arggroup.add_argument(*n, **a)\n+        file_or_dir_arg = optparser.add_argument(FILE_OR_DIR, nargs=\"*\")\n+        # bash like autocompletion for dirs (appending '/')\n+        # Type ignored because typeshed doesn't know about argcomplete.\n+        file_or_dir_arg.completer = filescompleter  # type: ignore\n+        return optparser\n+\n+    def parse_setoption(\n+        self,\n+        args: Sequence[str | os.PathLike[str]],\n+        option: argparse.Namespace,\n+        namespace: argparse.Namespace | None = None,\n+    ) -> list[str]:\n+        parsedoption = self.parse(args, namespace=namespace)\n+        for name, value in parsedoption.__dict__.items():\n+            setattr(option, name, value)\n+        return cast(list[str], getattr(parsedoption, FILE_OR_DIR))\n+\n+    def parse_known_args(\n+        self,\n+        args: Sequence[str | os.PathLike[str]],\n+        namespace: argparse.Namespace | None = None,\n+    ) -> argparse.Namespace:\n+        \"\"\"Parse the known arguments at this point.\n+\n+        :returns: An argparse namespace object.\n+        \"\"\"\n+        return self.parse_known_and_unknown_args(args, namespace=namespace)[0]\n+\n+    def parse_known_and_unknown_args(\n+        self,\n+        args: Sequence[str | os.PathLike[str]],\n+        namespace: argparse.Namespace | None = None,\n+    ) -> tuple[argparse.Namespace, list[str]]:\n+        \"\"\"Parse the known arguments at this point, and also return the\n+        remaining unknown arguments.\n+\n+        :returns:\n+            A tuple containing an argparse namespace object for the known\n+            arguments, and a list of the unknown arguments.\n+        \"\"\"\n+        optparser = self._getparser()\n+        strargs = [os.fspath(x) for x in args]\n+        return optparser.parse_known_args(strargs, namespace=namespace)\n+\n+    def addini(\n+        self,\n+        name: str,\n+        help: str,\n+        type: Literal[\"string\", \"paths\", \"pathlist\", \"args\", \"linelist\", \"bool\"]\n+        | None = None,\n+        default: Any = NOT_SET,\n+    ) -> None:\n+        \"\"\"Register an ini-file option.\n+\n+        :param name:\n+            Name of the ini-variable.\n+        :param type:\n+            Type of the variable. Can be:\n+\n+                * ``string``: a string\n+                * ``bool``: a boolean\n+                * ``args``: a list of strings, separated as in a shell\n+                * ``linelist``: a list of strings, separated by line breaks\n+                * ``paths``: a list of :class:`pathlib.Path`, separated as in a shell\n+                * ``pathlist``: a list of ``py.path``, separated as in a shell\n+                * ``int``: an integer\n+                * ``float``: a floating-point number\n+\n+                .. versionadded:: 8.4\n+\n+                    The ``float`` and ``int`` types.\n+\n+            For ``paths`` and ``pathlist`` types, they are considered relative to the ini-file.\n+            In case the execution is happening without an ini-file defined,\n+            they will be considered relative to the current working directory (for example with ``--override-ini``).\n+\n+            .. versionadded:: 7.0\n+                The ``paths`` variable type.\n+\n+            .. versionadded:: 8.1\n+                Use the current working directory to resolve ``paths`` and ``pathlist`` in the absence of an ini-file.\n+\n+            Defaults to ``string`` if ``None`` or not passed.\n+        :param default:\n+            Default value if no ini-file option exists but is queried.\n+\n+        The value of ini-variables can be retrieved via a call to\n+        :py:func:`config.getini(name) <pytest.Config.getini>`.\n+        \"\"\"\n+        assert type in (\n+            None,\n+            \"string\",\n+            \"paths\",\n+            \"pathlist\",\n+            \"args\",\n+            \"linelist\",\n+            \"bool\",\n+            \"int\",\n+            \"float\",\n+        )\n+        if default is NOT_SET:\n+            default = get_ini_default_for_type(type)\n+\n+        self._inidict[name] = (help, type, default)\n+        self._ininames.append(name)\n+\n+\n+def get_ini_default_for_type(\n+    type: Literal[\n+        \"string\", \"paths\", \"pathlist\", \"args\", \"linelist\", \"bool\", \"int\", \"float\"\n+    ]\n+    | None,\n+) -> Any:\n+    \"\"\"\n+    Used by addini to get the default value for a given ini-option type, when\n+    default is not supplied.\n+    \"\"\"\n+    if type is None:\n+        return \"\"\n+    elif type in (\"paths\", \"pathlist\", \"args\", \"linelist\"):\n+        return []\n+    elif type == \"bool\":\n+        return False\n+    elif type == \"int\":\n+        return 0\n+    elif type == \"float\":\n+        return 0.0\n+    else:\n+        return \"\"\n+\n+\n+class ArgumentError(Exception):\n+    \"\"\"Raised if an Argument instance is created with invalid or\n+    inconsistent arguments.\"\"\"\n+\n+    def __init__(self, msg: str, option: Argument | str) -> None:\n+        self.msg = msg\n+        self.option_id = str(option)\n+\n+    def __str__(self) -> str:\n+        if self.option_id:\n+            return f\"option {self.option_id}: {self.msg}\"\n+        else:\n+            return self.msg\n+\n+\n+class Argument:\n+    \"\"\"Class that mimics the necessary behaviour of optparse.Option.\n+\n+    It's currently a least effort implementation and ignoring choices\n+    and integer prefixes.\n+\n+    https://docs.python.org/3/library/optparse.html#optparse-standard-option-types\n+    \"\"\"\n+\n+    def __init__(self, *names: str, **attrs: Any) -> None:\n+        \"\"\"Store params in private vars for use in add_argument.\"\"\"\n+        self._attrs = attrs\n+        self._short_opts: list[str] = []\n+        self._long_opts: list[str] = []\n+        try:\n+            self.type = attrs[\"type\"]\n+        except KeyError:\n+            pass\n+        try:\n+            # Attribute existence is tested in Config._processopt.\n+            self.default = attrs[\"default\"]\n+        except KeyError:\n+            pass\n+        self._set_opt_strings(names)\n+        dest: str | None = attrs.get(\"dest\")\n+        if dest:\n+            self.dest = dest\n+        elif self._long_opts:\n+            self.dest = self._long_opts[0][2:].replace(\"-\", \"_\")\n+        else:\n+            try:\n+                self.dest = self._short_opts[0][1:]\n+            except IndexError as e:\n+                self.dest = \"???\"  # Needed for the error repr.\n+                raise ArgumentError(\"need a long or short option\", self) from e\n+\n+    def names(self) -> list[str]:\n+        return self._short_opts + self._long_opts\n+\n+    def attrs(self) -> Mapping[str, Any]:\n+        # Update any attributes set by processopt.\n+        attrs = \"default dest help\".split()\n+        attrs.append(self.dest)\n+        for attr in attrs:\n+            try:\n+                self._attrs[attr] = getattr(self, attr)\n+            except AttributeError:\n+                pass\n+        return self._attrs\n+\n+    def _set_opt_strings(self, opts: Sequence[str]) -> None:\n+        \"\"\"Directly from optparse.\n+\n+        Might not be necessary as this is passed to argparse later on.\n+        \"\"\"\n+        for opt in opts:\n+            if len(opt) < 2:\n+                raise ArgumentError(\n+                    f\"invalid option string {opt!r}: \"\n+                    \"must be at least two characters long\",\n+                    self,\n+                )\n+            elif len(opt) == 2:\n+                if not (opt[0] == \"-\" and opt[1] != \"-\"):\n+                    raise ArgumentError(\n+                        f\"invalid short option string {opt!r}: \"\n+                        \"must be of the form -x, (x any non-dash char)\",\n+                        self,\n+                    )\n+                self._short_opts.append(opt)\n+            else:\n+                if not (opt[0:2] == \"--\" and opt[2] != \"-\"):\n+                    raise ArgumentError(\n+                        f\"invalid long option string {opt!r}: \"\n+                        \"must start with --, followed by non-dash\",\n+                        self,\n+                    )\n+                self._long_opts.append(opt)\n+\n+    def __repr__(self) -> str:\n+        args: list[str] = []\n+        if self._short_opts:\n+            args += [\"_short_opts: \" + repr(self._short_opts)]\n+        if self._long_opts:\n+            args += [\"_long_opts: \" + repr(self._long_opts)]\n+        args += [\"dest: \" + repr(self.dest)]\n+        if hasattr(self, \"type\"):\n+            args += [\"type: \" + repr(self.type)]\n+        if hasattr(self, \"default\"):\n+            args += [\"default: \" + repr(self.default)]\n+        return \"Argument({})\".format(\", \".join(args))\n+\n+\n+class OptionGroup:\n+    \"\"\"A group of options shown in its own section.\"\"\"\n+\n+    def __init__(\n+        self,\n+        name: str,\n+        description: str = \"\",\n+        parser: Parser | None = None,\n+        *,\n+        _ispytest: bool = False,\n+    ) -> None:\n+        check_ispytest(_ispytest)\n+        self.name = name\n+        self.description = description\n+        self.options: list[Argument] = []\n+        self.parser = parser\n+\n+    def addoption(self, *opts: str, **attrs: Any) -> None:\n+        \"\"\"Add an option to this group.\n+\n+        If a shortened version of a long option is specified, it will\n+        be suppressed in the help. ``addoption('--twowords', '--two-words')``\n+        results in help showing ``--two-words`` only, but ``--twowords`` gets\n+        accepted **and** the automatic destination is in ``args.twowords``.\n+\n+        :param opts:\n+            Option names, can be short or long options.\n+        :param attrs:\n+            Same attributes as the argparse library's :meth:`add_argument()\n+            <argparse.ArgumentParser.add_argument>` function accepts.\n+        \"\"\"\n+        conflict = set(opts).intersection(\n+            name for opt in self.options for name in opt.names()\n+        )\n+        if conflict:\n+            raise ValueError(f\"option names {conflict} already added\")\n+        option = Argument(*opts, **attrs)\n+        self._addoption_instance(option, shortupper=False)\n+\n+    def _addoption(self, *opts: str, **attrs: Any) -> None:\n+        option = Argument(*opts, **attrs)\n+        self._addoption_instance(option, shortupper=True)\n+\n+    def _addoption_instance(self, option: Argument, shortupper: bool = False) -> None:\n+        if not shortupper:\n+            for opt in option._short_opts:\n+                if opt[0] == \"-\" and opt[1].islower():\n+                    raise ValueError(\"lowercase shortoptions reserved\")\n+        if self.parser:\n+            self.parser.processoption(option)\n+        self.options.append(option)\n+\n+\n+class MyOptionParser(argparse.ArgumentParser):\n+    def __init__(\n+        self,\n+        parser: Parser,\n+        extra_info: dict[str, Any] | None = None,\n+        prog: str | None = None,\n+    ) -> None:\n+        self._parser = parser\n+        super().__init__(\n+            prog=prog,\n+            usage=parser._usage,\n+            add_help=False,\n+            formatter_class=DropShorterLongHelpFormatter,\n+            allow_abbrev=False,\n+            fromfile_prefix_chars=\"@\",\n+        )\n+        # extra_info is a dict of (param -> value) to display if there's\n+        # an usage error to provide more contextual information to the user.\n+        self.extra_info = extra_info if extra_info else {}\n+\n+    def error(self, message: str) -> NoReturn:\n+        \"\"\"Transform argparse error message into UsageError.\"\"\"\n+        msg = f\"{self.prog}: error: {message}\"\n+\n+        if hasattr(self._parser, \"_config_source_hint\"):\n+            msg = f\"{msg} ({self._parser._config_source_hint})\"\n+\n+        raise UsageError(self.format_usage() + msg)\n+\n+    # Type ignored because typeshed has a very complex type in the superclass.\n+    def parse_args(  # type: ignore\n+        self,\n+        args: Sequence[str] | None = None,\n+        namespace: argparse.Namespace | None = None,\n+    ) -> argparse.Namespace:\n+        \"\"\"Allow splitting of positional arguments.\"\"\"\n+        parsed, unrecognized = self.parse_known_args(args, namespace)\n+        if unrecognized:\n+            for arg in unrecognized:\n+                if arg and arg[0] == \"-\":\n+                    lines = [\n+                        \"unrecognized arguments: {}\".format(\" \".join(unrecognized))\n+                    ]\n+                    for k, v in sorted(self.extra_info.items()):\n+                        lines.append(f\"  {k}: {v}\")\n+                    self.error(\"\\n\".join(lines))\n+            getattr(parsed, FILE_OR_DIR).extend(unrecognized)\n+        return parsed\n+\n+\n+class DropShorterLongHelpFormatter(argparse.HelpFormatter):\n+    \"\"\"Shorten help for long options that differ only in extra hyphens.\n+\n+    - Collapse **long** options that are the same except for extra hyphens.\n+    - Shortcut if there are only two options and one of them is a short one.\n+    - Cache result on the action object as this is called at least 2 times.\n+    \"\"\"\n+\n+    def __init__(self, *args: Any, **kwargs: Any) -> None:\n+        # Use more accurate terminal width.\n+        if \"width\" not in kwargs:\n+            kwargs[\"width\"] = _pytest._io.get_terminal_width()\n+        super().__init__(*args, **kwargs)\n+\n+    def _format_action_invocation(self, action: argparse.Action) -> str:\n+        orgstr = super()._format_action_invocation(action)\n+        if orgstr and orgstr[0] != \"-\":  # only optional arguments\n+            return orgstr\n+        res: str | None = getattr(action, \"_formatted_action_invocation\", None)\n+        if res:\n+            return res\n+        options = orgstr.split(\", \")\n+        if len(options) == 2 and (len(options[0]) == 2 or len(options[1]) == 2):\n+            # a shortcut for '-h, --help' or '--abc', '-a'\n+            action._formatted_action_invocation = orgstr  # type: ignore\n+            return orgstr\n+        return_list = []\n+        short_long: dict[str, str] = {}\n+        for option in options:\n+            if len(option) == 2 or option[2] == \" \":\n+                continue\n+            if not option.startswith(\"--\"):\n+                raise ArgumentError(\n+                    f'long optional argument without \"--\": [{option}]', option\n+                )\n+            xxoption = option[2:]\n+            shortened = xxoption.replace(\"-\", \"\")\n+            if shortened not in short_long or len(short_long[shortened]) < len(\n+                xxoption\n+            ):\n+                short_long[shortened] = xxoption\n+        # now short_long has been filled out to the longest with dashes\n+        # **and** we keep the right option ordering from add_argument\n+        for option in options:\n+            if len(option) == 2 or option[2] == \" \":\n+                return_list.append(option)\n+            if option[2:] == short_long.get(option.replace(\"-\", \"\")):\n+                return_list.append(option.replace(\" \", \"=\", 1))\n+        formatted_action_invocation = \", \".join(return_list)\n+        action._formatted_action_invocation = formatted_action_invocation  # type: ignore\n+        return formatted_action_invocation\n+\n+    def _split_lines(self, text, width):\n+        \"\"\"Wrap lines after splitting on original newlines.\n+\n+        This allows to have explicit line breaks in the help text.\n+        \"\"\"\n+        import textwrap\n+\n+        lines = []\n+        for line in text.splitlines():\n+            lines.extend(textwrap.wrap(line.strip(), width))\n+        return lines",
      "patch_lines": [
        "@@ -0,0 +1,533 @@\n",
        "+# mypy: allow-untyped-defs\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+import argparse\n",
        "+from collections.abc import Callable\n",
        "+from collections.abc import Mapping\n",
        "+from collections.abc import Sequence\n",
        "+import os\n",
        "+from typing import Any\n",
        "+from typing import cast\n",
        "+from typing import final\n",
        "+from typing import Literal\n",
        "+from typing import NoReturn\n",
        "+\n",
        "+import _pytest._io\n",
        "+from _pytest.config.exceptions import UsageError\n",
        "+from _pytest.deprecated import check_ispytest\n",
        "+\n",
        "+\n",
        "+FILE_OR_DIR = \"file_or_dir\"\n",
        "+\n",
        "+\n",
        "+class NotSet:\n",
        "+    def __repr__(self) -> str:\n",
        "+        return \"<notset>\"\n",
        "+\n",
        "+\n",
        "+NOT_SET = NotSet()\n",
        "+\n",
        "+\n",
        "+@final\n",
        "+class Parser:\n",
        "+    \"\"\"Parser for command line arguments and ini-file values.\n",
        "+\n",
        "+    :ivar extra_info: Dict of generic param -> value to display in case\n",
        "+        there's an error processing the command line arguments.\n",
        "+    \"\"\"\n",
        "+\n",
        "+    prog: str | None = None\n",
        "+\n",
        "+    def __init__(\n",
        "+        self,\n",
        "+        usage: str | None = None,\n",
        "+        processopt: Callable[[Argument], None] | None = None,\n",
        "+        *,\n",
        "+        _ispytest: bool = False,\n",
        "+    ) -> None:\n",
        "+        check_ispytest(_ispytest)\n",
        "+        self._anonymous = OptionGroup(\"Custom options\", parser=self, _ispytest=True)\n",
        "+        self._groups: list[OptionGroup] = []\n",
        "+        self._processopt = processopt\n",
        "+        self._usage = usage\n",
        "+        self._inidict: dict[str, tuple[str, str | None, Any]] = {}\n",
        "+        self._ininames: list[str] = []\n",
        "+        self.extra_info: dict[str, Any] = {}\n",
        "+\n",
        "+    def processoption(self, option: Argument) -> None:\n",
        "+        if self._processopt:\n",
        "+            if option.dest:\n",
        "+                self._processopt(option)\n",
        "+\n",
        "+    def getgroup(\n",
        "+        self, name: str, description: str = \"\", after: str | None = None\n",
        "+    ) -> OptionGroup:\n",
        "+        \"\"\"Get (or create) a named option Group.\n",
        "+\n",
        "+        :param name: Name of the option group.\n",
        "+        :param description: Long description for --help output.\n",
        "+        :param after: Name of another group, used for ordering --help output.\n",
        "+        :returns: The option group.\n",
        "+\n",
        "+        The returned group object has an ``addoption`` method with the same\n",
        "+        signature as :func:`parser.addoption <pytest.Parser.addoption>` but\n",
        "+        will be shown in the respective group in the output of\n",
        "+        ``pytest --help``.\n",
        "+        \"\"\"\n",
        "+        for group in self._groups:\n",
        "+            if group.name == name:\n",
        "+                return group\n",
        "+        group = OptionGroup(name, description, parser=self, _ispytest=True)\n",
        "+        i = 0\n",
        "+        for i, grp in enumerate(self._groups):\n",
        "+            if grp.name == after:\n",
        "+                break\n",
        "+        self._groups.insert(i + 1, group)\n",
        "+        return group\n",
        "+\n",
        "+    def addoption(self, *opts: str, **attrs: Any) -> None:\n",
        "+        \"\"\"Register a command line option.\n",
        "+\n",
        "+        :param opts:\n",
        "+            Option names, can be short or long options.\n",
        "+        :param attrs:\n",
        "+            Same attributes as the argparse library's :meth:`add_argument()\n",
        "+            <argparse.ArgumentParser.add_argument>` function accepts.\n",
        "+\n",
        "+        After command line parsing, options are available on the pytest config\n",
        "+        object via ``config.option.NAME`` where ``NAME`` is usually set\n",
        "+        by passing a ``dest`` attribute, for example\n",
        "+        ``addoption(\"--long\", dest=\"NAME\", ...)``.\n",
        "+        \"\"\"\n",
        "+        self._anonymous.addoption(*opts, **attrs)\n",
        "+\n",
        "+    def parse(\n",
        "+        self,\n",
        "+        args: Sequence[str | os.PathLike[str]],\n",
        "+        namespace: argparse.Namespace | None = None,\n",
        "+    ) -> argparse.Namespace:\n",
        "+        from _pytest._argcomplete import try_argcomplete\n",
        "+\n",
        "+        self.optparser = self._getparser()\n",
        "+        try_argcomplete(self.optparser)\n",
        "+        strargs = [os.fspath(x) for x in args]\n",
        "+        return self.optparser.parse_args(strargs, namespace=namespace)\n",
        "+\n",
        "+    def _getparser(self) -> MyOptionParser:\n",
        "+        from _pytest._argcomplete import filescompleter\n",
        "+\n",
        "+        optparser = MyOptionParser(self, self.extra_info, prog=self.prog)\n",
        "+        groups = [*self._groups, self._anonymous]\n",
        "+        for group in groups:\n",
        "+            if group.options:\n",
        "+                desc = group.description or group.name\n",
        "+                arggroup = optparser.add_argument_group(desc)\n",
        "+                for option in group.options:\n",
        "+                    n = option.names()\n",
        "+                    a = option.attrs()\n",
        "+                    arggroup.add_argument(*n, **a)\n",
        "+        file_or_dir_arg = optparser.add_argument(FILE_OR_DIR, nargs=\"*\")\n",
        "+        # bash like autocompletion for dirs (appending '/')\n",
        "+        # Type ignored because typeshed doesn't know about argcomplete.\n",
        "+        file_or_dir_arg.completer = filescompleter  # type: ignore\n",
        "+        return optparser\n",
        "+\n",
        "+    def parse_setoption(\n",
        "+        self,\n",
        "+        args: Sequence[str | os.PathLike[str]],\n",
        "+        option: argparse.Namespace,\n",
        "+        namespace: argparse.Namespace | None = None,\n",
        "+    ) -> list[str]:\n",
        "+        parsedoption = self.parse(args, namespace=namespace)\n",
        "+        for name, value in parsedoption.__dict__.items():\n",
        "+            setattr(option, name, value)\n",
        "+        return cast(list[str], getattr(parsedoption, FILE_OR_DIR))\n",
        "+\n",
        "+    def parse_known_args(\n",
        "+        self,\n",
        "+        args: Sequence[str | os.PathLike[str]],\n",
        "+        namespace: argparse.Namespace | None = None,\n",
        "+    ) -> argparse.Namespace:\n",
        "+        \"\"\"Parse the known arguments at this point.\n",
        "+\n",
        "+        :returns: An argparse namespace object.\n",
        "+        \"\"\"\n",
        "+        return self.parse_known_and_unknown_args(args, namespace=namespace)[0]\n",
        "+\n",
        "+    def parse_known_and_unknown_args(\n",
        "+        self,\n",
        "+        args: Sequence[str | os.PathLike[str]],\n",
        "+        namespace: argparse.Namespace | None = None,\n",
        "+    ) -> tuple[argparse.Namespace, list[str]]:\n",
        "+        \"\"\"Parse the known arguments at this point, and also return the\n",
        "+        remaining unknown arguments.\n",
        "+\n",
        "+        :returns:\n",
        "+            A tuple containing an argparse namespace object for the known\n",
        "+            arguments, and a list of the unknown arguments.\n",
        "+        \"\"\"\n",
        "+        optparser = self._getparser()\n",
        "+        strargs = [os.fspath(x) for x in args]\n",
        "+        return optparser.parse_known_args(strargs, namespace=namespace)\n",
        "+\n",
        "+    def addini(\n",
        "+        self,\n",
        "+        name: str,\n",
        "+        help: str,\n",
        "+        type: Literal[\"string\", \"paths\", \"pathlist\", \"args\", \"linelist\", \"bool\"]\n",
        "+        | None = None,\n",
        "+        default: Any = NOT_SET,\n",
        "+    ) -> None:\n",
        "+        \"\"\"Register an ini-file option.\n",
        "+\n",
        "+        :param name:\n",
        "+            Name of the ini-variable.\n",
        "+        :param type:\n",
        "+            Type of the variable. Can be:\n",
        "+\n",
        "+                * ``string``: a string\n",
        "+                * ``bool``: a boolean\n",
        "+                * ``args``: a list of strings, separated as in a shell\n",
        "+                * ``linelist``: a list of strings, separated by line breaks\n",
        "+                * ``paths``: a list of :class:`pathlib.Path`, separated as in a shell\n",
        "+                * ``pathlist``: a list of ``py.path``, separated as in a shell\n",
        "+                * ``int``: an integer\n",
        "+                * ``float``: a floating-point number\n",
        "+\n",
        "+                .. versionadded:: 8.4\n",
        "+\n",
        "+                    The ``float`` and ``int`` types.\n",
        "+\n",
        "+            For ``paths`` and ``pathlist`` types, they are considered relative to the ini-file.\n",
        "+            In case the execution is happening without an ini-file defined,\n",
        "+            they will be considered relative to the current working directory (for example with ``--override-ini``).\n",
        "+\n",
        "+            .. versionadded:: 7.0\n",
        "+                The ``paths`` variable type.\n",
        "+\n",
        "+            .. versionadded:: 8.1\n",
        "+                Use the current working directory to resolve ``paths`` and ``pathlist`` in the absence of an ini-file.\n",
        "+\n",
        "+            Defaults to ``string`` if ``None`` or not passed.\n",
        "+        :param default:\n",
        "+            Default value if no ini-file option exists but is queried.\n",
        "+\n",
        "+        The value of ini-variables can be retrieved via a call to\n",
        "+        :py:func:`config.getini(name) <pytest.Config.getini>`.\n",
        "+        \"\"\"\n",
        "+        assert type in (\n",
        "+            None,\n",
        "+            \"string\",\n",
        "+            \"paths\",\n",
        "+            \"pathlist\",\n",
        "+            \"args\",\n",
        "+            \"linelist\",\n",
        "+            \"bool\",\n",
        "+            \"int\",\n",
        "+            \"float\",\n",
        "+        )\n",
        "+        if default is NOT_SET:\n",
        "+            default = get_ini_default_for_type(type)\n",
        "+\n",
        "+        self._inidict[name] = (help, type, default)\n",
        "+        self._ininames.append(name)\n",
        "+\n",
        "+\n",
        "+def get_ini_default_for_type(\n",
        "+    type: Literal[\n",
        "+        \"string\", \"paths\", \"pathlist\", \"args\", \"linelist\", \"bool\", \"int\", \"float\"\n",
        "+    ]\n",
        "+    | None,\n",
        "+) -> Any:\n",
        "+    \"\"\"\n",
        "+    Used by addini to get the default value for a given ini-option type, when\n",
        "+    default is not supplied.\n",
        "+    \"\"\"\n",
        "+    if type is None:\n",
        "+        return \"\"\n",
        "+    elif type in (\"paths\", \"pathlist\", \"args\", \"linelist\"):\n",
        "+        return []\n",
        "+    elif type == \"bool\":\n",
        "+        return False\n",
        "+    elif type == \"int\":\n",
        "+        return 0\n",
        "+    elif type == \"float\":\n",
        "+        return 0.0\n",
        "+    else:\n",
        "+        return \"\"\n",
        "+\n",
        "+\n",
        "+class ArgumentError(Exception):\n",
        "+    \"\"\"Raised if an Argument instance is created with invalid or\n",
        "+    inconsistent arguments.\"\"\"\n",
        "+\n",
        "+    def __init__(self, msg: str, option: Argument | str) -> None:\n",
        "+        self.msg = msg\n",
        "+        self.option_id = str(option)\n",
        "+\n",
        "+    def __str__(self) -> str:\n",
        "+        if self.option_id:\n",
        "+            return f\"option {self.option_id}: {self.msg}\"\n",
        "+        else:\n",
        "+            return self.msg\n",
        "+\n",
        "+\n",
        "+class Argument:\n",
        "+    \"\"\"Class that mimics the necessary behaviour of optparse.Option.\n",
        "+\n",
        "+    It's currently a least effort implementation and ignoring choices\n",
        "+    and integer prefixes.\n",
        "+\n",
        "+    https://docs.python.org/3/library/optparse.html#optparse-standard-option-types\n",
        "+    \"\"\"\n",
        "+\n",
        "+    def __init__(self, *names: str, **attrs: Any) -> None:\n",
        "+        \"\"\"Store params in private vars for use in add_argument.\"\"\"\n",
        "+        self._attrs = attrs\n",
        "+        self._short_opts: list[str] = []\n",
        "+        self._long_opts: list[str] = []\n",
        "+        try:\n",
        "+            self.type = attrs[\"type\"]\n",
        "+        except KeyError:\n",
        "+            pass\n",
        "+        try:\n",
        "+            # Attribute existence is tested in Config._processopt.\n",
        "+            self.default = attrs[\"default\"]\n",
        "+        except KeyError:\n",
        "+            pass\n",
        "+        self._set_opt_strings(names)\n",
        "+        dest: str | None = attrs.get(\"dest\")\n",
        "+        if dest:\n",
        "+            self.dest = dest\n",
        "+        elif self._long_opts:\n",
        "+            self.dest = self._long_opts[0][2:].replace(\"-\", \"_\")\n",
        "+        else:\n",
        "+            try:\n",
        "+                self.dest = self._short_opts[0][1:]\n",
        "+            except IndexError as e:\n",
        "+                self.dest = \"???\"  # Needed for the error repr.\n",
        "+                raise ArgumentError(\"need a long or short option\", self) from e\n",
        "+\n",
        "+    def names(self) -> list[str]:\n",
        "+        return self._short_opts + self._long_opts\n",
        "+\n",
        "+    def attrs(self) -> Mapping[str, Any]:\n",
        "+        # Update any attributes set by processopt.\n",
        "+        attrs = \"default dest help\".split()\n",
        "+        attrs.append(self.dest)\n",
        "+        for attr in attrs:\n",
        "+            try:\n",
        "+                self._attrs[attr] = getattr(self, attr)\n",
        "+            except AttributeError:\n",
        "+                pass\n",
        "+        return self._attrs\n",
        "+\n",
        "+    def _set_opt_strings(self, opts: Sequence[str]) -> None:\n",
        "+        \"\"\"Directly from optparse.\n",
        "+\n",
        "+        Might not be necessary as this is passed to argparse later on.\n",
        "+        \"\"\"\n",
        "+        for opt in opts:\n",
        "+            if len(opt) < 2:\n",
        "+                raise ArgumentError(\n",
        "+                    f\"invalid option string {opt!r}: \"\n",
        "+                    \"must be at least two characters long\",\n",
        "+                    self,\n",
        "+                )\n",
        "+            elif len(opt) == 2:\n",
        "+                if not (opt[0] == \"-\" and opt[1] != \"-\"):\n",
        "+                    raise ArgumentError(\n",
        "+                        f\"invalid short option string {opt!r}: \"\n",
        "+                        \"must be of the form -x, (x any non-dash char)\",\n",
        "+                        self,\n",
        "+                    )\n",
        "+                self._short_opts.append(opt)\n",
        "+            else:\n",
        "+                if not (opt[0:2] == \"--\" and opt[2] != \"-\"):\n",
        "+                    raise ArgumentError(\n",
        "+                        f\"invalid long option string {opt!r}: \"\n",
        "+                        \"must start with --, followed by non-dash\",\n",
        "+                        self,\n",
        "+                    )\n",
        "+                self._long_opts.append(opt)\n",
        "+\n",
        "+    def __repr__(self) -> str:\n",
        "+        args: list[str] = []\n",
        "+        if self._short_opts:\n",
        "+            args += [\"_short_opts: \" + repr(self._short_opts)]\n",
        "+        if self._long_opts:\n",
        "+            args += [\"_long_opts: \" + repr(self._long_opts)]\n",
        "+        args += [\"dest: \" + repr(self.dest)]\n",
        "+        if hasattr(self, \"type\"):\n",
        "+            args += [\"type: \" + repr(self.type)]\n",
        "+        if hasattr(self, \"default\"):\n",
        "+            args += [\"default: \" + repr(self.default)]\n",
        "+        return \"Argument({})\".format(\", \".join(args))\n",
        "+\n",
        "+\n",
        "+class OptionGroup:\n",
        "+    \"\"\"A group of options shown in its own section.\"\"\"\n",
        "+\n",
        "+    def __init__(\n",
        "+        self,\n",
        "+        name: str,\n",
        "+        description: str = \"\",\n",
        "+        parser: Parser | None = None,\n",
        "+        *,\n",
        "+        _ispytest: bool = False,\n",
        "+    ) -> None:\n",
        "+        check_ispytest(_ispytest)\n",
        "+        self.name = name\n",
        "+        self.description = description\n",
        "+        self.options: list[Argument] = []\n",
        "+        self.parser = parser\n",
        "+\n",
        "+    def addoption(self, *opts: str, **attrs: Any) -> None:\n",
        "+        \"\"\"Add an option to this group.\n",
        "+\n",
        "+        If a shortened version of a long option is specified, it will\n",
        "+        be suppressed in the help. ``addoption('--twowords', '--two-words')``\n",
        "+        results in help showing ``--two-words`` only, but ``--twowords`` gets\n",
        "+        accepted **and** the automatic destination is in ``args.twowords``.\n",
        "+\n",
        "+        :param opts:\n",
        "+            Option names, can be short or long options.\n",
        "+        :param attrs:\n",
        "+            Same attributes as the argparse library's :meth:`add_argument()\n",
        "+            <argparse.ArgumentParser.add_argument>` function accepts.\n",
        "+        \"\"\"\n",
        "+        conflict = set(opts).intersection(\n",
        "+            name for opt in self.options for name in opt.names()\n",
        "+        )\n",
        "+        if conflict:\n",
        "+            raise ValueError(f\"option names {conflict} already added\")\n",
        "+        option = Argument(*opts, **attrs)\n",
        "+        self._addoption_instance(option, shortupper=False)\n",
        "+\n",
        "+    def _addoption(self, *opts: str, **attrs: Any) -> None:\n",
        "+        option = Argument(*opts, **attrs)\n",
        "+        self._addoption_instance(option, shortupper=True)\n",
        "+\n",
        "+    def _addoption_instance(self, option: Argument, shortupper: bool = False) -> None:\n",
        "+        if not shortupper:\n",
        "+            for opt in option._short_opts:\n",
        "+                if opt[0] == \"-\" and opt[1].islower():\n",
        "+                    raise ValueError(\"lowercase shortoptions reserved\")\n",
        "+        if self.parser:\n",
        "+            self.parser.processoption(option)\n",
        "+        self.options.append(option)\n",
        "+\n",
        "+\n",
        "+class MyOptionParser(argparse.ArgumentParser):\n",
        "+    def __init__(\n",
        "+        self,\n",
        "+        parser: Parser,\n",
        "+        extra_info: dict[str, Any] | None = None,\n",
        "+        prog: str | None = None,\n",
        "+    ) -> None:\n",
        "+        self._parser = parser\n",
        "+        super().__init__(\n",
        "+            prog=prog,\n",
        "+            usage=parser._usage,\n",
        "+            add_help=False,\n",
        "+            formatter_class=DropShorterLongHelpFormatter,\n",
        "+            allow_abbrev=False,\n",
        "+            fromfile_prefix_chars=\"@\",\n",
        "+        )\n",
        "+        # extra_info is a dict of (param -> value) to display if there's\n",
        "+        # an usage error to provide more contextual information to the user.\n",
        "+        self.extra_info = extra_info if extra_info else {}\n",
        "+\n",
        "+    def error(self, message: str) -> NoReturn:\n",
        "+        \"\"\"Transform argparse error message into UsageError.\"\"\"\n",
        "+        msg = f\"{self.prog}: error: {message}\"\n",
        "+\n",
        "+        if hasattr(self._parser, \"_config_source_hint\"):\n",
        "+            msg = f\"{msg} ({self._parser._config_source_hint})\"\n",
        "+\n",
        "+        raise UsageError(self.format_usage() + msg)\n",
        "+\n",
        "+    # Type ignored because typeshed has a very complex type in the superclass.\n",
        "+    def parse_args(  # type: ignore\n",
        "+        self,\n",
        "+        args: Sequence[str] | None = None,\n",
        "+        namespace: argparse.Namespace | None = None,\n",
        "+    ) -> argparse.Namespace:\n",
        "+        \"\"\"Allow splitting of positional arguments.\"\"\"\n",
        "+        parsed, unrecognized = self.parse_known_args(args, namespace)\n",
        "+        if unrecognized:\n",
        "+            for arg in unrecognized:\n",
        "+                if arg and arg[0] == \"-\":\n",
        "+                    lines = [\n",
        "+                        \"unrecognized arguments: {}\".format(\" \".join(unrecognized))\n",
        "+                    ]\n",
        "+                    for k, v in sorted(self.extra_info.items()):\n",
        "+                        lines.append(f\"  {k}: {v}\")\n",
        "+                    self.error(\"\\n\".join(lines))\n",
        "+            getattr(parsed, FILE_OR_DIR).extend(unrecognized)\n",
        "+        return parsed\n",
        "+\n",
        "+\n",
        "+class DropShorterLongHelpFormatter(argparse.HelpFormatter):\n",
        "+    \"\"\"Shorten help for long options that differ only in extra hyphens.\n",
        "+\n",
        "+    - Collapse **long** options that are the same except for extra hyphens.\n",
        "+    - Shortcut if there are only two options and one of them is a short one.\n",
        "+    - Cache result on the action object as this is called at least 2 times.\n",
        "+    \"\"\"\n",
        "+\n",
        "+    def __init__(self, *args: Any, **kwargs: Any) -> None:\n",
        "+        # Use more accurate terminal width.\n",
        "+        if \"width\" not in kwargs:\n",
        "+            kwargs[\"width\"] = _pytest._io.get_terminal_width()\n",
        "+        super().__init__(*args, **kwargs)\n",
        "+\n",
        "+    def _format_action_invocation(self, action: argparse.Action) -> str:\n",
        "+        orgstr = super()._format_action_invocation(action)\n",
        "+        if orgstr and orgstr[0] != \"-\":  # only optional arguments\n",
        "+            return orgstr\n",
        "+        res: str | None = getattr(action, \"_formatted_action_invocation\", None)\n",
        "+        if res:\n",
        "+            return res\n",
        "+        options = orgstr.split(\", \")\n",
        "+        if len(options) == 2 and (len(options[0]) == 2 or len(options[1]) == 2):\n",
        "+            # a shortcut for '-h, --help' or '--abc', '-a'\n",
        "+            action._formatted_action_invocation = orgstr  # type: ignore\n",
        "+            return orgstr\n",
        "+        return_list = []\n",
        "+        short_long: dict[str, str] = {}\n",
        "+        for option in options:\n",
        "+            if len(option) == 2 or option[2] == \" \":\n",
        "+                continue\n",
        "+            if not option.startswith(\"--\"):\n",
        "+                raise ArgumentError(\n",
        "+                    f'long optional argument without \"--\": [{option}]', option\n",
        "+                )\n",
        "+            xxoption = option[2:]\n",
        "+            shortened = xxoption.replace(\"-\", \"\")\n",
        "+            if shortened not in short_long or len(short_long[shortened]) < len(\n",
        "+                xxoption\n",
        "+            ):\n",
        "+                short_long[shortened] = xxoption\n",
        "+        # now short_long has been filled out to the longest with dashes\n",
        "+        # **and** we keep the right option ordering from add_argument\n",
        "+        for option in options:\n",
        "+            if len(option) == 2 or option[2] == \" \":\n",
        "+                return_list.append(option)\n",
        "+            if option[2:] == short_long.get(option.replace(\"-\", \"\")):\n",
        "+                return_list.append(option.replace(\" \", \"=\", 1))\n",
        "+        formatted_action_invocation = \", \".join(return_list)\n",
        "+        action._formatted_action_invocation = formatted_action_invocation  # type: ignore\n",
        "+        return formatted_action_invocation\n",
        "+\n",
        "+    def _split_lines(self, text, width):\n",
        "+        \"\"\"Wrap lines after splitting on original newlines.\n",
        "+\n",
        "+        This allows to have explicit line breaks in the help text.\n",
        "+        \"\"\"\n",
        "+        import textwrap\n",
        "+\n",
        "+        lines = []\n",
        "+        for line in text.splitlines():\n",
        "+            lines.extend(textwrap.wrap(line.strip(), width))\n",
        "+        return lines\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/config/compat.py",
      "status": "added",
      "additions": 85,
      "deletions": 0,
      "patch": "@@ -0,0 +1,85 @@\n+from __future__ import annotations\n+\n+from collections.abc import Mapping\n+import functools\n+from pathlib import Path\n+from typing import Any\n+import warnings\n+\n+import pluggy\n+\n+from ..compat import LEGACY_PATH\n+from ..compat import legacy_path\n+from ..deprecated import HOOK_LEGACY_PATH_ARG\n+\n+\n+# hookname: (Path, LEGACY_PATH)\n+imply_paths_hooks: Mapping[str, tuple[str, str]] = {\n+    \"pytest_ignore_collect\": (\"collection_path\", \"path\"),\n+    \"pytest_collect_file\": (\"file_path\", \"path\"),\n+    \"pytest_pycollect_makemodule\": (\"module_path\", \"path\"),\n+    \"pytest_report_header\": (\"start_path\", \"startdir\"),\n+    \"pytest_report_collectionfinish\": (\"start_path\", \"startdir\"),\n+}\n+\n+\n+def _check_path(path: Path, fspath: LEGACY_PATH) -> None:\n+    if Path(fspath) != path:\n+        raise ValueError(\n+            f\"Path({fspath!r}) != {path!r}\\n\"\n+            \"if both path and fspath are given they need to be equal\"\n+        )\n+\n+\n+class PathAwareHookProxy:\n+    \"\"\"\n+    this helper wraps around hook callers\n+    until pluggy supports fixingcalls, this one will do\n+\n+    it currently doesn't return full hook caller proxies for fixed hooks,\n+    this may have to be changed later depending on bugs\n+    \"\"\"\n+\n+    def __init__(self, hook_relay: pluggy.HookRelay) -> None:\n+        self._hook_relay = hook_relay\n+\n+    def __dir__(self) -> list[str]:\n+        return dir(self._hook_relay)\n+\n+    def __getattr__(self, key: str) -> pluggy.HookCaller:\n+        hook: pluggy.HookCaller = getattr(self._hook_relay, key)\n+        if key not in imply_paths_hooks:\n+            self.__dict__[key] = hook\n+            return hook\n+        else:\n+            path_var, fspath_var = imply_paths_hooks[key]\n+\n+            @functools.wraps(hook)\n+            def fixed_hook(**kw: Any) -> Any:\n+                path_value: Path | None = kw.pop(path_var, None)\n+                fspath_value: LEGACY_PATH | None = kw.pop(fspath_var, None)\n+                if fspath_value is not None:\n+                    warnings.warn(\n+                        HOOK_LEGACY_PATH_ARG.format(\n+                            pylib_path_arg=fspath_var, pathlib_path_arg=path_var\n+                        ),\n+                        stacklevel=2,\n+                    )\n+                if path_value is not None:\n+                    if fspath_value is not None:\n+                        _check_path(path_value, fspath_value)\n+                    else:\n+                        fspath_value = legacy_path(path_value)\n+                else:\n+                    assert fspath_value is not None\n+                    path_value = Path(fspath_value)\n+\n+                kw[path_var] = path_value\n+                kw[fspath_var] = fspath_value\n+                return hook(**kw)\n+\n+            fixed_hook.name = hook.name  # type: ignore[attr-defined]\n+            fixed_hook.spec = hook.spec  # type: ignore[attr-defined]\n+            fixed_hook.__name__ = key\n+            self.__dict__[key] = fixed_hook\n+            return fixed_hook  # type: ignore[return-value]",
      "patch_lines": [
        "@@ -0,0 +1,85 @@\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+from collections.abc import Mapping\n",
        "+import functools\n",
        "+from pathlib import Path\n",
        "+from typing import Any\n",
        "+import warnings\n",
        "+\n",
        "+import pluggy\n",
        "+\n",
        "+from ..compat import LEGACY_PATH\n",
        "+from ..compat import legacy_path\n",
        "+from ..deprecated import HOOK_LEGACY_PATH_ARG\n",
        "+\n",
        "+\n",
        "+# hookname: (Path, LEGACY_PATH)\n",
        "+imply_paths_hooks: Mapping[str, tuple[str, str]] = {\n",
        "+    \"pytest_ignore_collect\": (\"collection_path\", \"path\"),\n",
        "+    \"pytest_collect_file\": (\"file_path\", \"path\"),\n",
        "+    \"pytest_pycollect_makemodule\": (\"module_path\", \"path\"),\n",
        "+    \"pytest_report_header\": (\"start_path\", \"startdir\"),\n",
        "+    \"pytest_report_collectionfinish\": (\"start_path\", \"startdir\"),\n",
        "+}\n",
        "+\n",
        "+\n",
        "+def _check_path(path: Path, fspath: LEGACY_PATH) -> None:\n",
        "+    if Path(fspath) != path:\n",
        "+        raise ValueError(\n",
        "+            f\"Path({fspath!r}) != {path!r}\\n\"\n",
        "+            \"if both path and fspath are given they need to be equal\"\n",
        "+        )\n",
        "+\n",
        "+\n",
        "+class PathAwareHookProxy:\n",
        "+    \"\"\"\n",
        "+    this helper wraps around hook callers\n",
        "+    until pluggy supports fixingcalls, this one will do\n",
        "+\n",
        "+    it currently doesn't return full hook caller proxies for fixed hooks,\n",
        "+    this may have to be changed later depending on bugs\n",
        "+    \"\"\"\n",
        "+\n",
        "+    def __init__(self, hook_relay: pluggy.HookRelay) -> None:\n",
        "+        self._hook_relay = hook_relay\n",
        "+\n",
        "+    def __dir__(self) -> list[str]:\n",
        "+        return dir(self._hook_relay)\n",
        "+\n",
        "+    def __getattr__(self, key: str) -> pluggy.HookCaller:\n",
        "+        hook: pluggy.HookCaller = getattr(self._hook_relay, key)\n",
        "+        if key not in imply_paths_hooks:\n",
        "+            self.__dict__[key] = hook\n",
        "+            return hook\n",
        "+        else:\n",
        "+            path_var, fspath_var = imply_paths_hooks[key]\n",
        "+\n",
        "+            @functools.wraps(hook)\n",
        "+            def fixed_hook(**kw: Any) -> Any:\n",
        "+                path_value: Path | None = kw.pop(path_var, None)\n",
        "+                fspath_value: LEGACY_PATH | None = kw.pop(fspath_var, None)\n",
        "+                if fspath_value is not None:\n",
        "+                    warnings.warn(\n",
        "+                        HOOK_LEGACY_PATH_ARG.format(\n",
        "+                            pylib_path_arg=fspath_var, pathlib_path_arg=path_var\n",
        "+                        ),\n",
        "+                        stacklevel=2,\n",
        "+                    )\n",
        "+                if path_value is not None:\n",
        "+                    if fspath_value is not None:\n",
        "+                        _check_path(path_value, fspath_value)\n",
        "+                    else:\n",
        "+                        fspath_value = legacy_path(path_value)\n",
        "+                else:\n",
        "+                    assert fspath_value is not None\n",
        "+                    path_value = Path(fspath_value)\n",
        "+\n",
        "+                kw[path_var] = path_value\n",
        "+                kw[fspath_var] = fspath_value\n",
        "+                return hook(**kw)\n",
        "+\n",
        "+            fixed_hook.name = hook.name  # type: ignore[attr-defined]\n",
        "+            fixed_hook.spec = hook.spec  # type: ignore[attr-defined]\n",
        "+            fixed_hook.__name__ = key\n",
        "+            self.__dict__[key] = fixed_hook\n",
        "+            return fixed_hook  # type: ignore[return-value]\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/config/exceptions.py",
      "status": "added",
      "additions": 13,
      "deletions": 0,
      "patch": "@@ -0,0 +1,13 @@\n+from __future__ import annotations\n+\n+from typing import final\n+\n+\n+@final\n+class UsageError(Exception):\n+    \"\"\"Error in pytest usage or invocation.\"\"\"\n+\n+\n+class PrintHelp(Exception):\n+    \"\"\"Raised when pytest should print its help to skip the rest of the\n+    argument parsing and validation.\"\"\"",
      "patch_lines": [
        "@@ -0,0 +1,13 @@\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+from typing import final\n",
        "+\n",
        "+\n",
        "+@final\n",
        "+class UsageError(Exception):\n",
        "+    \"\"\"Error in pytest usage or invocation.\"\"\"\n",
        "+\n",
        "+\n",
        "+class PrintHelp(Exception):\n",
        "+    \"\"\"Raised when pytest should print its help to skip the rest of the\n",
        "+    argument parsing and validation.\"\"\"\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/config/findpaths.py",
      "status": "added",
      "additions": 239,
      "deletions": 0,
      "patch": "@@ -0,0 +1,239 @@\n+from __future__ import annotations\n+\n+from collections.abc import Iterable\n+from collections.abc import Sequence\n+import os\n+from pathlib import Path\n+import sys\n+from typing import TYPE_CHECKING\n+\n+import iniconfig\n+\n+from .exceptions import UsageError\n+from _pytest.outcomes import fail\n+from _pytest.pathlib import absolutepath\n+from _pytest.pathlib import commonpath\n+from _pytest.pathlib import safe_exists\n+\n+\n+if TYPE_CHECKING:\n+    from typing import Union\n+\n+    from typing_extensions import TypeAlias\n+\n+    # Even though TOML supports richer data types, all values are converted to str/list[str] during\n+    # parsing to maintain compatibility with the rest of the configuration system.\n+    ConfigDict: TypeAlias = dict[str, Union[str, list[str]]]\n+\n+\n+def _parse_ini_config(path: Path) -> iniconfig.IniConfig:\n+    \"\"\"Parse the given generic '.ini' file using legacy IniConfig parser, returning\n+    the parsed object.\n+\n+    Raise UsageError if the file cannot be parsed.\n+    \"\"\"\n+    try:\n+        return iniconfig.IniConfig(str(path))\n+    except iniconfig.ParseError as exc:\n+        raise UsageError(str(exc)) from exc\n+\n+\n+def load_config_dict_from_file(\n+    filepath: Path,\n+) -> ConfigDict | None:\n+    \"\"\"Load pytest configuration from the given file path, if supported.\n+\n+    Return None if the file does not contain valid pytest configuration.\n+    \"\"\"\n+    # Configuration from ini files are obtained from the [pytest] section, if present.\n+    if filepath.suffix == \".ini\":\n+        iniconfig = _parse_ini_config(filepath)\n+\n+        if \"pytest\" in iniconfig:\n+            return dict(iniconfig[\"pytest\"].items())\n+        else:\n+            # \"pytest.ini\" files are always the source of configuration, even if empty.\n+            if filepath.name == \"pytest.ini\":\n+                return {}\n+\n+    # '.cfg' files are considered if they contain a \"[tool:pytest]\" section.\n+    elif filepath.suffix == \".cfg\":\n+        iniconfig = _parse_ini_config(filepath)\n+\n+        if \"tool:pytest\" in iniconfig.sections:\n+            return dict(iniconfig[\"tool:pytest\"].items())\n+        elif \"pytest\" in iniconfig.sections:\n+            # If a setup.cfg contains a \"[pytest]\" section, we raise a failure to indicate users that\n+            # plain \"[pytest]\" sections in setup.cfg files is no longer supported (#3086).\n+            fail(CFG_PYTEST_SECTION.format(filename=\"setup.cfg\"), pytrace=False)\n+\n+    # '.toml' files are considered if they contain a [tool.pytest.ini_options] table.\n+    elif filepath.suffix == \".toml\":\n+        if sys.version_info >= (3, 11):\n+            import tomllib\n+        else:\n+            import tomli as tomllib\n+\n+        toml_text = filepath.read_text(encoding=\"utf-8\")\n+        try:\n+            config = tomllib.loads(toml_text)\n+        except tomllib.TOMLDecodeError as exc:\n+            raise UsageError(f\"{filepath}: {exc}\") from exc\n+\n+        result = config.get(\"tool\", {}).get(\"pytest\", {}).get(\"ini_options\", None)\n+        if result is not None:\n+            # TOML supports richer data types than ini files (strings, arrays, floats, ints, etc),\n+            # however we need to convert all scalar values to str for compatibility with the rest\n+            # of the configuration system, which expects strings only.\n+            def make_scalar(v: object) -> str | list[str]:\n+                return v if isinstance(v, list) else str(v)\n+\n+            return {k: make_scalar(v) for k, v in result.items()}\n+\n+    return None\n+\n+\n+def locate_config(\n+    invocation_dir: Path,\n+    args: Iterable[Path],\n+) -> tuple[Path | None, Path | None, ConfigDict]:\n+    \"\"\"Search in the list of arguments for a valid ini-file for pytest,\n+    and return a tuple of (rootdir, inifile, cfg-dict).\"\"\"\n+    config_names = [\n+        \"pytest.ini\",\n+        \".pytest.ini\",\n+        \"pyproject.toml\",\n+        \"tox.ini\",\n+        \"setup.cfg\",\n+    ]\n+    args = [x for x in args if not str(x).startswith(\"-\")]\n+    if not args:\n+        args = [invocation_dir]\n+    found_pyproject_toml: Path | None = None\n+    for arg in args:\n+        argpath = absolutepath(arg)\n+        for base in (argpath, *argpath.parents):\n+            for config_name in config_names:\n+                p = base / config_name\n+                if p.is_file():\n+                    if p.name == \"pyproject.toml\" and found_pyproject_toml is None:\n+                        found_pyproject_toml = p\n+                    ini_config = load_config_dict_from_file(p)\n+                    if ini_config is not None:\n+                        return base, p, ini_config\n+    if found_pyproject_toml is not None:\n+        return found_pyproject_toml.parent, found_pyproject_toml, {}\n+    return None, None, {}\n+\n+\n+def get_common_ancestor(\n+    invocation_dir: Path,\n+    paths: Iterable[Path],\n+) -> Path:\n+    common_ancestor: Path | None = None\n+    for path in paths:\n+        if not path.exists():\n+            continue\n+        if common_ancestor is None:\n+            common_ancestor = path\n+        else:\n+            if common_ancestor in path.parents or path == common_ancestor:\n+                continue\n+            elif path in common_ancestor.parents:\n+                common_ancestor = path\n+            else:\n+                shared = commonpath(path, common_ancestor)\n+                if shared is not None:\n+                    common_ancestor = shared\n+    if common_ancestor is None:\n+        common_ancestor = invocation_dir\n+    elif common_ancestor.is_file():\n+        common_ancestor = common_ancestor.parent\n+    return common_ancestor\n+\n+\n+def get_dirs_from_args(args: Iterable[str]) -> list[Path]:\n+    def is_option(x: str) -> bool:\n+        return x.startswith(\"-\")\n+\n+    def get_file_part_from_node_id(x: str) -> str:\n+        return x.split(\"::\")[0]\n+\n+    def get_dir_from_path(path: Path) -> Path:\n+        if path.is_dir():\n+            return path\n+        return path.parent\n+\n+    # These look like paths but may not exist\n+    possible_paths = (\n+        absolutepath(get_file_part_from_node_id(arg))\n+        for arg in args\n+        if not is_option(arg)\n+    )\n+\n+    return [get_dir_from_path(path) for path in possible_paths if safe_exists(path)]\n+\n+\n+CFG_PYTEST_SECTION = \"[pytest] section in {filename} files is no longer supported, change to [tool:pytest] instead.\"\n+\n+\n+def determine_setup(\n+    *,\n+    inifile: str | None,\n+    args: Sequence[str],\n+    rootdir_cmd_arg: str | None,\n+    invocation_dir: Path,\n+) -> tuple[Path, Path | None, ConfigDict]:\n+    \"\"\"Determine the rootdir, inifile and ini configuration values from the\n+    command line arguments.\n+\n+    :param inifile:\n+        The `--inifile` command line argument, if given.\n+    :param args:\n+        The free command line arguments.\n+    :param rootdir_cmd_arg:\n+        The `--rootdir` command line argument, if given.\n+    :param invocation_dir:\n+        The working directory when pytest was invoked.\n+    \"\"\"\n+    rootdir = None\n+    dirs = get_dirs_from_args(args)\n+    if inifile:\n+        inipath_ = absolutepath(inifile)\n+        inipath: Path | None = inipath_\n+        inicfg = load_config_dict_from_file(inipath_) or {}\n+        if rootdir_cmd_arg is None:\n+            rootdir = inipath_.parent\n+    else:\n+        ancestor = get_common_ancestor(invocation_dir, dirs)\n+        rootdir, inipath, inicfg = locate_config(invocation_dir, [ancestor])\n+        if rootdir is None and rootdir_cmd_arg is None:\n+            for possible_rootdir in (ancestor, *ancestor.parents):\n+                if (possible_rootdir / \"setup.py\").is_file():\n+                    rootdir = possible_rootdir\n+                    break\n+            else:\n+                if dirs != [ancestor]:\n+                    rootdir, inipath, inicfg = locate_config(invocation_dir, dirs)\n+                if rootdir is None:\n+                    rootdir = get_common_ancestor(\n+                        invocation_dir, [invocation_dir, ancestor]\n+                    )\n+                    if is_fs_root(rootdir):\n+                        rootdir = ancestor\n+    if rootdir_cmd_arg:\n+        rootdir = absolutepath(os.path.expandvars(rootdir_cmd_arg))\n+        if not rootdir.is_dir():\n+            raise UsageError(\n+                f\"Directory '{rootdir}' not found. Check your '--rootdir' option.\"\n+            )\n+    assert rootdir is not None\n+    return rootdir, inipath, inicfg or {}\n+\n+\n+def is_fs_root(p: Path) -> bool:\n+    r\"\"\"\n+    Return True if the given path is pointing to the root of the\n+    file system (\"/\" on Unix and \"C:\\\\\" on Windows for example).\n+    \"\"\"\n+    return os.path.splitdrive(str(p))[1] == os.sep",
      "patch_lines": [
        "@@ -0,0 +1,239 @@\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+from collections.abc import Iterable\n",
        "+from collections.abc import Sequence\n",
        "+import os\n",
        "+from pathlib import Path\n",
        "+import sys\n",
        "+from typing import TYPE_CHECKING\n",
        "+\n",
        "+import iniconfig\n",
        "+\n",
        "+from .exceptions import UsageError\n",
        "+from _pytest.outcomes import fail\n",
        "+from _pytest.pathlib import absolutepath\n",
        "+from _pytest.pathlib import commonpath\n",
        "+from _pytest.pathlib import safe_exists\n",
        "+\n",
        "+\n",
        "+if TYPE_CHECKING:\n",
        "+    from typing import Union\n",
        "+\n",
        "+    from typing_extensions import TypeAlias\n",
        "+\n",
        "+    # Even though TOML supports richer data types, all values are converted to str/list[str] during\n",
        "+    # parsing to maintain compatibility with the rest of the configuration system.\n",
        "+    ConfigDict: TypeAlias = dict[str, Union[str, list[str]]]\n",
        "+\n",
        "+\n",
        "+def _parse_ini_config(path: Path) -> iniconfig.IniConfig:\n",
        "+    \"\"\"Parse the given generic '.ini' file using legacy IniConfig parser, returning\n",
        "+    the parsed object.\n",
        "+\n",
        "+    Raise UsageError if the file cannot be parsed.\n",
        "+    \"\"\"\n",
        "+    try:\n",
        "+        return iniconfig.IniConfig(str(path))\n",
        "+    except iniconfig.ParseError as exc:\n",
        "+        raise UsageError(str(exc)) from exc\n",
        "+\n",
        "+\n",
        "+def load_config_dict_from_file(\n",
        "+    filepath: Path,\n",
        "+) -> ConfigDict | None:\n",
        "+    \"\"\"Load pytest configuration from the given file path, if supported.\n",
        "+\n",
        "+    Return None if the file does not contain valid pytest configuration.\n",
        "+    \"\"\"\n",
        "+    # Configuration from ini files are obtained from the [pytest] section, if present.\n",
        "+    if filepath.suffix == \".ini\":\n",
        "+        iniconfig = _parse_ini_config(filepath)\n",
        "+\n",
        "+        if \"pytest\" in iniconfig:\n",
        "+            return dict(iniconfig[\"pytest\"].items())\n",
        "+        else:\n",
        "+            # \"pytest.ini\" files are always the source of configuration, even if empty.\n",
        "+            if filepath.name == \"pytest.ini\":\n",
        "+                return {}\n",
        "+\n",
        "+    # '.cfg' files are considered if they contain a \"[tool:pytest]\" section.\n",
        "+    elif filepath.suffix == \".cfg\":\n",
        "+        iniconfig = _parse_ini_config(filepath)\n",
        "+\n",
        "+        if \"tool:pytest\" in iniconfig.sections:\n",
        "+            return dict(iniconfig[\"tool:pytest\"].items())\n",
        "+        elif \"pytest\" in iniconfig.sections:\n",
        "+            # If a setup.cfg contains a \"[pytest]\" section, we raise a failure to indicate users that\n",
        "+            # plain \"[pytest]\" sections in setup.cfg files is no longer supported (#3086).\n",
        "+            fail(CFG_PYTEST_SECTION.format(filename=\"setup.cfg\"), pytrace=False)\n",
        "+\n",
        "+    # '.toml' files are considered if they contain a [tool.pytest.ini_options] table.\n",
        "+    elif filepath.suffix == \".toml\":\n",
        "+        if sys.version_info >= (3, 11):\n",
        "+            import tomllib\n",
        "+        else:\n",
        "+            import tomli as tomllib\n",
        "+\n",
        "+        toml_text = filepath.read_text(encoding=\"utf-8\")\n",
        "+        try:\n",
        "+            config = tomllib.loads(toml_text)\n",
        "+        except tomllib.TOMLDecodeError as exc:\n",
        "+            raise UsageError(f\"{filepath}: {exc}\") from exc\n",
        "+\n",
        "+        result = config.get(\"tool\", {}).get(\"pytest\", {}).get(\"ini_options\", None)\n",
        "+        if result is not None:\n",
        "+            # TOML supports richer data types than ini files (strings, arrays, floats, ints, etc),\n",
        "+            # however we need to convert all scalar values to str for compatibility with the rest\n",
        "+            # of the configuration system, which expects strings only.\n",
        "+            def make_scalar(v: object) -> str | list[str]:\n",
        "+                return v if isinstance(v, list) else str(v)\n",
        "+\n",
        "+            return {k: make_scalar(v) for k, v in result.items()}\n",
        "+\n",
        "+    return None\n",
        "+\n",
        "+\n",
        "+def locate_config(\n",
        "+    invocation_dir: Path,\n",
        "+    args: Iterable[Path],\n",
        "+) -> tuple[Path | None, Path | None, ConfigDict]:\n",
        "+    \"\"\"Search in the list of arguments for a valid ini-file for pytest,\n",
        "+    and return a tuple of (rootdir, inifile, cfg-dict).\"\"\"\n",
        "+    config_names = [\n",
        "+        \"pytest.ini\",\n",
        "+        \".pytest.ini\",\n",
        "+        \"pyproject.toml\",\n",
        "+        \"tox.ini\",\n",
        "+        \"setup.cfg\",\n",
        "+    ]\n",
        "+    args = [x for x in args if not str(x).startswith(\"-\")]\n",
        "+    if not args:\n",
        "+        args = [invocation_dir]\n",
        "+    found_pyproject_toml: Path | None = None\n",
        "+    for arg in args:\n",
        "+        argpath = absolutepath(arg)\n",
        "+        for base in (argpath, *argpath.parents):\n",
        "+            for config_name in config_names:\n",
        "+                p = base / config_name\n",
        "+                if p.is_file():\n",
        "+                    if p.name == \"pyproject.toml\" and found_pyproject_toml is None:\n",
        "+                        found_pyproject_toml = p\n",
        "+                    ini_config = load_config_dict_from_file(p)\n",
        "+                    if ini_config is not None:\n",
        "+                        return base, p, ini_config\n",
        "+    if found_pyproject_toml is not None:\n",
        "+        return found_pyproject_toml.parent, found_pyproject_toml, {}\n",
        "+    return None, None, {}\n",
        "+\n",
        "+\n",
        "+def get_common_ancestor(\n",
        "+    invocation_dir: Path,\n",
        "+    paths: Iterable[Path],\n",
        "+) -> Path:\n",
        "+    common_ancestor: Path | None = None\n",
        "+    for path in paths:\n",
        "+        if not path.exists():\n",
        "+            continue\n",
        "+        if common_ancestor is None:\n",
        "+            common_ancestor = path\n",
        "+        else:\n",
        "+            if common_ancestor in path.parents or path == common_ancestor:\n",
        "+                continue\n",
        "+            elif path in common_ancestor.parents:\n",
        "+                common_ancestor = path\n",
        "+            else:\n",
        "+                shared = commonpath(path, common_ancestor)\n",
        "+                if shared is not None:\n",
        "+                    common_ancestor = shared\n",
        "+    if common_ancestor is None:\n",
        "+        common_ancestor = invocation_dir\n",
        "+    elif common_ancestor.is_file():\n",
        "+        common_ancestor = common_ancestor.parent\n",
        "+    return common_ancestor\n",
        "+\n",
        "+\n",
        "+def get_dirs_from_args(args: Iterable[str]) -> list[Path]:\n",
        "+    def is_option(x: str) -> bool:\n",
        "+        return x.startswith(\"-\")\n",
        "+\n",
        "+    def get_file_part_from_node_id(x: str) -> str:\n",
        "+        return x.split(\"::\")[0]\n",
        "+\n",
        "+    def get_dir_from_path(path: Path) -> Path:\n",
        "+        if path.is_dir():\n",
        "+            return path\n",
        "+        return path.parent\n",
        "+\n",
        "+    # These look like paths but may not exist\n",
        "+    possible_paths = (\n",
        "+        absolutepath(get_file_part_from_node_id(arg))\n",
        "+        for arg in args\n",
        "+        if not is_option(arg)\n",
        "+    )\n",
        "+\n",
        "+    return [get_dir_from_path(path) for path in possible_paths if safe_exists(path)]\n",
        "+\n",
        "+\n",
        "+CFG_PYTEST_SECTION = \"[pytest] section in {filename} files is no longer supported, change to [tool:pytest] instead.\"\n",
        "+\n",
        "+\n",
        "+def determine_setup(\n",
        "+    *,\n",
        "+    inifile: str | None,\n",
        "+    args: Sequence[str],\n",
        "+    rootdir_cmd_arg: str | None,\n",
        "+    invocation_dir: Path,\n",
        "+) -> tuple[Path, Path | None, ConfigDict]:\n",
        "+    \"\"\"Determine the rootdir, inifile and ini configuration values from the\n",
        "+    command line arguments.\n",
        "+\n",
        "+    :param inifile:\n",
        "+        The `--inifile` command line argument, if given.\n",
        "+    :param args:\n",
        "+        The free command line arguments.\n",
        "+    :param rootdir_cmd_arg:\n",
        "+        The `--rootdir` command line argument, if given.\n",
        "+    :param invocation_dir:\n",
        "+        The working directory when pytest was invoked.\n",
        "+    \"\"\"\n",
        "+    rootdir = None\n",
        "+    dirs = get_dirs_from_args(args)\n",
        "+    if inifile:\n",
        "+        inipath_ = absolutepath(inifile)\n",
        "+        inipath: Path | None = inipath_\n",
        "+        inicfg = load_config_dict_from_file(inipath_) or {}\n",
        "+        if rootdir_cmd_arg is None:\n",
        "+            rootdir = inipath_.parent\n",
        "+    else:\n",
        "+        ancestor = get_common_ancestor(invocation_dir, dirs)\n",
        "+        rootdir, inipath, inicfg = locate_config(invocation_dir, [ancestor])\n",
        "+        if rootdir is None and rootdir_cmd_arg is None:\n",
        "+            for possible_rootdir in (ancestor, *ancestor.parents):\n",
        "+                if (possible_rootdir / \"setup.py\").is_file():\n",
        "+                    rootdir = possible_rootdir\n",
        "+                    break\n",
        "+            else:\n",
        "+                if dirs != [ancestor]:\n",
        "+                    rootdir, inipath, inicfg = locate_config(invocation_dir, dirs)\n",
        "+                if rootdir is None:\n",
        "+                    rootdir = get_common_ancestor(\n",
        "+                        invocation_dir, [invocation_dir, ancestor]\n",
        "+                    )\n",
        "+                    if is_fs_root(rootdir):\n",
        "+                        rootdir = ancestor\n",
        "+    if rootdir_cmd_arg:\n",
        "+        rootdir = absolutepath(os.path.expandvars(rootdir_cmd_arg))\n",
        "+        if not rootdir.is_dir():\n",
        "+            raise UsageError(\n",
        "+                f\"Directory '{rootdir}' not found. Check your '--rootdir' option.\"\n",
        "+            )\n",
        "+    assert rootdir is not None\n",
        "+    return rootdir, inipath, inicfg or {}\n",
        "+\n",
        "+\n",
        "+def is_fs_root(p: Path) -> bool:\n",
        "+    r\"\"\"\n",
        "+    Return True if the given path is pointing to the root of the\n",
        "+    file system (\"/\" on Unix and \"C:\\\\\" on Windows for example).\n",
        "+    \"\"\"\n",
        "+    return os.path.splitdrive(str(p))[1] == os.sep\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/debugging.py",
      "status": "added",
      "additions": 407,
      "deletions": 0,
      "patch": "@@ -0,0 +1,407 @@\n+# mypy: allow-untyped-defs\n+# ruff: noqa: T100\n+\"\"\"Interactive debugging with PDB, the Python Debugger.\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+from collections.abc import Callable\n+from collections.abc import Generator\n+import functools\n+import sys\n+import types\n+from typing import Any\n+import unittest\n+\n+from _pytest import outcomes\n+from _pytest._code import ExceptionInfo\n+from _pytest.capture import CaptureManager\n+from _pytest.config import Config\n+from _pytest.config import ConftestImportFailure\n+from _pytest.config import hookimpl\n+from _pytest.config import PytestPluginManager\n+from _pytest.config.argparsing import Parser\n+from _pytest.config.exceptions import UsageError\n+from _pytest.nodes import Node\n+from _pytest.reports import BaseReport\n+from _pytest.runner import CallInfo\n+\n+\n+def _validate_usepdb_cls(value: str) -> tuple[str, str]:\n+    \"\"\"Validate syntax of --pdbcls option.\"\"\"\n+    try:\n+        modname, classname = value.split(\":\")\n+    except ValueError as e:\n+        raise argparse.ArgumentTypeError(\n+            f\"{value!r} is not in the format 'modname:classname'\"\n+        ) from e\n+    return (modname, classname)\n+\n+\n+def pytest_addoption(parser: Parser) -> None:\n+    group = parser.getgroup(\"general\")\n+    group.addoption(\n+        \"--pdb\",\n+        dest=\"usepdb\",\n+        action=\"store_true\",\n+        help=\"Start the interactive Python debugger on errors or KeyboardInterrupt\",\n+    )\n+    group.addoption(\n+        \"--pdbcls\",\n+        dest=\"usepdb_cls\",\n+        metavar=\"modulename:classname\",\n+        type=_validate_usepdb_cls,\n+        help=\"Specify a custom interactive Python debugger for use with --pdb.\"\n+        \"For example: --pdbcls=IPython.terminal.debugger:TerminalPdb\",\n+    )\n+    group.addoption(\n+        \"--trace\",\n+        dest=\"trace\",\n+        action=\"store_true\",\n+        help=\"Immediately break when running each test\",\n+    )\n+\n+\n+def pytest_configure(config: Config) -> None:\n+    import pdb\n+\n+    if config.getvalue(\"trace\"):\n+        config.pluginmanager.register(PdbTrace(), \"pdbtrace\")\n+    if config.getvalue(\"usepdb\"):\n+        config.pluginmanager.register(PdbInvoke(), \"pdbinvoke\")\n+\n+    pytestPDB._saved.append(\n+        (pdb.set_trace, pytestPDB._pluginmanager, pytestPDB._config)\n+    )\n+    pdb.set_trace = pytestPDB.set_trace\n+    pytestPDB._pluginmanager = config.pluginmanager\n+    pytestPDB._config = config\n+\n+    # NOTE: not using pytest_unconfigure, since it might get called although\n+    #       pytest_configure was not (if another plugin raises UsageError).\n+    def fin() -> None:\n+        (\n+            pdb.set_trace,\n+            pytestPDB._pluginmanager,\n+            pytestPDB._config,\n+        ) = pytestPDB._saved.pop()\n+\n+    config.add_cleanup(fin)\n+\n+\n+class pytestPDB:\n+    \"\"\"Pseudo PDB that defers to the real pdb.\"\"\"\n+\n+    _pluginmanager: PytestPluginManager | None = None\n+    _config: Config | None = None\n+    _saved: list[\n+        tuple[Callable[..., None], PytestPluginManager | None, Config | None]\n+    ] = []\n+    _recursive_debug = 0\n+    _wrapped_pdb_cls: tuple[type[Any], type[Any]] | None = None\n+\n+    @classmethod\n+    def _is_capturing(cls, capman: CaptureManager | None) -> str | bool:\n+        if capman:\n+            return capman.is_capturing()\n+        return False\n+\n+    @classmethod\n+    def _import_pdb_cls(cls, capman: CaptureManager | None):\n+        if not cls._config:\n+            import pdb\n+\n+            # Happens when using pytest.set_trace outside of a test.\n+            return pdb.Pdb\n+\n+        usepdb_cls = cls._config.getvalue(\"usepdb_cls\")\n+\n+        if cls._wrapped_pdb_cls and cls._wrapped_pdb_cls[0] == usepdb_cls:\n+            return cls._wrapped_pdb_cls[1]\n+\n+        if usepdb_cls:\n+            modname, classname = usepdb_cls\n+\n+            try:\n+                __import__(modname)\n+                mod = sys.modules[modname]\n+\n+                # Handle --pdbcls=pdb:pdb.Pdb (useful e.g. with pdbpp).\n+                parts = classname.split(\".\")\n+                pdb_cls = getattr(mod, parts[0])\n+                for part in parts[1:]:\n+                    pdb_cls = getattr(pdb_cls, part)\n+            except Exception as exc:\n+                value = \":\".join((modname, classname))\n+                raise UsageError(\n+                    f\"--pdbcls: could not import {value!r}: {exc}\"\n+                ) from exc\n+        else:\n+            import pdb\n+\n+            pdb_cls = pdb.Pdb\n+\n+        wrapped_cls = cls._get_pdb_wrapper_class(pdb_cls, capman)\n+        cls._wrapped_pdb_cls = (usepdb_cls, wrapped_cls)\n+        return wrapped_cls\n+\n+    @classmethod\n+    def _get_pdb_wrapper_class(cls, pdb_cls, capman: CaptureManager | None):\n+        import _pytest.config\n+\n+        class PytestPdbWrapper(pdb_cls):\n+            _pytest_capman = capman\n+            _continued = False\n+\n+            def do_debug(self, arg):\n+                cls._recursive_debug += 1\n+                ret = super().do_debug(arg)\n+                cls._recursive_debug -= 1\n+                return ret\n+\n+            if hasattr(pdb_cls, \"do_debug\"):\n+                do_debug.__doc__ = pdb_cls.do_debug.__doc__\n+\n+            def do_continue(self, arg):\n+                ret = super().do_continue(arg)\n+                if cls._recursive_debug == 0:\n+                    assert cls._config is not None\n+                    tw = _pytest.config.create_terminal_writer(cls._config)\n+                    tw.line()\n+\n+                    capman = self._pytest_capman\n+                    capturing = pytestPDB._is_capturing(capman)\n+                    if capturing:\n+                        if capturing == \"global\":\n+                            tw.sep(\">\", \"PDB continue (IO-capturing resumed)\")\n+                        else:\n+                            tw.sep(\n+                                \">\",\n+                                f\"PDB continue (IO-capturing resumed for {capturing})\",\n+                            )\n+                        assert capman is not None\n+                        capman.resume()\n+                    else:\n+                        tw.sep(\">\", \"PDB continue\")\n+                assert cls._pluginmanager is not None\n+                cls._pluginmanager.hook.pytest_leave_pdb(config=cls._config, pdb=self)\n+                self._continued = True\n+                return ret\n+\n+            if hasattr(pdb_cls, \"do_continue\"):\n+                do_continue.__doc__ = pdb_cls.do_continue.__doc__\n+\n+            do_c = do_cont = do_continue\n+\n+            def do_quit(self, arg):\n+                # Raise Exit outcome when quit command is used in pdb.\n+                #\n+                # This is a bit of a hack - it would be better if BdbQuit\n+                # could be handled, but this would require to wrap the\n+                # whole pytest run, and adjust the report etc.\n+                ret = super().do_quit(arg)\n+\n+                if cls._recursive_debug == 0:\n+                    outcomes.exit(\"Quitting debugger\")\n+\n+                return ret\n+\n+            if hasattr(pdb_cls, \"do_quit\"):\n+                do_quit.__doc__ = pdb_cls.do_quit.__doc__\n+\n+            do_q = do_quit\n+            do_exit = do_quit\n+\n+            def setup(self, f, tb):\n+                \"\"\"Suspend on setup().\n+\n+                Needed after do_continue resumed, and entering another\n+                breakpoint again.\n+                \"\"\"\n+                ret = super().setup(f, tb)\n+                if not ret and self._continued:\n+                    # pdb.setup() returns True if the command wants to exit\n+                    # from the interaction: do not suspend capturing then.\n+                    if self._pytest_capman:\n+                        self._pytest_capman.suspend_global_capture(in_=True)\n+                return ret\n+\n+            def get_stack(self, f, t):\n+                stack, i = super().get_stack(f, t)\n+                if f is None:\n+                    # Find last non-hidden frame.\n+                    i = max(0, len(stack) - 1)\n+                    while i and stack[i][0].f_locals.get(\"__tracebackhide__\", False):\n+                        i -= 1\n+                return stack, i\n+\n+        return PytestPdbWrapper\n+\n+    @classmethod\n+    def _init_pdb(cls, method, *args, **kwargs):\n+        \"\"\"Initialize PDB debugging, dropping any IO capturing.\"\"\"\n+        import _pytest.config\n+\n+        if cls._pluginmanager is None:\n+            capman: CaptureManager | None = None\n+        else:\n+            capman = cls._pluginmanager.getplugin(\"capturemanager\")\n+        if capman:\n+            capman.suspend(in_=True)\n+\n+        if cls._config:\n+            tw = _pytest.config.create_terminal_writer(cls._config)\n+            tw.line()\n+\n+            if cls._recursive_debug == 0:\n+                # Handle header similar to pdb.set_trace in py37+.\n+                header = kwargs.pop(\"header\", None)\n+                if header is not None:\n+                    tw.sep(\">\", header)\n+                else:\n+                    capturing = cls._is_capturing(capman)\n+                    if capturing == \"global\":\n+                        tw.sep(\">\", f\"PDB {method} (IO-capturing turned off)\")\n+                    elif capturing:\n+                        tw.sep(\n+                            \">\",\n+                            f\"PDB {method} (IO-capturing turned off for {capturing})\",\n+                        )\n+                    else:\n+                        tw.sep(\">\", f\"PDB {method}\")\n+\n+        _pdb = cls._import_pdb_cls(capman)(**kwargs)\n+\n+        if cls._pluginmanager:\n+            cls._pluginmanager.hook.pytest_enter_pdb(config=cls._config, pdb=_pdb)\n+        return _pdb\n+\n+    @classmethod\n+    def set_trace(cls, *args, **kwargs) -> None:\n+        \"\"\"Invoke debugging via ``Pdb.set_trace``, dropping any IO capturing.\"\"\"\n+        frame = sys._getframe().f_back\n+        _pdb = cls._init_pdb(\"set_trace\", *args, **kwargs)\n+        _pdb.set_trace(frame)\n+\n+\n+class PdbInvoke:\n+    def pytest_exception_interact(\n+        self, node: Node, call: CallInfo[Any], report: BaseReport\n+    ) -> None:\n+        capman = node.config.pluginmanager.getplugin(\"capturemanager\")\n+        if capman:\n+            capman.suspend_global_capture(in_=True)\n+            out, err = capman.read_global_capture()\n+            sys.stdout.write(out)\n+            sys.stdout.write(err)\n+        assert call.excinfo is not None\n+\n+        if not isinstance(call.excinfo.value, unittest.SkipTest):\n+            _enter_pdb(node, call.excinfo, report)\n+\n+    def pytest_internalerror(self, excinfo: ExceptionInfo[BaseException]) -> None:\n+        exc_or_tb = _postmortem_exc_or_tb(excinfo)\n+        post_mortem(exc_or_tb)\n+\n+\n+class PdbTrace:\n+    @hookimpl(wrapper=True)\n+    def pytest_pyfunc_call(self, pyfuncitem) -> Generator[None, object, object]:\n+        wrap_pytest_function_for_tracing(pyfuncitem)\n+        return (yield)\n+\n+\n+def wrap_pytest_function_for_tracing(pyfuncitem) -> None:\n+    \"\"\"Change the Python function object of the given Function item by a\n+    wrapper which actually enters pdb before calling the python function\n+    itself, effectively leaving the user in the pdb prompt in the first\n+    statement of the function.\"\"\"\n+    _pdb = pytestPDB._init_pdb(\"runcall\")\n+    testfunction = pyfuncitem.obj\n+\n+    # we can't just return `partial(pdb.runcall, testfunction)` because (on\n+    # python < 3.7.4) runcall's first param is `func`, which means we'd get\n+    # an exception if one of the kwargs to testfunction was called `func`.\n+    @functools.wraps(testfunction)\n+    def wrapper(*args, **kwargs) -> None:\n+        func = functools.partial(testfunction, *args, **kwargs)\n+        _pdb.runcall(func)\n+\n+    pyfuncitem.obj = wrapper\n+\n+\n+def maybe_wrap_pytest_function_for_tracing(pyfuncitem) -> None:\n+    \"\"\"Wrap the given pytestfunct item for tracing support if --trace was given in\n+    the command line.\"\"\"\n+    if pyfuncitem.config.getvalue(\"trace\"):\n+        wrap_pytest_function_for_tracing(pyfuncitem)\n+\n+\n+def _enter_pdb(\n+    node: Node, excinfo: ExceptionInfo[BaseException], rep: BaseReport\n+) -> BaseReport:\n+    # XXX we reuse the TerminalReporter's terminalwriter\n+    # because this seems to avoid some encoding related troubles\n+    # for not completely clear reasons.\n+    tw = node.config.pluginmanager.getplugin(\"terminalreporter\")._tw\n+    tw.line()\n+\n+    showcapture = node.config.option.showcapture\n+\n+    for sectionname, content in (\n+        (\"stdout\", rep.capstdout),\n+        (\"stderr\", rep.capstderr),\n+        (\"log\", rep.caplog),\n+    ):\n+        if showcapture in (sectionname, \"all\") and content:\n+            tw.sep(\">\", \"captured \" + sectionname)\n+            if content[-1:] == \"\\n\":\n+                content = content[:-1]\n+            tw.line(content)\n+\n+    tw.sep(\">\", \"traceback\")\n+    rep.toterminal(tw)\n+    tw.sep(\">\", \"entering PDB\")\n+    tb_or_exc = _postmortem_exc_or_tb(excinfo)\n+    rep._pdbshown = True  # type: ignore[attr-defined]\n+    post_mortem(tb_or_exc)\n+    return rep\n+\n+\n+def _postmortem_exc_or_tb(\n+    excinfo: ExceptionInfo[BaseException],\n+) -> types.TracebackType | BaseException:\n+    from doctest import UnexpectedException\n+\n+    get_exc = sys.version_info >= (3, 13)\n+    if isinstance(excinfo.value, UnexpectedException):\n+        # A doctest.UnexpectedException is not useful for post_mortem.\n+        # Use the underlying exception instead:\n+        underlying_exc = excinfo.value\n+        if get_exc:\n+            return underlying_exc.exc_info[1]\n+\n+        return underlying_exc.exc_info[2]\n+    elif isinstance(excinfo.value, ConftestImportFailure):\n+        # A config.ConftestImportFailure is not useful for post_mortem.\n+        # Use the underlying exception instead:\n+        cause = excinfo.value.cause\n+        if get_exc:\n+            return cause\n+\n+        assert cause.__traceback__ is not None\n+        return cause.__traceback__\n+    else:\n+        assert excinfo._excinfo is not None\n+        if get_exc:\n+            return excinfo._excinfo[1]\n+\n+        return excinfo._excinfo[2]\n+\n+\n+def post_mortem(tb_or_exc: types.TracebackType | BaseException) -> None:\n+    p = pytestPDB._init_pdb(\"post_mortem\")\n+    p.reset()\n+    p.interaction(None, tb_or_exc)\n+    if p.quitting:\n+        outcomes.exit(\"Quitting debugger\")",
      "patch_lines": [
        "@@ -0,0 +1,407 @@\n",
        "+# mypy: allow-untyped-defs\n",
        "+# ruff: noqa: T100\n",
        "+\"\"\"Interactive debugging with PDB, the Python Debugger.\"\"\"\n",
        "+\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+import argparse\n",
        "+from collections.abc import Callable\n",
        "+from collections.abc import Generator\n",
        "+import functools\n",
        "+import sys\n",
        "+import types\n",
        "+from typing import Any\n",
        "+import unittest\n",
        "+\n",
        "+from _pytest import outcomes\n",
        "+from _pytest._code import ExceptionInfo\n",
        "+from _pytest.capture import CaptureManager\n",
        "+from _pytest.config import Config\n",
        "+from _pytest.config import ConftestImportFailure\n",
        "+from _pytest.config import hookimpl\n",
        "+from _pytest.config import PytestPluginManager\n",
        "+from _pytest.config.argparsing import Parser\n",
        "+from _pytest.config.exceptions import UsageError\n",
        "+from _pytest.nodes import Node\n",
        "+from _pytest.reports import BaseReport\n",
        "+from _pytest.runner import CallInfo\n",
        "+\n",
        "+\n",
        "+def _validate_usepdb_cls(value: str) -> tuple[str, str]:\n",
        "+    \"\"\"Validate syntax of --pdbcls option.\"\"\"\n",
        "+    try:\n",
        "+        modname, classname = value.split(\":\")\n",
        "+    except ValueError as e:\n",
        "+        raise argparse.ArgumentTypeError(\n",
        "+            f\"{value!r} is not in the format 'modname:classname'\"\n",
        "+        ) from e\n",
        "+    return (modname, classname)\n",
        "+\n",
        "+\n",
        "+def pytest_addoption(parser: Parser) -> None:\n",
        "+    group = parser.getgroup(\"general\")\n",
        "+    group.addoption(\n",
        "+        \"--pdb\",\n",
        "+        dest=\"usepdb\",\n",
        "+        action=\"store_true\",\n",
        "+        help=\"Start the interactive Python debugger on errors or KeyboardInterrupt\",\n",
        "+    )\n",
        "+    group.addoption(\n",
        "+        \"--pdbcls\",\n",
        "+        dest=\"usepdb_cls\",\n",
        "+        metavar=\"modulename:classname\",\n",
        "+        type=_validate_usepdb_cls,\n",
        "+        help=\"Specify a custom interactive Python debugger for use with --pdb.\"\n",
        "+        \"For example: --pdbcls=IPython.terminal.debugger:TerminalPdb\",\n",
        "+    )\n",
        "+    group.addoption(\n",
        "+        \"--trace\",\n",
        "+        dest=\"trace\",\n",
        "+        action=\"store_true\",\n",
        "+        help=\"Immediately break when running each test\",\n",
        "+    )\n",
        "+\n",
        "+\n",
        "+def pytest_configure(config: Config) -> None:\n",
        "+    import pdb\n",
        "+\n",
        "+    if config.getvalue(\"trace\"):\n",
        "+        config.pluginmanager.register(PdbTrace(), \"pdbtrace\")\n",
        "+    if config.getvalue(\"usepdb\"):\n",
        "+        config.pluginmanager.register(PdbInvoke(), \"pdbinvoke\")\n",
        "+\n",
        "+    pytestPDB._saved.append(\n",
        "+        (pdb.set_trace, pytestPDB._pluginmanager, pytestPDB._config)\n",
        "+    )\n",
        "+    pdb.set_trace = pytestPDB.set_trace\n",
        "+    pytestPDB._pluginmanager = config.pluginmanager\n",
        "+    pytestPDB._config = config\n",
        "+\n",
        "+    # NOTE: not using pytest_unconfigure, since it might get called although\n",
        "+    #       pytest_configure was not (if another plugin raises UsageError).\n",
        "+    def fin() -> None:\n",
        "+        (\n",
        "+            pdb.set_trace,\n",
        "+            pytestPDB._pluginmanager,\n",
        "+            pytestPDB._config,\n",
        "+        ) = pytestPDB._saved.pop()\n",
        "+\n",
        "+    config.add_cleanup(fin)\n",
        "+\n",
        "+\n",
        "+class pytestPDB:\n",
        "+    \"\"\"Pseudo PDB that defers to the real pdb.\"\"\"\n",
        "+\n",
        "+    _pluginmanager: PytestPluginManager | None = None\n",
        "+    _config: Config | None = None\n",
        "+    _saved: list[\n",
        "+        tuple[Callable[..., None], PytestPluginManager | None, Config | None]\n",
        "+    ] = []\n",
        "+    _recursive_debug = 0\n",
        "+    _wrapped_pdb_cls: tuple[type[Any], type[Any]] | None = None\n",
        "+\n",
        "+    @classmethod\n",
        "+    def _is_capturing(cls, capman: CaptureManager | None) -> str | bool:\n",
        "+        if capman:\n",
        "+            return capman.is_capturing()\n",
        "+        return False\n",
        "+\n",
        "+    @classmethod\n",
        "+    def _import_pdb_cls(cls, capman: CaptureManager | None):\n",
        "+        if not cls._config:\n",
        "+            import pdb\n",
        "+\n",
        "+            # Happens when using pytest.set_trace outside of a test.\n",
        "+            return pdb.Pdb\n",
        "+\n",
        "+        usepdb_cls = cls._config.getvalue(\"usepdb_cls\")\n",
        "+\n",
        "+        if cls._wrapped_pdb_cls and cls._wrapped_pdb_cls[0] == usepdb_cls:\n",
        "+            return cls._wrapped_pdb_cls[1]\n",
        "+\n",
        "+        if usepdb_cls:\n",
        "+            modname, classname = usepdb_cls\n",
        "+\n",
        "+            try:\n",
        "+                __import__(modname)\n",
        "+                mod = sys.modules[modname]\n",
        "+\n",
        "+                # Handle --pdbcls=pdb:pdb.Pdb (useful e.g. with pdbpp).\n",
        "+                parts = classname.split(\".\")\n",
        "+                pdb_cls = getattr(mod, parts[0])\n",
        "+                for part in parts[1:]:\n",
        "+                    pdb_cls = getattr(pdb_cls, part)\n",
        "+            except Exception as exc:\n",
        "+                value = \":\".join((modname, classname))\n",
        "+                raise UsageError(\n",
        "+                    f\"--pdbcls: could not import {value!r}: {exc}\"\n",
        "+                ) from exc\n",
        "+        else:\n",
        "+            import pdb\n",
        "+\n",
        "+            pdb_cls = pdb.Pdb\n",
        "+\n",
        "+        wrapped_cls = cls._get_pdb_wrapper_class(pdb_cls, capman)\n",
        "+        cls._wrapped_pdb_cls = (usepdb_cls, wrapped_cls)\n",
        "+        return wrapped_cls\n",
        "+\n",
        "+    @classmethod\n",
        "+    def _get_pdb_wrapper_class(cls, pdb_cls, capman: CaptureManager | None):\n",
        "+        import _pytest.config\n",
        "+\n",
        "+        class PytestPdbWrapper(pdb_cls):\n",
        "+            _pytest_capman = capman\n",
        "+            _continued = False\n",
        "+\n",
        "+            def do_debug(self, arg):\n",
        "+                cls._recursive_debug += 1\n",
        "+                ret = super().do_debug(arg)\n",
        "+                cls._recursive_debug -= 1\n",
        "+                return ret\n",
        "+\n",
        "+            if hasattr(pdb_cls, \"do_debug\"):\n",
        "+                do_debug.__doc__ = pdb_cls.do_debug.__doc__\n",
        "+\n",
        "+            def do_continue(self, arg):\n",
        "+                ret = super().do_continue(arg)\n",
        "+                if cls._recursive_debug == 0:\n",
        "+                    assert cls._config is not None\n",
        "+                    tw = _pytest.config.create_terminal_writer(cls._config)\n",
        "+                    tw.line()\n",
        "+\n",
        "+                    capman = self._pytest_capman\n",
        "+                    capturing = pytestPDB._is_capturing(capman)\n",
        "+                    if capturing:\n",
        "+                        if capturing == \"global\":\n",
        "+                            tw.sep(\">\", \"PDB continue (IO-capturing resumed)\")\n",
        "+                        else:\n",
        "+                            tw.sep(\n",
        "+                                \">\",\n",
        "+                                f\"PDB continue (IO-capturing resumed for {capturing})\",\n",
        "+                            )\n",
        "+                        assert capman is not None\n",
        "+                        capman.resume()\n",
        "+                    else:\n",
        "+                        tw.sep(\">\", \"PDB continue\")\n",
        "+                assert cls._pluginmanager is not None\n",
        "+                cls._pluginmanager.hook.pytest_leave_pdb(config=cls._config, pdb=self)\n",
        "+                self._continued = True\n",
        "+                return ret\n",
        "+\n",
        "+            if hasattr(pdb_cls, \"do_continue\"):\n",
        "+                do_continue.__doc__ = pdb_cls.do_continue.__doc__\n",
        "+\n",
        "+            do_c = do_cont = do_continue\n",
        "+\n",
        "+            def do_quit(self, arg):\n",
        "+                # Raise Exit outcome when quit command is used in pdb.\n",
        "+                #\n",
        "+                # This is a bit of a hack - it would be better if BdbQuit\n",
        "+                # could be handled, but this would require to wrap the\n",
        "+                # whole pytest run, and adjust the report etc.\n",
        "+                ret = super().do_quit(arg)\n",
        "+\n",
        "+                if cls._recursive_debug == 0:\n",
        "+                    outcomes.exit(\"Quitting debugger\")\n",
        "+\n",
        "+                return ret\n",
        "+\n",
        "+            if hasattr(pdb_cls, \"do_quit\"):\n",
        "+                do_quit.__doc__ = pdb_cls.do_quit.__doc__\n",
        "+\n",
        "+            do_q = do_quit\n",
        "+            do_exit = do_quit\n",
        "+\n",
        "+            def setup(self, f, tb):\n",
        "+                \"\"\"Suspend on setup().\n",
        "+\n",
        "+                Needed after do_continue resumed, and entering another\n",
        "+                breakpoint again.\n",
        "+                \"\"\"\n",
        "+                ret = super().setup(f, tb)\n",
        "+                if not ret and self._continued:\n",
        "+                    # pdb.setup() returns True if the command wants to exit\n",
        "+                    # from the interaction: do not suspend capturing then.\n",
        "+                    if self._pytest_capman:\n",
        "+                        self._pytest_capman.suspend_global_capture(in_=True)\n",
        "+                return ret\n",
        "+\n",
        "+            def get_stack(self, f, t):\n",
        "+                stack, i = super().get_stack(f, t)\n",
        "+                if f is None:\n",
        "+                    # Find last non-hidden frame.\n",
        "+                    i = max(0, len(stack) - 1)\n",
        "+                    while i and stack[i][0].f_locals.get(\"__tracebackhide__\", False):\n",
        "+                        i -= 1\n",
        "+                return stack, i\n",
        "+\n",
        "+        return PytestPdbWrapper\n",
        "+\n",
        "+    @classmethod\n",
        "+    def _init_pdb(cls, method, *args, **kwargs):\n",
        "+        \"\"\"Initialize PDB debugging, dropping any IO capturing.\"\"\"\n",
        "+        import _pytest.config\n",
        "+\n",
        "+        if cls._pluginmanager is None:\n",
        "+            capman: CaptureManager | None = None\n",
        "+        else:\n",
        "+            capman = cls._pluginmanager.getplugin(\"capturemanager\")\n",
        "+        if capman:\n",
        "+            capman.suspend(in_=True)\n",
        "+\n",
        "+        if cls._config:\n",
        "+            tw = _pytest.config.create_terminal_writer(cls._config)\n",
        "+            tw.line()\n",
        "+\n",
        "+            if cls._recursive_debug == 0:\n",
        "+                # Handle header similar to pdb.set_trace in py37+.\n",
        "+                header = kwargs.pop(\"header\", None)\n",
        "+                if header is not None:\n",
        "+                    tw.sep(\">\", header)\n",
        "+                else:\n",
        "+                    capturing = cls._is_capturing(capman)\n",
        "+                    if capturing == \"global\":\n",
        "+                        tw.sep(\">\", f\"PDB {method} (IO-capturing turned off)\")\n",
        "+                    elif capturing:\n",
        "+                        tw.sep(\n",
        "+                            \">\",\n",
        "+                            f\"PDB {method} (IO-capturing turned off for {capturing})\",\n",
        "+                        )\n",
        "+                    else:\n",
        "+                        tw.sep(\">\", f\"PDB {method}\")\n",
        "+\n",
        "+        _pdb = cls._import_pdb_cls(capman)(**kwargs)\n",
        "+\n",
        "+        if cls._pluginmanager:\n",
        "+            cls._pluginmanager.hook.pytest_enter_pdb(config=cls._config, pdb=_pdb)\n",
        "+        return _pdb\n",
        "+\n",
        "+    @classmethod\n",
        "+    def set_trace(cls, *args, **kwargs) -> None:\n",
        "+        \"\"\"Invoke debugging via ``Pdb.set_trace``, dropping any IO capturing.\"\"\"\n",
        "+        frame = sys._getframe().f_back\n",
        "+        _pdb = cls._init_pdb(\"set_trace\", *args, **kwargs)\n",
        "+        _pdb.set_trace(frame)\n",
        "+\n",
        "+\n",
        "+class PdbInvoke:\n",
        "+    def pytest_exception_interact(\n",
        "+        self, node: Node, call: CallInfo[Any], report: BaseReport\n",
        "+    ) -> None:\n",
        "+        capman = node.config.pluginmanager.getplugin(\"capturemanager\")\n",
        "+        if capman:\n",
        "+            capman.suspend_global_capture(in_=True)\n",
        "+            out, err = capman.read_global_capture()\n",
        "+            sys.stdout.write(out)\n",
        "+            sys.stdout.write(err)\n",
        "+        assert call.excinfo is not None\n",
        "+\n",
        "+        if not isinstance(call.excinfo.value, unittest.SkipTest):\n",
        "+            _enter_pdb(node, call.excinfo, report)\n",
        "+\n",
        "+    def pytest_internalerror(self, excinfo: ExceptionInfo[BaseException]) -> None:\n",
        "+        exc_or_tb = _postmortem_exc_or_tb(excinfo)\n",
        "+        post_mortem(exc_or_tb)\n",
        "+\n",
        "+\n",
        "+class PdbTrace:\n",
        "+    @hookimpl(wrapper=True)\n",
        "+    def pytest_pyfunc_call(self, pyfuncitem) -> Generator[None, object, object]:\n",
        "+        wrap_pytest_function_for_tracing(pyfuncitem)\n",
        "+        return (yield)\n",
        "+\n",
        "+\n",
        "+def wrap_pytest_function_for_tracing(pyfuncitem) -> None:\n",
        "+    \"\"\"Change the Python function object of the given Function item by a\n",
        "+    wrapper which actually enters pdb before calling the python function\n",
        "+    itself, effectively leaving the user in the pdb prompt in the first\n",
        "+    statement of the function.\"\"\"\n",
        "+    _pdb = pytestPDB._init_pdb(\"runcall\")\n",
        "+    testfunction = pyfuncitem.obj\n",
        "+\n",
        "+    # we can't just return `partial(pdb.runcall, testfunction)` because (on\n",
        "+    # python < 3.7.4) runcall's first param is `func`, which means we'd get\n",
        "+    # an exception if one of the kwargs to testfunction was called `func`.\n",
        "+    @functools.wraps(testfunction)\n",
        "+    def wrapper(*args, **kwargs) -> None:\n",
        "+        func = functools.partial(testfunction, *args, **kwargs)\n",
        "+        _pdb.runcall(func)\n",
        "+\n",
        "+    pyfuncitem.obj = wrapper\n",
        "+\n",
        "+\n",
        "+def maybe_wrap_pytest_function_for_tracing(pyfuncitem) -> None:\n",
        "+    \"\"\"Wrap the given pytestfunct item for tracing support if --trace was given in\n",
        "+    the command line.\"\"\"\n",
        "+    if pyfuncitem.config.getvalue(\"trace\"):\n",
        "+        wrap_pytest_function_for_tracing(pyfuncitem)\n",
        "+\n",
        "+\n",
        "+def _enter_pdb(\n",
        "+    node: Node, excinfo: ExceptionInfo[BaseException], rep: BaseReport\n",
        "+) -> BaseReport:\n",
        "+    # XXX we reuse the TerminalReporter's terminalwriter\n",
        "+    # because this seems to avoid some encoding related troubles\n",
        "+    # for not completely clear reasons.\n",
        "+    tw = node.config.pluginmanager.getplugin(\"terminalreporter\")._tw\n",
        "+    tw.line()\n",
        "+\n",
        "+    showcapture = node.config.option.showcapture\n",
        "+\n",
        "+    for sectionname, content in (\n",
        "+        (\"stdout\", rep.capstdout),\n",
        "+        (\"stderr\", rep.capstderr),\n",
        "+        (\"log\", rep.caplog),\n",
        "+    ):\n",
        "+        if showcapture in (sectionname, \"all\") and content:\n",
        "+            tw.sep(\">\", \"captured \" + sectionname)\n",
        "+            if content[-1:] == \"\\n\":\n",
        "+                content = content[:-1]\n",
        "+            tw.line(content)\n",
        "+\n",
        "+    tw.sep(\">\", \"traceback\")\n",
        "+    rep.toterminal(tw)\n",
        "+    tw.sep(\">\", \"entering PDB\")\n",
        "+    tb_or_exc = _postmortem_exc_or_tb(excinfo)\n",
        "+    rep._pdbshown = True  # type: ignore[attr-defined]\n",
        "+    post_mortem(tb_or_exc)\n",
        "+    return rep\n",
        "+\n",
        "+\n",
        "+def _postmortem_exc_or_tb(\n",
        "+    excinfo: ExceptionInfo[BaseException],\n",
        "+) -> types.TracebackType | BaseException:\n",
        "+    from doctest import UnexpectedException\n",
        "+\n",
        "+    get_exc = sys.version_info >= (3, 13)\n",
        "+    if isinstance(excinfo.value, UnexpectedException):\n",
        "+        # A doctest.UnexpectedException is not useful for post_mortem.\n",
        "+        # Use the underlying exception instead:\n",
        "+        underlying_exc = excinfo.value\n",
        "+        if get_exc:\n",
        "+            return underlying_exc.exc_info[1]\n",
        "+\n",
        "+        return underlying_exc.exc_info[2]\n",
        "+    elif isinstance(excinfo.value, ConftestImportFailure):\n",
        "+        # A config.ConftestImportFailure is not useful for post_mortem.\n",
        "+        # Use the underlying exception instead:\n",
        "+        cause = excinfo.value.cause\n",
        "+        if get_exc:\n",
        "+            return cause\n",
        "+\n",
        "+        assert cause.__traceback__ is not None\n",
        "+        return cause.__traceback__\n",
        "+    else:\n",
        "+        assert excinfo._excinfo is not None\n",
        "+        if get_exc:\n",
        "+            return excinfo._excinfo[1]\n",
        "+\n",
        "+        return excinfo._excinfo[2]\n",
        "+\n",
        "+\n",
        "+def post_mortem(tb_or_exc: types.TracebackType | BaseException) -> None:\n",
        "+    p = pytestPDB._init_pdb(\"post_mortem\")\n",
        "+    p.reset()\n",
        "+    p.interaction(None, tb_or_exc)\n",
        "+    if p.quitting:\n",
        "+        outcomes.exit(\"Quitting debugger\")\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/deprecated.py",
      "status": "added",
      "additions": 91,
      "deletions": 0,
      "patch": "@@ -0,0 +1,91 @@\n+\"\"\"Deprecation messages and bits of code used elsewhere in the codebase that\n+is planned to be removed in the next pytest release.\n+\n+Keeping it in a central location makes it easy to track what is deprecated and should\n+be removed when the time comes.\n+\n+All constants defined in this module should be either instances of\n+:class:`PytestWarning`, or :class:`UnformattedWarning`\n+in case of warnings which need to format their messages.\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from warnings import warn\n+\n+from _pytest.warning_types import PytestDeprecationWarning\n+from _pytest.warning_types import PytestRemovedIn9Warning\n+from _pytest.warning_types import UnformattedWarning\n+\n+\n+# set of plugins which have been integrated into the core; we use this list to ignore\n+# them during registration to avoid conflicts\n+DEPRECATED_EXTERNAL_PLUGINS = {\n+    \"pytest_catchlog\",\n+    \"pytest_capturelog\",\n+    \"pytest_faulthandler\",\n+}\n+\n+\n+# This can be* removed pytest 8, but it's harmless and common, so no rush to remove.\n+# * If you're in the future: \"could have been\".\n+YIELD_FIXTURE = PytestDeprecationWarning(\n+    \"@pytest.yield_fixture is deprecated.\\n\"\n+    \"Use @pytest.fixture instead; they are the same.\"\n+)\n+\n+# This deprecation is never really meant to be removed.\n+PRIVATE = PytestDeprecationWarning(\"A private pytest class or function was used.\")\n+\n+\n+HOOK_LEGACY_PATH_ARG = UnformattedWarning(\n+    PytestRemovedIn9Warning,\n+    \"The ({pylib_path_arg}: py.path.local) argument is deprecated, please use ({pathlib_path_arg}: pathlib.Path)\\n\"\n+    \"see https://docs.pytest.org/en/latest/deprecations.html\"\n+    \"#py-path-local-arguments-for-hooks-replaced-with-pathlib-path\",\n+)\n+\n+NODE_CTOR_FSPATH_ARG = UnformattedWarning(\n+    PytestRemovedIn9Warning,\n+    \"The (fspath: py.path.local) argument to {node_type_name} is deprecated. \"\n+    \"Please use the (path: pathlib.Path) argument instead.\\n\"\n+    \"See https://docs.pytest.org/en/latest/deprecations.html\"\n+    \"#fspath-argument-for-node-constructors-replaced-with-pathlib-path\",\n+)\n+\n+HOOK_LEGACY_MARKING = UnformattedWarning(\n+    PytestDeprecationWarning,\n+    \"The hook{type} {fullname} uses old-style configuration options (marks or attributes).\\n\"\n+    \"Please use the pytest.hook{type}({hook_opts}) decorator instead\\n\"\n+    \" to configure the hooks.\\n\"\n+    \" See https://docs.pytest.org/en/latest/deprecations.html\"\n+    \"#configuring-hook-specs-impls-using-markers\",\n+)\n+\n+MARKED_FIXTURE = PytestRemovedIn9Warning(\n+    \"Marks applied to fixtures have no effect\\n\"\n+    \"See docs: https://docs.pytest.org/en/stable/deprecations.html#applying-a-mark-to-a-fixture-function\"\n+)\n+\n+# You want to make some `__init__` or function \"private\".\n+#\n+#   def my_private_function(some, args):\n+#       ...\n+#\n+# Do this:\n+#\n+#   def my_private_function(some, args, *, _ispytest: bool = False):\n+#       check_ispytest(_ispytest)\n+#       ...\n+#\n+# Change all internal/allowed calls to\n+#\n+#   my_private_function(some, args, _ispytest=True)\n+#\n+# All other calls will get the default _ispytest=False and trigger\n+# the warning (possibly error in the future).\n+\n+\n+def check_ispytest(ispytest: bool) -> None:\n+    if not ispytest:\n+        warn(PRIVATE, stacklevel=3)",
      "patch_lines": [
        "@@ -0,0 +1,91 @@\n",
        "+\"\"\"Deprecation messages and bits of code used elsewhere in the codebase that\n",
        "+is planned to be removed in the next pytest release.\n",
        "+\n",
        "+Keeping it in a central location makes it easy to track what is deprecated and should\n",
        "+be removed when the time comes.\n",
        "+\n",
        "+All constants defined in this module should be either instances of\n",
        "+:class:`PytestWarning`, or :class:`UnformattedWarning`\n",
        "+in case of warnings which need to format their messages.\n",
        "+\"\"\"\n",
        "+\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+from warnings import warn\n",
        "+\n",
        "+from _pytest.warning_types import PytestDeprecationWarning\n",
        "+from _pytest.warning_types import PytestRemovedIn9Warning\n",
        "+from _pytest.warning_types import UnformattedWarning\n",
        "+\n",
        "+\n",
        "+# set of plugins which have been integrated into the core; we use this list to ignore\n",
        "+# them during registration to avoid conflicts\n",
        "+DEPRECATED_EXTERNAL_PLUGINS = {\n",
        "+    \"pytest_catchlog\",\n",
        "+    \"pytest_capturelog\",\n",
        "+    \"pytest_faulthandler\",\n",
        "+}\n",
        "+\n",
        "+\n",
        "+# This can be* removed pytest 8, but it's harmless and common, so no rush to remove.\n",
        "+# * If you're in the future: \"could have been\".\n",
        "+YIELD_FIXTURE = PytestDeprecationWarning(\n",
        "+    \"@pytest.yield_fixture is deprecated.\\n\"\n",
        "+    \"Use @pytest.fixture instead; they are the same.\"\n",
        "+)\n",
        "+\n",
        "+# This deprecation is never really meant to be removed.\n",
        "+PRIVATE = PytestDeprecationWarning(\"A private pytest class or function was used.\")\n",
        "+\n",
        "+\n",
        "+HOOK_LEGACY_PATH_ARG = UnformattedWarning(\n",
        "+    PytestRemovedIn9Warning,\n",
        "+    \"The ({pylib_path_arg}: py.path.local) argument is deprecated, please use ({pathlib_path_arg}: pathlib.Path)\\n\"\n",
        "+    \"see https://docs.pytest.org/en/latest/deprecations.html\"\n",
        "+    \"#py-path-local-arguments-for-hooks-replaced-with-pathlib-path\",\n",
        "+)\n",
        "+\n",
        "+NODE_CTOR_FSPATH_ARG = UnformattedWarning(\n",
        "+    PytestRemovedIn9Warning,\n",
        "+    \"The (fspath: py.path.local) argument to {node_type_name} is deprecated. \"\n",
        "+    \"Please use the (path: pathlib.Path) argument instead.\\n\"\n",
        "+    \"See https://docs.pytest.org/en/latest/deprecations.html\"\n",
        "+    \"#fspath-argument-for-node-constructors-replaced-with-pathlib-path\",\n",
        "+)\n",
        "+\n",
        "+HOOK_LEGACY_MARKING = UnformattedWarning(\n",
        "+    PytestDeprecationWarning,\n",
        "+    \"The hook{type} {fullname} uses old-style configuration options (marks or attributes).\\n\"\n",
        "+    \"Please use the pytest.hook{type}({hook_opts}) decorator instead\\n\"\n",
        "+    \" to configure the hooks.\\n\"\n",
        "+    \" See https://docs.pytest.org/en/latest/deprecations.html\"\n",
        "+    \"#configuring-hook-specs-impls-using-markers\",\n",
        "+)\n",
        "+\n",
        "+MARKED_FIXTURE = PytestRemovedIn9Warning(\n",
        "+    \"Marks applied to fixtures have no effect\\n\"\n",
        "+    \"See docs: https://docs.pytest.org/en/stable/deprecations.html#applying-a-mark-to-a-fixture-function\"\n",
        "+)\n",
        "+\n",
        "+# You want to make some `__init__` or function \"private\".\n",
        "+#\n",
        "+#   def my_private_function(some, args):\n",
        "+#       ...\n",
        "+#\n",
        "+# Do this:\n",
        "+#\n",
        "+#   def my_private_function(some, args, *, _ispytest: bool = False):\n",
        "+#       check_ispytest(_ispytest)\n",
        "+#       ...\n",
        "+#\n",
        "+# Change all internal/allowed calls to\n",
        "+#\n",
        "+#   my_private_function(some, args, _ispytest=True)\n",
        "+#\n",
        "+# All other calls will get the default _ispytest=False and trigger\n",
        "+# the warning (possibly error in the future).\n",
        "+\n",
        "+\n",
        "+def check_ispytest(ispytest: bool) -> None:\n",
        "+    if not ispytest:\n",
        "+        warn(PRIVATE, stacklevel=3)\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/doctest.py",
      "status": "added",
      "additions": 754,
      "deletions": 0,
      "patch": "@@ -0,0 +1,754 @@\n+# mypy: allow-untyped-defs\n+\"\"\"Discover and run doctests in modules and test files.\"\"\"\n+\n+from __future__ import annotations\n+\n+import bdb\n+from collections.abc import Callable\n+from collections.abc import Generator\n+from collections.abc import Iterable\n+from collections.abc import Sequence\n+from contextlib import contextmanager\n+import functools\n+import inspect\n+import os\n+from pathlib import Path\n+import platform\n+import re\n+import sys\n+import traceback\n+import types\n+from typing import Any\n+from typing import TYPE_CHECKING\n+import warnings\n+\n+from _pytest import outcomes\n+from _pytest._code.code import ExceptionInfo\n+from _pytest._code.code import ReprFileLocation\n+from _pytest._code.code import TerminalRepr\n+from _pytest._io import TerminalWriter\n+from _pytest.compat import safe_getattr\n+from _pytest.config import Config\n+from _pytest.config.argparsing import Parser\n+from _pytest.fixtures import fixture\n+from _pytest.fixtures import TopRequest\n+from _pytest.nodes import Collector\n+from _pytest.nodes import Item\n+from _pytest.outcomes import OutcomeException\n+from _pytest.outcomes import skip\n+from _pytest.pathlib import fnmatch_ex\n+from _pytest.python import Module\n+from _pytest.python_api import approx\n+from _pytest.warning_types import PytestWarning\n+\n+\n+if TYPE_CHECKING:\n+    import doctest\n+\n+    from typing_extensions import Self\n+\n+DOCTEST_REPORT_CHOICE_NONE = \"none\"\n+DOCTEST_REPORT_CHOICE_CDIFF = \"cdiff\"\n+DOCTEST_REPORT_CHOICE_NDIFF = \"ndiff\"\n+DOCTEST_REPORT_CHOICE_UDIFF = \"udiff\"\n+DOCTEST_REPORT_CHOICE_ONLY_FIRST_FAILURE = \"only_first_failure\"\n+\n+DOCTEST_REPORT_CHOICES = (\n+    DOCTEST_REPORT_CHOICE_NONE,\n+    DOCTEST_REPORT_CHOICE_CDIFF,\n+    DOCTEST_REPORT_CHOICE_NDIFF,\n+    DOCTEST_REPORT_CHOICE_UDIFF,\n+    DOCTEST_REPORT_CHOICE_ONLY_FIRST_FAILURE,\n+)\n+\n+# Lazy definition of runner class\n+RUNNER_CLASS = None\n+# Lazy definition of output checker class\n+CHECKER_CLASS: type[doctest.OutputChecker] | None = None\n+\n+\n+def pytest_addoption(parser: Parser) -> None:\n+    parser.addini(\n+        \"doctest_optionflags\",\n+        \"Option flags for doctests\",\n+        type=\"args\",\n+        default=[\"ELLIPSIS\"],\n+    )\n+    parser.addini(\n+        \"doctest_encoding\", \"Encoding used for doctest files\", default=\"utf-8\"\n+    )\n+    group = parser.getgroup(\"collect\")\n+    group.addoption(\n+        \"--doctest-modules\",\n+        action=\"store_true\",\n+        default=False,\n+        help=\"Run doctests in all .py modules\",\n+        dest=\"doctestmodules\",\n+    )\n+    group.addoption(\n+        \"--doctest-report\",\n+        type=str.lower,\n+        default=\"udiff\",\n+        help=\"Choose another output format for diffs on doctest failure\",\n+        choices=DOCTEST_REPORT_CHOICES,\n+        dest=\"doctestreport\",\n+    )\n+    group.addoption(\n+        \"--doctest-glob\",\n+        action=\"append\",\n+        default=[],\n+        metavar=\"pat\",\n+        help=\"Doctests file matching pattern, default: test*.txt\",\n+        dest=\"doctestglob\",\n+    )\n+    group.addoption(\n+        \"--doctest-ignore-import-errors\",\n+        action=\"store_true\",\n+        default=False,\n+        help=\"Ignore doctest collection errors\",\n+        dest=\"doctest_ignore_import_errors\",\n+    )\n+    group.addoption(\n+        \"--doctest-continue-on-failure\",\n+        action=\"store_true\",\n+        default=False,\n+        help=\"For a given doctest, continue to run after the first failure\",\n+        dest=\"doctest_continue_on_failure\",\n+    )\n+\n+\n+def pytest_unconfigure() -> None:\n+    global RUNNER_CLASS\n+\n+    RUNNER_CLASS = None\n+\n+\n+def pytest_collect_file(\n+    file_path: Path,\n+    parent: Collector,\n+) -> DoctestModule | DoctestTextfile | None:\n+    config = parent.config\n+    if file_path.suffix == \".py\":\n+        if config.option.doctestmodules and not any(\n+            (_is_setup_py(file_path), _is_main_py(file_path))\n+        ):\n+            return DoctestModule.from_parent(parent, path=file_path)\n+    elif _is_doctest(config, file_path, parent):\n+        return DoctestTextfile.from_parent(parent, path=file_path)\n+    return None\n+\n+\n+def _is_setup_py(path: Path) -> bool:\n+    if path.name != \"setup.py\":\n+        return False\n+    contents = path.read_bytes()\n+    return b\"setuptools\" in contents or b\"distutils\" in contents\n+\n+\n+def _is_doctest(config: Config, path: Path, parent: Collector) -> bool:\n+    if path.suffix in (\".txt\", \".rst\") and parent.session.isinitpath(path):\n+        return True\n+    globs = config.getoption(\"doctestglob\") or [\"test*.txt\"]\n+    return any(fnmatch_ex(glob, path) for glob in globs)\n+\n+\n+def _is_main_py(path: Path) -> bool:\n+    return path.name == \"__main__.py\"\n+\n+\n+class ReprFailDoctest(TerminalRepr):\n+    def __init__(\n+        self, reprlocation_lines: Sequence[tuple[ReprFileLocation, Sequence[str]]]\n+    ) -> None:\n+        self.reprlocation_lines = reprlocation_lines\n+\n+    def toterminal(self, tw: TerminalWriter) -> None:\n+        for reprlocation, lines in self.reprlocation_lines:\n+            for line in lines:\n+                tw.line(line)\n+            reprlocation.toterminal(tw)\n+\n+\n+class MultipleDoctestFailures(Exception):\n+    def __init__(self, failures: Sequence[doctest.DocTestFailure]) -> None:\n+        super().__init__()\n+        self.failures = failures\n+\n+\n+def _init_runner_class() -> type[doctest.DocTestRunner]:\n+    import doctest\n+\n+    class PytestDoctestRunner(doctest.DebugRunner):\n+        \"\"\"Runner to collect failures.\n+\n+        Note that the out variable in this case is a list instead of a\n+        stdout-like object.\n+        \"\"\"\n+\n+        def __init__(\n+            self,\n+            checker: doctest.OutputChecker | None = None,\n+            verbose: bool | None = None,\n+            optionflags: int = 0,\n+            continue_on_failure: bool = True,\n+        ) -> None:\n+            super().__init__(checker=checker, verbose=verbose, optionflags=optionflags)\n+            self.continue_on_failure = continue_on_failure\n+\n+        def report_failure(\n+            self,\n+            out,\n+            test: doctest.DocTest,\n+            example: doctest.Example,\n+            got: str,\n+        ) -> None:\n+            failure = doctest.DocTestFailure(test, example, got)\n+            if self.continue_on_failure:\n+                out.append(failure)\n+            else:\n+                raise failure\n+\n+        def report_unexpected_exception(\n+            self,\n+            out,\n+            test: doctest.DocTest,\n+            example: doctest.Example,\n+            exc_info: tuple[type[BaseException], BaseException, types.TracebackType],\n+        ) -> None:\n+            if isinstance(exc_info[1], OutcomeException):\n+                raise exc_info[1]\n+            if isinstance(exc_info[1], bdb.BdbQuit):\n+                outcomes.exit(\"Quitting debugger\")\n+            failure = doctest.UnexpectedException(test, example, exc_info)\n+            if self.continue_on_failure:\n+                out.append(failure)\n+            else:\n+                raise failure\n+\n+    return PytestDoctestRunner\n+\n+\n+def _get_runner(\n+    checker: doctest.OutputChecker | None = None,\n+    verbose: bool | None = None,\n+    optionflags: int = 0,\n+    continue_on_failure: bool = True,\n+) -> doctest.DocTestRunner:\n+    # We need this in order to do a lazy import on doctest\n+    global RUNNER_CLASS\n+    if RUNNER_CLASS is None:\n+        RUNNER_CLASS = _init_runner_class()\n+    # Type ignored because the continue_on_failure argument is only defined on\n+    # PytestDoctestRunner, which is lazily defined so can't be used as a type.\n+    return RUNNER_CLASS(  # type: ignore\n+        checker=checker,\n+        verbose=verbose,\n+        optionflags=optionflags,\n+        continue_on_failure=continue_on_failure,\n+    )\n+\n+\n+class DoctestItem(Item):\n+    def __init__(\n+        self,\n+        name: str,\n+        parent: DoctestTextfile | DoctestModule,\n+        runner: doctest.DocTestRunner,\n+        dtest: doctest.DocTest,\n+    ) -> None:\n+        super().__init__(name, parent)\n+        self.runner = runner\n+        self.dtest = dtest\n+\n+        # Stuff needed for fixture support.\n+        self.obj = None\n+        fm = self.session._fixturemanager\n+        fixtureinfo = fm.getfixtureinfo(node=self, func=None, cls=None)\n+        self._fixtureinfo = fixtureinfo\n+        self.fixturenames = fixtureinfo.names_closure\n+        self._initrequest()\n+\n+    @classmethod\n+    def from_parent(  # type: ignore[override]\n+        cls,\n+        parent: DoctestTextfile | DoctestModule,\n+        *,\n+        name: str,\n+        runner: doctest.DocTestRunner,\n+        dtest: doctest.DocTest,\n+    ) -> Self:\n+        # incompatible signature due to imposed limits on subclass\n+        \"\"\"The public named constructor.\"\"\"\n+        return super().from_parent(name=name, parent=parent, runner=runner, dtest=dtest)\n+\n+    def _initrequest(self) -> None:\n+        self.funcargs: dict[str, object] = {}\n+        self._request = TopRequest(self, _ispytest=True)  # type: ignore[arg-type]\n+\n+    def setup(self) -> None:\n+        self._request._fillfixtures()\n+        globs = dict(getfixture=self._request.getfixturevalue)\n+        for name, value in self._request.getfixturevalue(\"doctest_namespace\").items():\n+            globs[name] = value\n+        self.dtest.globs.update(globs)\n+\n+    def runtest(self) -> None:\n+        _check_all_skipped(self.dtest)\n+        self._disable_output_capturing_for_darwin()\n+        failures: list[doctest.DocTestFailure] = []\n+        # Type ignored because we change the type of `out` from what\n+        # doctest expects.\n+        self.runner.run(self.dtest, out=failures)  # type: ignore[arg-type]\n+        if failures:\n+            raise MultipleDoctestFailures(failures)\n+\n+    def _disable_output_capturing_for_darwin(self) -> None:\n+        \"\"\"Disable output capturing. Otherwise, stdout is lost to doctest (#985).\"\"\"\n+        if platform.system() != \"Darwin\":\n+            return\n+        capman = self.config.pluginmanager.getplugin(\"capturemanager\")\n+        if capman:\n+            capman.suspend_global_capture(in_=True)\n+            out, err = capman.read_global_capture()\n+            sys.stdout.write(out)\n+            sys.stderr.write(err)\n+\n+    # TODO: Type ignored -- breaks Liskov Substitution.\n+    def repr_failure(  # type: ignore[override]\n+        self,\n+        excinfo: ExceptionInfo[BaseException],\n+    ) -> str | TerminalRepr:\n+        import doctest\n+\n+        failures: (\n+            Sequence[doctest.DocTestFailure | doctest.UnexpectedException] | None\n+        ) = None\n+        if isinstance(\n+            excinfo.value, (doctest.DocTestFailure, doctest.UnexpectedException)\n+        ):\n+            failures = [excinfo.value]\n+        elif isinstance(excinfo.value, MultipleDoctestFailures):\n+            failures = excinfo.value.failures\n+\n+        if failures is None:\n+            return super().repr_failure(excinfo)\n+\n+        reprlocation_lines = []\n+        for failure in failures:\n+            example = failure.example\n+            test = failure.test\n+            filename = test.filename\n+            if test.lineno is None:\n+                lineno = None\n+            else:\n+                lineno = test.lineno + example.lineno + 1\n+            message = type(failure).__name__\n+            # TODO: ReprFileLocation doesn't expect a None lineno.\n+            reprlocation = ReprFileLocation(filename, lineno, message)  # type: ignore[arg-type]\n+            checker = _get_checker()\n+            report_choice = _get_report_choice(self.config.getoption(\"doctestreport\"))\n+            if lineno is not None:\n+                assert failure.test.docstring is not None\n+                lines = failure.test.docstring.splitlines(False)\n+                # add line numbers to the left of the error message\n+                assert test.lineno is not None\n+                lines = [\n+                    f\"{i + test.lineno + 1:03d} {x}\" for (i, x) in enumerate(lines)\n+                ]\n+                # trim docstring error lines to 10\n+                lines = lines[max(example.lineno - 9, 0) : example.lineno + 1]\n+            else:\n+                lines = [\n+                    \"EXAMPLE LOCATION UNKNOWN, not showing all tests of that example\"\n+                ]\n+                indent = \">>>\"\n+                for line in example.source.splitlines():\n+                    lines.append(f\"??? {indent} {line}\")\n+                    indent = \"...\"\n+            if isinstance(failure, doctest.DocTestFailure):\n+                lines += checker.output_difference(\n+                    example, failure.got, report_choice\n+                ).split(\"\\n\")\n+            else:\n+                inner_excinfo = ExceptionInfo.from_exc_info(failure.exc_info)\n+                lines += [f\"UNEXPECTED EXCEPTION: {inner_excinfo.value!r}\"]\n+                lines += [\n+                    x.strip(\"\\n\") for x in traceback.format_exception(*failure.exc_info)\n+                ]\n+            reprlocation_lines.append((reprlocation, lines))\n+        return ReprFailDoctest(reprlocation_lines)\n+\n+    def reportinfo(self) -> tuple[os.PathLike[str] | str, int | None, str]:\n+        return self.path, self.dtest.lineno, f\"[doctest] {self.name}\"\n+\n+\n+def _get_flag_lookup() -> dict[str, int]:\n+    import doctest\n+\n+    return dict(\n+        DONT_ACCEPT_TRUE_FOR_1=doctest.DONT_ACCEPT_TRUE_FOR_1,\n+        DONT_ACCEPT_BLANKLINE=doctest.DONT_ACCEPT_BLANKLINE,\n+        NORMALIZE_WHITESPACE=doctest.NORMALIZE_WHITESPACE,\n+        ELLIPSIS=doctest.ELLIPSIS,\n+        IGNORE_EXCEPTION_DETAIL=doctest.IGNORE_EXCEPTION_DETAIL,\n+        COMPARISON_FLAGS=doctest.COMPARISON_FLAGS,\n+        ALLOW_UNICODE=_get_allow_unicode_flag(),\n+        ALLOW_BYTES=_get_allow_bytes_flag(),\n+        NUMBER=_get_number_flag(),\n+    )\n+\n+\n+def get_optionflags(config: Config) -> int:\n+    optionflags_str = config.getini(\"doctest_optionflags\")\n+    flag_lookup_table = _get_flag_lookup()\n+    flag_acc = 0\n+    for flag in optionflags_str:\n+        flag_acc |= flag_lookup_table[flag]\n+    return flag_acc\n+\n+\n+def _get_continue_on_failure(config: Config) -> bool:\n+    continue_on_failure: bool = config.getvalue(\"doctest_continue_on_failure\")\n+    if continue_on_failure:\n+        # We need to turn off this if we use pdb since we should stop at\n+        # the first failure.\n+        if config.getvalue(\"usepdb\"):\n+            continue_on_failure = False\n+    return continue_on_failure\n+\n+\n+class DoctestTextfile(Module):\n+    obj = None\n+\n+    def collect(self) -> Iterable[DoctestItem]:\n+        import doctest\n+\n+        # Inspired by doctest.testfile; ideally we would use it directly,\n+        # but it doesn't support passing a custom checker.\n+        encoding = self.config.getini(\"doctest_encoding\")\n+        text = self.path.read_text(encoding)\n+        filename = str(self.path)\n+        name = self.path.name\n+        globs = {\"__name__\": \"__main__\"}\n+\n+        optionflags = get_optionflags(self.config)\n+\n+        runner = _get_runner(\n+            verbose=False,\n+            optionflags=optionflags,\n+            checker=_get_checker(),\n+            continue_on_failure=_get_continue_on_failure(self.config),\n+        )\n+\n+        parser = doctest.DocTestParser()\n+        test = parser.get_doctest(text, globs, name, filename, 0)\n+        if test.examples:\n+            yield DoctestItem.from_parent(\n+                self, name=test.name, runner=runner, dtest=test\n+            )\n+\n+\n+def _check_all_skipped(test: doctest.DocTest) -> None:\n+    \"\"\"Raise pytest.skip() if all examples in the given DocTest have the SKIP\n+    option set.\"\"\"\n+    import doctest\n+\n+    all_skipped = all(x.options.get(doctest.SKIP, False) for x in test.examples)\n+    if all_skipped:\n+        skip(\"all tests skipped by +SKIP option\")\n+\n+\n+def _is_mocked(obj: object) -> bool:\n+    \"\"\"Return if an object is possibly a mock object by checking the\n+    existence of a highly improbable attribute.\"\"\"\n+    return (\n+        safe_getattr(obj, \"pytest_mock_example_attribute_that_shouldnt_exist\", None)\n+        is not None\n+    )\n+\n+\n+@contextmanager\n+def _patch_unwrap_mock_aware() -> Generator[None]:\n+    \"\"\"Context manager which replaces ``inspect.unwrap`` with a version\n+    that's aware of mock objects and doesn't recurse into them.\"\"\"\n+    real_unwrap = inspect.unwrap\n+\n+    def _mock_aware_unwrap(\n+        func: Callable[..., Any], *, stop: Callable[[Any], Any] | None = None\n+    ) -> Any:\n+        try:\n+            if stop is None or stop is _is_mocked:\n+                return real_unwrap(func, stop=_is_mocked)\n+            _stop = stop\n+            return real_unwrap(func, stop=lambda obj: _is_mocked(obj) or _stop(func))\n+        except Exception as e:\n+            warnings.warn(\n+                f\"Got {e!r} when unwrapping {func!r}.  This is usually caused \"\n+                \"by a violation of Python's object protocol; see e.g. \"\n+                \"https://github.com/pytest-dev/pytest/issues/5080\",\n+                PytestWarning,\n+            )\n+            raise\n+\n+    inspect.unwrap = _mock_aware_unwrap\n+    try:\n+        yield\n+    finally:\n+        inspect.unwrap = real_unwrap\n+\n+\n+class DoctestModule(Module):\n+    def collect(self) -> Iterable[DoctestItem]:\n+        import doctest\n+\n+        class MockAwareDocTestFinder(doctest.DocTestFinder):\n+            py_ver_info_minor = sys.version_info[:2]\n+            is_find_lineno_broken = (\n+                py_ver_info_minor < (3, 11)\n+                or (py_ver_info_minor == (3, 11) and sys.version_info.micro < 9)\n+                or (py_ver_info_minor == (3, 12) and sys.version_info.micro < 3)\n+            )\n+            if is_find_lineno_broken:\n+\n+                def _find_lineno(self, obj, source_lines):\n+                    \"\"\"On older Pythons, doctest code does not take into account\n+                    `@property`. https://github.com/python/cpython/issues/61648\n+\n+                    Moreover, wrapped Doctests need to be unwrapped so the correct\n+                    line number is returned. #8796\n+                    \"\"\"\n+                    if isinstance(obj, property):\n+                        obj = getattr(obj, \"fget\", obj)\n+\n+                    if hasattr(obj, \"__wrapped__\"):\n+                        # Get the main obj in case of it being wrapped\n+                        obj = inspect.unwrap(obj)\n+\n+                    # Type ignored because this is a private function.\n+                    return super()._find_lineno(  # type:ignore[misc]\n+                        obj,\n+                        source_lines,\n+                    )\n+\n+            if sys.version_info < (3, 10):\n+\n+                def _find(\n+                    self, tests, obj, name, module, source_lines, globs, seen\n+                ) -> None:\n+                    \"\"\"Override _find to work around issue in stdlib.\n+\n+                    https://github.com/pytest-dev/pytest/issues/3456\n+                    https://github.com/python/cpython/issues/69718\n+                    \"\"\"\n+                    if _is_mocked(obj):\n+                        return  # pragma: no cover\n+                    with _patch_unwrap_mock_aware():\n+                        # Type ignored because this is a private function.\n+                        super()._find(  # type:ignore[misc]\n+                            tests, obj, name, module, source_lines, globs, seen\n+                        )\n+\n+            if sys.version_info < (3, 13):\n+\n+                def _from_module(self, module, object):\n+                    \"\"\"`cached_property` objects are never considered a part\n+                    of the 'current module'. As such they are skipped by doctest.\n+                    Here we override `_from_module` to check the underlying\n+                    function instead. https://github.com/python/cpython/issues/107995\n+                    \"\"\"\n+                    if isinstance(object, functools.cached_property):\n+                        object = object.func\n+\n+                    # Type ignored because this is a private function.\n+                    return super()._from_module(module, object)  # type: ignore[misc]\n+\n+        try:\n+            module = self.obj\n+        except Collector.CollectError:\n+            if self.config.getvalue(\"doctest_ignore_import_errors\"):\n+                skip(f\"unable to import module {self.path!r}\")\n+            else:\n+                raise\n+\n+        # While doctests currently don't support fixtures directly, we still\n+        # need to pick up autouse fixtures.\n+        self.session._fixturemanager.parsefactories(self)\n+\n+        # Uses internal doctest module parsing mechanism.\n+        finder = MockAwareDocTestFinder()\n+        optionflags = get_optionflags(self.config)\n+        runner = _get_runner(\n+            verbose=False,\n+            optionflags=optionflags,\n+            checker=_get_checker(),\n+            continue_on_failure=_get_continue_on_failure(self.config),\n+        )\n+\n+        for test in finder.find(module, module.__name__):\n+            if test.examples:  # skip empty doctests\n+                yield DoctestItem.from_parent(\n+                    self, name=test.name, runner=runner, dtest=test\n+                )\n+\n+\n+def _init_checker_class() -> type[doctest.OutputChecker]:\n+    import doctest\n+\n+    class LiteralsOutputChecker(doctest.OutputChecker):\n+        # Based on doctest_nose_plugin.py from the nltk project\n+        # (https://github.com/nltk/nltk) and on the \"numtest\" doctest extension\n+        # by Sebastien Boisgerault (https://github.com/boisgera/numtest).\n+\n+        _unicode_literal_re = re.compile(r\"(\\W|^)[uU]([rR]?[\\'\\\"])\", re.UNICODE)\n+        _bytes_literal_re = re.compile(r\"(\\W|^)[bB]([rR]?[\\'\\\"])\", re.UNICODE)\n+        _number_re = re.compile(\n+            r\"\"\"\n+            (?P<number>\n+              (?P<mantissa>\n+                (?P<integer1> [+-]?\\d*)\\.(?P<fraction>\\d+)\n+                |\n+                (?P<integer2> [+-]?\\d+)\\.\n+              )\n+              (?:\n+                [Ee]\n+                (?P<exponent1> [+-]?\\d+)\n+              )?\n+              |\n+              (?P<integer3> [+-]?\\d+)\n+              (?:\n+                [Ee]\n+                (?P<exponent2> [+-]?\\d+)\n+              )\n+            )\n+            \"\"\",\n+            re.VERBOSE,\n+        )\n+\n+        def check_output(self, want: str, got: str, optionflags: int) -> bool:\n+            if super().check_output(want, got, optionflags):\n+                return True\n+\n+            allow_unicode = optionflags & _get_allow_unicode_flag()\n+            allow_bytes = optionflags & _get_allow_bytes_flag()\n+            allow_number = optionflags & _get_number_flag()\n+\n+            if not allow_unicode and not allow_bytes and not allow_number:\n+                return False\n+\n+            def remove_prefixes(regex: re.Pattern[str], txt: str) -> str:\n+                return re.sub(regex, r\"\\1\\2\", txt)\n+\n+            if allow_unicode:\n+                want = remove_prefixes(self._unicode_literal_re, want)\n+                got = remove_prefixes(self._unicode_literal_re, got)\n+\n+            if allow_bytes:\n+                want = remove_prefixes(self._bytes_literal_re, want)\n+                got = remove_prefixes(self._bytes_literal_re, got)\n+\n+            if allow_number:\n+                got = self._remove_unwanted_precision(want, got)\n+\n+            return super().check_output(want, got, optionflags)\n+\n+        def _remove_unwanted_precision(self, want: str, got: str) -> str:\n+            wants = list(self._number_re.finditer(want))\n+            gots = list(self._number_re.finditer(got))\n+            if len(wants) != len(gots):\n+                return got\n+            offset = 0\n+            for w, g in zip(wants, gots):\n+                fraction: str | None = w.group(\"fraction\")\n+                exponent: str | None = w.group(\"exponent1\")\n+                if exponent is None:\n+                    exponent = w.group(\"exponent2\")\n+                precision = 0 if fraction is None else len(fraction)\n+                if exponent is not None:\n+                    precision -= int(exponent)\n+                if float(w.group()) == approx(float(g.group()), abs=10**-precision):\n+                    # They're close enough. Replace the text we actually\n+                    # got with the text we want, so that it will match when we\n+                    # check the string literally.\n+                    got = (\n+                        got[: g.start() + offset] + w.group() + got[g.end() + offset :]\n+                    )\n+                    offset += w.end() - w.start() - (g.end() - g.start())\n+            return got\n+\n+    return LiteralsOutputChecker\n+\n+\n+def _get_checker() -> doctest.OutputChecker:\n+    \"\"\"Return a doctest.OutputChecker subclass that supports some\n+    additional options:\n+\n+    * ALLOW_UNICODE and ALLOW_BYTES options to ignore u'' and b''\n+      prefixes (respectively) in string literals. Useful when the same\n+      doctest should run in Python 2 and Python 3.\n+\n+    * NUMBER to ignore floating-point differences smaller than the\n+      precision of the literal number in the doctest.\n+\n+    An inner class is used to avoid importing \"doctest\" at the module\n+    level.\n+    \"\"\"\n+    global CHECKER_CLASS\n+    if CHECKER_CLASS is None:\n+        CHECKER_CLASS = _init_checker_class()\n+    return CHECKER_CLASS()\n+\n+\n+def _get_allow_unicode_flag() -> int:\n+    \"\"\"Register and return the ALLOW_UNICODE flag.\"\"\"\n+    import doctest\n+\n+    return doctest.register_optionflag(\"ALLOW_UNICODE\")\n+\n+\n+def _get_allow_bytes_flag() -> int:\n+    \"\"\"Register and return the ALLOW_BYTES flag.\"\"\"\n+    import doctest\n+\n+    return doctest.register_optionflag(\"ALLOW_BYTES\")\n+\n+\n+def _get_number_flag() -> int:\n+    \"\"\"Register and return the NUMBER flag.\"\"\"\n+    import doctest\n+\n+    return doctest.register_optionflag(\"NUMBER\")\n+\n+\n+def _get_report_choice(key: str) -> int:\n+    \"\"\"Return the actual `doctest` module flag value.\n+\n+    We want to do it as late as possible to avoid importing `doctest` and all\n+    its dependencies when parsing options, as it adds overhead and breaks tests.\n+    \"\"\"\n+    import doctest\n+\n+    return {\n+        DOCTEST_REPORT_CHOICE_UDIFF: doctest.REPORT_UDIFF,\n+        DOCTEST_REPORT_CHOICE_CDIFF: doctest.REPORT_CDIFF,\n+        DOCTEST_REPORT_CHOICE_NDIFF: doctest.REPORT_NDIFF,\n+        DOCTEST_REPORT_CHOICE_ONLY_FIRST_FAILURE: doctest.REPORT_ONLY_FIRST_FAILURE,\n+        DOCTEST_REPORT_CHOICE_NONE: 0,\n+    }[key]\n+\n+\n+@fixture(scope=\"session\")\n+def doctest_namespace() -> dict[str, Any]:\n+    \"\"\"Fixture that returns a :py:class:`dict` that will be injected into the\n+    namespace of doctests.\n+\n+    Usually this fixture is used in conjunction with another ``autouse`` fixture:\n+\n+    .. code-block:: python\n+\n+        @pytest.fixture(autouse=True)\n+        def add_np(doctest_namespace):\n+            doctest_namespace[\"np\"] = numpy\n+\n+    For more details: :ref:`doctest_namespace`.\n+    \"\"\"\n+    return dict()",
      "patch_lines": [
        "@@ -0,0 +1,754 @@\n",
        "+# mypy: allow-untyped-defs\n",
        "+\"\"\"Discover and run doctests in modules and test files.\"\"\"\n",
        "+\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+import bdb\n",
        "+from collections.abc import Callable\n",
        "+from collections.abc import Generator\n",
        "+from collections.abc import Iterable\n",
        "+from collections.abc import Sequence\n",
        "+from contextlib import contextmanager\n",
        "+import functools\n",
        "+import inspect\n",
        "+import os\n",
        "+from pathlib import Path\n",
        "+import platform\n",
        "+import re\n",
        "+import sys\n",
        "+import traceback\n",
        "+import types\n",
        "+from typing import Any\n",
        "+from typing import TYPE_CHECKING\n",
        "+import warnings\n",
        "+\n",
        "+from _pytest import outcomes\n",
        "+from _pytest._code.code import ExceptionInfo\n",
        "+from _pytest._code.code import ReprFileLocation\n",
        "+from _pytest._code.code import TerminalRepr\n",
        "+from _pytest._io import TerminalWriter\n",
        "+from _pytest.compat import safe_getattr\n",
        "+from _pytest.config import Config\n",
        "+from _pytest.config.argparsing import Parser\n",
        "+from _pytest.fixtures import fixture\n",
        "+from _pytest.fixtures import TopRequest\n",
        "+from _pytest.nodes import Collector\n",
        "+from _pytest.nodes import Item\n",
        "+from _pytest.outcomes import OutcomeException\n",
        "+from _pytest.outcomes import skip\n",
        "+from _pytest.pathlib import fnmatch_ex\n",
        "+from _pytest.python import Module\n",
        "+from _pytest.python_api import approx\n",
        "+from _pytest.warning_types import PytestWarning\n",
        "+\n",
        "+\n",
        "+if TYPE_CHECKING:\n",
        "+    import doctest\n",
        "+\n",
        "+    from typing_extensions import Self\n",
        "+\n",
        "+DOCTEST_REPORT_CHOICE_NONE = \"none\"\n",
        "+DOCTEST_REPORT_CHOICE_CDIFF = \"cdiff\"\n",
        "+DOCTEST_REPORT_CHOICE_NDIFF = \"ndiff\"\n",
        "+DOCTEST_REPORT_CHOICE_UDIFF = \"udiff\"\n",
        "+DOCTEST_REPORT_CHOICE_ONLY_FIRST_FAILURE = \"only_first_failure\"\n",
        "+\n",
        "+DOCTEST_REPORT_CHOICES = (\n",
        "+    DOCTEST_REPORT_CHOICE_NONE,\n",
        "+    DOCTEST_REPORT_CHOICE_CDIFF,\n",
        "+    DOCTEST_REPORT_CHOICE_NDIFF,\n",
        "+    DOCTEST_REPORT_CHOICE_UDIFF,\n",
        "+    DOCTEST_REPORT_CHOICE_ONLY_FIRST_FAILURE,\n",
        "+)\n",
        "+\n",
        "+# Lazy definition of runner class\n",
        "+RUNNER_CLASS = None\n",
        "+# Lazy definition of output checker class\n",
        "+CHECKER_CLASS: type[doctest.OutputChecker] | None = None\n",
        "+\n",
        "+\n",
        "+def pytest_addoption(parser: Parser) -> None:\n",
        "+    parser.addini(\n",
        "+        \"doctest_optionflags\",\n",
        "+        \"Option flags for doctests\",\n",
        "+        type=\"args\",\n",
        "+        default=[\"ELLIPSIS\"],\n",
        "+    )\n",
        "+    parser.addini(\n",
        "+        \"doctest_encoding\", \"Encoding used for doctest files\", default=\"utf-8\"\n",
        "+    )\n",
        "+    group = parser.getgroup(\"collect\")\n",
        "+    group.addoption(\n",
        "+        \"--doctest-modules\",\n",
        "+        action=\"store_true\",\n",
        "+        default=False,\n",
        "+        help=\"Run doctests in all .py modules\",\n",
        "+        dest=\"doctestmodules\",\n",
        "+    )\n",
        "+    group.addoption(\n",
        "+        \"--doctest-report\",\n",
        "+        type=str.lower,\n",
        "+        default=\"udiff\",\n",
        "+        help=\"Choose another output format for diffs on doctest failure\",\n",
        "+        choices=DOCTEST_REPORT_CHOICES,\n",
        "+        dest=\"doctestreport\",\n",
        "+    )\n",
        "+    group.addoption(\n",
        "+        \"--doctest-glob\",\n",
        "+        action=\"append\",\n",
        "+        default=[],\n",
        "+        metavar=\"pat\",\n",
        "+        help=\"Doctests file matching pattern, default: test*.txt\",\n",
        "+        dest=\"doctestglob\",\n",
        "+    )\n",
        "+    group.addoption(\n",
        "+        \"--doctest-ignore-import-errors\",\n",
        "+        action=\"store_true\",\n",
        "+        default=False,\n",
        "+        help=\"Ignore doctest collection errors\",\n",
        "+        dest=\"doctest_ignore_import_errors\",\n",
        "+    )\n",
        "+    group.addoption(\n",
        "+        \"--doctest-continue-on-failure\",\n",
        "+        action=\"store_true\",\n",
        "+        default=False,\n",
        "+        help=\"For a given doctest, continue to run after the first failure\",\n",
        "+        dest=\"doctest_continue_on_failure\",\n",
        "+    )\n",
        "+\n",
        "+\n",
        "+def pytest_unconfigure() -> None:\n",
        "+    global RUNNER_CLASS\n",
        "+\n",
        "+    RUNNER_CLASS = None\n",
        "+\n",
        "+\n",
        "+def pytest_collect_file(\n",
        "+    file_path: Path,\n",
        "+    parent: Collector,\n",
        "+) -> DoctestModule | DoctestTextfile | None:\n",
        "+    config = parent.config\n",
        "+    if file_path.suffix == \".py\":\n",
        "+        if config.option.doctestmodules and not any(\n",
        "+            (_is_setup_py(file_path), _is_main_py(file_path))\n",
        "+        ):\n",
        "+            return DoctestModule.from_parent(parent, path=file_path)\n",
        "+    elif _is_doctest(config, file_path, parent):\n",
        "+        return DoctestTextfile.from_parent(parent, path=file_path)\n",
        "+    return None\n",
        "+\n",
        "+\n",
        "+def _is_setup_py(path: Path) -> bool:\n",
        "+    if path.name != \"setup.py\":\n",
        "+        return False\n",
        "+    contents = path.read_bytes()\n",
        "+    return b\"setuptools\" in contents or b\"distutils\" in contents\n",
        "+\n",
        "+\n",
        "+def _is_doctest(config: Config, path: Path, parent: Collector) -> bool:\n",
        "+    if path.suffix in (\".txt\", \".rst\") and parent.session.isinitpath(path):\n",
        "+        return True\n",
        "+    globs = config.getoption(\"doctestglob\") or [\"test*.txt\"]\n",
        "+    return any(fnmatch_ex(glob, path) for glob in globs)\n",
        "+\n",
        "+\n",
        "+def _is_main_py(path: Path) -> bool:\n",
        "+    return path.name == \"__main__.py\"\n",
        "+\n",
        "+\n",
        "+class ReprFailDoctest(TerminalRepr):\n",
        "+    def __init__(\n",
        "+        self, reprlocation_lines: Sequence[tuple[ReprFileLocation, Sequence[str]]]\n",
        "+    ) -> None:\n",
        "+        self.reprlocation_lines = reprlocation_lines\n",
        "+\n",
        "+    def toterminal(self, tw: TerminalWriter) -> None:\n",
        "+        for reprlocation, lines in self.reprlocation_lines:\n",
        "+            for line in lines:\n",
        "+                tw.line(line)\n",
        "+            reprlocation.toterminal(tw)\n",
        "+\n",
        "+\n",
        "+class MultipleDoctestFailures(Exception):\n",
        "+    def __init__(self, failures: Sequence[doctest.DocTestFailure]) -> None:\n",
        "+        super().__init__()\n",
        "+        self.failures = failures\n",
        "+\n",
        "+\n",
        "+def _init_runner_class() -> type[doctest.DocTestRunner]:\n",
        "+    import doctest\n",
        "+\n",
        "+    class PytestDoctestRunner(doctest.DebugRunner):\n",
        "+        \"\"\"Runner to collect failures.\n",
        "+\n",
        "+        Note that the out variable in this case is a list instead of a\n",
        "+        stdout-like object.\n",
        "+        \"\"\"\n",
        "+\n",
        "+        def __init__(\n",
        "+            self,\n",
        "+            checker: doctest.OutputChecker | None = None,\n",
        "+            verbose: bool | None = None,\n",
        "+            optionflags: int = 0,\n",
        "+            continue_on_failure: bool = True,\n",
        "+        ) -> None:\n",
        "+            super().__init__(checker=checker, verbose=verbose, optionflags=optionflags)\n",
        "+            self.continue_on_failure = continue_on_failure\n",
        "+\n",
        "+        def report_failure(\n",
        "+            self,\n",
        "+            out,\n",
        "+            test: doctest.DocTest,\n",
        "+            example: doctest.Example,\n",
        "+            got: str,\n",
        "+        ) -> None:\n",
        "+            failure = doctest.DocTestFailure(test, example, got)\n",
        "+            if self.continue_on_failure:\n",
        "+                out.append(failure)\n",
        "+            else:\n",
        "+                raise failure\n",
        "+\n",
        "+        def report_unexpected_exception(\n",
        "+            self,\n",
        "+            out,\n",
        "+            test: doctest.DocTest,\n",
        "+            example: doctest.Example,\n",
        "+            exc_info: tuple[type[BaseException], BaseException, types.TracebackType],\n",
        "+        ) -> None:\n",
        "+            if isinstance(exc_info[1], OutcomeException):\n",
        "+                raise exc_info[1]\n",
        "+            if isinstance(exc_info[1], bdb.BdbQuit):\n",
        "+                outcomes.exit(\"Quitting debugger\")\n",
        "+            failure = doctest.UnexpectedException(test, example, exc_info)\n",
        "+            if self.continue_on_failure:\n",
        "+                out.append(failure)\n",
        "+            else:\n",
        "+                raise failure\n",
        "+\n",
        "+    return PytestDoctestRunner\n",
        "+\n",
        "+\n",
        "+def _get_runner(\n",
        "+    checker: doctest.OutputChecker | None = None,\n",
        "+    verbose: bool | None = None,\n",
        "+    optionflags: int = 0,\n",
        "+    continue_on_failure: bool = True,\n",
        "+) -> doctest.DocTestRunner:\n",
        "+    # We need this in order to do a lazy import on doctest\n",
        "+    global RUNNER_CLASS\n",
        "+    if RUNNER_CLASS is None:\n",
        "+        RUNNER_CLASS = _init_runner_class()\n",
        "+    # Type ignored because the continue_on_failure argument is only defined on\n",
        "+    # PytestDoctestRunner, which is lazily defined so can't be used as a type.\n",
        "+    return RUNNER_CLASS(  # type: ignore\n",
        "+        checker=checker,\n",
        "+        verbose=verbose,\n",
        "+        optionflags=optionflags,\n",
        "+        continue_on_failure=continue_on_failure,\n",
        "+    )\n",
        "+\n",
        "+\n",
        "+class DoctestItem(Item):\n",
        "+    def __init__(\n",
        "+        self,\n",
        "+        name: str,\n",
        "+        parent: DoctestTextfile | DoctestModule,\n",
        "+        runner: doctest.DocTestRunner,\n",
        "+        dtest: doctest.DocTest,\n",
        "+    ) -> None:\n",
        "+        super().__init__(name, parent)\n",
        "+        self.runner = runner\n",
        "+        self.dtest = dtest\n",
        "+\n",
        "+        # Stuff needed for fixture support.\n",
        "+        self.obj = None\n",
        "+        fm = self.session._fixturemanager\n",
        "+        fixtureinfo = fm.getfixtureinfo(node=self, func=None, cls=None)\n",
        "+        self._fixtureinfo = fixtureinfo\n",
        "+        self.fixturenames = fixtureinfo.names_closure\n",
        "+        self._initrequest()\n",
        "+\n",
        "+    @classmethod\n",
        "+    def from_parent(  # type: ignore[override]\n",
        "+        cls,\n",
        "+        parent: DoctestTextfile | DoctestModule,\n",
        "+        *,\n",
        "+        name: str,\n",
        "+        runner: doctest.DocTestRunner,\n",
        "+        dtest: doctest.DocTest,\n",
        "+    ) -> Self:\n",
        "+        # incompatible signature due to imposed limits on subclass\n",
        "+        \"\"\"The public named constructor.\"\"\"\n",
        "+        return super().from_parent(name=name, parent=parent, runner=runner, dtest=dtest)\n",
        "+\n",
        "+    def _initrequest(self) -> None:\n",
        "+        self.funcargs: dict[str, object] = {}\n",
        "+        self._request = TopRequest(self, _ispytest=True)  # type: ignore[arg-type]\n",
        "+\n",
        "+    def setup(self) -> None:\n",
        "+        self._request._fillfixtures()\n",
        "+        globs = dict(getfixture=self._request.getfixturevalue)\n",
        "+        for name, value in self._request.getfixturevalue(\"doctest_namespace\").items():\n",
        "+            globs[name] = value\n",
        "+        self.dtest.globs.update(globs)\n",
        "+\n",
        "+    def runtest(self) -> None:\n",
        "+        _check_all_skipped(self.dtest)\n",
        "+        self._disable_output_capturing_for_darwin()\n",
        "+        failures: list[doctest.DocTestFailure] = []\n",
        "+        # Type ignored because we change the type of `out` from what\n",
        "+        # doctest expects.\n",
        "+        self.runner.run(self.dtest, out=failures)  # type: ignore[arg-type]\n",
        "+        if failures:\n",
        "+            raise MultipleDoctestFailures(failures)\n",
        "+\n",
        "+    def _disable_output_capturing_for_darwin(self) -> None:\n",
        "+        \"\"\"Disable output capturing. Otherwise, stdout is lost to doctest (#985).\"\"\"\n",
        "+        if platform.system() != \"Darwin\":\n",
        "+            return\n",
        "+        capman = self.config.pluginmanager.getplugin(\"capturemanager\")\n",
        "+        if capman:\n",
        "+            capman.suspend_global_capture(in_=True)\n",
        "+            out, err = capman.read_global_capture()\n",
        "+            sys.stdout.write(out)\n",
        "+            sys.stderr.write(err)\n",
        "+\n",
        "+    # TODO: Type ignored -- breaks Liskov Substitution.\n",
        "+    def repr_failure(  # type: ignore[override]\n",
        "+        self,\n",
        "+        excinfo: ExceptionInfo[BaseException],\n",
        "+    ) -> str | TerminalRepr:\n",
        "+        import doctest\n",
        "+\n",
        "+        failures: (\n",
        "+            Sequence[doctest.DocTestFailure | doctest.UnexpectedException] | None\n",
        "+        ) = None\n",
        "+        if isinstance(\n",
        "+            excinfo.value, (doctest.DocTestFailure, doctest.UnexpectedException)\n",
        "+        ):\n",
        "+            failures = [excinfo.value]\n",
        "+        elif isinstance(excinfo.value, MultipleDoctestFailures):\n",
        "+            failures = excinfo.value.failures\n",
        "+\n",
        "+        if failures is None:\n",
        "+            return super().repr_failure(excinfo)\n",
        "+\n",
        "+        reprlocation_lines = []\n",
        "+        for failure in failures:\n",
        "+            example = failure.example\n",
        "+            test = failure.test\n",
        "+            filename = test.filename\n",
        "+            if test.lineno is None:\n",
        "+                lineno = None\n",
        "+            else:\n",
        "+                lineno = test.lineno + example.lineno + 1\n",
        "+            message = type(failure).__name__\n",
        "+            # TODO: ReprFileLocation doesn't expect a None lineno.\n",
        "+            reprlocation = ReprFileLocation(filename, lineno, message)  # type: ignore[arg-type]\n",
        "+            checker = _get_checker()\n",
        "+            report_choice = _get_report_choice(self.config.getoption(\"doctestreport\"))\n",
        "+            if lineno is not None:\n",
        "+                assert failure.test.docstring is not None\n",
        "+                lines = failure.test.docstring.splitlines(False)\n",
        "+                # add line numbers to the left of the error message\n",
        "+                assert test.lineno is not None\n",
        "+                lines = [\n",
        "+                    f\"{i + test.lineno + 1:03d} {x}\" for (i, x) in enumerate(lines)\n",
        "+                ]\n",
        "+                # trim docstring error lines to 10\n",
        "+                lines = lines[max(example.lineno - 9, 0) : example.lineno + 1]\n",
        "+            else:\n",
        "+                lines = [\n",
        "+                    \"EXAMPLE LOCATION UNKNOWN, not showing all tests of that example\"\n",
        "+                ]\n",
        "+                indent = \">>>\"\n",
        "+                for line in example.source.splitlines():\n",
        "+                    lines.append(f\"??? {indent} {line}\")\n",
        "+                    indent = \"...\"\n",
        "+            if isinstance(failure, doctest.DocTestFailure):\n",
        "+                lines += checker.output_difference(\n",
        "+                    example, failure.got, report_choice\n",
        "+                ).split(\"\\n\")\n",
        "+            else:\n",
        "+                inner_excinfo = ExceptionInfo.from_exc_info(failure.exc_info)\n",
        "+                lines += [f\"UNEXPECTED EXCEPTION: {inner_excinfo.value!r}\"]\n",
        "+                lines += [\n",
        "+                    x.strip(\"\\n\") for x in traceback.format_exception(*failure.exc_info)\n",
        "+                ]\n",
        "+            reprlocation_lines.append((reprlocation, lines))\n",
        "+        return ReprFailDoctest(reprlocation_lines)\n",
        "+\n",
        "+    def reportinfo(self) -> tuple[os.PathLike[str] | str, int | None, str]:\n",
        "+        return self.path, self.dtest.lineno, f\"[doctest] {self.name}\"\n",
        "+\n",
        "+\n",
        "+def _get_flag_lookup() -> dict[str, int]:\n",
        "+    import doctest\n",
        "+\n",
        "+    return dict(\n",
        "+        DONT_ACCEPT_TRUE_FOR_1=doctest.DONT_ACCEPT_TRUE_FOR_1,\n",
        "+        DONT_ACCEPT_BLANKLINE=doctest.DONT_ACCEPT_BLANKLINE,\n",
        "+        NORMALIZE_WHITESPACE=doctest.NORMALIZE_WHITESPACE,\n",
        "+        ELLIPSIS=doctest.ELLIPSIS,\n",
        "+        IGNORE_EXCEPTION_DETAIL=doctest.IGNORE_EXCEPTION_DETAIL,\n",
        "+        COMPARISON_FLAGS=doctest.COMPARISON_FLAGS,\n",
        "+        ALLOW_UNICODE=_get_allow_unicode_flag(),\n",
        "+        ALLOW_BYTES=_get_allow_bytes_flag(),\n",
        "+        NUMBER=_get_number_flag(),\n",
        "+    )\n",
        "+\n",
        "+\n",
        "+def get_optionflags(config: Config) -> int:\n",
        "+    optionflags_str = config.getini(\"doctest_optionflags\")\n",
        "+    flag_lookup_table = _get_flag_lookup()\n",
        "+    flag_acc = 0\n",
        "+    for flag in optionflags_str:\n",
        "+        flag_acc |= flag_lookup_table[flag]\n",
        "+    return flag_acc\n",
        "+\n",
        "+\n",
        "+def _get_continue_on_failure(config: Config) -> bool:\n",
        "+    continue_on_failure: bool = config.getvalue(\"doctest_continue_on_failure\")\n",
        "+    if continue_on_failure:\n",
        "+        # We need to turn off this if we use pdb since we should stop at\n",
        "+        # the first failure.\n",
        "+        if config.getvalue(\"usepdb\"):\n",
        "+            continue_on_failure = False\n",
        "+    return continue_on_failure\n",
        "+\n",
        "+\n",
        "+class DoctestTextfile(Module):\n",
        "+    obj = None\n",
        "+\n",
        "+    def collect(self) -> Iterable[DoctestItem]:\n",
        "+        import doctest\n",
        "+\n",
        "+        # Inspired by doctest.testfile; ideally we would use it directly,\n",
        "+        # but it doesn't support passing a custom checker.\n",
        "+        encoding = self.config.getini(\"doctest_encoding\")\n",
        "+        text = self.path.read_text(encoding)\n",
        "+        filename = str(self.path)\n",
        "+        name = self.path.name\n",
        "+        globs = {\"__name__\": \"__main__\"}\n",
        "+\n",
        "+        optionflags = get_optionflags(self.config)\n",
        "+\n",
        "+        runner = _get_runner(\n",
        "+            verbose=False,\n",
        "+            optionflags=optionflags,\n",
        "+            checker=_get_checker(),\n",
        "+            continue_on_failure=_get_continue_on_failure(self.config),\n",
        "+        )\n",
        "+\n",
        "+        parser = doctest.DocTestParser()\n",
        "+        test = parser.get_doctest(text, globs, name, filename, 0)\n",
        "+        if test.examples:\n",
        "+            yield DoctestItem.from_parent(\n",
        "+                self, name=test.name, runner=runner, dtest=test\n",
        "+            )\n",
        "+\n",
        "+\n",
        "+def _check_all_skipped(test: doctest.DocTest) -> None:\n",
        "+    \"\"\"Raise pytest.skip() if all examples in the given DocTest have the SKIP\n",
        "+    option set.\"\"\"\n",
        "+    import doctest\n",
        "+\n",
        "+    all_skipped = all(x.options.get(doctest.SKIP, False) for x in test.examples)\n",
        "+    if all_skipped:\n",
        "+        skip(\"all tests skipped by +SKIP option\")\n",
        "+\n",
        "+\n",
        "+def _is_mocked(obj: object) -> bool:\n",
        "+    \"\"\"Return if an object is possibly a mock object by checking the\n",
        "+    existence of a highly improbable attribute.\"\"\"\n",
        "+    return (\n",
        "+        safe_getattr(obj, \"pytest_mock_example_attribute_that_shouldnt_exist\", None)\n",
        "+        is not None\n",
        "+    )\n",
        "+\n",
        "+\n",
        "+@contextmanager\n",
        "+def _patch_unwrap_mock_aware() -> Generator[None]:\n",
        "+    \"\"\"Context manager which replaces ``inspect.unwrap`` with a version\n",
        "+    that's aware of mock objects and doesn't recurse into them.\"\"\"\n",
        "+    real_unwrap = inspect.unwrap\n",
        "+\n",
        "+    def _mock_aware_unwrap(\n",
        "+        func: Callable[..., Any], *, stop: Callable[[Any], Any] | None = None\n",
        "+    ) -> Any:\n",
        "+        try:\n",
        "+            if stop is None or stop is _is_mocked:\n",
        "+                return real_unwrap(func, stop=_is_mocked)\n",
        "+            _stop = stop\n",
        "+            return real_unwrap(func, stop=lambda obj: _is_mocked(obj) or _stop(func))\n",
        "+        except Exception as e:\n",
        "+            warnings.warn(\n",
        "+                f\"Got {e!r} when unwrapping {func!r}.  This is usually caused \"\n",
        "+                \"by a violation of Python's object protocol; see e.g. \"\n",
        "+                \"https://github.com/pytest-dev/pytest/issues/5080\",\n",
        "+                PytestWarning,\n",
        "+            )\n",
        "+            raise\n",
        "+\n",
        "+    inspect.unwrap = _mock_aware_unwrap\n",
        "+    try:\n",
        "+        yield\n",
        "+    finally:\n",
        "+        inspect.unwrap = real_unwrap\n",
        "+\n",
        "+\n",
        "+class DoctestModule(Module):\n",
        "+    def collect(self) -> Iterable[DoctestItem]:\n",
        "+        import doctest\n",
        "+\n",
        "+        class MockAwareDocTestFinder(doctest.DocTestFinder):\n",
        "+            py_ver_info_minor = sys.version_info[:2]\n",
        "+            is_find_lineno_broken = (\n",
        "+                py_ver_info_minor < (3, 11)\n",
        "+                or (py_ver_info_minor == (3, 11) and sys.version_info.micro < 9)\n",
        "+                or (py_ver_info_minor == (3, 12) and sys.version_info.micro < 3)\n",
        "+            )\n",
        "+            if is_find_lineno_broken:\n",
        "+\n",
        "+                def _find_lineno(self, obj, source_lines):\n",
        "+                    \"\"\"On older Pythons, doctest code does not take into account\n",
        "+                    `@property`. https://github.com/python/cpython/issues/61648\n",
        "+\n",
        "+                    Moreover, wrapped Doctests need to be unwrapped so the correct\n",
        "+                    line number is returned. #8796\n",
        "+                    \"\"\"\n",
        "+                    if isinstance(obj, property):\n",
        "+                        obj = getattr(obj, \"fget\", obj)\n",
        "+\n",
        "+                    if hasattr(obj, \"__wrapped__\"):\n",
        "+                        # Get the main obj in case of it being wrapped\n",
        "+                        obj = inspect.unwrap(obj)\n",
        "+\n",
        "+                    # Type ignored because this is a private function.\n",
        "+                    return super()._find_lineno(  # type:ignore[misc]\n",
        "+                        obj,\n",
        "+                        source_lines,\n",
        "+                    )\n",
        "+\n",
        "+            if sys.version_info < (3, 10):\n",
        "+\n",
        "+                def _find(\n",
        "+                    self, tests, obj, name, module, source_lines, globs, seen\n",
        "+                ) -> None:\n",
        "+                    \"\"\"Override _find to work around issue in stdlib.\n",
        "+\n",
        "+                    https://github.com/pytest-dev/pytest/issues/3456\n",
        "+                    https://github.com/python/cpython/issues/69718\n",
        "+                    \"\"\"\n",
        "+                    if _is_mocked(obj):\n",
        "+                        return  # pragma: no cover\n",
        "+                    with _patch_unwrap_mock_aware():\n",
        "+                        # Type ignored because this is a private function.\n",
        "+                        super()._find(  # type:ignore[misc]\n",
        "+                            tests, obj, name, module, source_lines, globs, seen\n",
        "+                        )\n",
        "+\n",
        "+            if sys.version_info < (3, 13):\n",
        "+\n",
        "+                def _from_module(self, module, object):\n",
        "+                    \"\"\"`cached_property` objects are never considered a part\n",
        "+                    of the 'current module'. As such they are skipped by doctest.\n",
        "+                    Here we override `_from_module` to check the underlying\n",
        "+                    function instead. https://github.com/python/cpython/issues/107995\n",
        "+                    \"\"\"\n",
        "+                    if isinstance(object, functools.cached_property):\n",
        "+                        object = object.func\n",
        "+\n",
        "+                    # Type ignored because this is a private function.\n",
        "+                    return super()._from_module(module, object)  # type: ignore[misc]\n",
        "+\n",
        "+        try:\n",
        "+            module = self.obj\n",
        "+        except Collector.CollectError:\n",
        "+            if self.config.getvalue(\"doctest_ignore_import_errors\"):\n",
        "+                skip(f\"unable to import module {self.path!r}\")\n",
        "+            else:\n",
        "+                raise\n",
        "+\n",
        "+        # While doctests currently don't support fixtures directly, we still\n",
        "+        # need to pick up autouse fixtures.\n",
        "+        self.session._fixturemanager.parsefactories(self)\n",
        "+\n",
        "+        # Uses internal doctest module parsing mechanism.\n",
        "+        finder = MockAwareDocTestFinder()\n",
        "+        optionflags = get_optionflags(self.config)\n",
        "+        runner = _get_runner(\n",
        "+            verbose=False,\n",
        "+            optionflags=optionflags,\n",
        "+            checker=_get_checker(),\n",
        "+            continue_on_failure=_get_continue_on_failure(self.config),\n",
        "+        )\n",
        "+\n",
        "+        for test in finder.find(module, module.__name__):\n",
        "+            if test.examples:  # skip empty doctests\n",
        "+                yield DoctestItem.from_parent(\n",
        "+                    self, name=test.name, runner=runner, dtest=test\n",
        "+                )\n",
        "+\n",
        "+\n",
        "+def _init_checker_class() -> type[doctest.OutputChecker]:\n",
        "+    import doctest\n",
        "+\n",
        "+    class LiteralsOutputChecker(doctest.OutputChecker):\n",
        "+        # Based on doctest_nose_plugin.py from the nltk project\n",
        "+        # (https://github.com/nltk/nltk) and on the \"numtest\" doctest extension\n",
        "+        # by Sebastien Boisgerault (https://github.com/boisgera/numtest).\n",
        "+\n",
        "+        _unicode_literal_re = re.compile(r\"(\\W|^)[uU]([rR]?[\\'\\\"])\", re.UNICODE)\n",
        "+        _bytes_literal_re = re.compile(r\"(\\W|^)[bB]([rR]?[\\'\\\"])\", re.UNICODE)\n",
        "+        _number_re = re.compile(\n",
        "+            r\"\"\"\n",
        "+            (?P<number>\n",
        "+              (?P<mantissa>\n",
        "+                (?P<integer1> [+-]?\\d*)\\.(?P<fraction>\\d+)\n",
        "+                |\n",
        "+                (?P<integer2> [+-]?\\d+)\\.\n",
        "+              )\n",
        "+              (?:\n",
        "+                [Ee]\n",
        "+                (?P<exponent1> [+-]?\\d+)\n",
        "+              )?\n",
        "+              |\n",
        "+              (?P<integer3> [+-]?\\d+)\n",
        "+              (?:\n",
        "+                [Ee]\n",
        "+                (?P<exponent2> [+-]?\\d+)\n",
        "+              )\n",
        "+            )\n",
        "+            \"\"\",\n",
        "+            re.VERBOSE,\n",
        "+        )\n",
        "+\n",
        "+        def check_output(self, want: str, got: str, optionflags: int) -> bool:\n",
        "+            if super().check_output(want, got, optionflags):\n",
        "+                return True\n",
        "+\n",
        "+            allow_unicode = optionflags & _get_allow_unicode_flag()\n",
        "+            allow_bytes = optionflags & _get_allow_bytes_flag()\n",
        "+            allow_number = optionflags & _get_number_flag()\n",
        "+\n",
        "+            if not allow_unicode and not allow_bytes and not allow_number:\n",
        "+                return False\n",
        "+\n",
        "+            def remove_prefixes(regex: re.Pattern[str], txt: str) -> str:\n",
        "+                return re.sub(regex, r\"\\1\\2\", txt)\n",
        "+\n",
        "+            if allow_unicode:\n",
        "+                want = remove_prefixes(self._unicode_literal_re, want)\n",
        "+                got = remove_prefixes(self._unicode_literal_re, got)\n",
        "+\n",
        "+            if allow_bytes:\n",
        "+                want = remove_prefixes(self._bytes_literal_re, want)\n",
        "+                got = remove_prefixes(self._bytes_literal_re, got)\n",
        "+\n",
        "+            if allow_number:\n",
        "+                got = self._remove_unwanted_precision(want, got)\n",
        "+\n",
        "+            return super().check_output(want, got, optionflags)\n",
        "+\n",
        "+        def _remove_unwanted_precision(self, want: str, got: str) -> str:\n",
        "+            wants = list(self._number_re.finditer(want))\n",
        "+            gots = list(self._number_re.finditer(got))\n",
        "+            if len(wants) != len(gots):\n",
        "+                return got\n",
        "+            offset = 0\n",
        "+            for w, g in zip(wants, gots):\n",
        "+                fraction: str | None = w.group(\"fraction\")\n",
        "+                exponent: str | None = w.group(\"exponent1\")\n",
        "+                if exponent is None:\n",
        "+                    exponent = w.group(\"exponent2\")\n",
        "+                precision = 0 if fraction is None else len(fraction)\n",
        "+                if exponent is not None:\n",
        "+                    precision -= int(exponent)\n",
        "+                if float(w.group()) == approx(float(g.group()), abs=10**-precision):\n",
        "+                    # They're close enough. Replace the text we actually\n",
        "+                    # got with the text we want, so that it will match when we\n",
        "+                    # check the string literally.\n",
        "+                    got = (\n",
        "+                        got[: g.start() + offset] + w.group() + got[g.end() + offset :]\n",
        "+                    )\n",
        "+                    offset += w.end() - w.start() - (g.end() - g.start())\n",
        "+            return got\n",
        "+\n",
        "+    return LiteralsOutputChecker\n",
        "+\n",
        "+\n",
        "+def _get_checker() -> doctest.OutputChecker:\n",
        "+    \"\"\"Return a doctest.OutputChecker subclass that supports some\n",
        "+    additional options:\n",
        "+\n",
        "+    * ALLOW_UNICODE and ALLOW_BYTES options to ignore u'' and b''\n",
        "+      prefixes (respectively) in string literals. Useful when the same\n",
        "+      doctest should run in Python 2 and Python 3.\n",
        "+\n",
        "+    * NUMBER to ignore floating-point differences smaller than the\n",
        "+      precision of the literal number in the doctest.\n",
        "+\n",
        "+    An inner class is used to avoid importing \"doctest\" at the module\n",
        "+    level.\n",
        "+    \"\"\"\n",
        "+    global CHECKER_CLASS\n",
        "+    if CHECKER_CLASS is None:\n",
        "+        CHECKER_CLASS = _init_checker_class()\n",
        "+    return CHECKER_CLASS()\n",
        "+\n",
        "+\n",
        "+def _get_allow_unicode_flag() -> int:\n",
        "+    \"\"\"Register and return the ALLOW_UNICODE flag.\"\"\"\n",
        "+    import doctest\n",
        "+\n",
        "+    return doctest.register_optionflag(\"ALLOW_UNICODE\")\n",
        "+\n",
        "+\n",
        "+def _get_allow_bytes_flag() -> int:\n",
        "+    \"\"\"Register and return the ALLOW_BYTES flag.\"\"\"\n",
        "+    import doctest\n",
        "+\n",
        "+    return doctest.register_optionflag(\"ALLOW_BYTES\")\n",
        "+\n",
        "+\n",
        "+def _get_number_flag() -> int:\n",
        "+    \"\"\"Register and return the NUMBER flag.\"\"\"\n",
        "+    import doctest\n",
        "+\n",
        "+    return doctest.register_optionflag(\"NUMBER\")\n",
        "+\n",
        "+\n",
        "+def _get_report_choice(key: str) -> int:\n",
        "+    \"\"\"Return the actual `doctest` module flag value.\n",
        "+\n",
        "+    We want to do it as late as possible to avoid importing `doctest` and all\n",
        "+    its dependencies when parsing options, as it adds overhead and breaks tests.\n",
        "+    \"\"\"\n",
        "+    import doctest\n",
        "+\n",
        "+    return {\n",
        "+        DOCTEST_REPORT_CHOICE_UDIFF: doctest.REPORT_UDIFF,\n",
        "+        DOCTEST_REPORT_CHOICE_CDIFF: doctest.REPORT_CDIFF,\n",
        "+        DOCTEST_REPORT_CHOICE_NDIFF: doctest.REPORT_NDIFF,\n",
        "+        DOCTEST_REPORT_CHOICE_ONLY_FIRST_FAILURE: doctest.REPORT_ONLY_FIRST_FAILURE,\n",
        "+        DOCTEST_REPORT_CHOICE_NONE: 0,\n",
        "+    }[key]\n",
        "+\n",
        "+\n",
        "+@fixture(scope=\"session\")\n",
        "+def doctest_namespace() -> dict[str, Any]:\n",
        "+    \"\"\"Fixture that returns a :py:class:`dict` that will be injected into the\n",
        "+    namespace of doctests.\n",
        "+\n",
        "+    Usually this fixture is used in conjunction with another ``autouse`` fixture:\n",
        "+\n",
        "+    .. code-block:: python\n",
        "+\n",
        "+        @pytest.fixture(autouse=True)\n",
        "+        def add_np(doctest_namespace):\n",
        "+            doctest_namespace[\"np\"] = numpy\n",
        "+\n",
        "+    For more details: :ref:`doctest_namespace`.\n",
        "+    \"\"\"\n",
        "+    return dict()\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/faulthandler.py",
      "status": "added",
      "additions": 105,
      "deletions": 0,
      "patch": "@@ -0,0 +1,105 @@\n+from __future__ import annotations\n+\n+from collections.abc import Generator\n+import os\n+import sys\n+\n+from _pytest.config import Config\n+from _pytest.config.argparsing import Parser\n+from _pytest.nodes import Item\n+from _pytest.stash import StashKey\n+import pytest\n+\n+\n+fault_handler_original_stderr_fd_key = StashKey[int]()\n+fault_handler_stderr_fd_key = StashKey[int]()\n+\n+\n+def pytest_addoption(parser: Parser) -> None:\n+    help = (\n+        \"Dump the traceback of all threads if a test takes \"\n+        \"more than TIMEOUT seconds to finish\"\n+    )\n+    parser.addini(\"faulthandler_timeout\", help, default=0.0)\n+\n+\n+def pytest_configure(config: Config) -> None:\n+    import faulthandler\n+\n+    # at teardown we want to restore the original faulthandler fileno\n+    # but faulthandler has no api to return the original fileno\n+    # so here we stash the stderr fileno to be used at teardown\n+    # sys.stderr and sys.__stderr__ may be closed or patched during the session\n+    # so we can't rely on their values being good at that point (#11572).\n+    stderr_fileno = get_stderr_fileno()\n+    if faulthandler.is_enabled():\n+        config.stash[fault_handler_original_stderr_fd_key] = stderr_fileno\n+    config.stash[fault_handler_stderr_fd_key] = os.dup(stderr_fileno)\n+    faulthandler.enable(file=config.stash[fault_handler_stderr_fd_key])\n+\n+\n+def pytest_unconfigure(config: Config) -> None:\n+    import faulthandler\n+\n+    faulthandler.disable()\n+    # Close the dup file installed during pytest_configure.\n+    if fault_handler_stderr_fd_key in config.stash:\n+        os.close(config.stash[fault_handler_stderr_fd_key])\n+        del config.stash[fault_handler_stderr_fd_key]\n+    # Re-enable the faulthandler if it was originally enabled.\n+    if fault_handler_original_stderr_fd_key in config.stash:\n+        faulthandler.enable(config.stash[fault_handler_original_stderr_fd_key])\n+        del config.stash[fault_handler_original_stderr_fd_key]\n+\n+\n+def get_stderr_fileno() -> int:\n+    try:\n+        fileno = sys.stderr.fileno()\n+        # The Twisted Logger will return an invalid file descriptor since it is not backed\n+        # by an FD. So, let's also forward this to the same code path as with pytest-xdist.\n+        if fileno == -1:\n+            raise AttributeError()\n+        return fileno\n+    except (AttributeError, ValueError):\n+        # pytest-xdist monkeypatches sys.stderr with an object that is not an actual file.\n+        # https://docs.python.org/3/library/faulthandler.html#issue-with-file-descriptors\n+        # This is potentially dangerous, but the best we can do.\n+        assert sys.__stderr__ is not None\n+        return sys.__stderr__.fileno()\n+\n+\n+def get_timeout_config_value(config: Config) -> float:\n+    return float(config.getini(\"faulthandler_timeout\") or 0.0)\n+\n+\n+@pytest.hookimpl(wrapper=True, trylast=True)\n+def pytest_runtest_protocol(item: Item) -> Generator[None, object, object]:\n+    timeout = get_timeout_config_value(item.config)\n+    if timeout > 0:\n+        import faulthandler\n+\n+        stderr = item.config.stash[fault_handler_stderr_fd_key]\n+        faulthandler.dump_traceback_later(timeout, file=stderr)\n+        try:\n+            return (yield)\n+        finally:\n+            faulthandler.cancel_dump_traceback_later()\n+    else:\n+        return (yield)\n+\n+\n+@pytest.hookimpl(tryfirst=True)\n+def pytest_enter_pdb() -> None:\n+    \"\"\"Cancel any traceback dumping due to timeout before entering pdb.\"\"\"\n+    import faulthandler\n+\n+    faulthandler.cancel_dump_traceback_later()\n+\n+\n+@pytest.hookimpl(tryfirst=True)\n+def pytest_exception_interact() -> None:\n+    \"\"\"Cancel any traceback dumping due to an interactive exception being\n+    raised.\"\"\"\n+    import faulthandler\n+\n+    faulthandler.cancel_dump_traceback_later()",
      "patch_lines": [
        "@@ -0,0 +1,105 @@\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+from collections.abc import Generator\n",
        "+import os\n",
        "+import sys\n",
        "+\n",
        "+from _pytest.config import Config\n",
        "+from _pytest.config.argparsing import Parser\n",
        "+from _pytest.nodes import Item\n",
        "+from _pytest.stash import StashKey\n",
        "+import pytest\n",
        "+\n",
        "+\n",
        "+fault_handler_original_stderr_fd_key = StashKey[int]()\n",
        "+fault_handler_stderr_fd_key = StashKey[int]()\n",
        "+\n",
        "+\n",
        "+def pytest_addoption(parser: Parser) -> None:\n",
        "+    help = (\n",
        "+        \"Dump the traceback of all threads if a test takes \"\n",
        "+        \"more than TIMEOUT seconds to finish\"\n",
        "+    )\n",
        "+    parser.addini(\"faulthandler_timeout\", help, default=0.0)\n",
        "+\n",
        "+\n",
        "+def pytest_configure(config: Config) -> None:\n",
        "+    import faulthandler\n",
        "+\n",
        "+    # at teardown we want to restore the original faulthandler fileno\n",
        "+    # but faulthandler has no api to return the original fileno\n",
        "+    # so here we stash the stderr fileno to be used at teardown\n",
        "+    # sys.stderr and sys.__stderr__ may be closed or patched during the session\n",
        "+    # so we can't rely on their values being good at that point (#11572).\n",
        "+    stderr_fileno = get_stderr_fileno()\n",
        "+    if faulthandler.is_enabled():\n",
        "+        config.stash[fault_handler_original_stderr_fd_key] = stderr_fileno\n",
        "+    config.stash[fault_handler_stderr_fd_key] = os.dup(stderr_fileno)\n",
        "+    faulthandler.enable(file=config.stash[fault_handler_stderr_fd_key])\n",
        "+\n",
        "+\n",
        "+def pytest_unconfigure(config: Config) -> None:\n",
        "+    import faulthandler\n",
        "+\n",
        "+    faulthandler.disable()\n",
        "+    # Close the dup file installed during pytest_configure.\n",
        "+    if fault_handler_stderr_fd_key in config.stash:\n",
        "+        os.close(config.stash[fault_handler_stderr_fd_key])\n",
        "+        del config.stash[fault_handler_stderr_fd_key]\n",
        "+    # Re-enable the faulthandler if it was originally enabled.\n",
        "+    if fault_handler_original_stderr_fd_key in config.stash:\n",
        "+        faulthandler.enable(config.stash[fault_handler_original_stderr_fd_key])\n",
        "+        del config.stash[fault_handler_original_stderr_fd_key]\n",
        "+\n",
        "+\n",
        "+def get_stderr_fileno() -> int:\n",
        "+    try:\n",
        "+        fileno = sys.stderr.fileno()\n",
        "+        # The Twisted Logger will return an invalid file descriptor since it is not backed\n",
        "+        # by an FD. So, let's also forward this to the same code path as with pytest-xdist.\n",
        "+        if fileno == -1:\n",
        "+            raise AttributeError()\n",
        "+        return fileno\n",
        "+    except (AttributeError, ValueError):\n",
        "+        # pytest-xdist monkeypatches sys.stderr with an object that is not an actual file.\n",
        "+        # https://docs.python.org/3/library/faulthandler.html#issue-with-file-descriptors\n",
        "+        # This is potentially dangerous, but the best we can do.\n",
        "+        assert sys.__stderr__ is not None\n",
        "+        return sys.__stderr__.fileno()\n",
        "+\n",
        "+\n",
        "+def get_timeout_config_value(config: Config) -> float:\n",
        "+    return float(config.getini(\"faulthandler_timeout\") or 0.0)\n",
        "+\n",
        "+\n",
        "+@pytest.hookimpl(wrapper=True, trylast=True)\n",
        "+def pytest_runtest_protocol(item: Item) -> Generator[None, object, object]:\n",
        "+    timeout = get_timeout_config_value(item.config)\n",
        "+    if timeout > 0:\n",
        "+        import faulthandler\n",
        "+\n",
        "+        stderr = item.config.stash[fault_handler_stderr_fd_key]\n",
        "+        faulthandler.dump_traceback_later(timeout, file=stderr)\n",
        "+        try:\n",
        "+            return (yield)\n",
        "+        finally:\n",
        "+            faulthandler.cancel_dump_traceback_later()\n",
        "+    else:\n",
        "+        return (yield)\n",
        "+\n",
        "+\n",
        "+@pytest.hookimpl(tryfirst=True)\n",
        "+def pytest_enter_pdb() -> None:\n",
        "+    \"\"\"Cancel any traceback dumping due to timeout before entering pdb.\"\"\"\n",
        "+    import faulthandler\n",
        "+\n",
        "+    faulthandler.cancel_dump_traceback_later()\n",
        "+\n",
        "+\n",
        "+@pytest.hookimpl(tryfirst=True)\n",
        "+def pytest_exception_interact() -> None:\n",
        "+    \"\"\"Cancel any traceback dumping due to an interactive exception being\n",
        "+    raised.\"\"\"\n",
        "+    import faulthandler\n",
        "+\n",
        "+    faulthandler.cancel_dump_traceback_later()\n"
      ]
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/fixtures.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/freeze_support.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/helpconfig.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/hookspec.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/junitxml.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/legacypath.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/logging.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/main.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/mark/__init__.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/mark/expression.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/mark/structures.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/monkeypatch.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/nodes.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/outcomes.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/pastebin.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/pathlib.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/py.typed",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/pytester.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/pytester_assertions.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/python.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/python_api.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/raises.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/recwarn.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/reports.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/runner.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/scope.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/setuponly.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/setupplan.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/skipping.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/stash.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/stepwise.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/terminal.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/threadexception.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/timing.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/tmpdir.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/tracemalloc.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/unittest.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/unraisableexception.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/warning_types.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_pytest/warnings.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/_yaml/__init__.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohappyeyeballs-2.6.1.dist-info/INSTALLER",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohappyeyeballs-2.6.1.dist-info/LICENSE",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohappyeyeballs-2.6.1.dist-info/METADATA",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohappyeyeballs-2.6.1.dist-info/RECORD",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohappyeyeballs-2.6.1.dist-info/WHEEL",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohappyeyeballs/__init__.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohappyeyeballs/_staggered.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohappyeyeballs/impl.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohappyeyeballs/py.typed",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohappyeyeballs/types.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohappyeyeballs/utils.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp-3.12.15.dist-info/INSTALLER",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp-3.12.15.dist-info/METADATA",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp-3.12.15.dist-info/RECORD",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp-3.12.15.dist-info/WHEEL",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp-3.12.15.dist-info/licenses/LICENSE.txt",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp-3.12.15.dist-info/licenses/vendor/llhttp/LICENSE",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp-3.12.15.dist-info/top_level.txt",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/.hash/_cparser.pxd.hash",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/.hash/_find_header.pxd.hash",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/.hash/_http_parser.pyx.hash",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/.hash/_http_writer.pyx.hash",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/.hash/hdrs.py.hash",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/__init__.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/_cookie_helpers.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/_cparser.pxd",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/_find_header.pxd",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/_headers.pxi",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/_http_parser.pyx",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/_http_writer.pyx",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/_websocket/.hash/mask.pxd.hash",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/_websocket/.hash/mask.pyx.hash",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/_websocket/.hash/reader_c.pxd.hash",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/_websocket/__init__.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/_websocket/helpers.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/_websocket/mask.pxd",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/_websocket/mask.pyx",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/_websocket/models.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/_websocket/reader.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/_websocket/reader_c.pxd",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/_websocket/reader_c.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/_websocket/reader_py.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/_websocket/writer.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/abc.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/base_protocol.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/client.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/client_exceptions.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/client_middleware_digest_auth.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/client_middlewares.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/client_proto.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/client_reqrep.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/client_ws.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/compression_utils.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/connector.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/cookiejar.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/formdata.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/hdrs.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/helpers.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/http.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/http_exceptions.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/http_parser.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/http_websocket.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/http_writer.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/log.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/multipart.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/payload.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/payload_streamer.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/py.typed",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/pytest_plugin.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/resolver.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/streams.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/tcp_helpers.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/test_utils.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/tracing.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/typedefs.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/web.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/web_app.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/web_exceptions.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/web_fileresponse.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/web_log.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/web_middlewares.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/web_protocol.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/web_request.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/web_response.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/web_routedef.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/web_runner.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/web_server.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/web_urldispatcher.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/web_ws.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiohttp/worker.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiosignal-1.4.0.dist-info/INSTALLER",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiosignal-1.4.0.dist-info/METADATA",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiosignal-1.4.0.dist-info/RECORD",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiosignal-1.4.0.dist-info/WHEEL",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiosignal-1.4.0.dist-info/licenses/LICENSE",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiosignal-1.4.0.dist-info/top_level.txt",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiosignal/__init__.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/aiosignal/py.typed",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/annotated_types-0.7.0.dist-info/INSTALLER",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/annotated_types-0.7.0.dist-info/METADATA",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/annotated_types-0.7.0.dist-info/RECORD",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/annotated_types-0.7.0.dist-info/WHEEL",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/annotated_types-0.7.0.dist-info/licenses/LICENSE",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/annotated_types/__init__.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/annotated_types/py.typed",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/annotated_types/test_cases.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio-4.10.0.dist-info/INSTALLER",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio-4.10.0.dist-info/METADATA",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio-4.10.0.dist-info/RECORD",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio-4.10.0.dist-info/WHEEL",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio-4.10.0.dist-info/entry_points.txt",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio-4.10.0.dist-info/licenses/LICENSE",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio-4.10.0.dist-info/top_level.txt",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio/__init__.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio/_backends/__init__.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio/_backends/_trio.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio/_core/__init__.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio/_core/_asyncio_selector_thread.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio/_core/_contextmanagers.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio/_core/_eventloop.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio/_core/_exceptions.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio/_core/_fileio.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio/_core/_resources.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio/_core/_signals.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio/_core/_sockets.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio/_core/_streams.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio/_core/_subprocesses.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio/_core/_synchronization.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio/_core/_tasks.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio/_core/_tempfile.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio/_core/_testing.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio/_core/_typedattr.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio/abc/__init__.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio/abc/_eventloop.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio/abc/_resources.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "agent/venv/lib/python3.12/site-packages/anyio/abc/_sockets.py",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    }
  ]
}
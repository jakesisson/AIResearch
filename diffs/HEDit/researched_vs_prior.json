{
  "project": "Research Data/HEDit",
  "repo": "Annotation-Garden/HEDit",
  "prior_commit": "7567091434d09849ac0fe5b297c23bff5124e9ac",
  "researched_commit": "ef6ed6fa475a79cf6a52c3a5bf405c8d3a94414b",
  "compare_url": "https://github.com/Annotation-Garden/HEDit/compare/7567091434d09849ac0fe5b297c23bff5124e9ac...ef6ed6fa475a79cf6a52c3a5bf405c8d3a94414b",
  "ahead_by": 1,
  "behind_by": 0,
  "changed_files": [
    {
      "path": "pyproject.toml",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "patch": "@@ -4,7 +4,7 @@ build-backend = \"setuptools.build_meta\"\n \n [project]\n name = \"hedit\"\n-version = \"0.6.4.dev3\"\n+version = \"0.6.4.dev4\"\n description = \"Multi-agent system for HED annotation generation and validation\"\n readme = \"PKG_README.md\"\n requires-python = \">=3.12\"\n@@ -54,6 +54,8 @@ standalone = [\n     \"langchain-community>=0.3.0\",\n     \"langchain-core>=0.3.0\",\n     \"langchain-openai>=0.3.0\",\n+    \"litellm>=1.50.0\",  # LLM proxy with native prompt caching support\n+    \"langchain-litellm>=0.2.0\",  # LangChain integration for LiteLLM\n     \"hedtools>=0.5.0\",\n     \"lxml>=5.3.0\",\n     \"beautifulsoup4>=4.12.3\",",
      "patch_lines": [
        "@@ -4,7 +4,7 @@ build-backend = \"setuptools.build_meta\"\n",
        " \n",
        " [project]\n",
        " name = \"hedit\"\n",
        "-version = \"0.6.4.dev3\"\n",
        "+version = \"0.6.4.dev4\"\n",
        " description = \"Multi-agent system for HED annotation generation and validation\"\n",
        " readme = \"PKG_README.md\"\n",
        " requires-python = \">=3.12\"\n",
        "@@ -54,6 +54,8 @@ standalone = [\n",
        "     \"langchain-community>=0.3.0\",\n",
        "     \"langchain-core>=0.3.0\",\n",
        "     \"langchain-openai>=0.3.0\",\n",
        "+    \"litellm>=1.50.0\",  # LLM proxy with native prompt caching support\n",
        "+    \"langchain-litellm>=0.2.0\",  # LangChain integration for LiteLLM\n",
        "     \"hedtools>=0.5.0\",\n",
        "     \"lxml>=5.3.0\",\n",
        "     \"beautifulsoup4>=4.12.3\",\n"
      ]
    },
    {
      "path": "src/utils/hed_comprehensive_guide.py",
      "status": "modified",
      "additions": 352,
      "deletions": 15,
      "patch": "@@ -62,6 +62,12 @@ def get_comprehensive_hed_guide(vocabulary_sample: list[str], extendable_tags: l\n EXAMPLE: ((Red, Circle), (To-left-of, (Green, Square)))\n MEANING: \"A red circle is to the left of a green square\"\n \n+EXAMPLE: ((Dog), (Chases, (Cat)))\n+MEANING: \"A dog chases a cat\"\n+\n+EXAMPLE: ((Participant), (Focuses-on, (Target)))\n+MEANING: \"The participant focuses on the target\"\n+\n ### Rule 4: Group Event and Task-event-role at top level\n Event classification tags (Sensory-event, Agent-action) and Task-event-role tags\n (Experimental-stimulus, Participant-response) should be at the top level.\n@@ -81,6 +87,77 @@ def get_comprehensive_hed_guide(vocabulary_sample: list[str], extendable_tags: l\n \n ---\n \n+## RELATION TAGS\n+\n+Relation tags describe how entities relate to each other spatially, temporally, or logically.\n+Always use the pattern: (Entity-A, (Relation-tag, Entity-B))\n+\n+### Spatial Relations\n+Describe where objects are positioned relative to each other.\n+\n+COMMON SPATIAL RELATIONS:\n+- To-left-of, To-right-of - horizontal positioning\n+- Above, Below - vertical positioning\n+- In-front-of, Behind - depth positioning\n+- Inside-of, Outside-of - containment\n+- Near-to, Far-from - distance\n+- Center-of, Edge-of - position within\n+\n+EXAMPLES:\n+((Red, Circle), (To-left-of, (Blue, Square)))\n+\"Red circle is to the left of blue square\"\n+\n+((Face), (Center-of, (Screen)))\n+\"Face is at the center of the screen\"\n+\n+((Target), (Inside-of, (Boundary)))\n+\"Target is inside the boundary\"\n+\n+### Temporal Relations\n+Describe when events occur relative to each other.\n+\n+COMMON TEMPORAL RELATIONS:\n+- Before, After - sequence\n+- During - simultaneity\n+- Starts-during, Ends-during - partial overlap\n+\n+EXAMPLES:\n+((Cue), (Before, (Target)))\n+\"Cue appears before target\"\n+\n+((Sound), (During, (Visual-presentation)))\n+\"Sound occurs during visual presentation\"\n+\n+### Logical/Functional Relations\n+Describe functional relationships between entities.\n+\n+COMMON LOGICAL RELATIONS:\n+- Associated-with - general association\n+- Part-of - component relationship\n+- Related-to - loose connection\n+- Indicated-by - signaling relationship\n+- Linked-to - connected entities\n+\n+EXAMPLES:\n+((Response), (Associated-with, (Stimulus)))\n+\"Response is associated with stimulus\"\n+\n+((Button), (Part-of, (Response-box)))\n+\"Button is part of response box\"\n+\n+### Relation Pattern Rules\n+1. The subject comes first: (Subject, (Relation, Object))\n+2. Relations create directional links\n+3. Inverse relations may exist (Above/Below, Before/After)\n+4. Group related entities properly before applying relations\n+5. Relations can be nested for complex descriptions\n+\n+COMPLEX EXAMPLE:\n+((Target, (Red, Circle)), (Above, ((Distractor, (Blue, Square)), (To-left-of, (Green, Triangle)))))\n+\"A red circle target is above a blue square distractor that is to the left of a green triangle\"\n+\n+---\n+\n ## CRITICAL: EVENT AND AGENT SUBTREES CANNOT BE EXTENDED\n \n The Event subtree (7 tags) and Agent subtree (6 tags) do NOT allow extension.\n@@ -172,62 +249,177 @@ def get_comprehensive_hed_guide(vocabulary_sample: list[str], extendable_tags: l\n \n ## DEFINITION SYSTEM\n \n-Definitions allow naming reusable annotation patterns.\n+Definitions allow naming reusable annotation patterns for consistency and brevity.\n+They are essential for Onset/Offset temporal scoping and for reducing repetition.\n+\n+### Why Use Definitions\n+1. REUSABILITY: Define once, use many times with Def\n+2. CONSISTENCY: Ensure same annotation structure throughout dataset\n+3. TEMPORAL SCOPING: Required anchor for Onset/Offset/Inset\n+4. PARAMETERIZATION: Use # to create templates with variable values\n \n ### Creating Definitions (in sidecars only)\n Pattern: (Definition/Name, (tag1, tag2, tag3))\n With placeholder: (Definition/Name/#, (Tag1/# units, Tag2))\n \n EXAMPLE: (Definition/RedCircle, (Sensory-event, Visual-presentation, (Red, Circle)))\n-EXAMPLE: (Definition/Acc/#, (Acceleration/# m-per-s^2, Red))\n+MEANING: Defines \"RedCircle\" as a sensory event showing a red circle\n+\n+EXAMPLE: (Definition/Acc/#, (Acceleration/# m-per-s^2))\n+MEANING: Defines \"Acc\" as an acceleration value with m-per-s^2 units\n+\n+EXAMPLE: (Definition/ButtonPress, (Agent-action, Participant-response, (Press, Mouse-button)))\n+MEANING: Defines \"ButtonPress\" as a participant pressing a mouse button\n \n ### Using Definitions with Def\n Pattern: Def/Name or Def/Name/value (if definition has placeholder)\n \n EXAMPLE: Def/RedCircle\n-EXAMPLE: Def/Acc/4.5\n+EXPANDS TO: (Sensory-event, Visual-presentation, (Red, Circle))\n+\n+EXAMPLE: Def/Acc/9.8\n+EXPANDS TO: (Acceleration/9.8 m-per-s^2)\n+\n+### Definition Naming Conventions\n+- Use descriptive, meaningful names: RedCircle, TargetAppears, ResponseGiven\n+- Use CamelCase or hyphenated names: Red-circle, Target-appears\n+- Avoid generic names: Event1, Def1, Thing\n+- Keep names concise but clear\n+\n+### Definitions for Temporal Scoping\n+When using Onset/Offset, the Definition provides the anchor.\n+\n+SIDECAR DEFINITION:\n+(Definition/VideoPlayback, (Sensory-event, Visual-presentation, Movie))\n+\n+EVENT FILE USAGE:\n+Row 1: (Def/VideoPlayback, Onset)  # Video starts\n+Row 2: (Def/VideoPlayback, Offset)  # Video ends\n+\n+### Parameterized Definitions\n+Use # as placeholder for values that change.\n+\n+DEFINITION: (Definition/Tone/#, (Auditory-presentation, Tone, Frequency/# Hz))\n+\n+USAGE:\n+Def/Tone/440 -> (Auditory-presentation, Tone, Frequency/440 Hz)\n+Def/Tone/880 -> (Auditory-presentation, Tone, Frequency/880 Hz)\n \n ### Def-expand (DO NOT USE)\n-Def-expand is created by tools during processing. Never use it manually.\n+Def-expand is created automatically by HED tools during validation/processing.\n+It shows the expanded content for debugging. Never write Def-expand manually.\n \n ### Definition Rules\n-- Definitions can only appear in sidecars or external files\n-- Cannot contain Def, Def-expand, or nested Definition\n-- If using #, must have exactly two # characters\n-- Definition names must be unique\n+- Definitions can ONLY appear in sidecars or external definition files\n+- Definitions CANNOT appear in the HED column of event files directly\n+- Cannot contain Def, Def-expand, or nested Definition tags\n+- If using #, must have exactly TWO # characters (one in name, one in content)\n+- Definition names must be unique across the entire dataset\n+- Names are case-sensitive: RedCircle and redcircle are different\n \n ---\n \n-## TEMPORAL SCOPING (Onset/Offset/Duration)\n+## TEMPORAL SCOPING (Onset/Offset/Duration/Inset)\n+\n+HED provides several ways to annotate the temporal extent of events.\n \n-### Using Duration (simpler)\n+### Using Duration (simpler approach)\n Pattern: (Duration/value units, (event-content))\n+Use Duration when you know exactly how long something lasts.\n \n EXAMPLE: (Duration/2 s, (Sensory-event, Visual-presentation, Cue, (Cross)))\n MEANING: A cross cue is displayed for 2 seconds\n \n+EXAMPLE: (Duration/500 ms, (Auditory-presentation, Beep))\n+MEANING: A beep is presented for 500 milliseconds\n+\n ### Using Onset/Offset (for explicit start/end markers)\n-Requires a Definition anchor.\n+Use Onset/Offset when start and end are recorded as separate events in data.\n+Requires a Definition anchor to link start and end events.\n+\n+STEP 1: Define the event type in sidecar\n+(Definition/Fixation-display, (Sensory-event, Visual-presentation, Fixation-point))\n+\n+STEP 2: Mark start with Onset\n+(Def/Fixation-display, Onset)\n+\n+STEP 3: Mark end with Offset\n+(Def/Fixation-display, Offset)\n \n-START: (Def/Event, Onset)\n-END: (Def/Event, Offset)\n+### Onset/Offset Rules\n+- Onset and Offset MUST use the same Definition anchor\n+- Each Onset must have a matching Offset (eventually)\n+- Multiple instances can overlap if they use different Definition anchors\n+- Onset marks when the scoped event BEGINS\n+- Offset marks when the scoped event ENDS\n+\n+### Using Inset (for markers during ongoing events)\n+Use Inset to mark intermediate time points within an Onset/Offset scope.\n+\n+Pattern: (Def/Event-name, Inset)\n \n EXAMPLE:\n-  Start: (Def/Fixation-point, Onset)\n-  End: (Def/Fixation-point, Offset)\n+  t=0: (Def/Video-playback, Onset)  # Video starts\n+  t=5: (Def/Video-playback, Inset), (Scene-change)  # Scene change during video\n+  t=10: (Def/Video-playback, Inset), (Face, Appears)  # Face appears\n+  t=30: (Def/Video-playback, Offset)  # Video ends\n+\n+### When to Use Each\n+\n+Duration:\n+- Event duration is known and can be specified directly\n+- Event starts at the annotated time point\n+- Simpler when you have duration information\n+\n+Onset/Offset:\n+- Start and end are recorded as separate event rows\n+- Duration may vary or be unknown at start\n+- Need to track overlapping instances\n+\n+Inset:\n+- Need to mark points within a longer event\n+- Annotating sub-events or state changes\n+- Used between Onset and Offset of the same anchor\n+\n+### Combining with Delay\n+Use Delay for events that occur after the trigger point.\n+\n+EXAMPLE: (Delay/1 s, (Duration/2 s, (Sensory-event, Visual-presentation, Target)))\n+MEANING: Target appears 1 second after event marker, displays for 2 seconds\n \n ---\n \n ## SIDECAR SYNTAX (events.json)\n \n ### Value Placeholders (#)\n For columns with varying values, use # as placeholder.\n+The # indicates a value that will be substituted from the column.\n \n EXAMPLE: {{\"age\": {{\"HED\": \"Age/# years\"}}}}\n For age=25: assembles to \"Age/25 years\"\n \n+EXAMPLE: {{\"response_time\": {{\"HED\": \"Response-time/# ms\"}}}}\n+For response_time=350: assembles to \"Response-time/350 ms\"\n+\n+### Units with # Placeholders\n+When using # with value-taking tags, always include the unit.\n+Common unit patterns:\n+- Time: Duration/# s, Response-time/# ms, Delay/# s\n+- Frequency: Frequency/# Hz\n+- Distance: Distance/# m, Height/# cm\n+- Angle: Angle/# deg, Rotation-angle/# rad\n+- Acceleration: Acceleration/# m-per-s^2\n+- Proportion: Probability/# (unitless)\n+\n+WRONG: Duration/#, Frequency/# (missing units)\n+RIGHT: Duration/# s, Frequency/# Hz\n+\n ### Column References (curly braces)\n Reference other columns to assemble grouped annotations.\n+Curly braces control how annotations from multiple columns are assembled together.\n+\n+BASIC PATTERN: {{column_name}}\n+This inserts the HED annotation from that column at this position.\n \n EXAMPLE:\n {{\n@@ -243,11 +435,45 @@ def get_comprehensive_hed_guide(vocabulary_sample: list[str], extendable_tags: l\n For event_type=visual, color=red, shape=circle:\n ASSEMBLES TO: Experimental-stimulus, Sensory-event, Visual-presentation, (Red, Circle)\n \n+### Advanced Curly Brace Patterns\n+\n+PATTERN 1: Grouping properties together\n+When properties should form a single group, put curly braces inside parentheses:\n+HED: \"({{object_color}}, {{object_shape}}, {{object_size}})\"\n+\n+PATTERN 2: Multiple independent groups\n+HED: \"({{target_color}}, {{target_shape}}), ({{distractor_color}}, {{distractor_shape}})\"\n+\n+PATTERN 3: Nested relationships\n+HED: \"(({{agent_type}}, {{agent_name}}), ({{action}}, ({{object}})))\"\n+\n+### Handling n/a Values in Assembly\n+When a column value is \"n/a\" or empty, its annotation is omitted.\n+\n+EXAMPLE:\n+{{\n+  \"response\": {{\n+    \"HED\": {{\n+      \"button_press\": \"Participant-response, (Press, ({{response_hand}}, Mouse-button))\"\n+    }}\n+  }},\n+  \"response_hand\": {{\"HED\": {{\"left\": \"Left\", \"right\": \"Right\"}}}}\n+}}\n+\n+For response=button_press, response_hand=left:\n+ASSEMBLES TO: Participant-response, (Press, (Left, Mouse-button))\n+\n+For response=button_press, response_hand=n/a:\n+ASSEMBLES TO: Participant-response, (Press, (Mouse-button))\n+\n ### Curly Brace Rules\n - Only valid in sidecars (not in event file HED column directly)\n - Must reference existing columns with HED annotations\n - No circular references (A references B, B references A)\n - Use for grouping related properties from different columns\n+- Empty/n/a values in referenced columns are silently omitted\n+- Curly braces can appear multiple times in the same annotation\n+- Column references are case-sensitive\n \n ---\n \n@@ -369,6 +595,117 @@ def get_comprehensive_hed_guide(vocabulary_sample: list[str], extendable_tags: l\n \n ---\n \n+## COMMON ERRORS AND TROUBLESHOOTING\n+\n+### Error: TAG_EXTENSION_INVALID\n+CAUSE: Extending a tag with a child that already exists in schema vocabulary.\n+\n+EXAMPLE ERRORS:\n+- Red-color/Red/DarkRed  (DarkRed may exist in vocab, use it directly)\n+- Sensory-presentation/Red  (Red exists in vocab, don't re-extend)\n+- Item/Window  (Window exists in vocab, use it directly)\n+\n+FIX: Check vocabulary first. If tag exists, use it directly without slash extension.\n+\n+WRONG: Building/House  (if House is in vocabulary)\n+RIGHT: House\n+\n+WRONG: Action/Press  (if Press is in vocabulary)\n+RIGHT: Press\n+\n+### Error: TAG_INVALID\n+CAUSE: Tag or extension is not valid in the schema.\n+\n+EXAMPLE ERRORS:\n+- ReallyInvalid/Extension  (base tag doesn't exist)\n+- ReallyInvalid  (tag not in schema)\n+- Label #  (# used incorrectly outside sidecar)\n+\n+FIX: Use only tags from the vocabulary or valid extensions from extendable tags.\n+\n+WRONG: Stimulus/Visual  (Stimulus not in vocab)\n+RIGHT: Sensory-event, Visual-presentation\n+\n+WRONG: Response/Button  (Response not a valid base)\n+RIGHT: Participant-response, (Press, Button)\n+\n+### Error: VALUE_INVALID\n+CAUSE: Value substituted for placeholder (#) is incorrect format.\n+\n+EXAMPLE ERRORS:\n+- Def/Acc/MyMy  (text instead of number for acceleration)\n+- Distance/4mxxx  (malformed unit)\n+- Duration/fast  (text instead of number)\n+\n+FIX: Use correct value format with proper units.\n+\n+WRONG: Duration/fast\n+RIGHT: Duration/2 s\n+\n+WRONG: Frequency/high\n+RIGHT: Frequency/1000 Hz\n+\n+WRONG: Distance/4mxxx\n+RIGHT: Distance/4 m\n+\n+### Error: UNIT_CLASS_INVALID\n+CAUSE: Wrong unit type for the value.\n+\n+EXAMPLE ERRORS:\n+- Duration/5 Hz  (Hz is frequency, not time)\n+- Frequency/3 s  (s is time, not frequency)\n+\n+FIX: Match unit to tag's expected unit class.\n+\n+Time units: s, ms, second, seconds, minute, minutes, hour\n+Frequency units: Hz, kHz, mHz\n+Distance units: m, cm, mm, km, ft, mile\n+Angle units: rad, deg, degree\n+\n+### Error: CHARACTER_INVALID\n+CAUSE: Extension name contains invalid characters.\n+\n+EXAMPLE ERRORS:\n+- Red/Red$2  ($ not allowed)\n+- Red/R#d  (# not allowed in extension names)\n+\n+FIX: Use only letters, numbers, and hyphens in extension names.\n+\n+WRONG: Animal/Cat$1\n+RIGHT: Animal/Cat-1 or Animal/Cat1\n+\n+### Error: PARENTHESES_MISMATCH\n+CAUSE: Opening and closing parentheses don't match.\n+\n+EXAMPLE ERRORS:\n+- ((Red, Circle)  (missing closing paren)\n+- (Red, Circle))  (extra closing paren)\n+- ((A, (B, C)))  (correct - properly nested)\n+\n+FIX: Count parentheses; each ( must have matching ).\n+\n+### Error: DEFINITION_INVALID\n+CAUSE: Definition used incorrectly.\n+\n+EXAMPLE ERRORS:\n+- Definition/Name in HED column  (definitions only in sidecars)\n+- (Definition/X, (Def/Y))  (cannot nest Def inside Definition)\n+- (Definition/A, (Definition/B))  (cannot nest definitions)\n+\n+FIX: Definitions only in sidecars, cannot contain Def or nested Definition.\n+\n+### Quick Validation Checklist\n+Before submitting annotations:\n+1. Every tag exists in vocabulary OR is valid extension?\n+2. Extensions use most specific parent?\n+3. Event/Agent tags are NOT extended (use grouping)?\n+4. Value tags have proper units?\n+5. Parentheses are balanced?\n+6. Definitions only in sidecar, not event file?\n+7. Properties grouped with their objects?\n+\n+---\n+\n ## OUTPUT FORMAT\n \n Output ONLY the HED annotation string.",
      "patch_lines": [
        "@@ -62,6 +62,12 @@ def get_comprehensive_hed_guide(vocabulary_sample: list[str], extendable_tags: l\n",
        " EXAMPLE: ((Red, Circle), (To-left-of, (Green, Square)))\n",
        " MEANING: \"A red circle is to the left of a green square\"\n",
        " \n",
        "+EXAMPLE: ((Dog), (Chases, (Cat)))\n",
        "+MEANING: \"A dog chases a cat\"\n",
        "+\n",
        "+EXAMPLE: ((Participant), (Focuses-on, (Target)))\n",
        "+MEANING: \"The participant focuses on the target\"\n",
        "+\n",
        " ### Rule 4: Group Event and Task-event-role at top level\n",
        " Event classification tags (Sensory-event, Agent-action) and Task-event-role tags\n",
        " (Experimental-stimulus, Participant-response) should be at the top level.\n",
        "@@ -81,6 +87,77 @@ def get_comprehensive_hed_guide(vocabulary_sample: list[str], extendable_tags: l\n",
        " \n",
        " ---\n",
        " \n",
        "+## RELATION TAGS\n",
        "+\n",
        "+Relation tags describe how entities relate to each other spatially, temporally, or logically.\n",
        "+Always use the pattern: (Entity-A, (Relation-tag, Entity-B))\n",
        "+\n",
        "+### Spatial Relations\n",
        "+Describe where objects are positioned relative to each other.\n",
        "+\n",
        "+COMMON SPATIAL RELATIONS:\n",
        "+- To-left-of, To-right-of - horizontal positioning\n",
        "+- Above, Below - vertical positioning\n",
        "+- In-front-of, Behind - depth positioning\n",
        "+- Inside-of, Outside-of - containment\n",
        "+- Near-to, Far-from - distance\n",
        "+- Center-of, Edge-of - position within\n",
        "+\n",
        "+EXAMPLES:\n",
        "+((Red, Circle), (To-left-of, (Blue, Square)))\n",
        "+\"Red circle is to the left of blue square\"\n",
        "+\n",
        "+((Face), (Center-of, (Screen)))\n",
        "+\"Face is at the center of the screen\"\n",
        "+\n",
        "+((Target), (Inside-of, (Boundary)))\n",
        "+\"Target is inside the boundary\"\n",
        "+\n",
        "+### Temporal Relations\n",
        "+Describe when events occur relative to each other.\n",
        "+\n",
        "+COMMON TEMPORAL RELATIONS:\n",
        "+- Before, After - sequence\n",
        "+- During - simultaneity\n",
        "+- Starts-during, Ends-during - partial overlap\n",
        "+\n",
        "+EXAMPLES:\n",
        "+((Cue), (Before, (Target)))\n",
        "+\"Cue appears before target\"\n",
        "+\n",
        "+((Sound), (During, (Visual-presentation)))\n",
        "+\"Sound occurs during visual presentation\"\n",
        "+\n",
        "+### Logical/Functional Relations\n",
        "+Describe functional relationships between entities.\n",
        "+\n",
        "+COMMON LOGICAL RELATIONS:\n",
        "+- Associated-with - general association\n",
        "+- Part-of - component relationship\n",
        "+- Related-to - loose connection\n",
        "+- Indicated-by - signaling relationship\n",
        "+- Linked-to - connected entities\n",
        "+\n",
        "+EXAMPLES:\n",
        "+((Response), (Associated-with, (Stimulus)))\n",
        "+\"Response is associated with stimulus\"\n",
        "+\n",
        "+((Button), (Part-of, (Response-box)))\n",
        "+\"Button is part of response box\"\n",
        "+\n",
        "+### Relation Pattern Rules\n",
        "+1. The subject comes first: (Subject, (Relation, Object))\n",
        "+2. Relations create directional links\n",
        "+3. Inverse relations may exist (Above/Below, Before/After)\n",
        "+4. Group related entities properly before applying relations\n",
        "+5. Relations can be nested for complex descriptions\n",
        "+\n",
        "+COMPLEX EXAMPLE:\n",
        "+((Target, (Red, Circle)), (Above, ((Distractor, (Blue, Square)), (To-left-of, (Green, Triangle)))))\n",
        "+\"A red circle target is above a blue square distractor that is to the left of a green triangle\"\n",
        "+\n",
        "+---\n",
        "+\n",
        " ## CRITICAL: EVENT AND AGENT SUBTREES CANNOT BE EXTENDED\n",
        " \n",
        " The Event subtree (7 tags) and Agent subtree (6 tags) do NOT allow extension.\n",
        "@@ -172,62 +249,177 @@ def get_comprehensive_hed_guide(vocabulary_sample: list[str], extendable_tags: l\n",
        " \n",
        " ## DEFINITION SYSTEM\n",
        " \n",
        "-Definitions allow naming reusable annotation patterns.\n",
        "+Definitions allow naming reusable annotation patterns for consistency and brevity.\n",
        "+They are essential for Onset/Offset temporal scoping and for reducing repetition.\n",
        "+\n",
        "+### Why Use Definitions\n",
        "+1. REUSABILITY: Define once, use many times with Def\n",
        "+2. CONSISTENCY: Ensure same annotation structure throughout dataset\n",
        "+3. TEMPORAL SCOPING: Required anchor for Onset/Offset/Inset\n",
        "+4. PARAMETERIZATION: Use # to create templates with variable values\n",
        " \n",
        " ### Creating Definitions (in sidecars only)\n",
        " Pattern: (Definition/Name, (tag1, tag2, tag3))\n",
        " With placeholder: (Definition/Name/#, (Tag1/# units, Tag2))\n",
        " \n",
        " EXAMPLE: (Definition/RedCircle, (Sensory-event, Visual-presentation, (Red, Circle)))\n",
        "-EXAMPLE: (Definition/Acc/#, (Acceleration/# m-per-s^2, Red))\n",
        "+MEANING: Defines \"RedCircle\" as a sensory event showing a red circle\n",
        "+\n",
        "+EXAMPLE: (Definition/Acc/#, (Acceleration/# m-per-s^2))\n",
        "+MEANING: Defines \"Acc\" as an acceleration value with m-per-s^2 units\n",
        "+\n",
        "+EXAMPLE: (Definition/ButtonPress, (Agent-action, Participant-response, (Press, Mouse-button)))\n",
        "+MEANING: Defines \"ButtonPress\" as a participant pressing a mouse button\n",
        " \n",
        " ### Using Definitions with Def\n",
        " Pattern: Def/Name or Def/Name/value (if definition has placeholder)\n",
        " \n",
        " EXAMPLE: Def/RedCircle\n",
        "-EXAMPLE: Def/Acc/4.5\n",
        "+EXPANDS TO: (Sensory-event, Visual-presentation, (Red, Circle))\n",
        "+\n",
        "+EXAMPLE: Def/Acc/9.8\n",
        "+EXPANDS TO: (Acceleration/9.8 m-per-s^2)\n",
        "+\n",
        "+### Definition Naming Conventions\n",
        "+- Use descriptive, meaningful names: RedCircle, TargetAppears, ResponseGiven\n",
        "+- Use CamelCase or hyphenated names: Red-circle, Target-appears\n",
        "+- Avoid generic names: Event1, Def1, Thing\n",
        "+- Keep names concise but clear\n",
        "+\n",
        "+### Definitions for Temporal Scoping\n",
        "+When using Onset/Offset, the Definition provides the anchor.\n",
        "+\n",
        "+SIDECAR DEFINITION:\n",
        "+(Definition/VideoPlayback, (Sensory-event, Visual-presentation, Movie))\n",
        "+\n",
        "+EVENT FILE USAGE:\n",
        "+Row 1: (Def/VideoPlayback, Onset)  # Video starts\n",
        "+Row 2: (Def/VideoPlayback, Offset)  # Video ends\n",
        "+\n",
        "+### Parameterized Definitions\n",
        "+Use # as placeholder for values that change.\n",
        "+\n",
        "+DEFINITION: (Definition/Tone/#, (Auditory-presentation, Tone, Frequency/# Hz))\n",
        "+\n",
        "+USAGE:\n",
        "+Def/Tone/440 -> (Auditory-presentation, Tone, Frequency/440 Hz)\n",
        "+Def/Tone/880 -> (Auditory-presentation, Tone, Frequency/880 Hz)\n",
        " \n",
        " ### Def-expand (DO NOT USE)\n",
        "-Def-expand is created by tools during processing. Never use it manually.\n",
        "+Def-expand is created automatically by HED tools during validation/processing.\n",
        "+It shows the expanded content for debugging. Never write Def-expand manually.\n",
        " \n",
        " ### Definition Rules\n",
        "-- Definitions can only appear in sidecars or external files\n",
        "-- Cannot contain Def, Def-expand, or nested Definition\n",
        "-- If using #, must have exactly two # characters\n",
        "-- Definition names must be unique\n",
        "+- Definitions can ONLY appear in sidecars or external definition files\n",
        "+- Definitions CANNOT appear in the HED column of event files directly\n",
        "+- Cannot contain Def, Def-expand, or nested Definition tags\n",
        "+- If using #, must have exactly TWO # characters (one in name, one in content)\n",
        "+- Definition names must be unique across the entire dataset\n",
        "+- Names are case-sensitive: RedCircle and redcircle are different\n",
        " \n",
        " ---\n",
        " \n",
        "-## TEMPORAL SCOPING (Onset/Offset/Duration)\n",
        "+## TEMPORAL SCOPING (Onset/Offset/Duration/Inset)\n",
        "+\n",
        "+HED provides several ways to annotate the temporal extent of events.\n",
        " \n",
        "-### Using Duration (simpler)\n",
        "+### Using Duration (simpler approach)\n",
        " Pattern: (Duration/value units, (event-content))\n",
        "+Use Duration when you know exactly how long something lasts.\n",
        " \n",
        " EXAMPLE: (Duration/2 s, (Sensory-event, Visual-presentation, Cue, (Cross)))\n",
        " MEANING: A cross cue is displayed for 2 seconds\n",
        " \n",
        "+EXAMPLE: (Duration/500 ms, (Auditory-presentation, Beep))\n",
        "+MEANING: A beep is presented for 500 milliseconds\n",
        "+\n",
        " ### Using Onset/Offset (for explicit start/end markers)\n",
        "-Requires a Definition anchor.\n",
        "+Use Onset/Offset when start and end are recorded as separate events in data.\n",
        "+Requires a Definition anchor to link start and end events.\n",
        "+\n",
        "+STEP 1: Define the event type in sidecar\n",
        "+(Definition/Fixation-display, (Sensory-event, Visual-presentation, Fixation-point))\n",
        "+\n",
        "+STEP 2: Mark start with Onset\n",
        "+(Def/Fixation-display, Onset)\n",
        "+\n",
        "+STEP 3: Mark end with Offset\n",
        "+(Def/Fixation-display, Offset)\n",
        " \n",
        "-START: (Def/Event, Onset)\n",
        "-END: (Def/Event, Offset)\n",
        "+### Onset/Offset Rules\n",
        "+- Onset and Offset MUST use the same Definition anchor\n",
        "+- Each Onset must have a matching Offset (eventually)\n",
        "+- Multiple instances can overlap if they use different Definition anchors\n",
        "+- Onset marks when the scoped event BEGINS\n",
        "+- Offset marks when the scoped event ENDS\n",
        "+\n",
        "+### Using Inset (for markers during ongoing events)\n",
        "+Use Inset to mark intermediate time points within an Onset/Offset scope.\n",
        "+\n",
        "+Pattern: (Def/Event-name, Inset)\n",
        " \n",
        " EXAMPLE:\n",
        "-  Start: (Def/Fixation-point, Onset)\n",
        "-  End: (Def/Fixation-point, Offset)\n",
        "+  t=0: (Def/Video-playback, Onset)  # Video starts\n",
        "+  t=5: (Def/Video-playback, Inset), (Scene-change)  # Scene change during video\n",
        "+  t=10: (Def/Video-playback, Inset), (Face, Appears)  # Face appears\n",
        "+  t=30: (Def/Video-playback, Offset)  # Video ends\n",
        "+\n",
        "+### When to Use Each\n",
        "+\n",
        "+Duration:\n",
        "+- Event duration is known and can be specified directly\n",
        "+- Event starts at the annotated time point\n",
        "+- Simpler when you have duration information\n",
        "+\n",
        "+Onset/Offset:\n",
        "+- Start and end are recorded as separate event rows\n",
        "+- Duration may vary or be unknown at start\n",
        "+- Need to track overlapping instances\n",
        "+\n",
        "+Inset:\n",
        "+- Need to mark points within a longer event\n",
        "+- Annotating sub-events or state changes\n",
        "+- Used between Onset and Offset of the same anchor\n",
        "+\n",
        "+### Combining with Delay\n",
        "+Use Delay for events that occur after the trigger point.\n",
        "+\n",
        "+EXAMPLE: (Delay/1 s, (Duration/2 s, (Sensory-event, Visual-presentation, Target)))\n",
        "+MEANING: Target appears 1 second after event marker, displays for 2 seconds\n",
        " \n",
        " ---\n",
        " \n",
        " ## SIDECAR SYNTAX (events.json)\n",
        " \n",
        " ### Value Placeholders (#)\n",
        " For columns with varying values, use # as placeholder.\n",
        "+The # indicates a value that will be substituted from the column.\n",
        " \n",
        " EXAMPLE: {{\"age\": {{\"HED\": \"Age/# years\"}}}}\n",
        " For age=25: assembles to \"Age/25 years\"\n",
        " \n",
        "+EXAMPLE: {{\"response_time\": {{\"HED\": \"Response-time/# ms\"}}}}\n",
        "+For response_time=350: assembles to \"Response-time/350 ms\"\n",
        "+\n",
        "+### Units with # Placeholders\n",
        "+When using # with value-taking tags, always include the unit.\n",
        "+Common unit patterns:\n",
        "+- Time: Duration/# s, Response-time/# ms, Delay/# s\n",
        "+- Frequency: Frequency/# Hz\n",
        "+- Distance: Distance/# m, Height/# cm\n",
        "+- Angle: Angle/# deg, Rotation-angle/# rad\n",
        "+- Acceleration: Acceleration/# m-per-s^2\n",
        "+- Proportion: Probability/# (unitless)\n",
        "+\n",
        "+WRONG: Duration/#, Frequency/# (missing units)\n",
        "+RIGHT: Duration/# s, Frequency/# Hz\n",
        "+\n",
        " ### Column References (curly braces)\n",
        " Reference other columns to assemble grouped annotations.\n",
        "+Curly braces control how annotations from multiple columns are assembled together.\n",
        "+\n",
        "+BASIC PATTERN: {{column_name}}\n",
        "+This inserts the HED annotation from that column at this position.\n",
        " \n",
        " EXAMPLE:\n",
        " {{\n",
        "@@ -243,11 +435,45 @@ def get_comprehensive_hed_guide(vocabulary_sample: list[str], extendable_tags: l\n",
        " For event_type=visual, color=red, shape=circle:\n",
        " ASSEMBLES TO: Experimental-stimulus, Sensory-event, Visual-presentation, (Red, Circle)\n",
        " \n",
        "+### Advanced Curly Brace Patterns\n",
        "+\n",
        "+PATTERN 1: Grouping properties together\n",
        "+When properties should form a single group, put curly braces inside parentheses:\n",
        "+HED: \"({{object_color}}, {{object_shape}}, {{object_size}})\"\n",
        "+\n",
        "+PATTERN 2: Multiple independent groups\n",
        "+HED: \"({{target_color}}, {{target_shape}}), ({{distractor_color}}, {{distractor_shape}})\"\n",
        "+\n",
        "+PATTERN 3: Nested relationships\n",
        "+HED: \"(({{agent_type}}, {{agent_name}}), ({{action}}, ({{object}})))\"\n",
        "+\n",
        "+### Handling n/a Values in Assembly\n",
        "+When a column value is \"n/a\" or empty, its annotation is omitted.\n",
        "+\n",
        "+EXAMPLE:\n",
        "+{{\n",
        "+  \"response\": {{\n",
        "+    \"HED\": {{\n",
        "+      \"button_press\": \"Participant-response, (Press, ({{response_hand}}, Mouse-button))\"\n",
        "+    }}\n",
        "+  }},\n",
        "+  \"response_hand\": {{\"HED\": {{\"left\": \"Left\", \"right\": \"Right\"}}}}\n",
        "+}}\n",
        "+\n",
        "+For response=button_press, response_hand=left:\n",
        "+ASSEMBLES TO: Participant-response, (Press, (Left, Mouse-button))\n",
        "+\n",
        "+For response=button_press, response_hand=n/a:\n",
        "+ASSEMBLES TO: Participant-response, (Press, (Mouse-button))\n",
        "+\n",
        " ### Curly Brace Rules\n",
        " - Only valid in sidecars (not in event file HED column directly)\n",
        " - Must reference existing columns with HED annotations\n",
        " - No circular references (A references B, B references A)\n",
        " - Use for grouping related properties from different columns\n",
        "+- Empty/n/a values in referenced columns are silently omitted\n",
        "+- Curly braces can appear multiple times in the same annotation\n",
        "+- Column references are case-sensitive\n",
        " \n",
        " ---\n",
        " \n",
        "@@ -369,6 +595,117 @@ def get_comprehensive_hed_guide(vocabulary_sample: list[str], extendable_tags: l\n",
        " \n",
        " ---\n",
        " \n",
        "+## COMMON ERRORS AND TROUBLESHOOTING\n",
        "+\n",
        "+### Error: TAG_EXTENSION_INVALID\n",
        "+CAUSE: Extending a tag with a child that already exists in schema vocabulary.\n",
        "+\n",
        "+EXAMPLE ERRORS:\n",
        "+- Red-color/Red/DarkRed  (DarkRed may exist in vocab, use it directly)\n",
        "+- Sensory-presentation/Red  (Red exists in vocab, don't re-extend)\n",
        "+- Item/Window  (Window exists in vocab, use it directly)\n",
        "+\n",
        "+FIX: Check vocabulary first. If tag exists, use it directly without slash extension.\n",
        "+\n",
        "+WRONG: Building/House  (if House is in vocabulary)\n",
        "+RIGHT: House\n",
        "+\n",
        "+WRONG: Action/Press  (if Press is in vocabulary)\n",
        "+RIGHT: Press\n",
        "+\n",
        "+### Error: TAG_INVALID\n",
        "+CAUSE: Tag or extension is not valid in the schema.\n",
        "+\n",
        "+EXAMPLE ERRORS:\n",
        "+- ReallyInvalid/Extension  (base tag doesn't exist)\n",
        "+- ReallyInvalid  (tag not in schema)\n",
        "+- Label #  (# used incorrectly outside sidecar)\n",
        "+\n",
        "+FIX: Use only tags from the vocabulary or valid extensions from extendable tags.\n",
        "+\n",
        "+WRONG: Stimulus/Visual  (Stimulus not in vocab)\n",
        "+RIGHT: Sensory-event, Visual-presentation\n",
        "+\n",
        "+WRONG: Response/Button  (Response not a valid base)\n",
        "+RIGHT: Participant-response, (Press, Button)\n",
        "+\n",
        "+### Error: VALUE_INVALID\n",
        "+CAUSE: Value substituted for placeholder (#) is incorrect format.\n",
        "+\n",
        "+EXAMPLE ERRORS:\n",
        "+- Def/Acc/MyMy  (text instead of number for acceleration)\n",
        "+- Distance/4mxxx  (malformed unit)\n",
        "+- Duration/fast  (text instead of number)\n",
        "+\n",
        "+FIX: Use correct value format with proper units.\n",
        "+\n",
        "+WRONG: Duration/fast\n",
        "+RIGHT: Duration/2 s\n",
        "+\n",
        "+WRONG: Frequency/high\n",
        "+RIGHT: Frequency/1000 Hz\n",
        "+\n",
        "+WRONG: Distance/4mxxx\n",
        "+RIGHT: Distance/4 m\n",
        "+\n",
        "+### Error: UNIT_CLASS_INVALID\n",
        "+CAUSE: Wrong unit type for the value.\n",
        "+\n",
        "+EXAMPLE ERRORS:\n",
        "+- Duration/5 Hz  (Hz is frequency, not time)\n",
        "+- Frequency/3 s  (s is time, not frequency)\n",
        "+\n",
        "+FIX: Match unit to tag's expected unit class.\n",
        "+\n",
        "+Time units: s, ms, second, seconds, minute, minutes, hour\n",
        "+Frequency units: Hz, kHz, mHz\n",
        "+Distance units: m, cm, mm, km, ft, mile\n",
        "+Angle units: rad, deg, degree\n",
        "+\n",
        "+### Error: CHARACTER_INVALID\n",
        "+CAUSE: Extension name contains invalid characters.\n",
        "+\n",
        "+EXAMPLE ERRORS:\n",
        "+- Red/Red$2  ($ not allowed)\n",
        "+- Red/R#d  (# not allowed in extension names)\n",
        "+\n",
        "+FIX: Use only letters, numbers, and hyphens in extension names.\n",
        "+\n",
        "+WRONG: Animal/Cat$1\n",
        "+RIGHT: Animal/Cat-1 or Animal/Cat1\n",
        "+\n",
        "+### Error: PARENTHESES_MISMATCH\n",
        "+CAUSE: Opening and closing parentheses don't match.\n",
        "+\n",
        "+EXAMPLE ERRORS:\n",
        "+- ((Red, Circle)  (missing closing paren)\n",
        "+- (Red, Circle))  (extra closing paren)\n",
        "+- ((A, (B, C)))  (correct - properly nested)\n",
        "+\n",
        "+FIX: Count parentheses; each ( must have matching ).\n",
        "+\n",
        "+### Error: DEFINITION_INVALID\n",
        "+CAUSE: Definition used incorrectly.\n",
        "+\n",
        "+EXAMPLE ERRORS:\n",
        "+- Definition/Name in HED column  (definitions only in sidecars)\n",
        "+- (Definition/X, (Def/Y))  (cannot nest Def inside Definition)\n",
        "+- (Definition/A, (Definition/B))  (cannot nest definitions)\n",
        "+\n",
        "+FIX: Definitions only in sidecars, cannot contain Def or nested Definition.\n",
        "+\n",
        "+### Quick Validation Checklist\n",
        "+Before submitting annotations:\n",
        "+1. Every tag exists in vocabulary OR is valid extension?\n",
        "+2. Extensions use most specific parent?\n",
        "+3. Event/Agent tags are NOT extended (use grouping)?\n",
        "+4. Value tags have proper units?\n",
        "+5. Parentheses are balanced?\n",
        "+6. Definitions only in sidecar, not event file?\n",
        "+7. Properties grouped with their objects?\n",
        "+\n",
        "+---\n",
        "+\n",
        " ## OUTPUT FORMAT\n",
        " \n",
        " Output ONLY the HED annotation string.\n"
      ]
    },
    {
      "path": "src/utils/litellm_llm.py",
      "status": "added",
      "additions": 195,
      "deletions": 0,
      "patch": "@@ -0,0 +1,195 @@\n+\"\"\"LiteLLM integration for OpenRouter with prompt caching support.\n+\n+This module provides LLM access through LiteLLM, which natively supports\n+Anthropic's prompt caching via the cache_control parameter. This reduces\n+costs by up to 90% for repeated prompts with large static content.\n+\n+Usage:\n+    from src.utils.litellm_llm import create_litellm_openrouter\n+\n+    # Create LLM with caching enabled\n+    llm = create_litellm_openrouter(\n+        model=\"anthropic/claude-haiku-4.5\",\n+        api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n+        enable_caching=True,\n+    )\n+\n+    # Use with LangChain messages (cache_control added automatically to system)\n+    response = await llm.ainvoke([\n+        SystemMessage(content=large_system_prompt),\n+        HumanMessage(content=user_query),\n+    ])\n+\"\"\"\n+\n+import os\n+from typing import Any\n+\n+from langchain_core.language_models import BaseChatModel\n+from langchain_core.messages import BaseMessage\n+\n+\n+def create_litellm_openrouter(\n+    model: str = \"openai/gpt-oss-120b\",\n+    api_key: str | None = None,\n+    temperature: float = 0.1,\n+    max_tokens: int | None = None,\n+    provider: str | None = None,\n+    user_id: str | None = None,\n+    enable_caching: bool = False,\n+) -> BaseChatModel:\n+    \"\"\"Create a LiteLLM-backed LLM for OpenRouter with optional prompt caching.\n+\n+    Args:\n+        model: Model identifier (e.g., \"anthropic/claude-haiku-4.5\")\n+        api_key: OpenRouter API key\n+        temperature: Sampling temperature (0.0-1.0)\n+        max_tokens: Maximum tokens to generate\n+        provider: Specific provider to use (e.g., \"Anthropic\")\n+        user_id: User identifier for cache optimization (sticky routing)\n+        enable_caching: Enable Anthropic prompt caching (default: False)\n+\n+    Returns:\n+        ChatLiteLLM instance configured for OpenRouter\n+    \"\"\"\n+    from langchain_litellm import ChatLiteLLM\n+\n+    # LiteLLM uses openrouter/ prefix for OpenRouter models\n+    litellm_model = f\"openrouter/{model}\"\n+\n+    # Build extra parameters\n+    extra_params: dict[str, Any] = {\n+        # OpenRouter app identification\n+        \"extra_headers\": {\n+            \"HTTP-Referer\": \"https://annotation.garden/hedit\",\n+            \"X-Title\": \"HEDit - HED Annotation Generator\",\n+        },\n+    }\n+\n+    # Provider routing\n+    if provider:\n+        extra_params[\"provider\"] = {\"only\": [provider]}\n+\n+    # User ID for sticky cache routing\n+    if user_id:\n+        extra_params[\"user\"] = user_id\n+\n+    # Return wrapped LLM with caching if enabled\n+    llm = ChatLiteLLM(\n+        model=litellm_model,\n+        api_key=api_key or os.getenv(\"OPENROUTER_API_KEY\"),\n+        temperature=temperature,\n+        max_tokens=max_tokens,\n+        model_kwargs=extra_params,\n+    )\n+\n+    if enable_caching:\n+        return CachingLLMWrapper(llm)\n+\n+    return llm\n+\n+\n+class CachingLLMWrapper(BaseChatModel):\n+    \"\"\"Wrapper that adds cache_control to system messages for Anthropic caching.\n+\n+    This wrapper intercepts messages before they're sent to the LLM and\n+    transforms system messages to use the multipart format with cache_control.\n+\n+    The cache_control parameter tells Anthropic to cache the content, reducing\n+    costs by 90% on cache hits (after initial 25% cache write premium).\n+    \"\"\"\n+\n+    llm: BaseChatModel\n+    \"\"\"The underlying LLM to wrap.\"\"\"\n+\n+    model_config = {\"arbitrary_types_allowed\": True}\n+\n+    def __init__(self, llm: BaseChatModel, **kwargs):\n+        super().__init__(llm=llm, **kwargs)\n+\n+    @property\n+    def _llm_type(self) -> str:\n+        return \"caching_llm_wrapper\"\n+\n+    def _add_cache_control(self, messages: list[BaseMessage]) -> list[dict]:\n+        \"\"\"Transform messages to add cache_control to system messages.\n+\n+        Args:\n+            messages: List of LangChain messages\n+\n+        Returns:\n+            List of message dicts with cache_control on system messages\n+        \"\"\"\n+        from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n+\n+        result = []\n+        for msg in messages:\n+            if isinstance(msg, SystemMessage):\n+                # Transform system message to multipart format with cache_control\n+                result.append(\n+                    {\n+                        \"role\": \"system\",\n+                        \"content\": [\n+                            {\n+                                \"type\": \"text\",\n+                                \"text\": msg.content,\n+                                \"cache_control\": {\"type\": \"ephemeral\"},\n+                            }\n+                        ],\n+                    }\n+                )\n+            elif isinstance(msg, HumanMessage):\n+                result.append({\"role\": \"user\", \"content\": msg.content})\n+            elif isinstance(msg, AIMessage):\n+                result.append({\"role\": \"assistant\", \"content\": msg.content})\n+            else:\n+                # Fallback for other message types\n+                result.append({\"role\": \"user\", \"content\": str(msg.content)})\n+\n+        return result\n+\n+    def _generate(self, messages: list[BaseMessage], **kwargs) -> Any:\n+        \"\"\"Generate response with cache_control on system messages.\"\"\"\n+        cached_messages = self._add_cache_control(messages)\n+        return self.llm._generate(cached_messages, **kwargs)\n+\n+    async def _agenerate(self, messages: list[BaseMessage], **kwargs) -> Any:\n+        \"\"\"Async generate response with cache_control on system messages.\"\"\"\n+        cached_messages = self._add_cache_control(messages)\n+        return await self.llm._agenerate(cached_messages, **kwargs)\n+\n+    def invoke(self, messages: list[BaseMessage], **kwargs) -> Any:\n+        \"\"\"Invoke LLM with cache_control on system messages.\"\"\"\n+        cached_messages = self._add_cache_control(messages)\n+        return self.llm.invoke(cached_messages, **kwargs)\n+\n+    async def ainvoke(self, messages: list[BaseMessage], **kwargs) -> Any:\n+        \"\"\"Async invoke LLM with cache_control on system messages.\"\"\"\n+        cached_messages = self._add_cache_control(messages)\n+        return await self.llm.ainvoke(cached_messages, **kwargs)\n+\n+\n+# Supported models that benefit from prompt caching\n+# Anthropic models via OpenRouter that support cache_control\n+CACHEABLE_MODELS = {\n+    \"claude-opus-4.5\": \"anthropic/claude-opus-4-5\",\n+    \"claude-sonnet-4.5\": \"anthropic/claude-sonnet-4-5\",\n+    \"claude-sonnet-4\": \"anthropic/claude-sonnet-4\",\n+    \"claude-haiku-4.5\": \"anthropic/claude-haiku-4.5\",\n+    \"claude-haiku-3.5\": \"anthropic/claude-3.5-haiku\",\n+}\n+\n+\n+def is_cacheable_model(model: str) -> bool:\n+    \"\"\"Check if a model supports Anthropic prompt caching.\n+\n+    Args:\n+        model: Model identifier\n+\n+    Returns:\n+        True if the model supports cache_control\n+    \"\"\"\n+    # Check exact match in aliases\n+    if model in CACHEABLE_MODELS:\n+        return True\n+    # Check if it's an Anthropic model\n+    return model.startswith(\"anthropic/claude-\")",
      "patch_lines": [
        "@@ -0,0 +1,195 @@\n",
        "+\"\"\"LiteLLM integration for OpenRouter with prompt caching support.\n",
        "+\n",
        "+This module provides LLM access through LiteLLM, which natively supports\n",
        "+Anthropic's prompt caching via the cache_control parameter. This reduces\n",
        "+costs by up to 90% for repeated prompts with large static content.\n",
        "+\n",
        "+Usage:\n",
        "+    from src.utils.litellm_llm import create_litellm_openrouter\n",
        "+\n",
        "+    # Create LLM with caching enabled\n",
        "+    llm = create_litellm_openrouter(\n",
        "+        model=\"anthropic/claude-haiku-4.5\",\n",
        "+        api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
        "+        enable_caching=True,\n",
        "+    )\n",
        "+\n",
        "+    # Use with LangChain messages (cache_control added automatically to system)\n",
        "+    response = await llm.ainvoke([\n",
        "+        SystemMessage(content=large_system_prompt),\n",
        "+        HumanMessage(content=user_query),\n",
        "+    ])\n",
        "+\"\"\"\n",
        "+\n",
        "+import os\n",
        "+from typing import Any\n",
        "+\n",
        "+from langchain_core.language_models import BaseChatModel\n",
        "+from langchain_core.messages import BaseMessage\n",
        "+\n",
        "+\n",
        "+def create_litellm_openrouter(\n",
        "+    model: str = \"openai/gpt-oss-120b\",\n",
        "+    api_key: str | None = None,\n",
        "+    temperature: float = 0.1,\n",
        "+    max_tokens: int | None = None,\n",
        "+    provider: str | None = None,\n",
        "+    user_id: str | None = None,\n",
        "+    enable_caching: bool = False,\n",
        "+) -> BaseChatModel:\n",
        "+    \"\"\"Create a LiteLLM-backed LLM for OpenRouter with optional prompt caching.\n",
        "+\n",
        "+    Args:\n",
        "+        model: Model identifier (e.g., \"anthropic/claude-haiku-4.5\")\n",
        "+        api_key: OpenRouter API key\n",
        "+        temperature: Sampling temperature (0.0-1.0)\n",
        "+        max_tokens: Maximum tokens to generate\n",
        "+        provider: Specific provider to use (e.g., \"Anthropic\")\n",
        "+        user_id: User identifier for cache optimization (sticky routing)\n",
        "+        enable_caching: Enable Anthropic prompt caching (default: False)\n",
        "+\n",
        "+    Returns:\n",
        "+        ChatLiteLLM instance configured for OpenRouter\n",
        "+    \"\"\"\n",
        "+    from langchain_litellm import ChatLiteLLM\n",
        "+\n",
        "+    # LiteLLM uses openrouter/ prefix for OpenRouter models\n",
        "+    litellm_model = f\"openrouter/{model}\"\n",
        "+\n",
        "+    # Build extra parameters\n",
        "+    extra_params: dict[str, Any] = {\n",
        "+        # OpenRouter app identification\n",
        "+        \"extra_headers\": {\n",
        "+            \"HTTP-Referer\": \"https://annotation.garden/hedit\",\n",
        "+            \"X-Title\": \"HEDit - HED Annotation Generator\",\n",
        "+        },\n",
        "+    }\n",
        "+\n",
        "+    # Provider routing\n",
        "+    if provider:\n",
        "+        extra_params[\"provider\"] = {\"only\": [provider]}\n",
        "+\n",
        "+    # User ID for sticky cache routing\n",
        "+    if user_id:\n",
        "+        extra_params[\"user\"] = user_id\n",
        "+\n",
        "+    # Return wrapped LLM with caching if enabled\n",
        "+    llm = ChatLiteLLM(\n",
        "+        model=litellm_model,\n",
        "+        api_key=api_key or os.getenv(\"OPENROUTER_API_KEY\"),\n",
        "+        temperature=temperature,\n",
        "+        max_tokens=max_tokens,\n",
        "+        model_kwargs=extra_params,\n",
        "+    )\n",
        "+\n",
        "+    if enable_caching:\n",
        "+        return CachingLLMWrapper(llm)\n",
        "+\n",
        "+    return llm\n",
        "+\n",
        "+\n",
        "+class CachingLLMWrapper(BaseChatModel):\n",
        "+    \"\"\"Wrapper that adds cache_control to system messages for Anthropic caching.\n",
        "+\n",
        "+    This wrapper intercepts messages before they're sent to the LLM and\n",
        "+    transforms system messages to use the multipart format with cache_control.\n",
        "+\n",
        "+    The cache_control parameter tells Anthropic to cache the content, reducing\n",
        "+    costs by 90% on cache hits (after initial 25% cache write premium).\n",
        "+    \"\"\"\n",
        "+\n",
        "+    llm: BaseChatModel\n",
        "+    \"\"\"The underlying LLM to wrap.\"\"\"\n",
        "+\n",
        "+    model_config = {\"arbitrary_types_allowed\": True}\n",
        "+\n",
        "+    def __init__(self, llm: BaseChatModel, **kwargs):\n",
        "+        super().__init__(llm=llm, **kwargs)\n",
        "+\n",
        "+    @property\n",
        "+    def _llm_type(self) -> str:\n",
        "+        return \"caching_llm_wrapper\"\n",
        "+\n",
        "+    def _add_cache_control(self, messages: list[BaseMessage]) -> list[dict]:\n",
        "+        \"\"\"Transform messages to add cache_control to system messages.\n",
        "+\n",
        "+        Args:\n",
        "+            messages: List of LangChain messages\n",
        "+\n",
        "+        Returns:\n",
        "+            List of message dicts with cache_control on system messages\n",
        "+        \"\"\"\n",
        "+        from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "+\n",
        "+        result = []\n",
        "+        for msg in messages:\n",
        "+            if isinstance(msg, SystemMessage):\n",
        "+                # Transform system message to multipart format with cache_control\n",
        "+                result.append(\n",
        "+                    {\n",
        "+                        \"role\": \"system\",\n",
        "+                        \"content\": [\n",
        "+                            {\n",
        "+                                \"type\": \"text\",\n",
        "+                                \"text\": msg.content,\n",
        "+                                \"cache_control\": {\"type\": \"ephemeral\"},\n",
        "+                            }\n",
        "+                        ],\n",
        "+                    }\n",
        "+                )\n",
        "+            elif isinstance(msg, HumanMessage):\n",
        "+                result.append({\"role\": \"user\", \"content\": msg.content})\n",
        "+            elif isinstance(msg, AIMessage):\n",
        "+                result.append({\"role\": \"assistant\", \"content\": msg.content})\n",
        "+            else:\n",
        "+                # Fallback for other message types\n",
        "+                result.append({\"role\": \"user\", \"content\": str(msg.content)})\n",
        "+\n",
        "+        return result\n",
        "+\n",
        "+    def _generate(self, messages: list[BaseMessage], **kwargs) -> Any:\n",
        "+        \"\"\"Generate response with cache_control on system messages.\"\"\"\n",
        "+        cached_messages = self._add_cache_control(messages)\n",
        "+        return self.llm._generate(cached_messages, **kwargs)\n",
        "+\n",
        "+    async def _agenerate(self, messages: list[BaseMessage], **kwargs) -> Any:\n",
        "+        \"\"\"Async generate response with cache_control on system messages.\"\"\"\n",
        "+        cached_messages = self._add_cache_control(messages)\n",
        "+        return await self.llm._agenerate(cached_messages, **kwargs)\n",
        "+\n",
        "+    def invoke(self, messages: list[BaseMessage], **kwargs) -> Any:\n",
        "+        \"\"\"Invoke LLM with cache_control on system messages.\"\"\"\n",
        "+        cached_messages = self._add_cache_control(messages)\n",
        "+        return self.llm.invoke(cached_messages, **kwargs)\n",
        "+\n",
        "+    async def ainvoke(self, messages: list[BaseMessage], **kwargs) -> Any:\n",
        "+        \"\"\"Async invoke LLM with cache_control on system messages.\"\"\"\n",
        "+        cached_messages = self._add_cache_control(messages)\n",
        "+        return await self.llm.ainvoke(cached_messages, **kwargs)\n",
        "+\n",
        "+\n",
        "+# Supported models that benefit from prompt caching\n",
        "+# Anthropic models via OpenRouter that support cache_control\n",
        "+CACHEABLE_MODELS = {\n",
        "+    \"claude-opus-4.5\": \"anthropic/claude-opus-4-5\",\n",
        "+    \"claude-sonnet-4.5\": \"anthropic/claude-sonnet-4-5\",\n",
        "+    \"claude-sonnet-4\": \"anthropic/claude-sonnet-4\",\n",
        "+    \"claude-haiku-4.5\": \"anthropic/claude-haiku-4.5\",\n",
        "+    \"claude-haiku-3.5\": \"anthropic/claude-3.5-haiku\",\n",
        "+}\n",
        "+\n",
        "+\n",
        "+def is_cacheable_model(model: str) -> bool:\n",
        "+    \"\"\"Check if a model supports Anthropic prompt caching.\n",
        "+\n",
        "+    Args:\n",
        "+        model: Model identifier\n",
        "+\n",
        "+    Returns:\n",
        "+        True if the model supports cache_control\n",
        "+    \"\"\"\n",
        "+    # Check exact match in aliases\n",
        "+    if model in CACHEABLE_MODELS:\n",
        "+        return True\n",
        "+    # Check if it's an Anthropic model\n",
        "+    return model.startswith(\"anthropic/claude-\")\n"
      ]
    },
    {
      "path": "src/utils/openrouter_llm.py",
      "status": "modified",
      "additions": 162,
      "deletions": 26,
      "patch": "@@ -1,7 +1,15 @@\n-\"\"\"OpenRouter LLM integration for cloud model access.\"\"\"\n+\"\"\"OpenRouter LLM integration for cloud model access.\n+\n+This module uses LiteLLM under the hood for native Anthropic prompt caching support.\n+Cache control is automatically enabled for Anthropic models, reducing costs by up to\n+90% for repeated prompts with large static content (like the HED vocabulary guide).\n+\"\"\"\n+\n+import os\n+from typing import Any\n \n from langchain_core.language_models import BaseChatModel\n-from langchain_openai import ChatOpenAI\n+from langchain_core.messages import BaseMessage\n \n \n def create_openrouter_llm(\n@@ -11,49 +19,151 @@ def create_openrouter_llm(\n     max_tokens: int | None = None,\n     provider: str | None = None,\n     user_id: str | None = None,\n+    enable_caching: bool | None = None,\n ) -> BaseChatModel:\n-    \"\"\"Create an OpenRouter LLM instance.\n+    \"\"\"Create an OpenRouter LLM instance with optional prompt caching.\n+\n+    Uses LiteLLM for native support of Anthropic's prompt caching feature.\n+    When caching is enabled, system messages are automatically transformed\n+    to include cache_control markers for 90% cost reduction on cache hits.\n \n     Args:\n-        model: Model identifier (e.g., \"openai/gpt-oss-120b\")\n-        api_key: OpenRouter API key\n+        model: Model identifier (e.g., \"openai/gpt-oss-120b\", \"anthropic/claude-haiku-4.5\")\n+        api_key: OpenRouter API key (defaults to OPENROUTER_API_KEY env var)\n         temperature: Sampling temperature (0.0-1.0)\n         max_tokens: Maximum tokens to generate\n-        provider: Specific provider to use (e.g., \"Cerebras\")\n+        provider: Specific provider to use (e.g., \"Cerebras\", \"Anthropic\")\n         user_id: User identifier for cache optimization (sticky routing)\n+        enable_caching: Enable Anthropic prompt caching. If None (default),\n+            auto-enables for Anthropic Claude models.\n \n     Returns:\n-        ChatOpenAI instance configured for OpenRouter\n+        LLM instance configured for OpenRouter\n     \"\"\"\n-    # OpenRouter requires these headers for app identification\n-    # Using default_headers (not model_kwargs) to ensure headers are sent\n-    # HTTP-Referer: Primary URL for app identification in OpenRouter rankings\n-    # X-Title: Display name in OpenRouter analytics\n-    # Note: favicon_url, main_url, description are configured in OpenRouter Dashboard\n-    default_headers = {\n-        \"HTTP-Referer\": \"https://annotation.garden/hedit\",\n-        \"X-Title\": \"HEDit - HED Annotation Generator\",\n+    from langchain_litellm import ChatLiteLLM\n+\n+    # LiteLLM uses openrouter/ prefix for OpenRouter models\n+    litellm_model = f\"openrouter/{model}\"\n+\n+    # Build model_kwargs for OpenRouter-specific options\n+    model_kwargs: dict[str, Any] = {\n+        # OpenRouter app identification headers\n+        \"extra_headers\": {\n+            \"HTTP-Referer\": \"https://annotation.garden/hedit\",\n+            \"X-Title\": \"HEDit - HED Annotation Generator\",\n+        },\n     }\n \n-    # Build extra_body for provider preference\n-    extra_body = {}\n+    # Provider routing (e.g., {\"only\": [\"Cerebras\"]})\n     if provider:\n-        extra_body[\"provider\"] = {\"only\": [provider]}\n+        model_kwargs[\"provider\"] = {\"only\": [provider]}\n \n-    # Add user ID for sticky cache routing (reduces costs via prompt caching)\n+    # User ID for sticky cache routing\n     if user_id:\n-        extra_body[\"user\"] = user_id\n+        model_kwargs[\"user\"] = user_id\n \n-    return ChatOpenAI(\n-        model=model,\n-        openai_api_key=api_key,\n-        openai_api_base=\"https://openrouter.ai/api/v1\",\n+    # Create base LLM\n+    llm = ChatLiteLLM(\n+        model=litellm_model,\n+        api_key=api_key or os.getenv(\"OPENROUTER_API_KEY\"),\n         temperature=temperature,\n         max_tokens=max_tokens,\n-        default_headers=default_headers,\n-        extra_body=extra_body if extra_body else None,\n+        model_kwargs=model_kwargs,\n     )\n \n+    # Determine if caching should be enabled\n+    if enable_caching is None:\n+        # Auto-enable for Anthropic models\n+        enable_caching = is_cacheable_model(model)\n+\n+    if enable_caching:\n+        return CachingLLMWrapper(llm=llm)\n+\n+    return llm\n+\n+\n+class CachingLLMWrapper(BaseChatModel):\n+    \"\"\"Wrapper that adds cache_control to system messages for Anthropic caching.\n+\n+    This wrapper intercepts messages before they're sent to the LLM and\n+    transforms system messages to use the multipart format with cache_control.\n+\n+    The cache_control parameter tells Anthropic to cache the content, reducing\n+    costs by 90% on cache hits (after initial 25% cache write premium).\n+\n+    Minimum cacheable prompt: 1024 tokens for Claude Sonnet/Opus, 4096 for Haiku 4.5\n+    Cache TTL: 5 minutes (refreshed on each hit)\n+    \"\"\"\n+\n+    llm: BaseChatModel\n+    \"\"\"The underlying LLM to wrap.\"\"\"\n+\n+    model_config = {\"arbitrary_types_allowed\": True}\n+\n+    def __init__(self, llm: BaseChatModel, **kwargs):\n+        super().__init__(llm=llm, **kwargs)\n+\n+    @property\n+    def _llm_type(self) -> str:\n+        return \"caching_llm_wrapper\"\n+\n+    def _add_cache_control(self, messages: list[BaseMessage]) -> list[dict]:\n+        \"\"\"Transform messages to add cache_control to system messages.\n+\n+        Args:\n+            messages: List of LangChain messages\n+\n+        Returns:\n+            List of message dicts with cache_control on system messages\n+        \"\"\"\n+        from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n+\n+        result = []\n+        for msg in messages:\n+            if isinstance(msg, SystemMessage):\n+                # Transform system message to multipart format with cache_control\n+                result.append(\n+                    {\n+                        \"role\": \"system\",\n+                        \"content\": [\n+                            {\n+                                \"type\": \"text\",\n+                                \"text\": msg.content,\n+                                \"cache_control\": {\"type\": \"ephemeral\"},\n+                            }\n+                        ],\n+                    }\n+                )\n+            elif isinstance(msg, HumanMessage):\n+                result.append({\"role\": \"user\", \"content\": msg.content})\n+            elif isinstance(msg, AIMessage):\n+                result.append({\"role\": \"assistant\", \"content\": msg.content})\n+            else:\n+                # Fallback for other message types\n+                result.append({\"role\": \"user\", \"content\": str(msg.content)})\n+\n+        return result\n+\n+    def _generate(self, messages: list[BaseMessage], **kwargs) -> Any:\n+        \"\"\"Generate response with cache_control on system messages.\"\"\"\n+        cached_messages = self._add_cache_control(messages)\n+        return self.llm._generate(cached_messages, **kwargs)\n+\n+    async def _agenerate(self, messages: list[BaseMessage], **kwargs) -> Any:\n+        \"\"\"Async generate response with cache_control on system messages.\"\"\"\n+        cached_messages = self._add_cache_control(messages)\n+        return await self.llm._agenerate(cached_messages, **kwargs)\n+\n+    def invoke(self, messages: list[BaseMessage], **kwargs) -> Any:\n+        \"\"\"Invoke LLM with cache_control on system messages.\"\"\"\n+        cached_messages = self._add_cache_control(messages)\n+        return self.llm.invoke(cached_messages, **kwargs)\n+\n+    async def ainvoke(self, messages: list[BaseMessage], **kwargs) -> Any:\n+        \"\"\"Async invoke LLM with cache_control on system messages.\"\"\"\n+        cached_messages = self._add_cache_control(messages)\n+        return await self.llm.ainvoke(cached_messages, **kwargs)\n+\n \n # Model configuration - using gpt-oss-120b via Cerebras\n OPENROUTER_MODELS = {\n@@ -72,3 +182,29 @@ def get_model_name(alias: str) -> str:\n         Full model identifier for OpenRouter\n     \"\"\"\n     return OPENROUTER_MODELS.get(alias, alias)\n+\n+\n+# Anthropic models that support prompt caching\n+CACHEABLE_MODELS = {\n+    \"claude-opus-4.5\": \"anthropic/claude-opus-4-5\",\n+    \"claude-sonnet-4.5\": \"anthropic/claude-sonnet-4-5\",\n+    \"claude-sonnet-4\": \"anthropic/claude-sonnet-4\",\n+    \"claude-haiku-4.5\": \"anthropic/claude-haiku-4.5\",\n+    \"claude-haiku-3.5\": \"anthropic/claude-3.5-haiku\",\n+}\n+\n+\n+def is_cacheable_model(model: str) -> bool:\n+    \"\"\"Check if a model supports Anthropic prompt caching.\n+\n+    Args:\n+        model: Model identifier\n+\n+    Returns:\n+        True if the model supports cache_control\n+    \"\"\"\n+    # Check exact match in aliases\n+    if model in CACHEABLE_MODELS:\n+        return True\n+    # Check if it's an Anthropic Claude model\n+    return model.startswith(\"anthropic/claude-\")",
      "patch_lines": [
        "@@ -1,7 +1,15 @@\n",
        "-\"\"\"OpenRouter LLM integration for cloud model access.\"\"\"\n",
        "+\"\"\"OpenRouter LLM integration for cloud model access.\n",
        "+\n",
        "+This module uses LiteLLM under the hood for native Anthropic prompt caching support.\n",
        "+Cache control is automatically enabled for Anthropic models, reducing costs by up to\n",
        "+90% for repeated prompts with large static content (like the HED vocabulary guide).\n",
        "+\"\"\"\n",
        "+\n",
        "+import os\n",
        "+from typing import Any\n",
        " \n",
        " from langchain_core.language_models import BaseChatModel\n",
        "-from langchain_openai import ChatOpenAI\n",
        "+from langchain_core.messages import BaseMessage\n",
        " \n",
        " \n",
        " def create_openrouter_llm(\n",
        "@@ -11,49 +19,151 @@ def create_openrouter_llm(\n",
        "     max_tokens: int | None = None,\n",
        "     provider: str | None = None,\n",
        "     user_id: str | None = None,\n",
        "+    enable_caching: bool | None = None,\n",
        " ) -> BaseChatModel:\n",
        "-    \"\"\"Create an OpenRouter LLM instance.\n",
        "+    \"\"\"Create an OpenRouter LLM instance with optional prompt caching.\n",
        "+\n",
        "+    Uses LiteLLM for native support of Anthropic's prompt caching feature.\n",
        "+    When caching is enabled, system messages are automatically transformed\n",
        "+    to include cache_control markers for 90% cost reduction on cache hits.\n",
        " \n",
        "     Args:\n",
        "-        model: Model identifier (e.g., \"openai/gpt-oss-120b\")\n",
        "-        api_key: OpenRouter API key\n",
        "+        model: Model identifier (e.g., \"openai/gpt-oss-120b\", \"anthropic/claude-haiku-4.5\")\n",
        "+        api_key: OpenRouter API key (defaults to OPENROUTER_API_KEY env var)\n",
        "         temperature: Sampling temperature (0.0-1.0)\n",
        "         max_tokens: Maximum tokens to generate\n",
        "-        provider: Specific provider to use (e.g., \"Cerebras\")\n",
        "+        provider: Specific provider to use (e.g., \"Cerebras\", \"Anthropic\")\n",
        "         user_id: User identifier for cache optimization (sticky routing)\n",
        "+        enable_caching: Enable Anthropic prompt caching. If None (default),\n",
        "+            auto-enables for Anthropic Claude models.\n",
        " \n",
        "     Returns:\n",
        "-        ChatOpenAI instance configured for OpenRouter\n",
        "+        LLM instance configured for OpenRouter\n",
        "     \"\"\"\n",
        "-    # OpenRouter requires these headers for app identification\n",
        "-    # Using default_headers (not model_kwargs) to ensure headers are sent\n",
        "-    # HTTP-Referer: Primary URL for app identification in OpenRouter rankings\n",
        "-    # X-Title: Display name in OpenRouter analytics\n",
        "-    # Note: favicon_url, main_url, description are configured in OpenRouter Dashboard\n",
        "-    default_headers = {\n",
        "-        \"HTTP-Referer\": \"https://annotation.garden/hedit\",\n",
        "-        \"X-Title\": \"HEDit - HED Annotation Generator\",\n",
        "+    from langchain_litellm import ChatLiteLLM\n",
        "+\n",
        "+    # LiteLLM uses openrouter/ prefix for OpenRouter models\n",
        "+    litellm_model = f\"openrouter/{model}\"\n",
        "+\n",
        "+    # Build model_kwargs for OpenRouter-specific options\n",
        "+    model_kwargs: dict[str, Any] = {\n",
        "+        # OpenRouter app identification headers\n",
        "+        \"extra_headers\": {\n",
        "+            \"HTTP-Referer\": \"https://annotation.garden/hedit\",\n",
        "+            \"X-Title\": \"HEDit - HED Annotation Generator\",\n",
        "+        },\n",
        "     }\n",
        " \n",
        "-    # Build extra_body for provider preference\n",
        "-    extra_body = {}\n",
        "+    # Provider routing (e.g., {\"only\": [\"Cerebras\"]})\n",
        "     if provider:\n",
        "-        extra_body[\"provider\"] = {\"only\": [provider]}\n",
        "+        model_kwargs[\"provider\"] = {\"only\": [provider]}\n",
        " \n",
        "-    # Add user ID for sticky cache routing (reduces costs via prompt caching)\n",
        "+    # User ID for sticky cache routing\n",
        "     if user_id:\n",
        "-        extra_body[\"user\"] = user_id\n",
        "+        model_kwargs[\"user\"] = user_id\n",
        " \n",
        "-    return ChatOpenAI(\n",
        "-        model=model,\n",
        "-        openai_api_key=api_key,\n",
        "-        openai_api_base=\"https://openrouter.ai/api/v1\",\n",
        "+    # Create base LLM\n",
        "+    llm = ChatLiteLLM(\n",
        "+        model=litellm_model,\n",
        "+        api_key=api_key or os.getenv(\"OPENROUTER_API_KEY\"),\n",
        "         temperature=temperature,\n",
        "         max_tokens=max_tokens,\n",
        "-        default_headers=default_headers,\n",
        "-        extra_body=extra_body if extra_body else None,\n",
        "+        model_kwargs=model_kwargs,\n",
        "     )\n",
        " \n",
        "+    # Determine if caching should be enabled\n",
        "+    if enable_caching is None:\n",
        "+        # Auto-enable for Anthropic models\n",
        "+        enable_caching = is_cacheable_model(model)\n",
        "+\n",
        "+    if enable_caching:\n",
        "+        return CachingLLMWrapper(llm=llm)\n",
        "+\n",
        "+    return llm\n",
        "+\n",
        "+\n",
        "+class CachingLLMWrapper(BaseChatModel):\n",
        "+    \"\"\"Wrapper that adds cache_control to system messages for Anthropic caching.\n",
        "+\n",
        "+    This wrapper intercepts messages before they're sent to the LLM and\n",
        "+    transforms system messages to use the multipart format with cache_control.\n",
        "+\n",
        "+    The cache_control parameter tells Anthropic to cache the content, reducing\n",
        "+    costs by 90% on cache hits (after initial 25% cache write premium).\n",
        "+\n",
        "+    Minimum cacheable prompt: 1024 tokens for Claude Sonnet/Opus, 4096 for Haiku 4.5\n",
        "+    Cache TTL: 5 minutes (refreshed on each hit)\n",
        "+    \"\"\"\n",
        "+\n",
        "+    llm: BaseChatModel\n",
        "+    \"\"\"The underlying LLM to wrap.\"\"\"\n",
        "+\n",
        "+    model_config = {\"arbitrary_types_allowed\": True}\n",
        "+\n",
        "+    def __init__(self, llm: BaseChatModel, **kwargs):\n",
        "+        super().__init__(llm=llm, **kwargs)\n",
        "+\n",
        "+    @property\n",
        "+    def _llm_type(self) -> str:\n",
        "+        return \"caching_llm_wrapper\"\n",
        "+\n",
        "+    def _add_cache_control(self, messages: list[BaseMessage]) -> list[dict]:\n",
        "+        \"\"\"Transform messages to add cache_control to system messages.\n",
        "+\n",
        "+        Args:\n",
        "+            messages: List of LangChain messages\n",
        "+\n",
        "+        Returns:\n",
        "+            List of message dicts with cache_control on system messages\n",
        "+        \"\"\"\n",
        "+        from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "+\n",
        "+        result = []\n",
        "+        for msg in messages:\n",
        "+            if isinstance(msg, SystemMessage):\n",
        "+                # Transform system message to multipart format with cache_control\n",
        "+                result.append(\n",
        "+                    {\n",
        "+                        \"role\": \"system\",\n",
        "+                        \"content\": [\n",
        "+                            {\n",
        "+                                \"type\": \"text\",\n",
        "+                                \"text\": msg.content,\n",
        "+                                \"cache_control\": {\"type\": \"ephemeral\"},\n",
        "+                            }\n",
        "+                        ],\n",
        "+                    }\n",
        "+                )\n",
        "+            elif isinstance(msg, HumanMessage):\n",
        "+                result.append({\"role\": \"user\", \"content\": msg.content})\n",
        "+            elif isinstance(msg, AIMessage):\n",
        "+                result.append({\"role\": \"assistant\", \"content\": msg.content})\n",
        "+            else:\n",
        "+                # Fallback for other message types\n",
        "+                result.append({\"role\": \"user\", \"content\": str(msg.content)})\n",
        "+\n",
        "+        return result\n",
        "+\n",
        "+    def _generate(self, messages: list[BaseMessage], **kwargs) -> Any:\n",
        "+        \"\"\"Generate response with cache_control on system messages.\"\"\"\n",
        "+        cached_messages = self._add_cache_control(messages)\n",
        "+        return self.llm._generate(cached_messages, **kwargs)\n",
        "+\n",
        "+    async def _agenerate(self, messages: list[BaseMessage], **kwargs) -> Any:\n",
        "+        \"\"\"Async generate response with cache_control on system messages.\"\"\"\n",
        "+        cached_messages = self._add_cache_control(messages)\n",
        "+        return await self.llm._agenerate(cached_messages, **kwargs)\n",
        "+\n",
        "+    def invoke(self, messages: list[BaseMessage], **kwargs) -> Any:\n",
        "+        \"\"\"Invoke LLM with cache_control on system messages.\"\"\"\n",
        "+        cached_messages = self._add_cache_control(messages)\n",
        "+        return self.llm.invoke(cached_messages, **kwargs)\n",
        "+\n",
        "+    async def ainvoke(self, messages: list[BaseMessage], **kwargs) -> Any:\n",
        "+        \"\"\"Async invoke LLM with cache_control on system messages.\"\"\"\n",
        "+        cached_messages = self._add_cache_control(messages)\n",
        "+        return await self.llm.ainvoke(cached_messages, **kwargs)\n",
        "+\n",
        " \n",
        " # Model configuration - using gpt-oss-120b via Cerebras\n",
        " OPENROUTER_MODELS = {\n",
        "@@ -72,3 +182,29 @@ def get_model_name(alias: str) -> str:\n",
        "         Full model identifier for OpenRouter\n",
        "     \"\"\"\n",
        "     return OPENROUTER_MODELS.get(alias, alias)\n",
        "+\n",
        "+\n",
        "+# Anthropic models that support prompt caching\n",
        "+CACHEABLE_MODELS = {\n",
        "+    \"claude-opus-4.5\": \"anthropic/claude-opus-4-5\",\n",
        "+    \"claude-sonnet-4.5\": \"anthropic/claude-sonnet-4-5\",\n",
        "+    \"claude-sonnet-4\": \"anthropic/claude-sonnet-4\",\n",
        "+    \"claude-haiku-4.5\": \"anthropic/claude-haiku-4.5\",\n",
        "+    \"claude-haiku-3.5\": \"anthropic/claude-3.5-haiku\",\n",
        "+}\n",
        "+\n",
        "+\n",
        "+def is_cacheable_model(model: str) -> bool:\n",
        "+    \"\"\"Check if a model supports Anthropic prompt caching.\n",
        "+\n",
        "+    Args:\n",
        "+        model: Model identifier\n",
        "+\n",
        "+    Returns:\n",
        "+        True if the model supports cache_control\n",
        "+    \"\"\"\n",
        "+    # Check exact match in aliases\n",
        "+    if model in CACHEABLE_MODELS:\n",
        "+        return True\n",
        "+    # Check if it's an Anthropic Claude model\n",
        "+    return model.startswith(\"anthropic/claude-\")\n"
      ]
    },
    {
      "path": "src/version.py",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "patch": "@@ -1,6 +1,6 @@\n \"\"\"Version information for HEDit.\"\"\"\n \n-__version__ = \"0.6.4.dev3\"\n+__version__ = \"0.6.4.dev4\"\n __version_info__ = (0, 6, 4, \"dev\")\n \n ",
      "patch_lines": [
        "@@ -1,6 +1,6 @@\n",
        " \"\"\"Version information for HEDit.\"\"\"\n",
        " \n",
        "-__version__ = \"0.6.4.dev3\"\n",
        "+__version__ = \"0.6.4.dev4\"\n",
        " __version_info__ = (0, 6, 4, \"dev\")\n",
        " \n",
        " \n"
      ]
    },
    {
      "path": "tests/test_litellm_llm.py",
      "status": "added",
      "additions": 266,
      "deletions": 0,
      "patch": "@@ -0,0 +1,266 @@\n+\"\"\"Tests for LiteLLM integration with prompt caching.\n+\n+These tests verify that:\n+1. LiteLLM wrapper creates valid LLM instances\n+2. Cache control is properly added to system messages\n+3. Real API calls work with OpenRouter (integration tests)\n+\n+Run unit tests:\n+    uv run pytest tests/test_litellm_llm.py -v -m \"not integration\"\n+\n+Run integration tests (requires OPENROUTER_API_KEY_FOR_TESTING):\n+    uv run pytest tests/test_litellm_llm.py -v -m integration\n+\"\"\"\n+\n+import os\n+\n+import pytest\n+\n+# Skip all tests if litellm is not installed\n+pytest.importorskip(\"litellm\")\n+\n+\n+class TestCreateLiteLLMOpenRouter:\n+    \"\"\"Unit tests for create_litellm_openrouter function.\"\"\"\n+\n+    def test_creates_llm_with_default_params(self):\n+        \"\"\"Test creating LLM with default parameters.\"\"\"\n+        from src.utils.litellm_llm import create_litellm_openrouter\n+\n+        llm = create_litellm_openrouter(api_key=\"test-key\")\n+\n+        assert llm is not None\n+\n+    def test_creates_llm_with_custom_model(self):\n+        \"\"\"Test creating LLM with custom model.\"\"\"\n+        from src.utils.litellm_llm import create_litellm_openrouter\n+\n+        llm = create_litellm_openrouter(\n+            model=\"anthropic/claude-haiku-4.5\",\n+            api_key=\"test-key\",\n+        )\n+\n+        assert llm is not None\n+\n+    def test_creates_llm_with_caching_enabled(self):\n+        \"\"\"Test creating LLM with caching enabled returns wrapper.\"\"\"\n+        from src.utils.litellm_llm import CachingLLMWrapper, create_litellm_openrouter\n+\n+        llm = create_litellm_openrouter(\n+            model=\"anthropic/claude-haiku-4.5\",\n+            api_key=\"test-key\",\n+            enable_caching=True,\n+        )\n+\n+        assert isinstance(llm, CachingLLMWrapper)\n+\n+    def test_creates_llm_without_caching_returns_base(self):\n+        \"\"\"Test creating LLM without caching returns base LLM.\"\"\"\n+        from langchain_litellm import ChatLiteLLM\n+\n+        from src.utils.litellm_llm import create_litellm_openrouter\n+\n+        llm = create_litellm_openrouter(\n+            model=\"anthropic/claude-haiku-4.5\",\n+            api_key=\"test-key\",\n+            enable_caching=False,\n+        )\n+\n+        assert isinstance(llm, ChatLiteLLM)\n+\n+\n+class TestCachingLLMWrapper:\n+    \"\"\"Unit tests for CachingLLMWrapper.\"\"\"\n+\n+    def test_adds_cache_control_to_system_message(self):\n+        \"\"\"Test that cache_control is added to system messages.\"\"\"\n+        from langchain_core.messages import HumanMessage, SystemMessage\n+        from langchain_litellm import ChatLiteLLM\n+\n+        from src.utils.litellm_llm import CachingLLMWrapper\n+\n+        base_llm = ChatLiteLLM(model=\"openrouter/openai/gpt-3.5-turbo\", api_key=\"test\")\n+        wrapper = CachingLLMWrapper(llm=base_llm)\n+\n+        messages = [\n+            SystemMessage(content=\"You are a helpful assistant.\"),\n+            HumanMessage(content=\"Hello!\"),\n+        ]\n+\n+        cached = wrapper._add_cache_control(messages)\n+\n+        # System message should be transformed\n+        assert cached[0][\"role\"] == \"system\"\n+        assert isinstance(cached[0][\"content\"], list)\n+        assert cached[0][\"content\"][0][\"type\"] == \"text\"\n+        assert cached[0][\"content\"][0][\"text\"] == \"You are a helpful assistant.\"\n+        assert cached[0][\"content\"][0][\"cache_control\"] == {\"type\": \"ephemeral\"}\n+\n+        # Human message should be simple\n+        assert cached[1][\"role\"] == \"user\"\n+        assert cached[1][\"content\"] == \"Hello!\"\n+\n+    def test_handles_ai_message(self):\n+        \"\"\"Test that AI messages are handled correctly.\"\"\"\n+        from langchain_core.messages import AIMessage, HumanMessage\n+        from langchain_litellm import ChatLiteLLM\n+\n+        from src.utils.litellm_llm import CachingLLMWrapper\n+\n+        base_llm = ChatLiteLLM(model=\"openrouter/openai/gpt-3.5-turbo\", api_key=\"test\")\n+        wrapper = CachingLLMWrapper(llm=base_llm)\n+\n+        messages = [\n+            HumanMessage(content=\"Hello!\"),\n+            AIMessage(content=\"Hi there!\"),\n+        ]\n+\n+        cached = wrapper._add_cache_control(messages)\n+\n+        assert cached[0][\"role\"] == \"user\"\n+        assert cached[1][\"role\"] == \"assistant\"\n+        assert cached[1][\"content\"] == \"Hi there!\"\n+\n+\n+class TestIsCacheableModel:\n+    \"\"\"Tests for is_cacheable_model function.\"\"\"\n+\n+    def test_anthropic_models_are_cacheable(self):\n+        \"\"\"Test that Anthropic Claude models are cacheable.\"\"\"\n+        from src.utils.litellm_llm import is_cacheable_model\n+\n+        assert is_cacheable_model(\"anthropic/claude-haiku-4.5\") is True\n+        assert is_cacheable_model(\"anthropic/claude-sonnet-4\") is True\n+        assert is_cacheable_model(\"anthropic/claude-opus-4-5\") is True\n+\n+    def test_aliases_are_cacheable(self):\n+        \"\"\"Test that model aliases are recognized as cacheable.\"\"\"\n+        from src.utils.litellm_llm import is_cacheable_model\n+\n+        assert is_cacheable_model(\"claude-haiku-4.5\") is True\n+        assert is_cacheable_model(\"claude-sonnet-4.5\") is True\n+\n+    def test_non_anthropic_models_not_cacheable(self):\n+        \"\"\"Test that non-Anthropic models are not cacheable.\"\"\"\n+        from src.utils.litellm_llm import is_cacheable_model\n+\n+        assert is_cacheable_model(\"openai/gpt-4\") is False\n+        assert is_cacheable_model(\"openai/gpt-oss-120b\") is False\n+\n+\n+@pytest.mark.integration\n+class TestLiteLLMIntegration:\n+    \"\"\"Integration tests that make real API calls.\n+\n+    These tests require OPENROUTER_API_KEY_FOR_TESTING environment variable.\n+    Run with: uv run pytest tests/test_litellm_llm.py -v -m integration\n+    \"\"\"\n+\n+    @pytest.fixture\n+    def api_key(self):\n+        \"\"\"Get API key from environment.\"\"\"\n+        key = os.getenv(\"OPENROUTER_API_KEY_FOR_TESTING\")\n+        if not key:\n+            pytest.skip(\"OPENROUTER_API_KEY_FOR_TESTING not set\")\n+        return key\n+\n+    @pytest.mark.asyncio\n+    async def test_basic_completion_without_caching(self, api_key):\n+        \"\"\"Test basic completion without caching works.\"\"\"\n+        from langchain_core.messages import HumanMessage, SystemMessage\n+\n+        from src.utils.litellm_llm import create_litellm_openrouter\n+\n+        llm = create_litellm_openrouter(\n+            model=\"anthropic/claude-haiku-4.5\",\n+            api_key=api_key,\n+            temperature=0.1,\n+            max_tokens=50,\n+            enable_caching=False,\n+        )\n+\n+        messages = [\n+            SystemMessage(content=\"You are a helpful assistant. Respond briefly.\"),\n+            HumanMessage(content=\"Say 'Hello, World!' and nothing else.\"),\n+        ]\n+\n+        response = await llm.ainvoke(messages)\n+\n+        assert response is not None\n+        assert \"Hello\" in response.content or \"hello\" in response.content.lower()\n+\n+    @pytest.mark.asyncio\n+    async def test_completion_with_caching_enabled(self, api_key):\n+        \"\"\"Test completion with caching enabled works.\"\"\"\n+        from langchain_core.messages import HumanMessage, SystemMessage\n+\n+        from src.utils.litellm_llm import create_litellm_openrouter\n+\n+        llm = create_litellm_openrouter(\n+            model=\"anthropic/claude-haiku-4.5\",\n+            api_key=api_key,\n+            temperature=0.1,\n+            max_tokens=50,\n+            enable_caching=True,\n+        )\n+\n+        # Use a large system prompt that qualifies for caching (>1024 tokens)\n+        large_system_prompt = \"\"\"You are a helpful assistant specialized in HED annotations.\n+\n+        \"\"\" + (\"This is padding text to ensure the prompt is large enough for caching. \" * 100)\n+\n+        messages = [\n+            SystemMessage(content=large_system_prompt),\n+            HumanMessage(content=\"Say 'Cache test passed!' and nothing else.\"),\n+        ]\n+\n+        response = await llm.ainvoke(messages)\n+\n+        assert response is not None\n+        assert len(response.content) > 0\n+\n+    @pytest.mark.asyncio\n+    async def test_cache_reduces_cost_on_second_call(self, api_key):\n+        \"\"\"Test that making the same call twice shows caching works.\n+\n+        Note: OpenRouter doesn't return cache metrics directly, but we can verify\n+        the call succeeds with caching enabled.\n+        \"\"\"\n+        from langchain_core.messages import HumanMessage, SystemMessage\n+\n+        from src.utils.litellm_llm import create_litellm_openrouter\n+\n+        llm = create_litellm_openrouter(\n+            model=\"anthropic/claude-haiku-4.5\",\n+            api_key=api_key,\n+            temperature=0.0,  # Deterministic for caching\n+            max_tokens=50,\n+            enable_caching=True,\n+        )\n+\n+        # Large system prompt that qualifies for caching\n+        large_system_prompt = \"\"\"You are a HED annotation expert.\n+\n+        \"\"\" + (\"Reference material for HED annotations. \" * 150)\n+\n+        messages = [\n+            SystemMessage(content=large_system_prompt),\n+            HumanMessage(content=\"Respond with: 'First call'\"),\n+        ]\n+\n+        # First call - should cache the system prompt\n+        response1 = await llm.ainvoke(messages)\n+        assert response1 is not None\n+\n+        # Second call with same system prompt - should hit cache\n+        messages2 = [\n+            SystemMessage(content=large_system_prompt),\n+            HumanMessage(content=\"Respond with: 'Second call'\"),\n+        ]\n+\n+        response2 = await llm.ainvoke(messages2)\n+        assert response2 is not None\n+\n+        # Both calls should succeed\n+        assert len(response1.content) > 0\n+        assert len(response2.content) > 0",
      "patch_lines": [
        "@@ -0,0 +1,266 @@\n",
        "+\"\"\"Tests for LiteLLM integration with prompt caching.\n",
        "+\n",
        "+These tests verify that:\n",
        "+1. LiteLLM wrapper creates valid LLM instances\n",
        "+2. Cache control is properly added to system messages\n",
        "+3. Real API calls work with OpenRouter (integration tests)\n",
        "+\n",
        "+Run unit tests:\n",
        "+    uv run pytest tests/test_litellm_llm.py -v -m \"not integration\"\n",
        "+\n",
        "+Run integration tests (requires OPENROUTER_API_KEY_FOR_TESTING):\n",
        "+    uv run pytest tests/test_litellm_llm.py -v -m integration\n",
        "+\"\"\"\n",
        "+\n",
        "+import os\n",
        "+\n",
        "+import pytest\n",
        "+\n",
        "+# Skip all tests if litellm is not installed\n",
        "+pytest.importorskip(\"litellm\")\n",
        "+\n",
        "+\n",
        "+class TestCreateLiteLLMOpenRouter:\n",
        "+    \"\"\"Unit tests for create_litellm_openrouter function.\"\"\"\n",
        "+\n",
        "+    def test_creates_llm_with_default_params(self):\n",
        "+        \"\"\"Test creating LLM with default parameters.\"\"\"\n",
        "+        from src.utils.litellm_llm import create_litellm_openrouter\n",
        "+\n",
        "+        llm = create_litellm_openrouter(api_key=\"test-key\")\n",
        "+\n",
        "+        assert llm is not None\n",
        "+\n",
        "+    def test_creates_llm_with_custom_model(self):\n",
        "+        \"\"\"Test creating LLM with custom model.\"\"\"\n",
        "+        from src.utils.litellm_llm import create_litellm_openrouter\n",
        "+\n",
        "+        llm = create_litellm_openrouter(\n",
        "+            model=\"anthropic/claude-haiku-4.5\",\n",
        "+            api_key=\"test-key\",\n",
        "+        )\n",
        "+\n",
        "+        assert llm is not None\n",
        "+\n",
        "+    def test_creates_llm_with_caching_enabled(self):\n",
        "+        \"\"\"Test creating LLM with caching enabled returns wrapper.\"\"\"\n",
        "+        from src.utils.litellm_llm import CachingLLMWrapper, create_litellm_openrouter\n",
        "+\n",
        "+        llm = create_litellm_openrouter(\n",
        "+            model=\"anthropic/claude-haiku-4.5\",\n",
        "+            api_key=\"test-key\",\n",
        "+            enable_caching=True,\n",
        "+        )\n",
        "+\n",
        "+        assert isinstance(llm, CachingLLMWrapper)\n",
        "+\n",
        "+    def test_creates_llm_without_caching_returns_base(self):\n",
        "+        \"\"\"Test creating LLM without caching returns base LLM.\"\"\"\n",
        "+        from langchain_litellm import ChatLiteLLM\n",
        "+\n",
        "+        from src.utils.litellm_llm import create_litellm_openrouter\n",
        "+\n",
        "+        llm = create_litellm_openrouter(\n",
        "+            model=\"anthropic/claude-haiku-4.5\",\n",
        "+            api_key=\"test-key\",\n",
        "+            enable_caching=False,\n",
        "+        )\n",
        "+\n",
        "+        assert isinstance(llm, ChatLiteLLM)\n",
        "+\n",
        "+\n",
        "+class TestCachingLLMWrapper:\n",
        "+    \"\"\"Unit tests for CachingLLMWrapper.\"\"\"\n",
        "+\n",
        "+    def test_adds_cache_control_to_system_message(self):\n",
        "+        \"\"\"Test that cache_control is added to system messages.\"\"\"\n",
        "+        from langchain_core.messages import HumanMessage, SystemMessage\n",
        "+        from langchain_litellm import ChatLiteLLM\n",
        "+\n",
        "+        from src.utils.litellm_llm import CachingLLMWrapper\n",
        "+\n",
        "+        base_llm = ChatLiteLLM(model=\"openrouter/openai/gpt-3.5-turbo\", api_key=\"test\")\n",
        "+        wrapper = CachingLLMWrapper(llm=base_llm)\n",
        "+\n",
        "+        messages = [\n",
        "+            SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "+            HumanMessage(content=\"Hello!\"),\n",
        "+        ]\n",
        "+\n",
        "+        cached = wrapper._add_cache_control(messages)\n",
        "+\n",
        "+        # System message should be transformed\n",
        "+        assert cached[0][\"role\"] == \"system\"\n",
        "+        assert isinstance(cached[0][\"content\"], list)\n",
        "+        assert cached[0][\"content\"][0][\"type\"] == \"text\"\n",
        "+        assert cached[0][\"content\"][0][\"text\"] == \"You are a helpful assistant.\"\n",
        "+        assert cached[0][\"content\"][0][\"cache_control\"] == {\"type\": \"ephemeral\"}\n",
        "+\n",
        "+        # Human message should be simple\n",
        "+        assert cached[1][\"role\"] == \"user\"\n",
        "+        assert cached[1][\"content\"] == \"Hello!\"\n",
        "+\n",
        "+    def test_handles_ai_message(self):\n",
        "+        \"\"\"Test that AI messages are handled correctly.\"\"\"\n",
        "+        from langchain_core.messages import AIMessage, HumanMessage\n",
        "+        from langchain_litellm import ChatLiteLLM\n",
        "+\n",
        "+        from src.utils.litellm_llm import CachingLLMWrapper\n",
        "+\n",
        "+        base_llm = ChatLiteLLM(model=\"openrouter/openai/gpt-3.5-turbo\", api_key=\"test\")\n",
        "+        wrapper = CachingLLMWrapper(llm=base_llm)\n",
        "+\n",
        "+        messages = [\n",
        "+            HumanMessage(content=\"Hello!\"),\n",
        "+            AIMessage(content=\"Hi there!\"),\n",
        "+        ]\n",
        "+\n",
        "+        cached = wrapper._add_cache_control(messages)\n",
        "+\n",
        "+        assert cached[0][\"role\"] == \"user\"\n",
        "+        assert cached[1][\"role\"] == \"assistant\"\n",
        "+        assert cached[1][\"content\"] == \"Hi there!\"\n",
        "+\n",
        "+\n",
        "+class TestIsCacheableModel:\n",
        "+    \"\"\"Tests for is_cacheable_model function.\"\"\"\n",
        "+\n",
        "+    def test_anthropic_models_are_cacheable(self):\n",
        "+        \"\"\"Test that Anthropic Claude models are cacheable.\"\"\"\n",
        "+        from src.utils.litellm_llm import is_cacheable_model\n",
        "+\n",
        "+        assert is_cacheable_model(\"anthropic/claude-haiku-4.5\") is True\n",
        "+        assert is_cacheable_model(\"anthropic/claude-sonnet-4\") is True\n",
        "+        assert is_cacheable_model(\"anthropic/claude-opus-4-5\") is True\n",
        "+\n",
        "+    def test_aliases_are_cacheable(self):\n",
        "+        \"\"\"Test that model aliases are recognized as cacheable.\"\"\"\n",
        "+        from src.utils.litellm_llm import is_cacheable_model\n",
        "+\n",
        "+        assert is_cacheable_model(\"claude-haiku-4.5\") is True\n",
        "+        assert is_cacheable_model(\"claude-sonnet-4.5\") is True\n",
        "+\n",
        "+    def test_non_anthropic_models_not_cacheable(self):\n",
        "+        \"\"\"Test that non-Anthropic models are not cacheable.\"\"\"\n",
        "+        from src.utils.litellm_llm import is_cacheable_model\n",
        "+\n",
        "+        assert is_cacheable_model(\"openai/gpt-4\") is False\n",
        "+        assert is_cacheable_model(\"openai/gpt-oss-120b\") is False\n",
        "+\n",
        "+\n",
        "+@pytest.mark.integration\n",
        "+class TestLiteLLMIntegration:\n",
        "+    \"\"\"Integration tests that make real API calls.\n",
        "+\n",
        "+    These tests require OPENROUTER_API_KEY_FOR_TESTING environment variable.\n",
        "+    Run with: uv run pytest tests/test_litellm_llm.py -v -m integration\n",
        "+    \"\"\"\n",
        "+\n",
        "+    @pytest.fixture\n",
        "+    def api_key(self):\n",
        "+        \"\"\"Get API key from environment.\"\"\"\n",
        "+        key = os.getenv(\"OPENROUTER_API_KEY_FOR_TESTING\")\n",
        "+        if not key:\n",
        "+            pytest.skip(\"OPENROUTER_API_KEY_FOR_TESTING not set\")\n",
        "+        return key\n",
        "+\n",
        "+    @pytest.mark.asyncio\n",
        "+    async def test_basic_completion_without_caching(self, api_key):\n",
        "+        \"\"\"Test basic completion without caching works.\"\"\"\n",
        "+        from langchain_core.messages import HumanMessage, SystemMessage\n",
        "+\n",
        "+        from src.utils.litellm_llm import create_litellm_openrouter\n",
        "+\n",
        "+        llm = create_litellm_openrouter(\n",
        "+            model=\"anthropic/claude-haiku-4.5\",\n",
        "+            api_key=api_key,\n",
        "+            temperature=0.1,\n",
        "+            max_tokens=50,\n",
        "+            enable_caching=False,\n",
        "+        )\n",
        "+\n",
        "+        messages = [\n",
        "+            SystemMessage(content=\"You are a helpful assistant. Respond briefly.\"),\n",
        "+            HumanMessage(content=\"Say 'Hello, World!' and nothing else.\"),\n",
        "+        ]\n",
        "+\n",
        "+        response = await llm.ainvoke(messages)\n",
        "+\n",
        "+        assert response is not None\n",
        "+        assert \"Hello\" in response.content or \"hello\" in response.content.lower()\n",
        "+\n",
        "+    @pytest.mark.asyncio\n",
        "+    async def test_completion_with_caching_enabled(self, api_key):\n",
        "+        \"\"\"Test completion with caching enabled works.\"\"\"\n",
        "+        from langchain_core.messages import HumanMessage, SystemMessage\n",
        "+\n",
        "+        from src.utils.litellm_llm import create_litellm_openrouter\n",
        "+\n",
        "+        llm = create_litellm_openrouter(\n",
        "+            model=\"anthropic/claude-haiku-4.5\",\n",
        "+            api_key=api_key,\n",
        "+            temperature=0.1,\n",
        "+            max_tokens=50,\n",
        "+            enable_caching=True,\n",
        "+        )\n",
        "+\n",
        "+        # Use a large system prompt that qualifies for caching (>1024 tokens)\n",
        "+        large_system_prompt = \"\"\"You are a helpful assistant specialized in HED annotations.\n",
        "+\n",
        "+        \"\"\" + (\"This is padding text to ensure the prompt is large enough for caching. \" * 100)\n",
        "+\n",
        "+        messages = [\n",
        "+            SystemMessage(content=large_system_prompt),\n",
        "+            HumanMessage(content=\"Say 'Cache test passed!' and nothing else.\"),\n",
        "+        ]\n",
        "+\n",
        "+        response = await llm.ainvoke(messages)\n",
        "+\n",
        "+        assert response is not None\n",
        "+        assert len(response.content) > 0\n",
        "+\n",
        "+    @pytest.mark.asyncio\n",
        "+    async def test_cache_reduces_cost_on_second_call(self, api_key):\n",
        "+        \"\"\"Test that making the same call twice shows caching works.\n",
        "+\n",
        "+        Note: OpenRouter doesn't return cache metrics directly, but we can verify\n",
        "+        the call succeeds with caching enabled.\n",
        "+        \"\"\"\n",
        "+        from langchain_core.messages import HumanMessage, SystemMessage\n",
        "+\n",
        "+        from src.utils.litellm_llm import create_litellm_openrouter\n",
        "+\n",
        "+        llm = create_litellm_openrouter(\n",
        "+            model=\"anthropic/claude-haiku-4.5\",\n",
        "+            api_key=api_key,\n",
        "+            temperature=0.0,  # Deterministic for caching\n",
        "+            max_tokens=50,\n",
        "+            enable_caching=True,\n",
        "+        )\n",
        "+\n",
        "+        # Large system prompt that qualifies for caching\n",
        "+        large_system_prompt = \"\"\"You are a HED annotation expert.\n",
        "+\n",
        "+        \"\"\" + (\"Reference material for HED annotations. \" * 150)\n",
        "+\n",
        "+        messages = [\n",
        "+            SystemMessage(content=large_system_prompt),\n",
        "+            HumanMessage(content=\"Respond with: 'First call'\"),\n",
        "+        ]\n",
        "+\n",
        "+        # First call - should cache the system prompt\n",
        "+        response1 = await llm.ainvoke(messages)\n",
        "+        assert response1 is not None\n",
        "+\n",
        "+        # Second call with same system prompt - should hit cache\n",
        "+        messages2 = [\n",
        "+            SystemMessage(content=large_system_prompt),\n",
        "+            HumanMessage(content=\"Respond with: 'Second call'\"),\n",
        "+        ]\n",
        "+\n",
        "+        response2 = await llm.ainvoke(messages2)\n",
        "+        assert response2 is not None\n",
        "+\n",
        "+        # Both calls should succeed\n",
        "+        assert len(response1.content) > 0\n",
        "+        assert len(response2.content) > 0\n"
      ]
    },
    {
      "path": "tests/test_openrouter_llm.py",
      "status": "modified",
      "additions": 143,
      "deletions": 22,
      "patch": "@@ -1,4 +1,11 @@\n-\"\"\"Unit tests for OpenRouter LLM utility.\"\"\"\n+\"\"\"Unit tests for OpenRouter LLM utility.\n+\n+Tests the LiteLLM-based implementation with optional prompt caching support.\n+\"\"\"\n+\n+import pytest\n+\n+pytest.importorskip(\"litellm\")\n \n \n class TestCreateOpenRouterLLM:\n@@ -11,68 +18,73 @@ def test_creates_llm_with_default_params(self):\n         llm = create_openrouter_llm(api_key=\"test-key\")\n \n         assert llm is not None\n-        assert llm.model_name == \"openai/gpt-oss-120b\"\n-        assert llm.temperature == 0.1\n \n     def test_creates_llm_with_custom_model(self):\n         \"\"\"Test creating LLM with custom model.\"\"\"\n         from src.utils.openrouter_llm import create_openrouter_llm\n \n         llm = create_openrouter_llm(model=\"anthropic/claude-3-haiku\", api_key=\"test-key\")\n \n-        assert llm.model_name == \"anthropic/claude-3-haiku\"\n+        assert llm is not None\n \n     def test_creates_llm_with_provider(self):\n         \"\"\"Test creating LLM with provider preference.\"\"\"\n+        from langchain_litellm import ChatLiteLLM\n+\n         from src.utils.openrouter_llm import create_openrouter_llm\n \n+        # Use non-Anthropic model to avoid caching wrapper\n         llm = create_openrouter_llm(\n+            model=\"openai/gpt-3.5-turbo\",\n             api_key=\"test-key\",\n             provider=\"Cerebras\",\n+            enable_caching=False,\n         )\n \n         assert llm is not None\n-        # Provider is passed in extra_body\n-        assert llm.extra_body is not None\n-        assert llm.extra_body.get(\"provider\") == {\"only\": [\"Cerebras\"]}\n+        assert isinstance(llm, ChatLiteLLM)\n+        # Provider is passed in model_kwargs\n+        assert llm.model_kwargs is not None\n+        assert llm.model_kwargs.get(\"provider\") == {\"only\": [\"Cerebras\"]}\n \n     def test_creates_llm_with_user_id(self):\n         \"\"\"Test creating LLM with user_id for cache optimization.\"\"\"\n+        from langchain_litellm import ChatLiteLLM\n+\n         from src.utils.openrouter_llm import create_openrouter_llm\n \n         llm = create_openrouter_llm(\n+            model=\"openai/gpt-3.5-turbo\",\n             api_key=\"test-key\",\n             user_id=\"test-user-123\",\n+            enable_caching=False,\n         )\n \n         assert llm is not None\n-        # User ID is passed in extra_body for sticky cache routing\n-        assert llm.extra_body is not None\n-        assert llm.extra_body.get(\"user\") == \"test-user-123\"\n+        assert isinstance(llm, ChatLiteLLM)\n+        # User ID is passed in model_kwargs for sticky cache routing\n+        assert llm.model_kwargs is not None\n+        assert llm.model_kwargs.get(\"user\") == \"test-user-123\"\n \n     def test_creates_llm_with_provider_and_user_id(self):\n         \"\"\"Test creating LLM with both provider and user_id.\"\"\"\n+        from langchain_litellm import ChatLiteLLM\n+\n         from src.utils.openrouter_llm import create_openrouter_llm\n \n         llm = create_openrouter_llm(\n+            model=\"openai/gpt-3.5-turbo\",\n             api_key=\"test-key\",\n             provider=\"Cerebras\",\n             user_id=\"test-user-456\",\n+            enable_caching=False,\n         )\n \n         assert llm is not None\n-        assert llm.extra_body is not None\n-        assert llm.extra_body.get(\"provider\") == {\"only\": [\"Cerebras\"]}\n-        assert llm.extra_body.get(\"user\") == \"test-user-456\"\n-\n-    def test_creates_llm_without_extra_body(self):\n-        \"\"\"Test creating LLM without provider or user_id.\"\"\"\n-        from src.utils.openrouter_llm import create_openrouter_llm\n-\n-        llm = create_openrouter_llm(api_key=\"test-key\")\n-\n-        # extra_body should be None when no provider or user_id\n-        assert llm.extra_body is None\n+        assert isinstance(llm, ChatLiteLLM)\n+        assert llm.model_kwargs is not None\n+        assert llm.model_kwargs.get(\"provider\") == {\"only\": [\"Cerebras\"]}\n+        assert llm.model_kwargs.get(\"user\") == \"test-user-456\"\n \n     def test_creates_llm_with_max_tokens(self):\n         \"\"\"Test creating LLM with max_tokens.\"\"\"\n@@ -81,10 +93,93 @@ def test_creates_llm_with_max_tokens(self):\n         llm = create_openrouter_llm(\n             api_key=\"test-key\",\n             max_tokens=1000,\n+            enable_caching=False,\n         )\n \n         assert llm.max_tokens == 1000\n \n+    def test_auto_enables_caching_for_anthropic_models(self):\n+        \"\"\"Test that caching is auto-enabled for Anthropic models.\"\"\"\n+        from src.utils.openrouter_llm import CachingLLMWrapper, create_openrouter_llm\n+\n+        llm = create_openrouter_llm(\n+            model=\"anthropic/claude-haiku-4.5\",\n+            api_key=\"test-key\",\n+        )\n+\n+        assert isinstance(llm, CachingLLMWrapper)\n+\n+    def test_no_caching_for_non_anthropic_models(self):\n+        \"\"\"Test that caching is not enabled for non-Anthropic models.\"\"\"\n+        from langchain_litellm import ChatLiteLLM\n+\n+        from src.utils.openrouter_llm import create_openrouter_llm\n+\n+        llm = create_openrouter_llm(\n+            model=\"openai/gpt-oss-120b\",\n+            api_key=\"test-key\",\n+        )\n+\n+        assert isinstance(llm, ChatLiteLLM)\n+\n+    def test_can_force_caching_off(self):\n+        \"\"\"Test that caching can be explicitly disabled.\"\"\"\n+        from langchain_litellm import ChatLiteLLM\n+\n+        from src.utils.openrouter_llm import create_openrouter_llm\n+\n+        llm = create_openrouter_llm(\n+            model=\"anthropic/claude-haiku-4.5\",\n+            api_key=\"test-key\",\n+            enable_caching=False,\n+        )\n+\n+        assert isinstance(llm, ChatLiteLLM)\n+\n+    def test_can_force_caching_on(self):\n+        \"\"\"Test that caching can be explicitly enabled.\"\"\"\n+        from src.utils.openrouter_llm import CachingLLMWrapper, create_openrouter_llm\n+\n+        llm = create_openrouter_llm(\n+            model=\"openai/gpt-oss-120b\",\n+            api_key=\"test-key\",\n+            enable_caching=True,\n+        )\n+\n+        assert isinstance(llm, CachingLLMWrapper)\n+\n+\n+class TestCachingLLMWrapper:\n+    \"\"\"Tests for CachingLLMWrapper.\"\"\"\n+\n+    def test_adds_cache_control_to_system_message(self):\n+        \"\"\"Test that cache_control is added to system messages.\"\"\"\n+        from langchain_core.messages import HumanMessage, SystemMessage\n+        from langchain_litellm import ChatLiteLLM\n+\n+        from src.utils.openrouter_llm import CachingLLMWrapper\n+\n+        base_llm = ChatLiteLLM(model=\"openrouter/openai/gpt-3.5-turbo\", api_key=\"test\")\n+        wrapper = CachingLLMWrapper(llm=base_llm)\n+\n+        messages = [\n+            SystemMessage(content=\"You are a helpful assistant.\"),\n+            HumanMessage(content=\"Hello!\"),\n+        ]\n+\n+        cached = wrapper._add_cache_control(messages)\n+\n+        # System message should be transformed\n+        assert cached[0][\"role\"] == \"system\"\n+        assert isinstance(cached[0][\"content\"], list)\n+        assert cached[0][\"content\"][0][\"type\"] == \"text\"\n+        assert cached[0][\"content\"][0][\"text\"] == \"You are a helpful assistant.\"\n+        assert cached[0][\"content\"][0][\"cache_control\"] == {\"type\": \"ephemeral\"}\n+\n+        # Human message should be simple\n+        assert cached[1][\"role\"] == \"user\"\n+        assert cached[1][\"content\"] == \"Hello!\"\n+\n \n class TestGetModelName:\n     \"\"\"Tests for get_model_name function.\"\"\"\n@@ -104,3 +199,29 @@ def test_get_unknown_model_returns_input(self):\n         result = get_model_name(\"some-unknown-model\")\n \n         assert result == \"some-unknown-model\"\n+\n+\n+class TestIsCacheableModel:\n+    \"\"\"Tests for is_cacheable_model function.\"\"\"\n+\n+    def test_anthropic_models_are_cacheable(self):\n+        \"\"\"Test that Anthropic Claude models are cacheable.\"\"\"\n+        from src.utils.openrouter_llm import is_cacheable_model\n+\n+        assert is_cacheable_model(\"anthropic/claude-haiku-4.5\") is True\n+        assert is_cacheable_model(\"anthropic/claude-sonnet-4\") is True\n+        assert is_cacheable_model(\"anthropic/claude-opus-4-5\") is True\n+\n+    def test_aliases_are_cacheable(self):\n+        \"\"\"Test that model aliases are recognized as cacheable.\"\"\"\n+        from src.utils.openrouter_llm import is_cacheable_model\n+\n+        assert is_cacheable_model(\"claude-haiku-4.5\") is True\n+        assert is_cacheable_model(\"claude-sonnet-4.5\") is True\n+\n+    def test_non_anthropic_models_not_cacheable(self):\n+        \"\"\"Test that non-Anthropic models are not cacheable.\"\"\"\n+        from src.utils.openrouter_llm import is_cacheable_model\n+\n+        assert is_cacheable_model(\"openai/gpt-4\") is False\n+        assert is_cacheable_model(\"openai/gpt-oss-120b\") is False",
      "patch_lines": [
        "@@ -1,4 +1,11 @@\n",
        "-\"\"\"Unit tests for OpenRouter LLM utility.\"\"\"\n",
        "+\"\"\"Unit tests for OpenRouter LLM utility.\n",
        "+\n",
        "+Tests the LiteLLM-based implementation with optional prompt caching support.\n",
        "+\"\"\"\n",
        "+\n",
        "+import pytest\n",
        "+\n",
        "+pytest.importorskip(\"litellm\")\n",
        " \n",
        " \n",
        " class TestCreateOpenRouterLLM:\n",
        "@@ -11,68 +18,73 @@ def test_creates_llm_with_default_params(self):\n",
        "         llm = create_openrouter_llm(api_key=\"test-key\")\n",
        " \n",
        "         assert llm is not None\n",
        "-        assert llm.model_name == \"openai/gpt-oss-120b\"\n",
        "-        assert llm.temperature == 0.1\n",
        " \n",
        "     def test_creates_llm_with_custom_model(self):\n",
        "         \"\"\"Test creating LLM with custom model.\"\"\"\n",
        "         from src.utils.openrouter_llm import create_openrouter_llm\n",
        " \n",
        "         llm = create_openrouter_llm(model=\"anthropic/claude-3-haiku\", api_key=\"test-key\")\n",
        " \n",
        "-        assert llm.model_name == \"anthropic/claude-3-haiku\"\n",
        "+        assert llm is not None\n",
        " \n",
        "     def test_creates_llm_with_provider(self):\n",
        "         \"\"\"Test creating LLM with provider preference.\"\"\"\n",
        "+        from langchain_litellm import ChatLiteLLM\n",
        "+\n",
        "         from src.utils.openrouter_llm import create_openrouter_llm\n",
        " \n",
        "+        # Use non-Anthropic model to avoid caching wrapper\n",
        "         llm = create_openrouter_llm(\n",
        "+            model=\"openai/gpt-3.5-turbo\",\n",
        "             api_key=\"test-key\",\n",
        "             provider=\"Cerebras\",\n",
        "+            enable_caching=False,\n",
        "         )\n",
        " \n",
        "         assert llm is not None\n",
        "-        # Provider is passed in extra_body\n",
        "-        assert llm.extra_body is not None\n",
        "-        assert llm.extra_body.get(\"provider\") == {\"only\": [\"Cerebras\"]}\n",
        "+        assert isinstance(llm, ChatLiteLLM)\n",
        "+        # Provider is passed in model_kwargs\n",
        "+        assert llm.model_kwargs is not None\n",
        "+        assert llm.model_kwargs.get(\"provider\") == {\"only\": [\"Cerebras\"]}\n",
        " \n",
        "     def test_creates_llm_with_user_id(self):\n",
        "         \"\"\"Test creating LLM with user_id for cache optimization.\"\"\"\n",
        "+        from langchain_litellm import ChatLiteLLM\n",
        "+\n",
        "         from src.utils.openrouter_llm import create_openrouter_llm\n",
        " \n",
        "         llm = create_openrouter_llm(\n",
        "+            model=\"openai/gpt-3.5-turbo\",\n",
        "             api_key=\"test-key\",\n",
        "             user_id=\"test-user-123\",\n",
        "+            enable_caching=False,\n",
        "         )\n",
        " \n",
        "         assert llm is not None\n",
        "-        # User ID is passed in extra_body for sticky cache routing\n",
        "-        assert llm.extra_body is not None\n",
        "-        assert llm.extra_body.get(\"user\") == \"test-user-123\"\n",
        "+        assert isinstance(llm, ChatLiteLLM)\n",
        "+        # User ID is passed in model_kwargs for sticky cache routing\n",
        "+        assert llm.model_kwargs is not None\n",
        "+        assert llm.model_kwargs.get(\"user\") == \"test-user-123\"\n",
        " \n",
        "     def test_creates_llm_with_provider_and_user_id(self):\n",
        "         \"\"\"Test creating LLM with both provider and user_id.\"\"\"\n",
        "+        from langchain_litellm import ChatLiteLLM\n",
        "+\n",
        "         from src.utils.openrouter_llm import create_openrouter_llm\n",
        " \n",
        "         llm = create_openrouter_llm(\n",
        "+            model=\"openai/gpt-3.5-turbo\",\n",
        "             api_key=\"test-key\",\n",
        "             provider=\"Cerebras\",\n",
        "             user_id=\"test-user-456\",\n",
        "+            enable_caching=False,\n",
        "         )\n",
        " \n",
        "         assert llm is not None\n",
        "-        assert llm.extra_body is not None\n",
        "-        assert llm.extra_body.get(\"provider\") == {\"only\": [\"Cerebras\"]}\n",
        "-        assert llm.extra_body.get(\"user\") == \"test-user-456\"\n",
        "-\n",
        "-    def test_creates_llm_without_extra_body(self):\n",
        "-        \"\"\"Test creating LLM without provider or user_id.\"\"\"\n",
        "-        from src.utils.openrouter_llm import create_openrouter_llm\n",
        "-\n",
        "-        llm = create_openrouter_llm(api_key=\"test-key\")\n",
        "-\n",
        "-        # extra_body should be None when no provider or user_id\n",
        "-        assert llm.extra_body is None\n",
        "+        assert isinstance(llm, ChatLiteLLM)\n",
        "+        assert llm.model_kwargs is not None\n",
        "+        assert llm.model_kwargs.get(\"provider\") == {\"only\": [\"Cerebras\"]}\n",
        "+        assert llm.model_kwargs.get(\"user\") == \"test-user-456\"\n",
        " \n",
        "     def test_creates_llm_with_max_tokens(self):\n",
        "         \"\"\"Test creating LLM with max_tokens.\"\"\"\n",
        "@@ -81,10 +93,93 @@ def test_creates_llm_with_max_tokens(self):\n",
        "         llm = create_openrouter_llm(\n",
        "             api_key=\"test-key\",\n",
        "             max_tokens=1000,\n",
        "+            enable_caching=False,\n",
        "         )\n",
        " \n",
        "         assert llm.max_tokens == 1000\n",
        " \n",
        "+    def test_auto_enables_caching_for_anthropic_models(self):\n",
        "+        \"\"\"Test that caching is auto-enabled for Anthropic models.\"\"\"\n",
        "+        from src.utils.openrouter_llm import CachingLLMWrapper, create_openrouter_llm\n",
        "+\n",
        "+        llm = create_openrouter_llm(\n",
        "+            model=\"anthropic/claude-haiku-4.5\",\n",
        "+            api_key=\"test-key\",\n",
        "+        )\n",
        "+\n",
        "+        assert isinstance(llm, CachingLLMWrapper)\n",
        "+\n",
        "+    def test_no_caching_for_non_anthropic_models(self):\n",
        "+        \"\"\"Test that caching is not enabled for non-Anthropic models.\"\"\"\n",
        "+        from langchain_litellm import ChatLiteLLM\n",
        "+\n",
        "+        from src.utils.openrouter_llm import create_openrouter_llm\n",
        "+\n",
        "+        llm = create_openrouter_llm(\n",
        "+            model=\"openai/gpt-oss-120b\",\n",
        "+            api_key=\"test-key\",\n",
        "+        )\n",
        "+\n",
        "+        assert isinstance(llm, ChatLiteLLM)\n",
        "+\n",
        "+    def test_can_force_caching_off(self):\n",
        "+        \"\"\"Test that caching can be explicitly disabled.\"\"\"\n",
        "+        from langchain_litellm import ChatLiteLLM\n",
        "+\n",
        "+        from src.utils.openrouter_llm import create_openrouter_llm\n",
        "+\n",
        "+        llm = create_openrouter_llm(\n",
        "+            model=\"anthropic/claude-haiku-4.5\",\n",
        "+            api_key=\"test-key\",\n",
        "+            enable_caching=False,\n",
        "+        )\n",
        "+\n",
        "+        assert isinstance(llm, ChatLiteLLM)\n",
        "+\n",
        "+    def test_can_force_caching_on(self):\n",
        "+        \"\"\"Test that caching can be explicitly enabled.\"\"\"\n",
        "+        from src.utils.openrouter_llm import CachingLLMWrapper, create_openrouter_llm\n",
        "+\n",
        "+        llm = create_openrouter_llm(\n",
        "+            model=\"openai/gpt-oss-120b\",\n",
        "+            api_key=\"test-key\",\n",
        "+            enable_caching=True,\n",
        "+        )\n",
        "+\n",
        "+        assert isinstance(llm, CachingLLMWrapper)\n",
        "+\n",
        "+\n",
        "+class TestCachingLLMWrapper:\n",
        "+    \"\"\"Tests for CachingLLMWrapper.\"\"\"\n",
        "+\n",
        "+    def test_adds_cache_control_to_system_message(self):\n",
        "+        \"\"\"Test that cache_control is added to system messages.\"\"\"\n",
        "+        from langchain_core.messages import HumanMessage, SystemMessage\n",
        "+        from langchain_litellm import ChatLiteLLM\n",
        "+\n",
        "+        from src.utils.openrouter_llm import CachingLLMWrapper\n",
        "+\n",
        "+        base_llm = ChatLiteLLM(model=\"openrouter/openai/gpt-3.5-turbo\", api_key=\"test\")\n",
        "+        wrapper = CachingLLMWrapper(llm=base_llm)\n",
        "+\n",
        "+        messages = [\n",
        "+            SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "+            HumanMessage(content=\"Hello!\"),\n",
        "+        ]\n",
        "+\n",
        "+        cached = wrapper._add_cache_control(messages)\n",
        "+\n",
        "+        # System message should be transformed\n",
        "+        assert cached[0][\"role\"] == \"system\"\n",
        "+        assert isinstance(cached[0][\"content\"], list)\n",
        "+        assert cached[0][\"content\"][0][\"type\"] == \"text\"\n",
        "+        assert cached[0][\"content\"][0][\"text\"] == \"You are a helpful assistant.\"\n",
        "+        assert cached[0][\"content\"][0][\"cache_control\"] == {\"type\": \"ephemeral\"}\n",
        "+\n",
        "+        # Human message should be simple\n",
        "+        assert cached[1][\"role\"] == \"user\"\n",
        "+        assert cached[1][\"content\"] == \"Hello!\"\n",
        "+\n",
        " \n",
        " class TestGetModelName:\n",
        "     \"\"\"Tests for get_model_name function.\"\"\"\n",
        "@@ -104,3 +199,29 @@ def test_get_unknown_model_returns_input(self):\n",
        "         result = get_model_name(\"some-unknown-model\")\n",
        " \n",
        "         assert result == \"some-unknown-model\"\n",
        "+\n",
        "+\n",
        "+class TestIsCacheableModel:\n",
        "+    \"\"\"Tests for is_cacheable_model function.\"\"\"\n",
        "+\n",
        "+    def test_anthropic_models_are_cacheable(self):\n",
        "+        \"\"\"Test that Anthropic Claude models are cacheable.\"\"\"\n",
        "+        from src.utils.openrouter_llm import is_cacheable_model\n",
        "+\n",
        "+        assert is_cacheable_model(\"anthropic/claude-haiku-4.5\") is True\n",
        "+        assert is_cacheable_model(\"anthropic/claude-sonnet-4\") is True\n",
        "+        assert is_cacheable_model(\"anthropic/claude-opus-4-5\") is True\n",
        "+\n",
        "+    def test_aliases_are_cacheable(self):\n",
        "+        \"\"\"Test that model aliases are recognized as cacheable.\"\"\"\n",
        "+        from src.utils.openrouter_llm import is_cacheable_model\n",
        "+\n",
        "+        assert is_cacheable_model(\"claude-haiku-4.5\") is True\n",
        "+        assert is_cacheable_model(\"claude-sonnet-4.5\") is True\n",
        "+\n",
        "+    def test_non_anthropic_models_not_cacheable(self):\n",
        "+        \"\"\"Test that non-Anthropic models are not cacheable.\"\"\"\n",
        "+        from src.utils.openrouter_llm import is_cacheable_model\n",
        "+\n",
        "+        assert is_cacheable_model(\"openai/gpt-4\") is False\n",
        "+        assert is_cacheable_model(\"openai/gpt-oss-120b\") is False\n"
      ]
    }
  ]
}
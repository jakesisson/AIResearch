{
  "project": "Research Data/ecommerce-chat2",
  "repo": "arthurmorais12/ecommerce-chat",
  "prior_commit": "467f81b0e3fc51a46cb03d45a3d61a03dec1273a",
  "researched_commit": "8e3edabb925f892107438516701dd6f0ac1b3def",
  "compare_url": "https://github.com/arthurmorais12/ecommerce-chat/compare/467f81b0e3fc51a46cb03d45a3d61a03dec1273a...8e3edabb925f892107438516701dd6f0ac1b3def",
  "ahead_by": 1,
  "behind_by": 0,
  "changed_files": [
    {
      "path": "backend/pyproject.toml",
      "status": "modified",
      "additions": 1,
      "deletions": 0,
      "patch": "@@ -19,4 +19,5 @@ dependencies = [\n     \"uvicorn>=0.35.0\",\n     \"langgraph>=0.5.2\",\n     \"langgraph-checkpoint-sqlite>=2.0.10\",\n+    \"aiosqlite>=0.21.0\",\n ]",
      "patch_lines": [
        "@@ -19,4 +19,5 @@ dependencies = [\n",
        "     \"uvicorn>=0.35.0\",\n",
        "     \"langgraph>=0.5.2\",\n",
        "     \"langgraph-checkpoint-sqlite>=2.0.10\",\n",
        "+    \"aiosqlite>=0.21.0\",\n",
        " ]\n"
      ]
    },
    {
      "path": "backend/services/agent/agent.py",
      "status": "modified",
      "additions": 60,
      "deletions": 23,
      "patch": "@@ -1,8 +1,8 @@\n-import sqlite3\n+import uuid\n \n from dotenv import load_dotenv\n from langchain.chat_models import init_chat_model\n-from langgraph.checkpoint.sqlite import SqliteSaver\n+from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n from langgraph.prebuilt import create_react_agent\n \n from .prompt import VENDEDOR_PROMPT\n@@ -11,31 +11,68 @@\n load_dotenv()\n \n \n-def create_agent():\n-    # Modelo\n-    model = init_chat_model(\n-        model=\"llama-3.1-8b-instant\",\n-        model_provider=\"groq\",\n-    )\n+class Agent:\n+    def __init__(self, thread_id: str = None):\n+        self.model = init_chat_model(\n+            model=\"llama-3.1-8b-instant\", model_provider=\"groq\"\n+        )\n+        self.db_path = \"chat_memory.db\"\n+        self.tools = tools\n+        self.prompt = VENDEDOR_PROMPT\n+        self.thread_id = thread_id or str(uuid.uuid4())\n \n-    # Mem\u00f3ria SQLite (maneira correta)\n-    conn = sqlite3.connect(\"chat_memory.db\", check_same_thread=False)\n-    memory = SqliteSaver(conn)\n+    async def send_message(self, user_message: str) -> str:\n+        \"\"\"Envia mensagem e retorna resposta completa\"\"\"\n+        async with AsyncSqliteSaver.from_conn_string(self.db_path) as checkpointer:\n+            agent = create_react_agent(\n+                model=self.model,\n+                tools=self.tools,\n+                checkpointer=checkpointer,\n+                prompt=self.prompt,\n+            )\n \n-    # Agente React com mem\u00f3ria e prompt detalhado\n-    agent = create_react_agent(\n-        model=model, tools=tools, checkpointer=memory, prompt=VENDEDOR_PROMPT\n-    )\n+            config = {\"configurable\": {\"thread_id\": self.thread_id}}\n \n-    return agent\n+            result = await agent.ainvoke(\n+                {\"messages\": [(\"human\", user_message)]}, config\n+            )\n \n+            return result[\"messages\"][-1].content\n \n-def send_message(message: str, user_id: str = \"default\") -> str:\n-    agent = create_agent()\n+    async def stream_message(self, user_message: str):\n+        \"\"\"Envia mensagem e faz streaming da resposta\"\"\"\n+        async with AsyncSqliteSaver.from_conn_string(self.db_path) as checkpointer:\n+            agent = create_react_agent(\n+                model=self.model,\n+                tools=self.tools,\n+                checkpointer=checkpointer,\n+                prompt=self.prompt,\n+            )\n \n-    result = agent.invoke(\n-        {\"messages\": [{\"role\": \"user\", \"content\": message}]},\n-        config={\"configurable\": {\"thread_id\": user_id}},\n-    )\n+            config = {\"configurable\": {\"thread_id\": self.thread_id}}\n \n-    return result[\"messages\"][-1].content\n+            try:\n+                full_response = \"\"\n+\n+                async for chunk, metadata in agent.astream(\n+                    {\"messages\": [(\"human\", user_message)]},\n+                    config,\n+                    stream_mode=\"messages\",\n+                ):\n+                    chunk_type = chunk.__class__.__name__\n+                    if chunk_type in [\"AIMessageChunk\", \"AIMessage\"]:\n+                        token_content = chunk.content\n+                        full_response += token_content\n+\n+                        yield {\"type\": \"content\", \"data\": token_content}\n+\n+                # completion message\n+                yield {\"type\": \"complete\", \"data\": full_response}\n+            except Exception as e:\n+                print(f\"Error in streaming: {e}\")\n+                yield {\"type\": \"error\", \"data\": f\"Error processing message: {str(e)}\"}\n+\n+    @classmethod\n+    def build(cls, thread_id: str = None):\n+        \"\"\"Factory method para criar inst\u00e2ncia (n\u00e3o precisa mais ser async!)\"\"\"\n+        return cls(thread_id=thread_id)",
      "patch_lines": [
        "@@ -1,8 +1,8 @@\n",
        "-import sqlite3\n",
        "+import uuid\n",
        " \n",
        " from dotenv import load_dotenv\n",
        " from langchain.chat_models import init_chat_model\n",
        "-from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "+from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
        " from langgraph.prebuilt import create_react_agent\n",
        " \n",
        " from .prompt import VENDEDOR_PROMPT\n",
        "@@ -11,31 +11,68 @@\n",
        " load_dotenv()\n",
        " \n",
        " \n",
        "-def create_agent():\n",
        "-    # Modelo\n",
        "-    model = init_chat_model(\n",
        "-        model=\"llama-3.1-8b-instant\",\n",
        "-        model_provider=\"groq\",\n",
        "-    )\n",
        "+class Agent:\n",
        "+    def __init__(self, thread_id: str = None):\n",
        "+        self.model = init_chat_model(\n",
        "+            model=\"llama-3.1-8b-instant\", model_provider=\"groq\"\n",
        "+        )\n",
        "+        self.db_path = \"chat_memory.db\"\n",
        "+        self.tools = tools\n",
        "+        self.prompt = VENDEDOR_PROMPT\n",
        "+        self.thread_id = thread_id or str(uuid.uuid4())\n",
        " \n",
        "-    # Mem\u00f3ria SQLite (maneira correta)\n",
        "-    conn = sqlite3.connect(\"chat_memory.db\", check_same_thread=False)\n",
        "-    memory = SqliteSaver(conn)\n",
        "+    async def send_message(self, user_message: str) -> str:\n",
        "+        \"\"\"Envia mensagem e retorna resposta completa\"\"\"\n",
        "+        async with AsyncSqliteSaver.from_conn_string(self.db_path) as checkpointer:\n",
        "+            agent = create_react_agent(\n",
        "+                model=self.model,\n",
        "+                tools=self.tools,\n",
        "+                checkpointer=checkpointer,\n",
        "+                prompt=self.prompt,\n",
        "+            )\n",
        " \n",
        "-    # Agente React com mem\u00f3ria e prompt detalhado\n",
        "-    agent = create_react_agent(\n",
        "-        model=model, tools=tools, checkpointer=memory, prompt=VENDEDOR_PROMPT\n",
        "-    )\n",
        "+            config = {\"configurable\": {\"thread_id\": self.thread_id}}\n",
        " \n",
        "-    return agent\n",
        "+            result = await agent.ainvoke(\n",
        "+                {\"messages\": [(\"human\", user_message)]}, config\n",
        "+            )\n",
        " \n",
        "+            return result[\"messages\"][-1].content\n",
        " \n",
        "-def send_message(message: str, user_id: str = \"default\") -> str:\n",
        "-    agent = create_agent()\n",
        "+    async def stream_message(self, user_message: str):\n",
        "+        \"\"\"Envia mensagem e faz streaming da resposta\"\"\"\n",
        "+        async with AsyncSqliteSaver.from_conn_string(self.db_path) as checkpointer:\n",
        "+            agent = create_react_agent(\n",
        "+                model=self.model,\n",
        "+                tools=self.tools,\n",
        "+                checkpointer=checkpointer,\n",
        "+                prompt=self.prompt,\n",
        "+            )\n",
        " \n",
        "-    result = agent.invoke(\n",
        "-        {\"messages\": [{\"role\": \"user\", \"content\": message}]},\n",
        "-        config={\"configurable\": {\"thread_id\": user_id}},\n",
        "-    )\n",
        "+            config = {\"configurable\": {\"thread_id\": self.thread_id}}\n",
        " \n",
        "-    return result[\"messages\"][-1].content\n",
        "+            try:\n",
        "+                full_response = \"\"\n",
        "+\n",
        "+                async for chunk, metadata in agent.astream(\n",
        "+                    {\"messages\": [(\"human\", user_message)]},\n",
        "+                    config,\n",
        "+                    stream_mode=\"messages\",\n",
        "+                ):\n",
        "+                    chunk_type = chunk.__class__.__name__\n",
        "+                    if chunk_type in [\"AIMessageChunk\", \"AIMessage\"]:\n",
        "+                        token_content = chunk.content\n",
        "+                        full_response += token_content\n",
        "+\n",
        "+                        yield {\"type\": \"content\", \"data\": token_content}\n",
        "+\n",
        "+                # completion message\n",
        "+                yield {\"type\": \"complete\", \"data\": full_response}\n",
        "+            except Exception as e:\n",
        "+                print(f\"Error in streaming: {e}\")\n",
        "+                yield {\"type\": \"error\", \"data\": f\"Error processing message: {str(e)}\"}\n",
        "+\n",
        "+    @classmethod\n",
        "+    def build(cls, thread_id: str = None):\n",
        "+        \"\"\"Factory method para criar inst\u00e2ncia (n\u00e3o precisa mais ser async!)\"\"\"\n",
        "+        return cls(thread_id=thread_id)\n"
      ]
    },
    {
      "path": "backend/uv.lock",
      "status": "modified",
      "additions": 2,
      "deletions": 0,
      "patch": "@@ -104,6 +104,7 @@ name = \"backend\"\n version = \"0.1.0\"\n source = { virtual = \".\" }\n dependencies = [\n+    { name = \"aiosqlite\" },\n     { name = \"chromadb\" },\n     { name = \"fastapi\" },\n     { name = \"langchain\", extra = [\"google-genai\", \"groq\"] },\n@@ -122,6 +123,7 @@ dependencies = [\n \n [package.metadata]\n requires-dist = [\n+    { name = \"aiosqlite\", specifier = \">=0.21.0\" },\n     { name = \"chromadb\", specifier = \">=1.0.15\" },\n     { name = \"fastapi\", specifier = \">=0.116.1\" },\n     { name = \"langchain\", extras = [\"google-genai\", \"groq\"], specifier = \">=0.3.26\" },",
      "patch_lines": [
        "@@ -104,6 +104,7 @@ name = \"backend\"\n",
        " version = \"0.1.0\"\n",
        " source = { virtual = \".\" }\n",
        " dependencies = [\n",
        "+    { name = \"aiosqlite\" },\n",
        "     { name = \"chromadb\" },\n",
        "     { name = \"fastapi\" },\n",
        "     { name = \"langchain\", extra = [\"google-genai\", \"groq\"] },\n",
        "@@ -122,6 +123,7 @@ dependencies = [\n",
        " \n",
        " [package.metadata]\n",
        " requires-dist = [\n",
        "+    { name = \"aiosqlite\", specifier = \">=0.21.0\" },\n",
        "     { name = \"chromadb\", specifier = \">=1.0.15\" },\n",
        "     { name = \"fastapi\", specifier = \">=0.116.1\" },\n",
        "     { name = \"langchain\", extras = [\"google-genai\", \"groq\"], specifier = \">=0.3.26\" },\n"
      ]
    }
  ]
}
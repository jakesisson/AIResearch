{
  "project": "Research Data/Experimental",
  "repo": "nguyenhonglich111-1001/Experimental",
  "prior_commit": "2954648d4d35e4014a65fee1737259e7e5ef525c",
  "researched_commit": "3e970bfea0fb85405b07cd96dfc6e1d1ca862ed2",
  "compare_url": "https://github.com/nguyenhonglich111-1001/Experimental/compare/2954648d4d35e4014a65fee1737259e7e5ef525c...3e970bfea0fb85405b07cd96dfc6e1d1ca862ed2",
  "ahead_by": 1,
  "behind_by": 0,
  "changed_files": [
    {
      "path": "nlp/tools/langchain-file-processor/server.py",
      "status": "modified",
      "additions": 120,
      "deletions": 98,
      "patch": "@@ -1,139 +1,143 @@\n \"\"\"\n A Streamlit application for processing and chatting with uploaded files using LangChain.\n+This version incorporates best practices for Streamlit apps, including:\n+- Caching for expensive resources (@st.cache_resource) and data processing (@st.cache_data).\n+- Modularity and separation of concerns for better readability and maintenance.\n+- Advanced RAG techniques: ParentDocumentRetriever and LLM-based re-ranking.\n \"\"\"\n import os\n import tempfile\n from pathlib import Path\n-from typing import Optional\n+from typing import List, Optional\n \n import streamlit as st\n from dotenv import load_dotenv\n from langchain.chains import RetrievalQA\n+from langchain.retrievers import (ContextualCompressionRetriever,\n+                                  ParentDocumentRetriever)\n+from langchain.retrievers.document_compressors import LLMChainExtractor\n+from langchain.storage import InMemoryStore\n from langchain.text_splitter import RecursiveCharacterTextSplitter\n from langchain_chroma import Chroma\n from langchain_community.document_loaders import PyPDFLoader\n from langchain_core.documents import Document\n-from langchain_core.vectorstores import VectorStore\n from langchain_google_genai import (ChatGoogleGenerativeAI,\n                                     GoogleGenerativeAIEmbeddings)\n \n-# Define the path for the persistent Chroma database relative to the script file\n+# --- Configuration ---\n PERSIST_DIRECTORY = Path(__file__).parent / \".chroma_db\"\n+PARENT_CHUNK_SIZE = 2000\n+CHILD_CHUNK_SIZE = 400\n \n+# --- Helper Functions ---\n \n def get_google_api_key() -> Optional[str]:\n     \"\"\"\n     Fetches the Google API key from environment variables.\n-\n     Checks for GOOGLE_API_KEY first, then falls back to OPENROUTER_API_KEY.\n-\n-    Returns:\n-        Optional[str]: The API key if found, otherwise None.\n     \"\"\"\n     return os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"OPENROUTER_API_KEY\")\n \n+# --- Caching Functions for Expensive Resources ---\n \n-def initialize_session_state(embeddings: GoogleGenerativeAIEmbeddings):\n-    \"\"\"Initializes the Streamlit session state.\"\"\"\n-    if \"vector_store\" not in st.session_state:\n-        if PERSIST_DIRECTORY.exists():\n-            st.write(\"Loading existing vector store...\")\n-            st.session_state.vector_store = Chroma(\n-                persist_directory=str(PERSIST_DIRECTORY),\n-                embedding_function=embeddings,\n-            )\n-            st.write(\"Vector store loaded.\")\n-        else:\n-            st.session_state.vector_store = None\n-    if \"processed_file_id\" not in st.session_state:\n-        st.session_state.processed_file_id = None\n+@st.cache_resource\n+def get_llm(api_key: str) -> ChatGoogleGenerativeAI:\n+    \"\"\"Initializes and caches the LLM.\"\"\"\n+    return ChatGoogleGenerativeAI(\n+        model=\"gemini-2.5-flash-lite-preview-06-17\",\n+        google_api_key=api_key,\n+        temperature=0.7,\n+    )\n \n+@st.cache_resource\n+def get_embeddings(api_key: str) -> GoogleGenerativeAIEmbeddings:\n+    \"\"\"Initializes and caches the embeddings model.\"\"\"\n+    return GoogleGenerativeAIEmbeddings(\n+        model=\"models/embedding-001\", google_api_key=api_key\n+    )\n \n-def process_uploaded_file(uploaded_file, embeddings: GoogleGenerativeAIEmbeddings):\n+@st.cache_data\n+def load_and_split_docs(file_path: str) -> List[Document]:\n     \"\"\"\n-    Processes the uploaded PDF file, creates a vector store, and updates the session state.\n+    Loads a PDF and splits it into documents.\n+    Caches the result based on the file path.\n     \"\"\"\n-    try:\n-        with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n-            tmp_file.write(uploaded_file.getbuffer())\n-            tmp_file_path = tmp_file.name\n-\n-        st.write(f\"Processing `{uploaded_file.name}`...\")\n-        progress_bar = st.progress(0)\n-        status_text = st.empty()\n-\n-        # 1. Load the document\n-        status_text.text(\"Loading PDF...\")\n-        loader = PyPDFLoader(tmp_file_path)\n-        documents = loader.load()\n-        progress_bar.progress(25)\n-\n-        # 2. Split the document into chunks\n-        status_text.text(f\"Splitting {len(documents)} pages into chunks...\")\n-        text_splitter = RecursiveCharacterTextSplitter(\n-            chunk_size=1000,\n-            chunk_overlap=200,\n-            add_start_index=True,\n-            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n-        )\n-        texts: list[Document] = text_splitter.split_documents(documents)\n-        progress_bar.progress(50)\n+    loader = PyPDFLoader(file_path)\n+    return loader.load()\n \n-        # 3. Create embeddings and vector store\n-        status_text.text(f\"Creating embeddings for {len(texts)} chunks...\")\n-        st.session_state.vector_store = Chroma.from_documents(\n-            texts, embeddings, persist_directory=str(PERSIST_DIRECTORY)\n-        )\n-        progress_bar.progress(100)\n-        status_text.text(\"Processing complete!\")\n-        st.success(\"File processed and vector store created/updated successfully!\")\n-        st.session_state.processed_file_id = uploaded_file.file_id\n+@st.cache_resource\n+def build_retriever(\n+    _docs: List[Document], embeddings: GoogleGenerativeAIEmbeddings\n+) -> ParentDocumentRetriever:\n+    \"\"\"\n+    Builds and caches the advanced ParentDocumentRetriever.\n+    The '_docs' argument is used for caching purposes but the function\n+    relies on the documents being passed to retriever.add_documents.\n+    \"\"\"\n+    parent_splitter = RecursiveCharacterTextSplitter(chunk_size=PARENT_CHUNK_SIZE)\n+    child_splitter = RecursiveCharacterTextSplitter(chunk_size=CHILD_CHUNK_SIZE)\n+    \n+    vectorstore = Chroma(\n+        collection_name=\"split_parents\",\n+        embedding_function=embeddings,\n+        persist_directory=str(PERSIST_DIRECTORY),\n+    )\n+    store = InMemoryStore()\n \n-    except Exception as e:\n-        st.error(f\"An error occurred during file processing: {e}\")\n-    finally:\n-        if 'tmp_file_path' in locals() and os.path.exists(tmp_file_path):\n-            os.remove(tmp_file_path)\n+    retriever = ParentDocumentRetriever(\n+        vectorstore=vectorstore,\n+        docstore=store,\n+        child_splitter=child_splitter,\n+        parent_splitter=parent_splitter,\n+    )\n+    # This part is crucial and will run only when the documents change\n+    retriever.add_documents(_docs, ids=None)\n+    return retriever\n \n+# --- Main Application Logic ---\n \n-def handle_question_answering(google_api_key: str, vector_store: VectorStore):\n-    \"\"\"Handles the user input and the QA process.\"\"\"\n+def handle_question_answering(llm: ChatGoogleGenerativeAI, retriever):\n+    \"\"\"Handles the user input and the QA process with re-ranking.\"\"\"\n     st.header(\"Ask a question about the document\")\n     user_question = st.text_input(\"Your question:\")\n \n     if user_question:\n-        try:\n-            llm = ChatGoogleGenerativeAI(\n-                model=\"models/gemini-1.5-flash-latest\",\n-                google_api_key=google_api_key,\n-                temperature=0.7,\n-            )\n-            retriever = vector_store.as_retriever(\n-                search_type=\"similarity\", search_kwargs={\"k\": 3}\n-            )\n-            qa_chain = RetrievalQA.from_chain_type(\n-                llm=llm,\n-                chain_type=\"stuff\",\n-                retriever=retriever,\n-                return_source_documents=True,\n-            )\n-\n-            with st.spinner(\"Finding the answer...\"):\n+        with st.spinner(\"Retrieving, re-ranking, and finding the answer...\"):\n+            try:\n+                # 1. Set up the re-ranking compressor\n+                compressor = LLMChainExtractor.from_llm(llm)\n+                compression_retriever = ContextualCompressionRetriever(\n+                    base_compressor=compressor, base_retriever=retriever\n+                )\n+\n+                # 2. Set up the QA chain\n+                qa_chain = RetrievalQA.from_chain_type(\n+                    llm=llm,\n+                    chain_type=\"stuff\",\n+                    retriever=compression_retriever,\n+                    return_source_documents=True,\n+                )\n+\n+                # 3. Invoke the chain and display the response\n                 response = qa_chain.invoke({\"query\": user_question})\n                 st.write(\"### Answer\")\n                 st.write(response[\"result\"])\n \n                 with st.expander(\"Show source documents\"):\n                     st.write(response[\"source_documents\"])\n \n-        except Exception as e:\n-            st.error(f\"An error occurred while answering the question: {e}\")\n-\n+            except Exception as e:\n+                st.error(f\"An error occurred: {e}\")\n+                st.exception(e)\n \n def main():\n     \"\"\"Main function to run the Streamlit application.\"\"\"\n     st.set_page_config(page_title=\"Chat with your PDF\", layout=\"wide\")\n-    st.title(\"\ud83d\udcda Chat with Your Book\")\n+    st.title(\"\ud83d\udcda Advanced Chat with Your Book\")\n+    st.markdown(\n+        \"This app uses advanced RAG techniques for more accurate answers. \"\n+        \"Upload a PDF to get started.\"\n+    )\n \n     load_dotenv()\n     google_api_key = get_google_api_key()\n@@ -144,24 +148,42 @@ def main():\n         )\n         return\n \n-    embeddings = GoogleGenerativeAIEmbeddings(\n-        model=\"models/embedding-001\", google_api_key=google_api_key\n-    )\n-\n-    initialize_session_state(embeddings)\n+    # Initialize LLM and Embeddings using cached functions\n+    llm = get_llm(google_api_key)\n+    embeddings = get_embeddings(google_api_key)\n \n+    # File uploader\n     uploaded_file = st.file_uploader(\n-        \"Upload your PDF book to create or update the vector store\", type=\"pdf\"\n+        \"Upload your PDF book to create the retriever\", type=\"pdf\"\n     )\n \n-    if uploaded_file and uploaded_file.file_id != st.session_state.get(\n-        \"processed_file_id\"\n-    ):\n-        process_uploaded_file(uploaded_file, embeddings)\n+    if uploaded_file:\n+        try:\n+            # Use a temporary file to get a stable path for caching\n+            with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n+                tmp_file.write(uploaded_file.getbuffer())\n+                tmp_file_path = tmp_file.name\n+            \n+            # Load, split, and build the retriever using cached functions\n+            with st.spinner(\"Processing your document... This may take a moment.\"):\n+                docs = load_and_split_docs(tmp_file_path)\n+                retriever = build_retriever(docs, embeddings)\n+\n+            st.success(\"Document processed successfully! You can now ask questions.\")\n+            \n+            # Handle the QA logic\n+            handle_question_answering(llm, retriever)\n \n-    if st.session_state.get(\"vector_store\"):\n-        handle_question_answering(google_api_key, st.session_state.vector_store)\n+        except Exception as e:\n+            st.error(f\"An error occurred during file processing: {e}\")\n+            st.exception(e)\n+        finally:\n+            # Clean up the temporary file\n+            if \"tmp_file_path\" in locals() and os.path.exists(tmp_file_path):\n+                os.remove(tmp_file_path)\n+    else:\n+        st.info(\"Please upload a PDF file to begin.\")\n \n \n if __name__ == \"__main__\":\n-    main()\n\\ No newline at end of file\n+    main()",
      "patch_lines": [
        "@@ -1,139 +1,143 @@\n",
        " \"\"\"\n",
        " A Streamlit application for processing and chatting with uploaded files using LangChain.\n",
        "+This version incorporates best practices for Streamlit apps, including:\n",
        "+- Caching for expensive resources (@st.cache_resource) and data processing (@st.cache_data).\n",
        "+- Modularity and separation of concerns for better readability and maintenance.\n",
        "+- Advanced RAG techniques: ParentDocumentRetriever and LLM-based re-ranking.\n",
        " \"\"\"\n",
        " import os\n",
        " import tempfile\n",
        " from pathlib import Path\n",
        "-from typing import Optional\n",
        "+from typing import List, Optional\n",
        " \n",
        " import streamlit as st\n",
        " from dotenv import load_dotenv\n",
        " from langchain.chains import RetrievalQA\n",
        "+from langchain.retrievers import (ContextualCompressionRetriever,\n",
        "+                                  ParentDocumentRetriever)\n",
        "+from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "+from langchain.storage import InMemoryStore\n",
        " from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        " from langchain_chroma import Chroma\n",
        " from langchain_community.document_loaders import PyPDFLoader\n",
        " from langchain_core.documents import Document\n",
        "-from langchain_core.vectorstores import VectorStore\n",
        " from langchain_google_genai import (ChatGoogleGenerativeAI,\n",
        "                                     GoogleGenerativeAIEmbeddings)\n",
        " \n",
        "-# Define the path for the persistent Chroma database relative to the script file\n",
        "+# --- Configuration ---\n",
        " PERSIST_DIRECTORY = Path(__file__).parent / \".chroma_db\"\n",
        "+PARENT_CHUNK_SIZE = 2000\n",
        "+CHILD_CHUNK_SIZE = 400\n",
        " \n",
        "+# --- Helper Functions ---\n",
        " \n",
        " def get_google_api_key() -> Optional[str]:\n",
        "     \"\"\"\n",
        "     Fetches the Google API key from environment variables.\n",
        "-\n",
        "     Checks for GOOGLE_API_KEY first, then falls back to OPENROUTER_API_KEY.\n",
        "-\n",
        "-    Returns:\n",
        "-        Optional[str]: The API key if found, otherwise None.\n",
        "     \"\"\"\n",
        "     return os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"OPENROUTER_API_KEY\")\n",
        " \n",
        "+# --- Caching Functions for Expensive Resources ---\n",
        " \n",
        "-def initialize_session_state(embeddings: GoogleGenerativeAIEmbeddings):\n",
        "-    \"\"\"Initializes the Streamlit session state.\"\"\"\n",
        "-    if \"vector_store\" not in st.session_state:\n",
        "-        if PERSIST_DIRECTORY.exists():\n",
        "-            st.write(\"Loading existing vector store...\")\n",
        "-            st.session_state.vector_store = Chroma(\n",
        "-                persist_directory=str(PERSIST_DIRECTORY),\n",
        "-                embedding_function=embeddings,\n",
        "-            )\n",
        "-            st.write(\"Vector store loaded.\")\n",
        "-        else:\n",
        "-            st.session_state.vector_store = None\n",
        "-    if \"processed_file_id\" not in st.session_state:\n",
        "-        st.session_state.processed_file_id = None\n",
        "+@st.cache_resource\n",
        "+def get_llm(api_key: str) -> ChatGoogleGenerativeAI:\n",
        "+    \"\"\"Initializes and caches the LLM.\"\"\"\n",
        "+    return ChatGoogleGenerativeAI(\n",
        "+        model=\"gemini-2.5-flash-lite-preview-06-17\",\n",
        "+        google_api_key=api_key,\n",
        "+        temperature=0.7,\n",
        "+    )\n",
        " \n",
        "+@st.cache_resource\n",
        "+def get_embeddings(api_key: str) -> GoogleGenerativeAIEmbeddings:\n",
        "+    \"\"\"Initializes and caches the embeddings model.\"\"\"\n",
        "+    return GoogleGenerativeAIEmbeddings(\n",
        "+        model=\"models/embedding-001\", google_api_key=api_key\n",
        "+    )\n",
        " \n",
        "-def process_uploaded_file(uploaded_file, embeddings: GoogleGenerativeAIEmbeddings):\n",
        "+@st.cache_data\n",
        "+def load_and_split_docs(file_path: str) -> List[Document]:\n",
        "     \"\"\"\n",
        "-    Processes the uploaded PDF file, creates a vector store, and updates the session state.\n",
        "+    Loads a PDF and splits it into documents.\n",
        "+    Caches the result based on the file path.\n",
        "     \"\"\"\n",
        "-    try:\n",
        "-        with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
        "-            tmp_file.write(uploaded_file.getbuffer())\n",
        "-            tmp_file_path = tmp_file.name\n",
        "-\n",
        "-        st.write(f\"Processing `{uploaded_file.name}`...\")\n",
        "-        progress_bar = st.progress(0)\n",
        "-        status_text = st.empty()\n",
        "-\n",
        "-        # 1. Load the document\n",
        "-        status_text.text(\"Loading PDF...\")\n",
        "-        loader = PyPDFLoader(tmp_file_path)\n",
        "-        documents = loader.load()\n",
        "-        progress_bar.progress(25)\n",
        "-\n",
        "-        # 2. Split the document into chunks\n",
        "-        status_text.text(f\"Splitting {len(documents)} pages into chunks...\")\n",
        "-        text_splitter = RecursiveCharacterTextSplitter(\n",
        "-            chunk_size=1000,\n",
        "-            chunk_overlap=200,\n",
        "-            add_start_index=True,\n",
        "-            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
        "-        )\n",
        "-        texts: list[Document] = text_splitter.split_documents(documents)\n",
        "-        progress_bar.progress(50)\n",
        "+    loader = PyPDFLoader(file_path)\n",
        "+    return loader.load()\n",
        " \n",
        "-        # 3. Create embeddings and vector store\n",
        "-        status_text.text(f\"Creating embeddings for {len(texts)} chunks...\")\n",
        "-        st.session_state.vector_store = Chroma.from_documents(\n",
        "-            texts, embeddings, persist_directory=str(PERSIST_DIRECTORY)\n",
        "-        )\n",
        "-        progress_bar.progress(100)\n",
        "-        status_text.text(\"Processing complete!\")\n",
        "-        st.success(\"File processed and vector store created/updated successfully!\")\n",
        "-        st.session_state.processed_file_id = uploaded_file.file_id\n",
        "+@st.cache_resource\n",
        "+def build_retriever(\n",
        "+    _docs: List[Document], embeddings: GoogleGenerativeAIEmbeddings\n",
        "+) -> ParentDocumentRetriever:\n",
        "+    \"\"\"\n",
        "+    Builds and caches the advanced ParentDocumentRetriever.\n",
        "+    The '_docs' argument is used for caching purposes but the function\n",
        "+    relies on the documents being passed to retriever.add_documents.\n",
        "+    \"\"\"\n",
        "+    parent_splitter = RecursiveCharacterTextSplitter(chunk_size=PARENT_CHUNK_SIZE)\n",
        "+    child_splitter = RecursiveCharacterTextSplitter(chunk_size=CHILD_CHUNK_SIZE)\n",
        "+    \n",
        "+    vectorstore = Chroma(\n",
        "+        collection_name=\"split_parents\",\n",
        "+        embedding_function=embeddings,\n",
        "+        persist_directory=str(PERSIST_DIRECTORY),\n",
        "+    )\n",
        "+    store = InMemoryStore()\n",
        " \n",
        "-    except Exception as e:\n",
        "-        st.error(f\"An error occurred during file processing: {e}\")\n",
        "-    finally:\n",
        "-        if 'tmp_file_path' in locals() and os.path.exists(tmp_file_path):\n",
        "-            os.remove(tmp_file_path)\n",
        "+    retriever = ParentDocumentRetriever(\n",
        "+        vectorstore=vectorstore,\n",
        "+        docstore=store,\n",
        "+        child_splitter=child_splitter,\n",
        "+        parent_splitter=parent_splitter,\n",
        "+    )\n",
        "+    # This part is crucial and will run only when the documents change\n",
        "+    retriever.add_documents(_docs, ids=None)\n",
        "+    return retriever\n",
        " \n",
        "+# --- Main Application Logic ---\n",
        " \n",
        "-def handle_question_answering(google_api_key: str, vector_store: VectorStore):\n",
        "-    \"\"\"Handles the user input and the QA process.\"\"\"\n",
        "+def handle_question_answering(llm: ChatGoogleGenerativeAI, retriever):\n",
        "+    \"\"\"Handles the user input and the QA process with re-ranking.\"\"\"\n",
        "     st.header(\"Ask a question about the document\")\n",
        "     user_question = st.text_input(\"Your question:\")\n",
        " \n",
        "     if user_question:\n",
        "-        try:\n",
        "-            llm = ChatGoogleGenerativeAI(\n",
        "-                model=\"models/gemini-1.5-flash-latest\",\n",
        "-                google_api_key=google_api_key,\n",
        "-                temperature=0.7,\n",
        "-            )\n",
        "-            retriever = vector_store.as_retriever(\n",
        "-                search_type=\"similarity\", search_kwargs={\"k\": 3}\n",
        "-            )\n",
        "-            qa_chain = RetrievalQA.from_chain_type(\n",
        "-                llm=llm,\n",
        "-                chain_type=\"stuff\",\n",
        "-                retriever=retriever,\n",
        "-                return_source_documents=True,\n",
        "-            )\n",
        "-\n",
        "-            with st.spinner(\"Finding the answer...\"):\n",
        "+        with st.spinner(\"Retrieving, re-ranking, and finding the answer...\"):\n",
        "+            try:\n",
        "+                # 1. Set up the re-ranking compressor\n",
        "+                compressor = LLMChainExtractor.from_llm(llm)\n",
        "+                compression_retriever = ContextualCompressionRetriever(\n",
        "+                    base_compressor=compressor, base_retriever=retriever\n",
        "+                )\n",
        "+\n",
        "+                # 2. Set up the QA chain\n",
        "+                qa_chain = RetrievalQA.from_chain_type(\n",
        "+                    llm=llm,\n",
        "+                    chain_type=\"stuff\",\n",
        "+                    retriever=compression_retriever,\n",
        "+                    return_source_documents=True,\n",
        "+                )\n",
        "+\n",
        "+                # 3. Invoke the chain and display the response\n",
        "                 response = qa_chain.invoke({\"query\": user_question})\n",
        "                 st.write(\"### Answer\")\n",
        "                 st.write(response[\"result\"])\n",
        " \n",
        "                 with st.expander(\"Show source documents\"):\n",
        "                     st.write(response[\"source_documents\"])\n",
        " \n",
        "-        except Exception as e:\n",
        "-            st.error(f\"An error occurred while answering the question: {e}\")\n",
        "-\n",
        "+            except Exception as e:\n",
        "+                st.error(f\"An error occurred: {e}\")\n",
        "+                st.exception(e)\n",
        " \n",
        " def main():\n",
        "     \"\"\"Main function to run the Streamlit application.\"\"\"\n",
        "     st.set_page_config(page_title=\"Chat with your PDF\", layout=\"wide\")\n",
        "-    st.title(\"\ud83d\udcda Chat with Your Book\")\n",
        "+    st.title(\"\ud83d\udcda Advanced Chat with Your Book\")\n",
        "+    st.markdown(\n",
        "+        \"This app uses advanced RAG techniques for more accurate answers. \"\n",
        "+        \"Upload a PDF to get started.\"\n",
        "+    )\n",
        " \n",
        "     load_dotenv()\n",
        "     google_api_key = get_google_api_key()\n",
        "@@ -144,24 +148,42 @@ def main():\n",
        "         )\n",
        "         return\n",
        " \n",
        "-    embeddings = GoogleGenerativeAIEmbeddings(\n",
        "-        model=\"models/embedding-001\", google_api_key=google_api_key\n",
        "-    )\n",
        "-\n",
        "-    initialize_session_state(embeddings)\n",
        "+    # Initialize LLM and Embeddings using cached functions\n",
        "+    llm = get_llm(google_api_key)\n",
        "+    embeddings = get_embeddings(google_api_key)\n",
        " \n",
        "+    # File uploader\n",
        "     uploaded_file = st.file_uploader(\n",
        "-        \"Upload your PDF book to create or update the vector store\", type=\"pdf\"\n",
        "+        \"Upload your PDF book to create the retriever\", type=\"pdf\"\n",
        "     )\n",
        " \n",
        "-    if uploaded_file and uploaded_file.file_id != st.session_state.get(\n",
        "-        \"processed_file_id\"\n",
        "-    ):\n",
        "-        process_uploaded_file(uploaded_file, embeddings)\n",
        "+    if uploaded_file:\n",
        "+        try:\n",
        "+            # Use a temporary file to get a stable path for caching\n",
        "+            with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
        "+                tmp_file.write(uploaded_file.getbuffer())\n",
        "+                tmp_file_path = tmp_file.name\n",
        "+            \n",
        "+            # Load, split, and build the retriever using cached functions\n",
        "+            with st.spinner(\"Processing your document... This may take a moment.\"):\n",
        "+                docs = load_and_split_docs(tmp_file_path)\n",
        "+                retriever = build_retriever(docs, embeddings)\n",
        "+\n",
        "+            st.success(\"Document processed successfully! You can now ask questions.\")\n",
        "+            \n",
        "+            # Handle the QA logic\n",
        "+            handle_question_answering(llm, retriever)\n",
        " \n",
        "-    if st.session_state.get(\"vector_store\"):\n",
        "-        handle_question_answering(google_api_key, st.session_state.vector_store)\n",
        "+        except Exception as e:\n",
        "+            st.error(f\"An error occurred during file processing: {e}\")\n",
        "+            st.exception(e)\n",
        "+        finally:\n",
        "+            # Clean up the temporary file\n",
        "+            if \"tmp_file_path\" in locals() and os.path.exists(tmp_file_path):\n",
        "+                os.remove(tmp_file_path)\n",
        "+    else:\n",
        "+        st.info(\"Please upload a PDF file to begin.\")\n",
        " \n",
        " \n",
        " if __name__ == \"__main__\":\n",
        "-    main()\n",
        "\\ No newline at end of file\n",
        "+    main()\n"
      ]
    }
  ]
}
{
  "project": "Research Data/SSU_RAG",
  "repo": "Gael-Android/SSU_RAG",
  "prior_commit": "b9b2befc1271c08e73efc34212c8d4ea229f1b63",
  "researched_commit": "4228e4e133cd201ebb2e04d0e40c3bf27b775b4c",
  "compare_url": "https://github.com/Gael-Android/SSU_RAG/compare/b9b2befc1271c08e73efc34212c8d4ea229f1b63...4228e4e133cd201ebb2e04d0e40c3bf27b775b4c",
  "ahead_by": 1,
  "behind_by": 0,
  "changed_files": [
    {
      "path": "chains.py",
      "status": "modified",
      "additions": 129,
      "deletions": 99,
      "patch": "@@ -1,115 +1,145 @@\n import os\n-from typing import List, Dict\n-\n+from typing import List, Dict, Optional\n from dotenv import load_dotenv\n from langchain_openai import ChatOpenAI\n-from langchain_core.prompts import ChatPromptTemplate\n+from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n from langchain_core.output_parsers import StrOutputParser\n-\n+from langchain_core.runnables.history import RunnableWithMessageHistory\n+from langchain_community.chat_message_histories import ChatMessageHistory\n from embedding_processor import EmbeddingProcessor\n \n-\n-# \ud658\uacbd \ubcc0\uc218 \ub85c\ub4dc\n load_dotenv()\n \n+# ===== \uc124\uc815 =====\n+LLM = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3, api_key=os.getenv(\"OPENAI_API_KEY\"))\n \n-# ===== \uacf5\uc6a9 LLM =====\n-llm = ChatOpenAI(\n-    model=\"gpt-3.5-turbo\",\n-    temperature=0.7,\n-    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n-)\n-\n-\n-\n-\n-# ===== \ubca1\ud130 \uac80\uc0c9 \uc720\ud2f8 =====\n-_processor: EmbeddingProcessor | None = None\n-\n-\n-def get_processor() -> EmbeddingProcessor:\n-    global _processor\n-    if _processor is None:\n-        _processor = EmbeddingProcessor()\n-    return _processor\n-\n-\n-def _build_context(items: List[Dict]) -> str:\n-    lines: List[str] = []\n-    for i, item in enumerate(items, 1):\n-        title = item.get(\"title\") or \"\"\n-        description = item.get(\"description\") or \"\"\n-        content = item.get(\"content\") or \"\"\n-        author = item.get(\"author\") or \"\"\n-        category = item.get(\"category\") or \"\"\n-        published = item.get(\"published\") or \"\"\n-        link = item.get(\"link\") or \"\"\n-        snippet = (description or content)[:500]\n-        lines.append(\n-            f\"[{i}] Title: {title}\\nAuthor: {author} | Category: {category} | Published: {published}\\nSnippet: {snippet}\\nLink: {link}\"\n-        )\n-    return \"\\n\\n\".join(lines)\n-\n-\n-# ===== RAG \ub2f5\ubcc0 \uccb4\uc778 =====\n-_rag_prompt = ChatPromptTemplate.from_messages([\n-    (\n-        \"system\",\n-        \"\ub108\ub294 \uc81c\uacf5\ub41c \ubb38\ub9e5(Context)\ub9cc\uc744 \uadfc\uac70\ub85c \ud55c\uad6d\uc5b4\ub85c \uac04\uacb0\ud558\uace0 \uc815\ud655\ud558\uac8c \ub2f5\ubcc0\ud55c\ub2e4. \ubaa8\ub974\uba74 \ubaa8\ub978\ub2e4\uace0 \ub9d0\ud55c\ub2e4. \ubc18\ub4dc\uc2dc \ubcf8\ubb38 \ub0b4\uc5d0 \ubb38\ub9e5 \ud56d\ubaa9\uc758 \ub300\uad04\ud638 \ubc88\ud638([1], [2], ...)\ub85c \uadfc\uac70\ub97c \ud45c\uc2dc\ud558\ub77c. \ubcf8\ubb38\uc5d0\ub294 \ub9c1\ud06c\ub97c \ub123\uc9c0 \ub9d0\uace0, \ub9c1\ud06c \ubaa9\ub85d\uc740 \uc2dc\uc2a4\ud15c\uc774 \ub2f5\ubcc0 \ub9c8\uc9c0\ub9c9\uc5d0 '\ucc38\uace0 \ubb38\uc11c' \uc139\uc158\uc73c\ub85c \uc790\ub3d9 \ucca8\ubd80\ud55c\ub2e4.\",\n-    ),\n+# ===== \ud504\ub86c\ud504\ud2b8 =====\n+RAG_PROMPT = ChatPromptTemplate.from_messages([\n+    (\"system\", \"\ub108\ub294 \ubb38\ub9e5\uacfc \ub300\ud654 \uc774\ub825\uc744 \uadfc\uac70\ub85c \ud55c\uad6d\uc5b4\ub85c \ub2f5\ubcc0\ud55c\ub2e4. \ubb38\ub9e5 \ub0b4\uc6a9\uc740 [1], [2] \ubc88\ud638\ub85c \uc778\uc6a9\ud558\ub77c. \ubaa8\ub974\uba74 \ubaa8\ub978\ub2e4\uace0 \ub9d0\ud55c\ub2e4.\"),\n+    MessagesPlaceholder(\"history\"),\n     (\"human\", \"\uc9c8\ubb38: {question}\\n\\nContext:\\n{context}\"),\n ])\n \n-_rag_answer_chain = _rag_prompt | llm | StrOutputParser()\n+CONDENSE_PROMPT = ChatPromptTemplate.from_messages([\n+    (\"system\", \"\uc774\uc804 \ub300\ud654\ub97c \ucc38\uace0\ud574 \ucd5c\uc2e0 \uc9c8\ubb38\uc744 \ub3c5\ub9bd\uc801\uc778 \ud55c\uad6d\uc5b4 \uc9c8\ubb38\uc73c\ub85c \uc7ac\uc791\uc131\ud55c\ub2e4.\"),\n+    (\"human\", \"\ub300\ud654 \uc774\ub825:\\n{history}\\n\\n\ucd5c\uc2e0 \uc9c8\ubb38: {question}\\n\\n\uc7ac\uc791\uc131:\"),\n+])\n \n+# ===== \uc720\ud2f8\ub9ac\ud2f0 =====\n+def build_context(items: List[Dict]) -> str:\n+    lines = []\n+    for i, item in enumerate(items, 1):\n+        title = item.get(\"title\", \"\")\n+        snippet = (item.get(\"description\") or item.get(\"content\") or \"\")[:300]\n+        link = item.get(\"link\", \"\")\n+        lines.append(f\"[{i}] {title}\\n{snippet}\\n{link}\")\n+    return \"\\n\\n\".join(lines)\n \n-# ===== \ud1b5\ud569\ub41c RAG \uccb4\uc778 =====\n-def unified_rag_chain(query: str, limit: int = 5) -> Dict:\n-    \"\"\"\uc9c8\uc758\ub97c \ubc1b\uc544 \ub2f5\ubcc0, \uc0c1\uc138 \uac80\uc0c9 \uacb0\uacfc, \ucd9c\ucc98\ub97c \ubaa8\ub450 \ubc18\ud658\ud558\ub294 \ud1b5\ud569 \ud568\uc218\"\"\"\n-    processor = get_processor()\n-    results: List[Dict] = processor.search_similar_content(query, limit)\n-    context_text = _build_context(results)\n+def make_sources(items: List[Dict]) -> List[Dict]:\n+    return [{\n+        \"index\": i,\n+        \"title\": item.get(\"title\"),\n+        \"link\": item.get(\"link\"),\n+        \"author\": item.get(\"author\"),\n+        \"distance\": item.get(\"distance\"),\n+    } for i, item in enumerate(items, 1)]\n+\n+def history_to_text(messages) -> str:\n+    lines = []\n+    for m in messages:\n+        role = \"Human\" if m.__class__.__name__ == \"HumanMessage\" else \"AI\"\n+        lines.append(f\"{role}: {m.content}\")\n+    return \"\\n\".join(lines)\n+\n+# ===== \uc11c\ube44\uc2a4 =====\n+class RagService:\n+    def __init__(self):\n+        self.processor = EmbeddingProcessor()\n+        self.histories: Dict[str, ChatMessageHistory] = {}\n+        self.answer_chain = RAG_PROMPT | LLM | StrOutputParser()\n+        self.condense_chain = CONDENSE_PROMPT | LLM | StrOutputParser()\n     \n-    # AI \ub2f5\ubcc0 \uc0dd\uc131\n-    answer = _rag_answer_chain.invoke({\"question\": query, \"context\": context_text})\n+    def get_history(self, session_id: str) -> ChatMessageHistory:\n+        if session_id not in self.histories:\n+            self.histories[session_id] = ChatMessageHistory()\n+        return self.histories[session_id]\n     \n-    # \ucd9c\ucc98 \uc815\ubcf4 \uad6c\uc131\n-    sources = []\n-    sources_lines: List[str] = []\n-    for idx, r in enumerate(results, 1):\n-        title = r.get(\"title\")\n-        link = r.get(\"link\")\n-        sources.append(\n-            {\n-                \"index\": idx,\n-                \"title\": title,\n-                \"link\": link,\n-                \"published\": r.get(\"published\"),\n-                \"author\": r.get(\"author\"),\n-                \"category\": r.get(\"category\"),\n-                \"distance\": r.get(\"distance\"),\n-            }\n-        )\n-        sources_lines.append(f\"[{idx}] {title} - {link}\")\n-\n-    # # \ub2f5\ubcc0 \ubcf8\ubb38 \ub05d\uc5d0 \ucc38\uace0 \ubb38\uc11c \uc139\uc158 \uc790\ub3d9 \ucca8\ubd80\n-    # answer_with_sources = answer.strip()\n-    # if sources_lines:\n-    #     answer_with_sources += \"\\n\\n\ucc38\uace0 \ubb38\uc11c:\\n\" + \"\\n\".join(sources_lines)\n-\n-    # \ud1b5\ud569\ub41c \uacb0\uacfc \ubc18\ud658 (\uae30\uc874 run_rag_qa + _rag_items \uc815\ubcf4 \ubaa8\ub450 \ud3ec\ud568)\n-    return {\n-        \"query\": query,\n-        \"answer\": answer,\n-        \"sources\": sources,\n-        \"context\": context_text,\n-        \"items\": results,\n-        \"sources_text\": \"\\n\".join(sources_lines),\n-    }\n-\n-# \uae30\uc874 \ud568\uc218\ub4e4\uacfc\uc758 \ud638\ud658\uc131\uc744 \uc704\ud55c \ubcc4\uce6d\n-run_rag_qa = unified_rag_chain\n-\n-# LangServe \uad00\ub828 \uc2a4\ud0a4\ub9c8/\ub798\ud37c \uc81c\uac70\n-\n-\n+    def seed_messages(self, session_id: str, messages: List[Dict]):\n+        if not messages:\n+            return\n+        hist = self.get_history(session_id)\n+        for msg in messages:\n+            role = msg.get(\"role\")\n+            content = msg.get(\"content\", \"\").strip()\n+            if not content:\n+                continue\n+            if role == \"user\":\n+                hist.add_user_message(content)\n+            elif role == \"assistant\":\n+                hist.add_ai_message(content)\n+    \n+    def condense_query(self, session_id: str, query: str) -> str:\n+        hist = self.get_history(session_id)\n+        if not hist.messages:\n+            return query\n+        try:\n+            history_text = history_to_text(hist.messages)\n+            return self.condense_chain.invoke({\n+                \"history\": history_text,\n+                \"question\": query\n+            }).strip() or query\n+        except:\n+            return query\n+    \n+    def rag_query(self, query: str, session_id: str = None, messages: List[Dict] = None, limit: int = 5) -> Dict:\n+        # 1. \uba54\uc2dc\uc9c0 \uc2dc\ub4dc\n+        if session_id and messages:\n+            self.seed_messages(session_id, messages)\n+        \n+        # 2. \uc9c8\ubb38 \uc7ac\uc791\uc131\n+        effective_query = query\n+        if session_id:\n+            effective_query = self.condense_query(session_id, query)\n+        \n+        # 3. \uac80\uc0c9\n+        results = self.processor.search_similar_content(effective_query, limit)\n+        context = build_context(results)\n+        \n+        # 4. \ub2f5\ubcc0 \uc0dd\uc131 (\ud788\uc2a4\ud1a0\ub9ac \ud3ec\ud568)\n+        if session_id:\n+            chain_with_history = RunnableWithMessageHistory(\n+                self.answer_chain,\n+                self.get_history,\n+                input_messages_key=\"question\",\n+                history_messages_key=\"history\"\n+            )\n+            answer = chain_with_history.invoke(\n+                {\"question\": query, \"context\": context},\n+                config={\"configurable\": {\"session_id\": session_id}}\n+            )\n+            # \ud788\uc2a4\ud1a0\ub9ac\uc5d0 \uc774\ubc88 \ud134 \uc800\uc7a5\n+            hist = self.get_history(session_id)\n+            hist.add_user_message(query)\n+            hist.add_ai_message(answer)\n+        else:\n+            answer = self.answer_chain.invoke({\"question\": query, \"context\": context, \"history\": []})\n+        \n+        return {\n+            \"query\": query,\n+            \"rephrased_query\": effective_query,\n+            \"answer\": answer,\n+            \"sources\": make_sources(results),\n+            \"items\": results,\n+        }\n+\n+# ===== \uc804\uc5ed \uc778\uc2a4\ud134\uc2a4 =====\n+_service = None\n+\n+def get_service() -> RagService:\n+    global _service\n+    if _service is None:\n+        _service = RagService()\n+    return _service\n+\n+def run_rag_qa(query: str, limit: int = 5, messages: Optional[List[Dict]] = None, session_id: Optional[str] = None) -> Dict:\n+    return get_service().rag_query(query, session_id, messages, limit)\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -1,115 +1,145 @@\n",
        " import os\n",
        "-from typing import List, Dict\n",
        "-\n",
        "+from typing import List, Dict, Optional\n",
        " from dotenv import load_dotenv\n",
        " from langchain_openai import ChatOpenAI\n",
        "-from langchain_core.prompts import ChatPromptTemplate\n",
        "+from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        " from langchain_core.output_parsers import StrOutputParser\n",
        "-\n",
        "+from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "+from langchain_community.chat_message_histories import ChatMessageHistory\n",
        " from embedding_processor import EmbeddingProcessor\n",
        " \n",
        "-\n",
        "-# \ud658\uacbd \ubcc0\uc218 \ub85c\ub4dc\n",
        " load_dotenv()\n",
        " \n",
        "+# ===== \uc124\uc815 =====\n",
        "+LLM = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3, api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        " \n",
        "-# ===== \uacf5\uc6a9 LLM =====\n",
        "-llm = ChatOpenAI(\n",
        "-    model=\"gpt-3.5-turbo\",\n",
        "-    temperature=0.7,\n",
        "-    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "-)\n",
        "-\n",
        "-\n",
        "-\n",
        "-\n",
        "-# ===== \ubca1\ud130 \uac80\uc0c9 \uc720\ud2f8 =====\n",
        "-_processor: EmbeddingProcessor | None = None\n",
        "-\n",
        "-\n",
        "-def get_processor() -> EmbeddingProcessor:\n",
        "-    global _processor\n",
        "-    if _processor is None:\n",
        "-        _processor = EmbeddingProcessor()\n",
        "-    return _processor\n",
        "-\n",
        "-\n",
        "-def _build_context(items: List[Dict]) -> str:\n",
        "-    lines: List[str] = []\n",
        "-    for i, item in enumerate(items, 1):\n",
        "-        title = item.get(\"title\") or \"\"\n",
        "-        description = item.get(\"description\") or \"\"\n",
        "-        content = item.get(\"content\") or \"\"\n",
        "-        author = item.get(\"author\") or \"\"\n",
        "-        category = item.get(\"category\") or \"\"\n",
        "-        published = item.get(\"published\") or \"\"\n",
        "-        link = item.get(\"link\") or \"\"\n",
        "-        snippet = (description or content)[:500]\n",
        "-        lines.append(\n",
        "-            f\"[{i}] Title: {title}\\nAuthor: {author} | Category: {category} | Published: {published}\\nSnippet: {snippet}\\nLink: {link}\"\n",
        "-        )\n",
        "-    return \"\\n\\n\".join(lines)\n",
        "-\n",
        "-\n",
        "-# ===== RAG \ub2f5\ubcc0 \uccb4\uc778 =====\n",
        "-_rag_prompt = ChatPromptTemplate.from_messages([\n",
        "-    (\n",
        "-        \"system\",\n",
        "-        \"\ub108\ub294 \uc81c\uacf5\ub41c \ubb38\ub9e5(Context)\ub9cc\uc744 \uadfc\uac70\ub85c \ud55c\uad6d\uc5b4\ub85c \uac04\uacb0\ud558\uace0 \uc815\ud655\ud558\uac8c \ub2f5\ubcc0\ud55c\ub2e4. \ubaa8\ub974\uba74 \ubaa8\ub978\ub2e4\uace0 \ub9d0\ud55c\ub2e4. \ubc18\ub4dc\uc2dc \ubcf8\ubb38 \ub0b4\uc5d0 \ubb38\ub9e5 \ud56d\ubaa9\uc758 \ub300\uad04\ud638 \ubc88\ud638([1], [2], ...)\ub85c \uadfc\uac70\ub97c \ud45c\uc2dc\ud558\ub77c. \ubcf8\ubb38\uc5d0\ub294 \ub9c1\ud06c\ub97c \ub123\uc9c0 \ub9d0\uace0, \ub9c1\ud06c \ubaa9\ub85d\uc740 \uc2dc\uc2a4\ud15c\uc774 \ub2f5\ubcc0 \ub9c8\uc9c0\ub9c9\uc5d0 '\ucc38\uace0 \ubb38\uc11c' \uc139\uc158\uc73c\ub85c \uc790\ub3d9 \ucca8\ubd80\ud55c\ub2e4.\",\n",
        "-    ),\n",
        "+# ===== \ud504\ub86c\ud504\ud2b8 =====\n",
        "+RAG_PROMPT = ChatPromptTemplate.from_messages([\n",
        "+    (\"system\", \"\ub108\ub294 \ubb38\ub9e5\uacfc \ub300\ud654 \uc774\ub825\uc744 \uadfc\uac70\ub85c \ud55c\uad6d\uc5b4\ub85c \ub2f5\ubcc0\ud55c\ub2e4. \ubb38\ub9e5 \ub0b4\uc6a9\uc740 [1], [2] \ubc88\ud638\ub85c \uc778\uc6a9\ud558\ub77c. \ubaa8\ub974\uba74 \ubaa8\ub978\ub2e4\uace0 \ub9d0\ud55c\ub2e4.\"),\n",
        "+    MessagesPlaceholder(\"history\"),\n",
        "     (\"human\", \"\uc9c8\ubb38: {question}\\n\\nContext:\\n{context}\"),\n",
        " ])\n",
        " \n",
        "-_rag_answer_chain = _rag_prompt | llm | StrOutputParser()\n",
        "+CONDENSE_PROMPT = ChatPromptTemplate.from_messages([\n",
        "+    (\"system\", \"\uc774\uc804 \ub300\ud654\ub97c \ucc38\uace0\ud574 \ucd5c\uc2e0 \uc9c8\ubb38\uc744 \ub3c5\ub9bd\uc801\uc778 \ud55c\uad6d\uc5b4 \uc9c8\ubb38\uc73c\ub85c \uc7ac\uc791\uc131\ud55c\ub2e4.\"),\n",
        "+    (\"human\", \"\ub300\ud654 \uc774\ub825:\\n{history}\\n\\n\ucd5c\uc2e0 \uc9c8\ubb38: {question}\\n\\n\uc7ac\uc791\uc131:\"),\n",
        "+])\n",
        " \n",
        "+# ===== \uc720\ud2f8\ub9ac\ud2f0 =====\n",
        "+def build_context(items: List[Dict]) -> str:\n",
        "+    lines = []\n",
        "+    for i, item in enumerate(items, 1):\n",
        "+        title = item.get(\"title\", \"\")\n",
        "+        snippet = (item.get(\"description\") or item.get(\"content\") or \"\")[:300]\n",
        "+        link = item.get(\"link\", \"\")\n",
        "+        lines.append(f\"[{i}] {title}\\n{snippet}\\n{link}\")\n",
        "+    return \"\\n\\n\".join(lines)\n",
        " \n",
        "-# ===== \ud1b5\ud569\ub41c RAG \uccb4\uc778 =====\n",
        "-def unified_rag_chain(query: str, limit: int = 5) -> Dict:\n",
        "-    \"\"\"\uc9c8\uc758\ub97c \ubc1b\uc544 \ub2f5\ubcc0, \uc0c1\uc138 \uac80\uc0c9 \uacb0\uacfc, \ucd9c\ucc98\ub97c \ubaa8\ub450 \ubc18\ud658\ud558\ub294 \ud1b5\ud569 \ud568\uc218\"\"\"\n",
        "-    processor = get_processor()\n",
        "-    results: List[Dict] = processor.search_similar_content(query, limit)\n",
        "-    context_text = _build_context(results)\n",
        "+def make_sources(items: List[Dict]) -> List[Dict]:\n",
        "+    return [{\n",
        "+        \"index\": i,\n",
        "+        \"title\": item.get(\"title\"),\n",
        "+        \"link\": item.get(\"link\"),\n",
        "+        \"author\": item.get(\"author\"),\n",
        "+        \"distance\": item.get(\"distance\"),\n",
        "+    } for i, item in enumerate(items, 1)]\n",
        "+\n",
        "+def history_to_text(messages) -> str:\n",
        "+    lines = []\n",
        "+    for m in messages:\n",
        "+        role = \"Human\" if m.__class__.__name__ == \"HumanMessage\" else \"AI\"\n",
        "+        lines.append(f\"{role}: {m.content}\")\n",
        "+    return \"\\n\".join(lines)\n",
        "+\n",
        "+# ===== \uc11c\ube44\uc2a4 =====\n",
        "+class RagService:\n",
        "+    def __init__(self):\n",
        "+        self.processor = EmbeddingProcessor()\n",
        "+        self.histories: Dict[str, ChatMessageHistory] = {}\n",
        "+        self.answer_chain = RAG_PROMPT | LLM | StrOutputParser()\n",
        "+        self.condense_chain = CONDENSE_PROMPT | LLM | StrOutputParser()\n",
        "     \n",
        "-    # AI \ub2f5\ubcc0 \uc0dd\uc131\n",
        "-    answer = _rag_answer_chain.invoke({\"question\": query, \"context\": context_text})\n",
        "+    def get_history(self, session_id: str) -> ChatMessageHistory:\n",
        "+        if session_id not in self.histories:\n",
        "+            self.histories[session_id] = ChatMessageHistory()\n",
        "+        return self.histories[session_id]\n",
        "     \n",
        "-    # \ucd9c\ucc98 \uc815\ubcf4 \uad6c\uc131\n",
        "-    sources = []\n",
        "-    sources_lines: List[str] = []\n",
        "-    for idx, r in enumerate(results, 1):\n",
        "-        title = r.get(\"title\")\n",
        "-        link = r.get(\"link\")\n",
        "-        sources.append(\n",
        "-            {\n",
        "-                \"index\": idx,\n",
        "-                \"title\": title,\n",
        "-                \"link\": link,\n",
        "-                \"published\": r.get(\"published\"),\n",
        "-                \"author\": r.get(\"author\"),\n",
        "-                \"category\": r.get(\"category\"),\n",
        "-                \"distance\": r.get(\"distance\"),\n",
        "-            }\n",
        "-        )\n",
        "-        sources_lines.append(f\"[{idx}] {title} - {link}\")\n",
        "-\n",
        "-    # # \ub2f5\ubcc0 \ubcf8\ubb38 \ub05d\uc5d0 \ucc38\uace0 \ubb38\uc11c \uc139\uc158 \uc790\ub3d9 \ucca8\ubd80\n",
        "-    # answer_with_sources = answer.strip()\n",
        "-    # if sources_lines:\n",
        "-    #     answer_with_sources += \"\\n\\n\ucc38\uace0 \ubb38\uc11c:\\n\" + \"\\n\".join(sources_lines)\n",
        "-\n",
        "-    # \ud1b5\ud569\ub41c \uacb0\uacfc \ubc18\ud658 (\uae30\uc874 run_rag_qa + _rag_items \uc815\ubcf4 \ubaa8\ub450 \ud3ec\ud568)\n",
        "-    return {\n",
        "-        \"query\": query,\n",
        "-        \"answer\": answer,\n",
        "-        \"sources\": sources,\n",
        "-        \"context\": context_text,\n",
        "-        \"items\": results,\n",
        "-        \"sources_text\": \"\\n\".join(sources_lines),\n",
        "-    }\n",
        "-\n",
        "-# \uae30\uc874 \ud568\uc218\ub4e4\uacfc\uc758 \ud638\ud658\uc131\uc744 \uc704\ud55c \ubcc4\uce6d\n",
        "-run_rag_qa = unified_rag_chain\n",
        "-\n",
        "-# LangServe \uad00\ub828 \uc2a4\ud0a4\ub9c8/\ub798\ud37c \uc81c\uac70\n",
        "-\n",
        "-\n",
        "+    def seed_messages(self, session_id: str, messages: List[Dict]):\n",
        "+        if not messages:\n",
        "+            return\n",
        "+        hist = self.get_history(session_id)\n",
        "+        for msg in messages:\n",
        "+            role = msg.get(\"role\")\n",
        "+            content = msg.get(\"content\", \"\").strip()\n",
        "+            if not content:\n",
        "+                continue\n",
        "+            if role == \"user\":\n",
        "+                hist.add_user_message(content)\n",
        "+            elif role == \"assistant\":\n",
        "+                hist.add_ai_message(content)\n",
        "+    \n",
        "+    def condense_query(self, session_id: str, query: str) -> str:\n",
        "+        hist = self.get_history(session_id)\n",
        "+        if not hist.messages:\n",
        "+            return query\n",
        "+        try:\n",
        "+            history_text = history_to_text(hist.messages)\n",
        "+            return self.condense_chain.invoke({\n",
        "+                \"history\": history_text,\n",
        "+                \"question\": query\n",
        "+            }).strip() or query\n",
        "+        except:\n",
        "+            return query\n",
        "+    \n",
        "+    def rag_query(self, query: str, session_id: str = None, messages: List[Dict] = None, limit: int = 5) -> Dict:\n",
        "+        # 1. \uba54\uc2dc\uc9c0 \uc2dc\ub4dc\n",
        "+        if session_id and messages:\n",
        "+            self.seed_messages(session_id, messages)\n",
        "+        \n",
        "+        # 2. \uc9c8\ubb38 \uc7ac\uc791\uc131\n",
        "+        effective_query = query\n",
        "+        if session_id:\n",
        "+            effective_query = self.condense_query(session_id, query)\n",
        "+        \n",
        "+        # 3. \uac80\uc0c9\n",
        "+        results = self.processor.search_similar_content(effective_query, limit)\n",
        "+        context = build_context(results)\n",
        "+        \n",
        "+        # 4. \ub2f5\ubcc0 \uc0dd\uc131 (\ud788\uc2a4\ud1a0\ub9ac \ud3ec\ud568)\n",
        "+        if session_id:\n",
        "+            chain_with_history = RunnableWithMessageHistory(\n",
        "+                self.answer_chain,\n",
        "+                self.get_history,\n",
        "+                input_messages_key=\"question\",\n",
        "+                history_messages_key=\"history\"\n",
        "+            )\n",
        "+            answer = chain_with_history.invoke(\n",
        "+                {\"question\": query, \"context\": context},\n",
        "+                config={\"configurable\": {\"session_id\": session_id}}\n",
        "+            )\n",
        "+            # \ud788\uc2a4\ud1a0\ub9ac\uc5d0 \uc774\ubc88 \ud134 \uc800\uc7a5\n",
        "+            hist = self.get_history(session_id)\n",
        "+            hist.add_user_message(query)\n",
        "+            hist.add_ai_message(answer)\n",
        "+        else:\n",
        "+            answer = self.answer_chain.invoke({\"question\": query, \"context\": context, \"history\": []})\n",
        "+        \n",
        "+        return {\n",
        "+            \"query\": query,\n",
        "+            \"rephrased_query\": effective_query,\n",
        "+            \"answer\": answer,\n",
        "+            \"sources\": make_sources(results),\n",
        "+            \"items\": results,\n",
        "+        }\n",
        "+\n",
        "+# ===== \uc804\uc5ed \uc778\uc2a4\ud134\uc2a4 =====\n",
        "+_service = None\n",
        "+\n",
        "+def get_service() -> RagService:\n",
        "+    global _service\n",
        "+    if _service is None:\n",
        "+        _service = RagService()\n",
        "+    return _service\n",
        "+\n",
        "+def run_rag_qa(query: str, limit: int = 5, messages: Optional[List[Dict]] = None, session_id: Optional[str] = None) -> Dict:\n",
        "+    return get_service().rag_query(query, session_id, messages, limit)\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "frontend/src/components/ChatInterface.jsx",
      "status": "modified",
      "additions": 15,
      "deletions": 2,
      "patch": "@@ -8,6 +8,15 @@ function ChatInterface() {\n   const [messages, setMessages] = useState([]);\n   const [isLoading, setIsLoading] = useState(false);\n   const messagesEndRef = useRef(null);\n+  const [sessionId, setSessionId] = useState(() => {\n+    const existing = localStorage.getItem('ssu_rag_session_id');\n+    if (existing) return existing;\n+    const newId = (window.crypto && window.crypto.randomUUID)\n+      ? window.crypto.randomUUID()\n+      : Math.random().toString(36).slice(2);\n+    localStorage.setItem('ssu_rag_session_id', newId);\n+    return newId;\n+  });\n \n   const scrollToBottom = () => {\n     messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });\n@@ -27,13 +36,17 @@ function ChatInterface() {\n       timestamp: new Date()\n     };\n \n-    setMessages(prev => [...prev, userMessage]);\n+    const allMessages = [...messages, userMessage];\n+    setMessages(allMessages);\n     setIsLoading(true);\n \n     try {\n+      const minimalHistory = allMessages.map(m => ({ role: m.role, content: m.text }));\n       const response = await axios.post('/api/chat_api', {\n         query: text,\n-        limit: 5\n+        limit: 5,\n+        messages: minimalHistory,\n+        session_id: sessionId,\n       });\n \n       const aiMessage = {",
      "patch_lines": [
        "@@ -8,6 +8,15 @@ function ChatInterface() {\n",
        "   const [messages, setMessages] = useState([]);\n",
        "   const [isLoading, setIsLoading] = useState(false);\n",
        "   const messagesEndRef = useRef(null);\n",
        "+  const [sessionId, setSessionId] = useState(() => {\n",
        "+    const existing = localStorage.getItem('ssu_rag_session_id');\n",
        "+    if (existing) return existing;\n",
        "+    const newId = (window.crypto && window.crypto.randomUUID)\n",
        "+      ? window.crypto.randomUUID()\n",
        "+      : Math.random().toString(36).slice(2);\n",
        "+    localStorage.setItem('ssu_rag_session_id', newId);\n",
        "+    return newId;\n",
        "+  });\n",
        " \n",
        "   const scrollToBottom = () => {\n",
        "     messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });\n",
        "@@ -27,13 +36,17 @@ function ChatInterface() {\n",
        "       timestamp: new Date()\n",
        "     };\n",
        " \n",
        "-    setMessages(prev => [...prev, userMessage]);\n",
        "+    const allMessages = [...messages, userMessage];\n",
        "+    setMessages(allMessages);\n",
        "     setIsLoading(true);\n",
        " \n",
        "     try {\n",
        "+      const minimalHistory = allMessages.map(m => ({ role: m.role, content: m.text }));\n",
        "       const response = await axios.post('/api/chat_api', {\n",
        "         query: text,\n",
        "-        limit: 5\n",
        "+        limit: 5,\n",
        "+        messages: minimalHistory,\n",
        "+        session_id: sessionId,\n",
        "       });\n",
        " \n",
        "       const aiMessage = {\n"
      ]
    },
    {
      "path": "main.py",
      "status": "modified",
      "additions": 15,
      "deletions": 2,
      "patch": "@@ -171,7 +171,13 @@ async def rag_qa_post(payload: Dict) -> Dict:\n     \"\"\"POST \ubc14\ub514\ub85c \uc9c8\uc758\ub97c \ubc1b\uc544 \ucc98\ub9ac (\ud55c\uae00/\uc778\ucf54\ub529 \uc548\uc804)\"\"\"\n     query = (payload or {}).get(\"query\", \"\")\n     limit = int((payload or {}).get(\"limit\", 5))\n-    return run_rag_qa(query=query, limit=limit)\n+    messages = (payload or {}).get(\"messages\")\n+    session_id = (payload or {}).get(\"session_id\")\n+    try:\n+        print(f\"[POST /qa] sid={session_id} messages={len(messages) if isinstance(messages, list) else 0}\")\n+    except Exception:\n+        pass\n+    return run_rag_qa(query=query, limit=limit, messages=messages, session_id=session_id)\n \n # ===== \uac04\ub2e8\ud55c \ucc44\ud305 \uc5d4\ub4dc\ud3ec\uc778\ud2b8 =====\n @app.post(\"/chat_api\")\n@@ -190,10 +196,17 @@ async def chat_simple(payload: Dict) -> Dict:\n     if not query:\n         return {\"error\": \"\uc9c8\ubb38\uc744 \uc785\ub825\ud574\uc8fc\uc138\uc694.\"}\n     \n-    result = run_rag_qa(query=query)\n+    messages = (payload or {}).get(\"messages\")\n+    session_id = (payload or {}).get(\"session_id\")\n+    try:\n+        print(f\"[POST /chat_api] sid={session_id} messages={len(messages) if isinstance(messages, list) else 0}\")\n+    except Exception:\n+        pass\n+    result = run_rag_qa(query=query, messages=messages, session_id=session_id)\n     return {\n         \"message\": result.get(\"answer\", \"\ub2f5\ubcc0\uc744 \uc0dd\uc131\ud560 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4.\"),\n         \"query\": query,\n+        \"rephrased_query\": result.get(\"rephrased_query\"),\n         \"sources\": result.get(\"sources\", [])\n     }\n ",
      "patch_lines": [
        "@@ -171,7 +171,13 @@ async def rag_qa_post(payload: Dict) -> Dict:\n",
        "     \"\"\"POST \ubc14\ub514\ub85c \uc9c8\uc758\ub97c \ubc1b\uc544 \ucc98\ub9ac (\ud55c\uae00/\uc778\ucf54\ub529 \uc548\uc804)\"\"\"\n",
        "     query = (payload or {}).get(\"query\", \"\")\n",
        "     limit = int((payload or {}).get(\"limit\", 5))\n",
        "-    return run_rag_qa(query=query, limit=limit)\n",
        "+    messages = (payload or {}).get(\"messages\")\n",
        "+    session_id = (payload or {}).get(\"session_id\")\n",
        "+    try:\n",
        "+        print(f\"[POST /qa] sid={session_id} messages={len(messages) if isinstance(messages, list) else 0}\")\n",
        "+    except Exception:\n",
        "+        pass\n",
        "+    return run_rag_qa(query=query, limit=limit, messages=messages, session_id=session_id)\n",
        " \n",
        " # ===== \uac04\ub2e8\ud55c \ucc44\ud305 \uc5d4\ub4dc\ud3ec\uc778\ud2b8 =====\n",
        " @app.post(\"/chat_api\")\n",
        "@@ -190,10 +196,17 @@ async def chat_simple(payload: Dict) -> Dict:\n",
        "     if not query:\n",
        "         return {\"error\": \"\uc9c8\ubb38\uc744 \uc785\ub825\ud574\uc8fc\uc138\uc694.\"}\n",
        "     \n",
        "-    result = run_rag_qa(query=query)\n",
        "+    messages = (payload or {}).get(\"messages\")\n",
        "+    session_id = (payload or {}).get(\"session_id\")\n",
        "+    try:\n",
        "+        print(f\"[POST /chat_api] sid={session_id} messages={len(messages) if isinstance(messages, list) else 0}\")\n",
        "+    except Exception:\n",
        "+        pass\n",
        "+    result = run_rag_qa(query=query, messages=messages, session_id=session_id)\n",
        "     return {\n",
        "         \"message\": result.get(\"answer\", \"\ub2f5\ubcc0\uc744 \uc0dd\uc131\ud560 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4.\"),\n",
        "         \"query\": query,\n",
        "+        \"rephrased_query\": result.get(\"rephrased_query\"),\n",
        "         \"sources\": result.get(\"sources\", [])\n",
        "     }\n",
        " \n"
      ]
    },
    {
      "path": "pyproject.toml",
      "status": "modified",
      "additions": 1,
      "deletions": 0,
      "patch": "@@ -10,6 +10,7 @@ dependencies = [\n     \"feedparser>=6.0.11\",\n     \"langchain>=0.3.27\",\n     \"langchain-openai>=0.3.28\",\n+    \"langchain-community>=0.3.27\",\n     \"pymilvus>=2.6.0\",\n     \"python-dotenv>=1.1.1\",\n     \"requests>=2.32.4\",",
      "patch_lines": [
        "@@ -10,6 +10,7 @@ dependencies = [\n",
        "     \"feedparser>=6.0.11\",\n",
        "     \"langchain>=0.3.27\",\n",
        "     \"langchain-openai>=0.3.28\",\n",
        "+    \"langchain-community>=0.3.27\",\n",
        "     \"pymilvus>=2.6.0\",\n",
        "     \"python-dotenv>=1.1.1\",\n",
        "     \"requests>=2.32.4\",\n"
      ]
    }
  ]
}
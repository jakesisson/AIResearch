{
  "project": "Research Data/psychological-agent",
  "repo": "FurkanSahinnn/psychological-agent",
  "prior_commit": "124599544119a8487c1eb09ac05849bf6d5a0bc6",
  "researched_commit": "efcab0a6ae2707f8c9ab8b35a3eeddf8ca0e8565",
  "compare_url": "https://github.com/FurkanSahinnn/psychological-agent/compare/124599544119a8487c1eb09ac05849bf6d5a0bc6...efcab0a6ae2707f8c9ab8b35a3eeddf8ca0e8565",
  "ahead_by": 1,
  "behind_by": 0,
  "changed_files": [
    {
      "path": "app.py",
      "status": "modified",
      "additions": 26,
      "deletions": 12,
      "patch": "@@ -9,21 +9,35 @@ def ask_question():\n     question = data.get('question', '')\n     \n     if not question:\n-        return jsonify({\"error\": \"Soru bo\u015f olamaz\"}), 400\n+        return jsonify({\"error\": \"Question cannot be empty\"}), 400\n     \n-    # Agent'i \u00e7al\u0131\u015ft\u0131r\n-    initial_state = AgentState(user_query=question, retrieved_documents=None, final_prompt=None, final_response=None)\n-    events = psychology_agent.stream(initial_state)\n+    print(f\"\ud83d\udcdd Question received: {question}\")\n     \n-    final_response = \"\"\n-    for event in events:\n-        if \"final_response\" in event.get(\"generator\", {}):\n-            final_response = event[\"generator\"][\"final_response\"]\n+    # Simple state initialization\n+    initial_state = AgentState(\n+        user_query=question,\n+        search_results=None,\n+        final_response=None\n+    )\n     \n-    return jsonify({\n-        \"question\": question,\n-        \"answer\": final_response\n-    })\n+    # Run the pipeline\n+    try:\n+        result = psychology_agent.invoke(initial_state)\n+        final_response = result.get(\"final_response\", \"Sorry, I couldn't generate a response.\")\n+        \n+        print(f\"\u2705 Answer ready: {len(final_response)} characters\")\n+        \n+        return jsonify({\n+            \"question\": question,\n+            \"answer\": final_response\n+        })\n+        \n+    except Exception as e:\n+        print(f\"\u274c Error: {e}\")\n+        return jsonify({\n+            \"question\": question,\n+            \"answer\": f\"An error occurred: {str(e)}\"\n+        }), 500\n \n if __name__ == '__main__':\n     app.run(port=5000, debug=True) \n\\ No newline at end of file",
      "patch_lines": [
        "@@ -9,21 +9,35 @@ def ask_question():\n",
        "     question = data.get('question', '')\n",
        "     \n",
        "     if not question:\n",
        "-        return jsonify({\"error\": \"Soru bo\u015f olamaz\"}), 400\n",
        "+        return jsonify({\"error\": \"Question cannot be empty\"}), 400\n",
        "     \n",
        "-    # Agent'i \u00e7al\u0131\u015ft\u0131r\n",
        "-    initial_state = AgentState(user_query=question, retrieved_documents=None, final_prompt=None, final_response=None)\n",
        "-    events = psychology_agent.stream(initial_state)\n",
        "+    print(f\"\ud83d\udcdd Question received: {question}\")\n",
        "     \n",
        "-    final_response = \"\"\n",
        "-    for event in events:\n",
        "-        if \"final_response\" in event.get(\"generator\", {}):\n",
        "-            final_response = event[\"generator\"][\"final_response\"]\n",
        "+    # Simple state initialization\n",
        "+    initial_state = AgentState(\n",
        "+        user_query=question,\n",
        "+        search_results=None,\n",
        "+        final_response=None\n",
        "+    )\n",
        "     \n",
        "-    return jsonify({\n",
        "-        \"question\": question,\n",
        "-        \"answer\": final_response\n",
        "-    })\n",
        "+    # Run the pipeline\n",
        "+    try:\n",
        "+        result = psychology_agent.invoke(initial_state)\n",
        "+        final_response = result.get(\"final_response\", \"Sorry, I couldn't generate a response.\")\n",
        "+        \n",
        "+        print(f\"\u2705 Answer ready: {len(final_response)} characters\")\n",
        "+        \n",
        "+        return jsonify({\n",
        "+            \"question\": question,\n",
        "+            \"answer\": final_response\n",
        "+        })\n",
        "+        \n",
        "+    except Exception as e:\n",
        "+        print(f\"\u274c Error: {e}\")\n",
        "+        return jsonify({\n",
        "+            \"question\": question,\n",
        "+            \"answer\": f\"An error occurred: {str(e)}\"\n",
        "+        }), 500\n",
        " \n",
        " if __name__ == '__main__':\n",
        "     app.run(port=5000, debug=True) \n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "build_prompt.py",
      "status": "removed",
      "additions": 0,
      "deletions": 59,
      "patch": "@@ -1,59 +0,0 @@\n-from typing import List, Dict, Any\n-\n-def format_retrieved_documents(documents: List[Dict[str, Any]]) -> str:\n-    \"\"\"\n-    Formats the retrieved documents from Qdrant into a text format that the LLM can understand.\n-    \"\"\"\n-    formatted_docs = []\n-    for i, doc in enumerate(documents):\n-        if doc.get(\"error\"):\n-            continue\n-        \n-        document_text = doc.get('document', 'Content not found.')\n-        source = doc.get('source', 'Unknown Source')\n-        page = doc.get('page', 'Unknown Page')\n-        \n-        formatted_docs.append(\n-            f\"--- Document {i+1} (Source: {source}, Page: {page}) ---\\n\"\n-            f\"{document_text}\\n\"\n-            f\"--- Document {i+1} End ---\"\n-        )\n-    \n-    if not formatted_docs:\n-        return \"No document found related to the user's query.\"\n-        \n-    return \"\\n\\n\".join(formatted_docs)\n-\n-\n-def build_agent_prompt(user_query: str, retrieved_documents: List[Dict[str, Any]]) -> str:\n-    \"\"\"\n-    Builds a complete system prompt for the LangGraph agent using the user query and retrieved documents.\n-    \"\"\"\n-    formatted_documents_str = format_retrieved_documents(retrieved_documents)\n-\n-    prompt_template = f\"\"\"\n-# TASK AND ROLE\n-You are a specialized AI assistant in the field of psychology. Your task is to synthesize the information from the provided academic documents into a coherent and comprehensive text, answering the user's question. Your answers should be based solely on the documents provided to you.\n-\n-# ANSWERING RULES\n-1.  **Never use information from outside the provided documents:** Do not use information from outside the provided documents in your answer. If the documents are insufficient to answer the question, explicitly state this.\n-2.  **Use academic and professional language:** Your answer should be written in a way similar to how a psychologist would answer, with clear, informative, and professional language.\n-3.  **Always provide source information:** When providing information, always indicate which book and page number the information comes from. Use the format: (Source: [Book Name], Page: [Page Number]) after each piece of information.\n-4.  **Answer in the same language as the user's question:** Detect the language of the user's question and respond in the same language. If the question is in English, respond in English. If the question is in Turkish, respond in Turkish. If the question is in another language, respond in that language.\n-5.  **Provide a comprehensive and structured answer:** Address the user's question comprehensively. Structure your answer logically, using headings or lists if necessary.\n-6.  **Be direct and clear:** Start your answer directly with the user's question, then provide details.\n-7.  **Source citation format:** When citing information, use this format: \"Information content here (Source: Book Name, Page: X)\"\n-\n-# PROVIDED ACADEMIC DOCUMENTS\n-The following are the relevant academic texts that you should use to answer the user's question:\n-{formatted_documents_str}\n-\n-# USER QUESTION\n-Here is the question you need to answer:\n-\"{user_query}\"\n-\n-# ANSWER\n-Please follow the rules above and the documents provided. Create a comprehensive answer while ALWAYS including source information (book name and page number) for each piece of information you provide. Use the format: (Source: Book Name, Page: X) after each important statement or fact. Remember to respond in the same language as the user's question.\n-\"\"\"\n-    \n-    return prompt_template.strip()",
      "patch_lines": [
        "@@ -1,59 +0,0 @@\n",
        "-from typing import List, Dict, Any\n",
        "-\n",
        "-def format_retrieved_documents(documents: List[Dict[str, Any]]) -> str:\n",
        "-    \"\"\"\n",
        "-    Formats the retrieved documents from Qdrant into a text format that the LLM can understand.\n",
        "-    \"\"\"\n",
        "-    formatted_docs = []\n",
        "-    for i, doc in enumerate(documents):\n",
        "-        if doc.get(\"error\"):\n",
        "-            continue\n",
        "-        \n",
        "-        document_text = doc.get('document', 'Content not found.')\n",
        "-        source = doc.get('source', 'Unknown Source')\n",
        "-        page = doc.get('page', 'Unknown Page')\n",
        "-        \n",
        "-        formatted_docs.append(\n",
        "-            f\"--- Document {i+1} (Source: {source}, Page: {page}) ---\\n\"\n",
        "-            f\"{document_text}\\n\"\n",
        "-            f\"--- Document {i+1} End ---\"\n",
        "-        )\n",
        "-    \n",
        "-    if not formatted_docs:\n",
        "-        return \"No document found related to the user's query.\"\n",
        "-        \n",
        "-    return \"\\n\\n\".join(formatted_docs)\n",
        "-\n",
        "-\n",
        "-def build_agent_prompt(user_query: str, retrieved_documents: List[Dict[str, Any]]) -> str:\n",
        "-    \"\"\"\n",
        "-    Builds a complete system prompt for the LangGraph agent using the user query and retrieved documents.\n",
        "-    \"\"\"\n",
        "-    formatted_documents_str = format_retrieved_documents(retrieved_documents)\n",
        "-\n",
        "-    prompt_template = f\"\"\"\n",
        "-# TASK AND ROLE\n",
        "-You are a specialized AI assistant in the field of psychology. Your task is to synthesize the information from the provided academic documents into a coherent and comprehensive text, answering the user's question. Your answers should be based solely on the documents provided to you.\n",
        "-\n",
        "-# ANSWERING RULES\n",
        "-1.  **Never use information from outside the provided documents:** Do not use information from outside the provided documents in your answer. If the documents are insufficient to answer the question, explicitly state this.\n",
        "-2.  **Use academic and professional language:** Your answer should be written in a way similar to how a psychologist would answer, with clear, informative, and professional language.\n",
        "-3.  **Always provide source information:** When providing information, always indicate which book and page number the information comes from. Use the format: (Source: [Book Name], Page: [Page Number]) after each piece of information.\n",
        "-4.  **Answer in the same language as the user's question:** Detect the language of the user's question and respond in the same language. If the question is in English, respond in English. If the question is in Turkish, respond in Turkish. If the question is in another language, respond in that language.\n",
        "-5.  **Provide a comprehensive and structured answer:** Address the user's question comprehensively. Structure your answer logically, using headings or lists if necessary.\n",
        "-6.  **Be direct and clear:** Start your answer directly with the user's question, then provide details.\n",
        "-7.  **Source citation format:** When citing information, use this format: \"Information content here (Source: Book Name, Page: X)\"\n",
        "-\n",
        "-# PROVIDED ACADEMIC DOCUMENTS\n",
        "-The following are the relevant academic texts that you should use to answer the user's question:\n",
        "-{formatted_documents_str}\n",
        "-\n",
        "-# USER QUESTION\n",
        "-Here is the question you need to answer:\n",
        "-\"{user_query}\"\n",
        "-\n",
        "-# ANSWER\n",
        "-Please follow the rules above and the documents provided. Create a comprehensive answer while ALWAYS including source information (book name and page number) for each piece of information you provide. Use the format: (Source: Book Name, Page: X) after each important statement or fact. Remember to respond in the same language as the user's question.\n",
        "-\"\"\"\n",
        "-    \n",
        "-    return prompt_template.strip()\n"
      ]
    },
    {
      "path": "langgraph_pipeline.py",
      "status": "modified",
      "additions": 57,
      "deletions": 35,
      "patch": "@@ -1,57 +1,79 @@\n from langgraph.graph import StateGraph, END\n-from typing import TypedDict, Annotated, List, Dict, Any, Optional\n-import operator\n+from typing import TypedDict, Optional\n from conf import llm_client, LLM_MODEL\n from tools import search_psychology_knowledge_base\n-from build_prompt import build_agent_prompt\n+import json\n \n class AgentState(TypedDict):\n     user_query: str\n-    retrieved_documents: Optional[List[Dict[str, Any]]]\n-    final_prompt: Optional[str]\n+    search_results: Optional[str]\n     final_response: Optional[str]\n \n-def retrieve_documents_node(state: AgentState) -> dict:\n-    \"\"\"\n-    Retrieves relevant documents from Qdrant using the user's query.\n-    \"\"\"\n-    print(\"--- \ud83e\udde0 Searching the knowledge base... ---\")\n-    user_query = state[\"user_query\"]\n-    retrieved_documents = search_psychology_knowledge_base.invoke({\"query\": user_query})\n+def search_node(state: AgentState) -> dict:\n+    \"\"\"Always search first\"\"\"\n+    print(\"--- \ud83d\udd0d Searching knowledge base... ---\")\n     \n-    return {\"retrieved_documents\": retrieved_documents}\n+    results = search_psychology_knowledge_base.invoke({\"query\": state[\"user_query\"]})\n+    \n+    # Format results for LLM\n+    formatted_results = []\n+    for i, result in enumerate(results):\n+        if \"error\" not in result:\n+            formatted_results.append(\n+                f\"Document {i+1} (Source: {result['source']}, Page: {result['page']}):\\n\"\n+                f\"{result['document']}\\n\"\n+            )\n+    \n+    search_text = \"\\n\".join(formatted_results) if formatted_results else \"No relevant documents found.\"\n+    print(f\"--- \u2705 Found {len(results)} documents ---\")\n+    \n+    return {\"search_results\": search_text}\n+\n+def generate_node(state: AgentState) -> dict:\n+    \"\"\"Generate response based on search results\"\"\"\n+    print(\"--- \ud83e\udd16 Generating response... ---\")\n+    \n+    prompt = f\"\"\"You are an expert psychologist. Based on the search results below, answer the user's question.\n+\n+SEARCH RESULTS:\n+{state['search_results']}\n+\n+USER QUESTION:\n+{state['user_query']}\n \n-def generate_response_node(state: AgentState) -> dict:\n-    \"\"\"\n-    Generates a response from the LLM using the retrieved documents and the user's query.\n-    \"\"\"\n-    print(\"--- \ud83e\udd16 LLM is generating a response... ---\")\n-    user_query = state[\"user_query\"]\n-    retrieved_documents = state.get(\"retrieved_documents\") or []\n+IMPORTANT RULES:\n+1. Only use information from the search results above\n+2. Always cite sources: (Source: Book Name, Page: X)\n+3. Use academic and professional language\n+4. Respond in the same language as the question\n+5. If search results are insufficient, say so clearly\n \n-    final_prompt = build_agent_prompt(user_query, retrieved_documents)\n+Provide a comprehensive answer with proper citations.\"\"\"\n \n     response = llm_client.chat.completions.create(\n         model=LLM_MODEL,\n-        messages=[{\"role\": \"user\", \"content\": final_prompt}],\n-        temperature=0.3,\n+        messages=[{\"role\": \"user\", \"content\": prompt}],\n+        temperature=0.3\n     )\n-    final_response = response.choices[0].message.content or \"I'm sorry, I couldn't generate a response.\"\n     \n-    return {\"final_prompt\": final_prompt, \"final_response\": final_response}\n+    final_answer = response.choices[0].message.content\n+    print(\"--- \u2705 Response generated ---\")\n+    \n+    return {\"final_response\": final_answer}\n \n def create_psychology_agent_graph():\n-    \"\"\"\n-    Creates a LangGraph workflow (pipeline), defines nodes and edges.\n-    \"\"\"\n+    \"\"\"Simple two-step workflow: search -> generate\"\"\"\n     workflow = StateGraph(AgentState)\n-    workflow.add_node(\"retriever\", retrieve_documents_node)\n-    workflow.add_node(\"generator\", generate_response_node)\n-\n-    workflow.set_entry_point(\"retriever\")\n-    workflow.add_edge(\"retriever\", \"generator\")\n-    workflow.add_edge(\"generator\", END)\n-\n+    \n+    # Add nodes\n+    workflow.add_node(\"search\", search_node)\n+    workflow.add_node(\"generate\", generate_node)\n+    \n+    # Define flow: search -> generate -> end\n+    workflow.set_entry_point(\"search\")\n+    workflow.add_edge(\"search\", \"generate\")\n+    workflow.add_edge(\"generate\", END)\n+    \n     return workflow.compile()\n \n psychology_agent = create_psychology_agent_graph()",
      "patch_lines": [
        "@@ -1,57 +1,79 @@\n",
        " from langgraph.graph import StateGraph, END\n",
        "-from typing import TypedDict, Annotated, List, Dict, Any, Optional\n",
        "-import operator\n",
        "+from typing import TypedDict, Optional\n",
        " from conf import llm_client, LLM_MODEL\n",
        " from tools import search_psychology_knowledge_base\n",
        "-from build_prompt import build_agent_prompt\n",
        "+import json\n",
        " \n",
        " class AgentState(TypedDict):\n",
        "     user_query: str\n",
        "-    retrieved_documents: Optional[List[Dict[str, Any]]]\n",
        "-    final_prompt: Optional[str]\n",
        "+    search_results: Optional[str]\n",
        "     final_response: Optional[str]\n",
        " \n",
        "-def retrieve_documents_node(state: AgentState) -> dict:\n",
        "-    \"\"\"\n",
        "-    Retrieves relevant documents from Qdrant using the user's query.\n",
        "-    \"\"\"\n",
        "-    print(\"--- \ud83e\udde0 Searching the knowledge base... ---\")\n",
        "-    user_query = state[\"user_query\"]\n",
        "-    retrieved_documents = search_psychology_knowledge_base.invoke({\"query\": user_query})\n",
        "+def search_node(state: AgentState) -> dict:\n",
        "+    \"\"\"Always search first\"\"\"\n",
        "+    print(\"--- \ud83d\udd0d Searching knowledge base... ---\")\n",
        "     \n",
        "-    return {\"retrieved_documents\": retrieved_documents}\n",
        "+    results = search_psychology_knowledge_base.invoke({\"query\": state[\"user_query\"]})\n",
        "+    \n",
        "+    # Format results for LLM\n",
        "+    formatted_results = []\n",
        "+    for i, result in enumerate(results):\n",
        "+        if \"error\" not in result:\n",
        "+            formatted_results.append(\n",
        "+                f\"Document {i+1} (Source: {result['source']}, Page: {result['page']}):\\n\"\n",
        "+                f\"{result['document']}\\n\"\n",
        "+            )\n",
        "+    \n",
        "+    search_text = \"\\n\".join(formatted_results) if formatted_results else \"No relevant documents found.\"\n",
        "+    print(f\"--- \u2705 Found {len(results)} documents ---\")\n",
        "+    \n",
        "+    return {\"search_results\": search_text}\n",
        "+\n",
        "+def generate_node(state: AgentState) -> dict:\n",
        "+    \"\"\"Generate response based on search results\"\"\"\n",
        "+    print(\"--- \ud83e\udd16 Generating response... ---\")\n",
        "+    \n",
        "+    prompt = f\"\"\"You are an expert psychologist. Based on the search results below, answer the user's question.\n",
        "+\n",
        "+SEARCH RESULTS:\n",
        "+{state['search_results']}\n",
        "+\n",
        "+USER QUESTION:\n",
        "+{state['user_query']}\n",
        " \n",
        "-def generate_response_node(state: AgentState) -> dict:\n",
        "-    \"\"\"\n",
        "-    Generates a response from the LLM using the retrieved documents and the user's query.\n",
        "-    \"\"\"\n",
        "-    print(\"--- \ud83e\udd16 LLM is generating a response... ---\")\n",
        "-    user_query = state[\"user_query\"]\n",
        "-    retrieved_documents = state.get(\"retrieved_documents\") or []\n",
        "+IMPORTANT RULES:\n",
        "+1. Only use information from the search results above\n",
        "+2. Always cite sources: (Source: Book Name, Page: X)\n",
        "+3. Use academic and professional language\n",
        "+4. Respond in the same language as the question\n",
        "+5. If search results are insufficient, say so clearly\n",
        " \n",
        "-    final_prompt = build_agent_prompt(user_query, retrieved_documents)\n",
        "+Provide a comprehensive answer with proper citations.\"\"\"\n",
        " \n",
        "     response = llm_client.chat.completions.create(\n",
        "         model=LLM_MODEL,\n",
        "-        messages=[{\"role\": \"user\", \"content\": final_prompt}],\n",
        "-        temperature=0.3,\n",
        "+        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "+        temperature=0.3\n",
        "     )\n",
        "-    final_response = response.choices[0].message.content or \"I'm sorry, I couldn't generate a response.\"\n",
        "     \n",
        "-    return {\"final_prompt\": final_prompt, \"final_response\": final_response}\n",
        "+    final_answer = response.choices[0].message.content\n",
        "+    print(\"--- \u2705 Response generated ---\")\n",
        "+    \n",
        "+    return {\"final_response\": final_answer}\n",
        " \n",
        " def create_psychology_agent_graph():\n",
        "-    \"\"\"\n",
        "-    Creates a LangGraph workflow (pipeline), defines nodes and edges.\n",
        "-    \"\"\"\n",
        "+    \"\"\"Simple two-step workflow: search -> generate\"\"\"\n",
        "     workflow = StateGraph(AgentState)\n",
        "-    workflow.add_node(\"retriever\", retrieve_documents_node)\n",
        "-    workflow.add_node(\"generator\", generate_response_node)\n",
        "-\n",
        "-    workflow.set_entry_point(\"retriever\")\n",
        "-    workflow.add_edge(\"retriever\", \"generator\")\n",
        "-    workflow.add_edge(\"generator\", END)\n",
        "-\n",
        "+    \n",
        "+    # Add nodes\n",
        "+    workflow.add_node(\"search\", search_node)\n",
        "+    workflow.add_node(\"generate\", generate_node)\n",
        "+    \n",
        "+    # Define flow: search -> generate -> end\n",
        "+    workflow.set_entry_point(\"search\")\n",
        "+    workflow.add_edge(\"search\", \"generate\")\n",
        "+    workflow.add_edge(\"generate\", END)\n",
        "+    \n",
        "     return workflow.compile()\n",
        " \n",
        " psychology_agent = create_psychology_agent_graph()\n"
      ]
    },
    {
      "path": "requirements.txt",
      "status": "modified",
      "additions": 0,
      "deletions": 1,
      "patch": "@@ -16,7 +16,6 @@ tqdm\n \n # Flask API\n flask\n-flask-cors\n \n # PDF Processing (for upsert_pdfs.py)\n pypdf2",
      "patch_lines": [
        "@@ -16,7 +16,6 @@ tqdm\n",
        " \n",
        " # Flask API\n",
        " flask\n",
        "-flask-cors\n",
        " \n",
        " # PDF Processing (for upsert_pdfs.py)\n",
        " pypdf2\n"
      ]
    },
    {
      "path": "tools.py",
      "status": "modified",
      "additions": 4,
      "deletions": 6,
      "patch": "@@ -3,10 +3,10 @@\n from conf import embedding_client, qdrant_client, QDRANT_COLLECTION_NAME, EMBEDDING_MODEL\n \n @tool\n-def search_psychology_knowledge_base(query: str, top_k: int = 8) -> List[Dict[str, Any]]:\n+def search_psychology_knowledge_base(query: str) -> List[Dict[str, Any]]:\n     \"\"\"\n     Searches the psychology knowledge base in Qdrant for the most relevant results for the user's psychology-related question.\n-    Returns the top 'top_k' results.\n+    Returns the top 8 results with document text, source title, and page number.\n     \"\"\"\n     try:\n         # Convert the user query to a vector using the embedding model\n@@ -19,7 +19,7 @@ def search_psychology_knowledge_base(query: str, top_k: int = 8) -> List[Dict[st\n         search_results = qdrant_client.search(\n             collection_name=QDRANT_COLLECTION_NAME,\n             query_vector=query_embedding,\n-            limit=top_k,\n+            limit=8,\n             with_payload=True\n         )\n \n@@ -30,7 +30,7 @@ def search_psychology_knowledge_base(query: str, top_k: int = 8) -> List[Dict[st\n                 formatted_results.append({\n                     \"score\": result.score,\n                     \"document\": result.payload.get(\"text\", \"\"),\n-                    \"source\": result.payload.get(\"source_title\", \"Bilinmeyen Kaynak\"),\n+                    \"source\": result.payload.get(\"source_title\", \"Unknown Source\"),\n                     \"page\": result.payload.get(\"page_number\", 0)\n                 })\n         \n@@ -39,6 +39,4 @@ def search_psychology_knowledge_base(query: str, top_k: int = 8) -> List[Dict[st\n     except Exception as e:\n         print(f\"An error occurred while searching the knowledge base: {e}\")\n         return [{\"error\": \"An error occurred while searching the knowledge base.\"}]\n-# Add the search tool to the list. LangGraph will use this list.\n-tools = [search_psychology_knowledge_base]\n ",
      "patch_lines": [
        "@@ -3,10 +3,10 @@\n",
        " from conf import embedding_client, qdrant_client, QDRANT_COLLECTION_NAME, EMBEDDING_MODEL\n",
        " \n",
        " @tool\n",
        "-def search_psychology_knowledge_base(query: str, top_k: int = 8) -> List[Dict[str, Any]]:\n",
        "+def search_psychology_knowledge_base(query: str) -> List[Dict[str, Any]]:\n",
        "     \"\"\"\n",
        "     Searches the psychology knowledge base in Qdrant for the most relevant results for the user's psychology-related question.\n",
        "-    Returns the top 'top_k' results.\n",
        "+    Returns the top 8 results with document text, source title, and page number.\n",
        "     \"\"\"\n",
        "     try:\n",
        "         # Convert the user query to a vector using the embedding model\n",
        "@@ -19,7 +19,7 @@ def search_psychology_knowledge_base(query: str, top_k: int = 8) -> List[Dict[st\n",
        "         search_results = qdrant_client.search(\n",
        "             collection_name=QDRANT_COLLECTION_NAME,\n",
        "             query_vector=query_embedding,\n",
        "-            limit=top_k,\n",
        "+            limit=8,\n",
        "             with_payload=True\n",
        "         )\n",
        " \n",
        "@@ -30,7 +30,7 @@ def search_psychology_knowledge_base(query: str, top_k: int = 8) -> List[Dict[st\n",
        "                 formatted_results.append({\n",
        "                     \"score\": result.score,\n",
        "                     \"document\": result.payload.get(\"text\", \"\"),\n",
        "-                    \"source\": result.payload.get(\"source_title\", \"Bilinmeyen Kaynak\"),\n",
        "+                    \"source\": result.payload.get(\"source_title\", \"Unknown Source\"),\n",
        "                     \"page\": result.payload.get(\"page_number\", 0)\n",
        "                 })\n",
        "         \n",
        "@@ -39,6 +39,4 @@ def search_psychology_knowledge_base(query: str, top_k: int = 8) -> List[Dict[st\n",
        "     except Exception as e:\n",
        "         print(f\"An error occurred while searching the knowledge base: {e}\")\n",
        "         return [{\"error\": \"An error occurred while searching the knowledge base.\"}]\n",
        "-# Add the search tool to the list. LangGraph will use this list.\n",
        "-tools = [search_psychology_knowledge_base]\n",
        " \n"
      ]
    }
  ]
}
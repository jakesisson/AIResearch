{
  "project": "Research Data/RAGveda",
  "repo": "parth1609/RAGveda",
  "prior_commit": "1de749ce978d3c0911743ba07dbf89dbba3088db",
  "researched_commit": "a11d83969234368fd6c7a4a3118a23177b5e7ec8",
  "compare_url": "https://github.com/parth1609/RAGveda/compare/1de749ce978d3c0911743ba07dbf89dbba3088db...a11d83969234368fd6c7a4a3118a23177b5e7ec8",
  "ahead_by": 1,
  "behind_by": 0,
  "changed_files": [
    {
      "path": "langgraph_ragveda.py",
      "status": "modified",
      "additions": 41,
      "deletions": 38,
      "patch": "@@ -1,16 +1,20 @@\n \"\"\"\n RAGVeda LangGraph Implementation.\n Converts the RAGVeda application to use LangGraph for orchestration.\n+\n+Update: Memory is now handled via LangGraph's message-based state rather than a\n+custom summarization manager. We adopt MessagesState/add_messages so each turn\n+adds Human/AI messages directly to state, enabling downstream nodes to access\n+chat history without bespoke memory code.\n \"\"\"\n \n from typing import Literal, List, Dict, Any, Optional\n-from typing_extensions import TypedDict, Annotated\n-import operator\n+from typing_extensions import TypedDict\n import tempfile\n from pathlib import Path\n import pandas as pd\n \n-from langgraph.graph import StateGraph, START, END\n+from langgraph.graph import StateGraph, START, END, MessagesState\n from langchain_core.messages import AnyMessage, HumanMessage, AIMessage\n from langchain_core.documents import Document\n \n@@ -24,11 +28,15 @@\n \n # ==================== STATE DEFINITION ====================\n \n-class RAGVedaState(TypedDict):\n-    \"\"\"Global state that flows through all nodes.\"\"\"\n-    \n+class RAGVedaState(MessagesState):\n+    \"\"\"Global state that flows through all nodes.\n+\n+    Inherits from MessagesState to leverage LangGraph's built-in message\n+    reducer (add_messages). The \"messages\" field is provided by the parent\n+    class and accumulates LangChain messages (HumanMessage/AIMessage).\n+    \"\"\"\n+\n     # User interaction\n-    messages: Annotated[list[AnyMessage], operator.add]\n     user_query: str\n     \n     # File processing\n@@ -99,14 +107,12 @@ def initialize_services(state: RAGVedaState) -> Dict[str, Any]:\n         # Initialize LLM Chain\n         llm_chain = LLMChain()\n         \n-        # Initialize Memory Manager if enabled\n-        memory_manager = MemoryManager() if Config.MEMORY_ENABLED else None\n-        \n         return {\n             \"neo4j_manager\": neo4j_manager,\n             \"document_processor\": document_processor,\n             \"llm_chain\": llm_chain,\n-            \"memory_manager\": memory_manager,\n+            # Memory is handled via MessagesState; keep keys for backward-compat\n+            \"memory_manager\": None,\n             \"memory_enabled\": Config.MEMORY_ENABLED,\n             \"neo4j_connected\": True,\n             \"current_step\": \"services_initialized\",\n@@ -355,37 +361,31 @@ def handle_no_results(state: RAGVedaState) -> Dict[str, Any]:\n \n # ==================== MEMORY MANAGEMENT NODES ====================\n \n-def get_memory_context(state: RAGVedaState) -> Dict[str, Any]:\n-    \"\"\"Get conversation memory context if available.\"\"\"\n-    memory_manager = state.get(\"memory_manager\")\n-    \n-    if memory_manager and memory_manager.is_available():\n-        memory_context = memory_manager.get_memory_context()\n-        return {\n-            \"memory_context\": memory_context,\n-            \"current_step\": \"memory_retrieved\"\n-        }\n-    \n+def append_user_message(state: RAGVedaState) -> Dict[str, Any]:\n+    \"\"\"Append the current user query to messages using LangGraph reducer.\"\"\"\n     return {\n-        \"memory_context\": \"\",\n-        \"current_step\": \"memory_skipped\"\n+        \"messages\": [HumanMessage(content=state[\"user_query\"])],\n+        \"current_step\": \"user_message_appended\",\n     }\n \n+def get_memory_context(state: RAGVedaState) -> Dict[str, Any]:\n+    \"\"\"No-op memory context: we rely on message history directly in state.\n+\n+    Left in place for backward compatibility with code paths expecting a\n+    \"memory_context\" field. Always returns an empty string.\n+    \"\"\"\n+    return {\"memory_context\": \"\", \"current_step\": \"memory_skipped\"}\n+\n \n def save_to_memory(state: RAGVedaState) -> Dict[str, Any]:\n-    \"\"\"Save conversation turn to memory.\"\"\"\n-    memory_manager = state.get(\"memory_manager\")\n-    \n-    if memory_manager and memory_manager.is_available():\n-        response_text = state[\"llm_response\"].get(\"text\", \"\")\n-        memory_manager.save_conversation_turn(\n-            state[\"user_query\"],\n-            response_text\n-        )\n-    \n-    return {\n-        \"current_step\": \"memory_saved\"\n-    }\n+    \"\"\"Append assistant response to messages using the built-in reducer.\"\"\"\n+    response_text = state.get(\"llm_response\", {}).get(\"text\", \"\")\n+    if response_text:\n+        return {\n+            \"messages\": [AIMessage(content=response_text)],\n+            \"current_step\": \"memory_saved\",\n+        }\n+    return {\"current_step\": \"memory_saved\"}\n \n \n # ==================== RESPONSE GENERATION NODES ====================\n@@ -494,6 +494,7 @@ def build_query_processing_graph() -> StateGraph:\n     builder = StateGraph(RAGVedaState)\n     \n     # Add nodes\n+    builder.add_node(\"append_user_message\", append_user_message)\n     builder.add_node(\"check_query_complexity\", check_query_complexity)\n     builder.add_node(\"rewrite_query\", rewrite_query)\n     builder.add_node(\"retrieve_documents\", retrieve_documents)\n@@ -505,7 +506,8 @@ def build_query_processing_graph() -> StateGraph:\n     builder.add_node(\"save_to_memory\", save_to_memory)\n     \n     # Add edges\n-    builder.add_edge(START, \"check_query_complexity\")\n+    builder.add_edge(START, \"append_user_message\")\n+    builder.add_edge(\"append_user_message\", \"check_query_complexity\")\n     \n     # Conditional routing for query rewriting\n     builder.add_conditional_edges(\n@@ -557,6 +559,7 @@ def build_complete_ragveda_graph() -> StateGraph:\n     builder.add_node(\"create_file_relationships\", create_file_relationships)\n     \n     # Query processing nodes\n+    builder.add_node(\"append_user_message\", append_user_message)\n     builder.add_node(\"check_query_complexity\", check_query_complexity)\n     builder.add_node(\"rewrite_query\", rewrite_query)\n     builder.add_node(\"retrieve_documents\", retrieve_documents)",
      "patch_lines": [
        "@@ -1,16 +1,20 @@\n",
        " \"\"\"\n",
        " RAGVeda LangGraph Implementation.\n",
        " Converts the RAGVeda application to use LangGraph for orchestration.\n",
        "+\n",
        "+Update: Memory is now handled via LangGraph's message-based state rather than a\n",
        "+custom summarization manager. We adopt MessagesState/add_messages so each turn\n",
        "+adds Human/AI messages directly to state, enabling downstream nodes to access\n",
        "+chat history without bespoke memory code.\n",
        " \"\"\"\n",
        " \n",
        " from typing import Literal, List, Dict, Any, Optional\n",
        "-from typing_extensions import TypedDict, Annotated\n",
        "-import operator\n",
        "+from typing_extensions import TypedDict\n",
        " import tempfile\n",
        " from pathlib import Path\n",
        " import pandas as pd\n",
        " \n",
        "-from langgraph.graph import StateGraph, START, END\n",
        "+from langgraph.graph import StateGraph, START, END, MessagesState\n",
        " from langchain_core.messages import AnyMessage, HumanMessage, AIMessage\n",
        " from langchain_core.documents import Document\n",
        " \n",
        "@@ -24,11 +28,15 @@\n",
        " \n",
        " # ==================== STATE DEFINITION ====================\n",
        " \n",
        "-class RAGVedaState(TypedDict):\n",
        "-    \"\"\"Global state that flows through all nodes.\"\"\"\n",
        "-    \n",
        "+class RAGVedaState(MessagesState):\n",
        "+    \"\"\"Global state that flows through all nodes.\n",
        "+\n",
        "+    Inherits from MessagesState to leverage LangGraph's built-in message\n",
        "+    reducer (add_messages). The \"messages\" field is provided by the parent\n",
        "+    class and accumulates LangChain messages (HumanMessage/AIMessage).\n",
        "+    \"\"\"\n",
        "+\n",
        "     # User interaction\n",
        "-    messages: Annotated[list[AnyMessage], operator.add]\n",
        "     user_query: str\n",
        "     \n",
        "     # File processing\n",
        "@@ -99,14 +107,12 @@ def initialize_services(state: RAGVedaState) -> Dict[str, Any]:\n",
        "         # Initialize LLM Chain\n",
        "         llm_chain = LLMChain()\n",
        "         \n",
        "-        # Initialize Memory Manager if enabled\n",
        "-        memory_manager = MemoryManager() if Config.MEMORY_ENABLED else None\n",
        "-        \n",
        "         return {\n",
        "             \"neo4j_manager\": neo4j_manager,\n",
        "             \"document_processor\": document_processor,\n",
        "             \"llm_chain\": llm_chain,\n",
        "-            \"memory_manager\": memory_manager,\n",
        "+            # Memory is handled via MessagesState; keep keys for backward-compat\n",
        "+            \"memory_manager\": None,\n",
        "             \"memory_enabled\": Config.MEMORY_ENABLED,\n",
        "             \"neo4j_connected\": True,\n",
        "             \"current_step\": \"services_initialized\",\n",
        "@@ -355,37 +361,31 @@ def handle_no_results(state: RAGVedaState) -> Dict[str, Any]:\n",
        " \n",
        " # ==================== MEMORY MANAGEMENT NODES ====================\n",
        " \n",
        "-def get_memory_context(state: RAGVedaState) -> Dict[str, Any]:\n",
        "-    \"\"\"Get conversation memory context if available.\"\"\"\n",
        "-    memory_manager = state.get(\"memory_manager\")\n",
        "-    \n",
        "-    if memory_manager and memory_manager.is_available():\n",
        "-        memory_context = memory_manager.get_memory_context()\n",
        "-        return {\n",
        "-            \"memory_context\": memory_context,\n",
        "-            \"current_step\": \"memory_retrieved\"\n",
        "-        }\n",
        "-    \n",
        "+def append_user_message(state: RAGVedaState) -> Dict[str, Any]:\n",
        "+    \"\"\"Append the current user query to messages using LangGraph reducer.\"\"\"\n",
        "     return {\n",
        "-        \"memory_context\": \"\",\n",
        "-        \"current_step\": \"memory_skipped\"\n",
        "+        \"messages\": [HumanMessage(content=state[\"user_query\"])],\n",
        "+        \"current_step\": \"user_message_appended\",\n",
        "     }\n",
        " \n",
        "+def get_memory_context(state: RAGVedaState) -> Dict[str, Any]:\n",
        "+    \"\"\"No-op memory context: we rely on message history directly in state.\n",
        "+\n",
        "+    Left in place for backward compatibility with code paths expecting a\n",
        "+    \"memory_context\" field. Always returns an empty string.\n",
        "+    \"\"\"\n",
        "+    return {\"memory_context\": \"\", \"current_step\": \"memory_skipped\"}\n",
        "+\n",
        " \n",
        " def save_to_memory(state: RAGVedaState) -> Dict[str, Any]:\n",
        "-    \"\"\"Save conversation turn to memory.\"\"\"\n",
        "-    memory_manager = state.get(\"memory_manager\")\n",
        "-    \n",
        "-    if memory_manager and memory_manager.is_available():\n",
        "-        response_text = state[\"llm_response\"].get(\"text\", \"\")\n",
        "-        memory_manager.save_conversation_turn(\n",
        "-            state[\"user_query\"],\n",
        "-            response_text\n",
        "-        )\n",
        "-    \n",
        "-    return {\n",
        "-        \"current_step\": \"memory_saved\"\n",
        "-    }\n",
        "+    \"\"\"Append assistant response to messages using the built-in reducer.\"\"\"\n",
        "+    response_text = state.get(\"llm_response\", {}).get(\"text\", \"\")\n",
        "+    if response_text:\n",
        "+        return {\n",
        "+            \"messages\": [AIMessage(content=response_text)],\n",
        "+            \"current_step\": \"memory_saved\",\n",
        "+        }\n",
        "+    return {\"current_step\": \"memory_saved\"}\n",
        " \n",
        " \n",
        " # ==================== RESPONSE GENERATION NODES ====================\n",
        "@@ -494,6 +494,7 @@ def build_query_processing_graph() -> StateGraph:\n",
        "     builder = StateGraph(RAGVedaState)\n",
        "     \n",
        "     # Add nodes\n",
        "+    builder.add_node(\"append_user_message\", append_user_message)\n",
        "     builder.add_node(\"check_query_complexity\", check_query_complexity)\n",
        "     builder.add_node(\"rewrite_query\", rewrite_query)\n",
        "     builder.add_node(\"retrieve_documents\", retrieve_documents)\n",
        "@@ -505,7 +506,8 @@ def build_query_processing_graph() -> StateGraph:\n",
        "     builder.add_node(\"save_to_memory\", save_to_memory)\n",
        "     \n",
        "     # Add edges\n",
        "-    builder.add_edge(START, \"check_query_complexity\")\n",
        "+    builder.add_edge(START, \"append_user_message\")\n",
        "+    builder.add_edge(\"append_user_message\", \"check_query_complexity\")\n",
        "     \n",
        "     # Conditional routing for query rewriting\n",
        "     builder.add_conditional_edges(\n",
        "@@ -557,6 +559,7 @@ def build_complete_ragveda_graph() -> StateGraph:\n",
        "     builder.add_node(\"create_file_relationships\", create_file_relationships)\n",
        "     \n",
        "     # Query processing nodes\n",
        "+    builder.add_node(\"append_user_message\", append_user_message)\n",
        "     builder.add_node(\"check_query_complexity\", check_query_complexity)\n",
        "     builder.add_node(\"rewrite_query\", rewrite_query)\n",
        "     builder.add_node(\"retrieve_documents\", retrieve_documents)\n"
      ]
    },
    {
      "path": "main.py",
      "status": "modified",
      "additions": 3,
      "deletions": 3,
      "patch": "@@ -288,7 +288,7 @@ def run(self):\n         \"\"\"Run the main application.\"\"\"\n         # Page configuration\n         st.set_page_config(\n-            page_title=\"RAGVeda LangGraph\",\n+            page_title=\"RAGVeda\",\n             page_icon=\"\ud83d\udd49\ufe0f\",\n             layout=\"wide\",\n             initial_sidebar_state=\"expanded\"\n@@ -302,7 +302,7 @@ def run(self):\n         \n         # Render header\n         self.ui.render_header(\n-            title=\"RAGVeda LangGraph\",\n+            title=\"RAGVeda\",\n             subtitle=\"Intelligent Semantic Search powered by LangGraph & Neo4j\"\n         )\n         \n@@ -377,7 +377,7 @@ def run(self):\n         # Main content area\n         if st.session_state.embeddings_created:\n             # Chat interface\n-            st.markdown('<h2 class=\"sub-header\">\ud83d\udcac Chat with LangGraph</h2>', unsafe_allow_html=True)\n+            st.markdown('<h2 class=\"sub-header\">\ud83d\udcac Ask anything</h2>', unsafe_allow_html=True)\n             self.handle_chat_with_graph()\n         else:\n             # Welcome screen",
      "patch_lines": [
        "@@ -288,7 +288,7 @@ def run(self):\n",
        "         \"\"\"Run the main application.\"\"\"\n",
        "         # Page configuration\n",
        "         st.set_page_config(\n",
        "-            page_title=\"RAGVeda LangGraph\",\n",
        "+            page_title=\"RAGVeda\",\n",
        "             page_icon=\"\ud83d\udd49\ufe0f\",\n",
        "             layout=\"wide\",\n",
        "             initial_sidebar_state=\"expanded\"\n",
        "@@ -302,7 +302,7 @@ def run(self):\n",
        "         \n",
        "         # Render header\n",
        "         self.ui.render_header(\n",
        "-            title=\"RAGVeda LangGraph\",\n",
        "+            title=\"RAGVeda\",\n",
        "             subtitle=\"Intelligent Semantic Search powered by LangGraph & Neo4j\"\n",
        "         )\n",
        "         \n",
        "@@ -377,7 +377,7 @@ def run(self):\n",
        "         # Main content area\n",
        "         if st.session_state.embeddings_created:\n",
        "             # Chat interface\n",
        "-            st.markdown('<h2 class=\"sub-header\">\ud83d\udcac Chat with LangGraph</h2>', unsafe_allow_html=True)\n",
        "+            st.markdown('<h2 class=\"sub-header\">\ud83d\udcac Ask anything</h2>', unsafe_allow_html=True)\n",
        "             self.handle_chat_with_graph()\n",
        "         else:\n",
        "             # Welcome screen\n"
      ]
    },
    {
      "path": "modules/memory_manager.py",
      "status": "modified",
      "additions": 110,
      "deletions": 170,
      "patch": "@@ -1,184 +1,124 @@\n \"\"\"\n-Memory Manager Module.\n-Handles conversation memory using LangChain's ConversationSummaryMemory.\n+Memory Manager Module (Shim).\n+\n+Purpose:\n+- This module now serves as a lightweight adapter to support message-based\n+  memory using LangGraph's MessagesState and add_messages reducer, as described\n+  in the LangGraph Graph API documentation (\"Using Messages in your graph\").\n+- The previous summarization-based memory storage has been deprecated to avoid\n+  duplicating functionality that is already built into LangGraph.\n+\n+Notes for developers:\n+- Conversation history should be passed through the graph via the `messages`\n+  field (list of LangChain `Message` objects or their serialized dict form).\n+- This shim preserves the class and method surface so existing imports do not\n+  break, but most methods are now no-ops.\n \"\"\"\n \n from typing import Optional, Dict, Any, List\n-from langchain_core.messages import HumanMessage, AIMessage\n-from langchain_groq import ChatGroq\n-from modules.config import Config\n \n \n class MemoryManager:\n-    \"\"\"Manages conversation memory with summarization.\"\"\"\n-    \n-    def __init__(self, llm: Optional[ChatGroq] = None):\n-        \"\"\"\n-        Initialize memory manager.\n-        \n-        Args:\n-            llm: Optional LLM instance for summarization\n-        \"\"\"\n-        self.llm = llm or self._create_llm()\n-        self.memory = None\n-        self._reset_memory()\n-    \n-    def _create_llm(self) -> Optional[ChatGroq]:\n-        \"\"\"Create LLM instance for memory summarization.\"\"\"\n-        try:\n-            return ChatGroq(\n-                groq_api_key=Config.GROQ_API_KEY,\n-                model_name=Config.LLM_MODEL,\n-                temperature=Config.LLM_TEMPERATURE\n-            )\n-        except Exception as e:\n-            print(f\"Failed to create LLM for memory: {e}\")\n-            return None\n-    \n-    def _reset_memory(self):\n-        \"\"\"Reset memory to empty state.\"\"\"\n-        self.memory = {\n-            \"turns\": [],\n-            \"summary\": \"\"\n-        }\n-    \n-    def save_conversation_turn(self, human_input: str, ai_response: str):\n-        \"\"\"\n-        Save a conversation turn to memory.\n-        \n-        Args:\n-            human_input: User's message\n-            ai_response: Assistant's response\n+    \"\"\"Shim over message-based memory.\n+\n+    This class exists for backward compatibility with previous code paths that\n+    imported and used a MemoryManager. It no longer stores or summarizes\n+    conversation state. Instead, use LangGraph's `messages` in the graph state.\n+    \"\"\"\n+\n+    def __init__(self, *_args: Any, **_kwargs: Any):\n+        \"\"\"Initialize the shim. No services or state are created.\"\"\"\n+        self._deprecated = True\n+\n+    def save_conversation_turn(self, human_input: str, ai_response: str) -> None:\n+        \"\"\"No-op. Kept for API compatibility.\n+\n+        Parameters:\n+            human_input (str): User message text.\n+            ai_response (str): Assistant message text.\n+        Side Effects:\n+            None.\n         \"\"\"\n-        if not self.memory:\n-            return\n-        \n-        try:\n-            # Add turn to memory\n-            self.memory[\"turns\"].append({\n-                \"human\": human_input,\n-                \"ai\": ai_response\n-            })\n-            \n-            # If we have too many turns, create a summary\n-            if len(self.memory[\"turns\"]) > Config.MEMORY_MAX_TURNS_BEFORE_SUMMARY:\n-                self._create_summary()\n-        except Exception as e:\n-            print(f\"Failed to save conversation to memory: {e}\")\n-    \n+        return None\n+\n     def get_memory_context(self) -> str:\n+        \"\"\"Return an empty string to indicate no external memory context.\n+\n+        Return:\n+            str: Always \"\". Graph nodes should rely on `messages` instead.\n         \"\"\"\n-        Get the current memory context (summary).\n-        \n-        Returns:\n-            Memory context string\n-        \"\"\"\n-        if not self.memory:\n-            return \"\"\n-        \n-        try:\n-            # Return summary if available, otherwise recent turns\n-            if self.memory[\"summary\"]:\n-                return self.memory[\"summary\"]\n-            \n-            # If no summary yet, return recent conversation turns\n-            if len(self.memory[\"turns\"]) > 0:\n-                recent_turns = self.memory[\"turns\"][-3:]  # Last 3 turns\n-                context_parts = []\n-                for turn in recent_turns:\n-                    context_parts.append(f\"Human: {turn['human'][:100]}...\")\n-                    context_parts.append(f\"AI: {turn['ai'][:100]}...\")\n-                return \"\\n\".join(context_parts)\n-            \n-            return \"\"\n-        except Exception as e:\n-            print(f\"Failed to load memory context: {e}\")\n-            return \"\"\n-    \n-    def _create_summary(self):\n-        \"\"\"Create a summary of the conversation turns.\"\"\"\n-        if not self.llm or len(self.memory[\"turns\"]) == 0:\n-            return\n-        \n-        try:\n-            # Format conversation turns for summarization\n-            conversation_text = []\n-            for turn in self.memory[\"turns\"]:\n-                conversation_text.append(f\"Human: {turn['human']}\")\n-                conversation_text.append(f\"AI: {turn['ai']}\")\n-            \n-            full_conversation = \"\\n\".join(conversation_text)\n-            \n-            # Create summary using LLM\n-            summary_prompt = f\"\"\"Summarize this conversation concisely in 2-3 sentences, focusing on key topics discussed:\n-\n-{full_conversation}\n-\n-Summary:\"\"\"\n-            \n-            summary_response = self.llm.invoke(summary_prompt)\n-            self.memory[\"summary\"] = summary_response.content if hasattr(summary_response, 'content') else str(summary_response)\n-            \n-            # Keep only the most recent turns after summarizing\n-            self.memory[\"turns\"] = self.memory[\"turns\"][-2:]  # Keep last 2 turns\n-            \n-        except Exception as e:\n-            print(f\"Failed to create memory summary: {e}\")\n-    \n-    def clear_memory(self):\n-        \"\"\"Clear all memory contents.\"\"\"\n-        if self.memory:\n-            try:\n-                self.memory[\"turns\"] = []\n-                self.memory[\"summary\"] = \"\"\n-            except Exception as e:\n-                print(f\"Failed to clear memory: {e}\")\n-    \n-    def reset_session(self):\n-        \"\"\"Reset memory for new session.\"\"\"\n-        self._reset_memory()\n-    \n+        return \"\"\n+\n+    def clear_memory(self) -> None:\n+        \"\"\"No-op. Left for backward compatibility.\"\"\"\n+        return None\n+\n+    def reset_session(self) -> None:\n+        \"\"\"No-op. Left for backward compatibility.\"\"\"\n+        return None\n+\n     def is_available(self) -> bool:\n-        \"\"\"Check if memory is available.\"\"\"\n-        return self.memory is not None\n-    \n+        \"\"\"Indicate that external memory storage is not used.\n+\n+        Return:\n+            bool: Always False.\n+        \"\"\"\n+        return False\n+\n     def get_buffer_string(self) -> str:\n-        \"\"\"Get the raw memory buffer for debugging.\"\"\"\n-        if not self.memory:\n-            return \"\"\n-        \n-        try:\n-            return getattr(self.memory, 'buffer', '')\n-        except:\n-            return \"\"\n-    \n-    def from_chat_history(self, chat_history: List[Dict[str, Any]]):\n+        \"\"\"No-op buffer accessor kept for compatibility.\n+\n+        Return:\n+            str: Always \"\".\n         \"\"\"\n-        Initialize memory from existing chat history.\n-        \n-        Args:\n-            chat_history: List of chat messages with 'role' and 'content' keys\n+        return \"\"\n+\n+    # ---------- Adapters for UI <-> Graph messages ----------\n+    def to_messages(self, chat_history: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n+        \"\"\"Convert UI chat history to LangGraph-serializable message dicts.\n+\n+        Parameters:\n+            chat_history (List[Dict[str, Any]]): Items with keys 'role' and 'content'.\n+                For assistant turns, 'content' may be a dict with 'text'.\n+\n+        Return:\n+            List[Dict[str, Any]]: Each item has keys 'type' ('human'|'ai') and 'content' (str).\n+\n+        Example:\n+            >>> mgr = MemoryManager()\n+            >>> mgr.to_messages([\n+            ...   {'role': 'user', 'content': 'Hi'},\n+            ...   {'role': 'assistant', 'content': {'text': 'Hello'}}\n+            ... ])\n+            [{'type': 'human', 'content': 'Hi'}, {'type': 'ai', 'content': 'Hello'}]\n+        \"\"\"\n+        msgs: List[Dict[str, Any]] = []\n+        for m in chat_history or []:\n+            role = (m.get('role') or '').strip().lower()\n+            content = m.get('content', '')\n+            if role == 'assistant' and isinstance(content, dict):\n+                content = content.get('text', str(content))\n+            if role == 'user':\n+                msgs.append({\"type\": \"human\", \"content\": str(content)})\n+            elif role == 'assistant':\n+                msgs.append({\"type\": \"ai\", \"content\": str(content)})\n+        return msgs\n+\n+    def from_messages(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n+        \"\"\"Convert LangGraph message dicts back to simple UI chat entries.\n+\n+        Parameters:\n+            messages (List[Dict[str, Any]]): Items with keys 'type' and 'content'.\n+\n+        Return:\n+            List[Dict[str, Any]]: Items with 'role' and 'content'.\n         \"\"\"\n-        if not self.memory or not chat_history:\n-            return\n-        \n-        try:\n-            # Process chat history in pairs (user + assistant)\n-            for i in range(0, len(chat_history) - 1, 2):\n-                if i + 1 < len(chat_history):\n-                    user_msg = chat_history[i]\n-                    ai_msg = chat_history[i + 1]\n-                    \n-                    if (user_msg.get('role') == 'user' and \n-                        ai_msg.get('role') == 'assistant'):\n-                        \n-                        user_content = user_msg.get('content', '')\n-                        ai_content = ai_msg.get('content', '')\n-                        \n-                        # Handle AI content that might be a dict (with text and references)\n-                        if isinstance(ai_content, dict):\n-                            ai_content = ai_content.get('text', str(ai_content))\n-                        \n-                        self.save_conversation_turn(str(user_content), str(ai_content))\n-        except Exception as e:\n-            print(f\"Failed to initialize memory from chat history: {e}\")\n+        ui: List[Dict[str, Any]] = []\n+        for m in messages or []:\n+            t = (m.get('type') or '').strip().lower()\n+            c = m.get('content', '')\n+            if t == 'human':\n+                ui.append({'role': 'user', 'content': c})\n+            elif t == 'ai':\n+                ui.append({'role': 'assistant', 'content': {'text': c}})\n+        return ui",
      "patch_lines": [
        "@@ -1,184 +1,124 @@\n",
        " \"\"\"\n",
        "-Memory Manager Module.\n",
        "-Handles conversation memory using LangChain's ConversationSummaryMemory.\n",
        "+Memory Manager Module (Shim).\n",
        "+\n",
        "+Purpose:\n",
        "+- This module now serves as a lightweight adapter to support message-based\n",
        "+  memory using LangGraph's MessagesState and add_messages reducer, as described\n",
        "+  in the LangGraph Graph API documentation (\"Using Messages in your graph\").\n",
        "+- The previous summarization-based memory storage has been deprecated to avoid\n",
        "+  duplicating functionality that is already built into LangGraph.\n",
        "+\n",
        "+Notes for developers:\n",
        "+- Conversation history should be passed through the graph via the `messages`\n",
        "+  field (list of LangChain `Message` objects or their serialized dict form).\n",
        "+- This shim preserves the class and method surface so existing imports do not\n",
        "+  break, but most methods are now no-ops.\n",
        " \"\"\"\n",
        " \n",
        " from typing import Optional, Dict, Any, List\n",
        "-from langchain_core.messages import HumanMessage, AIMessage\n",
        "-from langchain_groq import ChatGroq\n",
        "-from modules.config import Config\n",
        " \n",
        " \n",
        " class MemoryManager:\n",
        "-    \"\"\"Manages conversation memory with summarization.\"\"\"\n",
        "-    \n",
        "-    def __init__(self, llm: Optional[ChatGroq] = None):\n",
        "-        \"\"\"\n",
        "-        Initialize memory manager.\n",
        "-        \n",
        "-        Args:\n",
        "-            llm: Optional LLM instance for summarization\n",
        "-        \"\"\"\n",
        "-        self.llm = llm or self._create_llm()\n",
        "-        self.memory = None\n",
        "-        self._reset_memory()\n",
        "-    \n",
        "-    def _create_llm(self) -> Optional[ChatGroq]:\n",
        "-        \"\"\"Create LLM instance for memory summarization.\"\"\"\n",
        "-        try:\n",
        "-            return ChatGroq(\n",
        "-                groq_api_key=Config.GROQ_API_KEY,\n",
        "-                model_name=Config.LLM_MODEL,\n",
        "-                temperature=Config.LLM_TEMPERATURE\n",
        "-            )\n",
        "-        except Exception as e:\n",
        "-            print(f\"Failed to create LLM for memory: {e}\")\n",
        "-            return None\n",
        "-    \n",
        "-    def _reset_memory(self):\n",
        "-        \"\"\"Reset memory to empty state.\"\"\"\n",
        "-        self.memory = {\n",
        "-            \"turns\": [],\n",
        "-            \"summary\": \"\"\n",
        "-        }\n",
        "-    \n",
        "-    def save_conversation_turn(self, human_input: str, ai_response: str):\n",
        "-        \"\"\"\n",
        "-        Save a conversation turn to memory.\n",
        "-        \n",
        "-        Args:\n",
        "-            human_input: User's message\n",
        "-            ai_response: Assistant's response\n",
        "+    \"\"\"Shim over message-based memory.\n",
        "+\n",
        "+    This class exists for backward compatibility with previous code paths that\n",
        "+    imported and used a MemoryManager. It no longer stores or summarizes\n",
        "+    conversation state. Instead, use LangGraph's `messages` in the graph state.\n",
        "+    \"\"\"\n",
        "+\n",
        "+    def __init__(self, *_args: Any, **_kwargs: Any):\n",
        "+        \"\"\"Initialize the shim. No services or state are created.\"\"\"\n",
        "+        self._deprecated = True\n",
        "+\n",
        "+    def save_conversation_turn(self, human_input: str, ai_response: str) -> None:\n",
        "+        \"\"\"No-op. Kept for API compatibility.\n",
        "+\n",
        "+        Parameters:\n",
        "+            human_input (str): User message text.\n",
        "+            ai_response (str): Assistant message text.\n",
        "+        Side Effects:\n",
        "+            None.\n",
        "         \"\"\"\n",
        "-        if not self.memory:\n",
        "-            return\n",
        "-        \n",
        "-        try:\n",
        "-            # Add turn to memory\n",
        "-            self.memory[\"turns\"].append({\n",
        "-                \"human\": human_input,\n",
        "-                \"ai\": ai_response\n",
        "-            })\n",
        "-            \n",
        "-            # If we have too many turns, create a summary\n",
        "-            if len(self.memory[\"turns\"]) > Config.MEMORY_MAX_TURNS_BEFORE_SUMMARY:\n",
        "-                self._create_summary()\n",
        "-        except Exception as e:\n",
        "-            print(f\"Failed to save conversation to memory: {e}\")\n",
        "-    \n",
        "+        return None\n",
        "+\n",
        "     def get_memory_context(self) -> str:\n",
        "+        \"\"\"Return an empty string to indicate no external memory context.\n",
        "+\n",
        "+        Return:\n",
        "+            str: Always \"\". Graph nodes should rely on `messages` instead.\n",
        "         \"\"\"\n",
        "-        Get the current memory context (summary).\n",
        "-        \n",
        "-        Returns:\n",
        "-            Memory context string\n",
        "-        \"\"\"\n",
        "-        if not self.memory:\n",
        "-            return \"\"\n",
        "-        \n",
        "-        try:\n",
        "-            # Return summary if available, otherwise recent turns\n",
        "-            if self.memory[\"summary\"]:\n",
        "-                return self.memory[\"summary\"]\n",
        "-            \n",
        "-            # If no summary yet, return recent conversation turns\n",
        "-            if len(self.memory[\"turns\"]) > 0:\n",
        "-                recent_turns = self.memory[\"turns\"][-3:]  # Last 3 turns\n",
        "-                context_parts = []\n",
        "-                for turn in recent_turns:\n",
        "-                    context_parts.append(f\"Human: {turn['human'][:100]}...\")\n",
        "-                    context_parts.append(f\"AI: {turn['ai'][:100]}...\")\n",
        "-                return \"\\n\".join(context_parts)\n",
        "-            \n",
        "-            return \"\"\n",
        "-        except Exception as e:\n",
        "-            print(f\"Failed to load memory context: {e}\")\n",
        "-            return \"\"\n",
        "-    \n",
        "-    def _create_summary(self):\n",
        "-        \"\"\"Create a summary of the conversation turns.\"\"\"\n",
        "-        if not self.llm or len(self.memory[\"turns\"]) == 0:\n",
        "-            return\n",
        "-        \n",
        "-        try:\n",
        "-            # Format conversation turns for summarization\n",
        "-            conversation_text = []\n",
        "-            for turn in self.memory[\"turns\"]:\n",
        "-                conversation_text.append(f\"Human: {turn['human']}\")\n",
        "-                conversation_text.append(f\"AI: {turn['ai']}\")\n",
        "-            \n",
        "-            full_conversation = \"\\n\".join(conversation_text)\n",
        "-            \n",
        "-            # Create summary using LLM\n",
        "-            summary_prompt = f\"\"\"Summarize this conversation concisely in 2-3 sentences, focusing on key topics discussed:\n",
        "-\n",
        "-{full_conversation}\n",
        "-\n",
        "-Summary:\"\"\"\n",
        "-            \n",
        "-            summary_response = self.llm.invoke(summary_prompt)\n",
        "-            self.memory[\"summary\"] = summary_response.content if hasattr(summary_response, 'content') else str(summary_response)\n",
        "-            \n",
        "-            # Keep only the most recent turns after summarizing\n",
        "-            self.memory[\"turns\"] = self.memory[\"turns\"][-2:]  # Keep last 2 turns\n",
        "-            \n",
        "-        except Exception as e:\n",
        "-            print(f\"Failed to create memory summary: {e}\")\n",
        "-    \n",
        "-    def clear_memory(self):\n",
        "-        \"\"\"Clear all memory contents.\"\"\"\n",
        "-        if self.memory:\n",
        "-            try:\n",
        "-                self.memory[\"turns\"] = []\n",
        "-                self.memory[\"summary\"] = \"\"\n",
        "-            except Exception as e:\n",
        "-                print(f\"Failed to clear memory: {e}\")\n",
        "-    \n",
        "-    def reset_session(self):\n",
        "-        \"\"\"Reset memory for new session.\"\"\"\n",
        "-        self._reset_memory()\n",
        "-    \n",
        "+        return \"\"\n",
        "+\n",
        "+    def clear_memory(self) -> None:\n",
        "+        \"\"\"No-op. Left for backward compatibility.\"\"\"\n",
        "+        return None\n",
        "+\n",
        "+    def reset_session(self) -> None:\n",
        "+        \"\"\"No-op. Left for backward compatibility.\"\"\"\n",
        "+        return None\n",
        "+\n",
        "     def is_available(self) -> bool:\n",
        "-        \"\"\"Check if memory is available.\"\"\"\n",
        "-        return self.memory is not None\n",
        "-    \n",
        "+        \"\"\"Indicate that external memory storage is not used.\n",
        "+\n",
        "+        Return:\n",
        "+            bool: Always False.\n",
        "+        \"\"\"\n",
        "+        return False\n",
        "+\n",
        "     def get_buffer_string(self) -> str:\n",
        "-        \"\"\"Get the raw memory buffer for debugging.\"\"\"\n",
        "-        if not self.memory:\n",
        "-            return \"\"\n",
        "-        \n",
        "-        try:\n",
        "-            return getattr(self.memory, 'buffer', '')\n",
        "-        except:\n",
        "-            return \"\"\n",
        "-    \n",
        "-    def from_chat_history(self, chat_history: List[Dict[str, Any]]):\n",
        "+        \"\"\"No-op buffer accessor kept for compatibility.\n",
        "+\n",
        "+        Return:\n",
        "+            str: Always \"\".\n",
        "         \"\"\"\n",
        "-        Initialize memory from existing chat history.\n",
        "-        \n",
        "-        Args:\n",
        "-            chat_history: List of chat messages with 'role' and 'content' keys\n",
        "+        return \"\"\n",
        "+\n",
        "+    # ---------- Adapters for UI <-> Graph messages ----------\n",
        "+    def to_messages(self, chat_history: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "+        \"\"\"Convert UI chat history to LangGraph-serializable message dicts.\n",
        "+\n",
        "+        Parameters:\n",
        "+            chat_history (List[Dict[str, Any]]): Items with keys 'role' and 'content'.\n",
        "+                For assistant turns, 'content' may be a dict with 'text'.\n",
        "+\n",
        "+        Return:\n",
        "+            List[Dict[str, Any]]: Each item has keys 'type' ('human'|'ai') and 'content' (str).\n",
        "+\n",
        "+        Example:\n",
        "+            >>> mgr = MemoryManager()\n",
        "+            >>> mgr.to_messages([\n",
        "+            ...   {'role': 'user', 'content': 'Hi'},\n",
        "+            ...   {'role': 'assistant', 'content': {'text': 'Hello'}}\n",
        "+            ... ])\n",
        "+            [{'type': 'human', 'content': 'Hi'}, {'type': 'ai', 'content': 'Hello'}]\n",
        "+        \"\"\"\n",
        "+        msgs: List[Dict[str, Any]] = []\n",
        "+        for m in chat_history or []:\n",
        "+            role = (m.get('role') or '').strip().lower()\n",
        "+            content = m.get('content', '')\n",
        "+            if role == 'assistant' and isinstance(content, dict):\n",
        "+                content = content.get('text', str(content))\n",
        "+            if role == 'user':\n",
        "+                msgs.append({\"type\": \"human\", \"content\": str(content)})\n",
        "+            elif role == 'assistant':\n",
        "+                msgs.append({\"type\": \"ai\", \"content\": str(content)})\n",
        "+        return msgs\n",
        "+\n",
        "+    def from_messages(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "+        \"\"\"Convert LangGraph message dicts back to simple UI chat entries.\n",
        "+\n",
        "+        Parameters:\n",
        "+            messages (List[Dict[str, Any]]): Items with keys 'type' and 'content'.\n",
        "+\n",
        "+        Return:\n",
        "+            List[Dict[str, Any]]: Items with 'role' and 'content'.\n",
        "         \"\"\"\n",
        "-        if not self.memory or not chat_history:\n",
        "-            return\n",
        "-        \n",
        "-        try:\n",
        "-            # Process chat history in pairs (user + assistant)\n",
        "-            for i in range(0, len(chat_history) - 1, 2):\n",
        "-                if i + 1 < len(chat_history):\n",
        "-                    user_msg = chat_history[i]\n",
        "-                    ai_msg = chat_history[i + 1]\n",
        "-                    \n",
        "-                    if (user_msg.get('role') == 'user' and \n",
        "-                        ai_msg.get('role') == 'assistant'):\n",
        "-                        \n",
        "-                        user_content = user_msg.get('content', '')\n",
        "-                        ai_content = ai_msg.get('content', '')\n",
        "-                        \n",
        "-                        # Handle AI content that might be a dict (with text and references)\n",
        "-                        if isinstance(ai_content, dict):\n",
        "-                            ai_content = ai_content.get('text', str(ai_content))\n",
        "-                        \n",
        "-                        self.save_conversation_turn(str(user_content), str(ai_content))\n",
        "-        except Exception as e:\n",
        "-            print(f\"Failed to initialize memory from chat history: {e}\")\n",
        "+        ui: List[Dict[str, Any]] = []\n",
        "+        for m in messages or []:\n",
        "+            t = (m.get('type') or '').strip().lower()\n",
        "+            c = m.get('content', '')\n",
        "+            if t == 'human':\n",
        "+                ui.append({'role': 'user', 'content': c})\n",
        "+            elif t == 'ai':\n",
        "+                ui.append({'role': 'assistant', 'content': {'text': c}})\n",
        "+        return ui\n"
      ]
    },
    {
      "path": "rag.ipynb",
      "status": "added",
      "additions": 550,
      "deletions": 0,
      "patch": "@@ -0,0 +1,550 @@\n+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 2,\n+   \"id\": \"1d2e74b1\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"Note: you may need to restart the kernel to use updated packages.\\n\"\n+     ]\n+    },\n+    {\n+     \"name\": \"stderr\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"\\n\",\n+      \"[notice] A new release of pip is available: 25.0.1 -> 25.2\\n\",\n+      \"[notice] To update, run: python.exe -m pip install --upgrade pip\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"pip install -qU langchain[groq] langchain-chroma\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 3,\n+   \"id\": \"a7522473\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"Defaulting to user installation because normal site-packages is not writeable\\n\",\n+      \"Requirement already satisfied: opentelemetry-api in c:\\\\users\\\\parth\\\\appdata\\\\roaming\\\\python\\\\python312\\\\site-packages (1.37.0)\\n\",\n+      \"Requirement already satisfied: langchain-chroma in c:\\\\users\\\\parth\\\\appdata\\\\roaming\\\\python\\\\python312\\\\site-packages (0.2.6)\\n\",\n+      \"Note: you may need to restart the kernel to use updated packages.\\n\"\n+     ]\n+    },\n+    {\n+     \"name\": \"stderr\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"ERROR: Could not find a version that satisfies the requirement chromadbpip (from versions: none)\\n\",\n+      \"\\n\",\n+      \"[notice] A new release of pip is available: 25.0.1 -> 25.2\\n\",\n+      \"[notice] To update, run: python.exe -m pip install --upgrade pip\\n\",\n+      \"ERROR: No matching distribution found for chromadbpip\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"pip install --upgrade opentelemetry-api langchain-chroma chromadbpip install -U opentelemetry-api opentelemetry-sdk\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 4,\n+   \"id\": \"04678c32\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import getpass\\n\",\n+    \"import os\\n\",\n+    \"import pandas as pd\\n\",\n+    \"import io\\n\",\n+    \"from modules.document_processor import DocumentProcessor\\n\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 5,\n+   \"id\": \"42221159\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"\\n\",\n+    \"if not os.environ.get(\\\"GROQ_API_KEY\\\"):\\n\",\n+    \"  os.environ[\\\"GROQ_API_KEY\\\"] = getpass.getpass(\\\"Enter API key for Groq: \\\")\\n\",\n+    \"\\n\",\n+    \"from langchain.chat_models import init_chat_model\\n\",\n+    \"\\n\",\n+    \"model = init_chat_model(\\\"llama3-8b-8192\\\", model_provider=\\\"groq\\\")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 6,\n+   \"id\": \"c8ddef6d\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"['chapter', 'verse', 'speaker', 'sanskrit', 'translation', 'question']\\n\",\n+      \"Original CSV had 700 rows.\\n\",\n+      \"Processed into 117 final document chunks.\\n\",\n+      \"\\n\",\n+      \"--- Example of the First Chunk ---\\n\",\n+      \"Content:\\n\",\n+      \"Chapter 1 Verse 1 \u2014 Dhritarashtra said, \\\"What did my people and the sons of Pandu do when they had assembled together, eager for battle, on the holy plain of Kurukshetra, O Sanjaya?\\\"\\n\",\n+      \"\\n\",\n+      \"Chapter 1 Verse 2 \u2014 Sanjaya said: Having seen the army of the Pandavas drawn up in battle array, King Duryodhana approached his teacher, Drona, and spoke these words.\\n\",\n+      \"\\n\",\n+      \"Chapter 1 Verse 3 \u2014 Behold, O Teacher! This mighty army of the sons of Pandu, arrayed by the son of Drupada, thy wise disciple.\\n\",\n+      \"\\n\",\n+      \"Chapter 1 Verse 4 \u2014 Here are heroes, mighty archers, equal in battle to Bhima and Arjuna, Yuyudhana (Satyaki), Virata, and Drupada\u2014all mighty warriors.\\n\",\n+      \"\\n\",\n+      \"Chapter 1 Verse 5 \u2014 Dhrishtaketu, Chekitana, the valiant king of Kasi, Purujit, Kuntibhoja, and Saibya\u2014the best of men.\\n\",\n+      \"\\n\",\n+      \"Chapter 1 Verse 6 \u2014 The strong Yudhamanyu and the brave Uttamaujas, the son of Subhadra (Abhimanyu, the son of Subhadra and Arjuna), and the sons of Draupadi, all of them great charioteers (great heroes).\\\"\\n\",\n+      \"\\n\",\n+      \"Metadata:\\n\",\n+      \"{'row_start': 0, 'row_end': 5, 'filename': 'gita.csv', 'chapters': [1, 1, 1, 1, 1, 1], 'verses': [1, 2, 3, 4, 5, 6]}\\n\",\n+      \"------------------------------\\n\",\n+      \"\\n\",\n+      \"--- Example of the Second Chunk ---\\n\",\n+      \"Content:\\n\",\n+      \"Chapter 1 Verse 7 \u2014 Know also, O best among the twice-born! the names of those who are the most distinguished amongst ourselves, the leaders of my army; these I name to you for your information.\\n\",\n+      \"\\n\",\n+      \"Chapter 1 Verse 8 \u2014 \\\"Thou thyself, Bhishma, Karna, Kripa, the victorious in war, Asvatthama, Vikarna, and Bhurisrava, the son of Somadatta\u2014all these are ready for battle.\\\"\\n\",\n+      \"\\n\",\n+      \"Chapter 1 Verse 9 \u2014 And also many other heroes, ready to give up their lives for my sake, armed with various weapons and missiles, all well-skilled in battle.\\n\",\n+      \"\\n\",\n+      \"Chapter 1 Verse 10 \u2014 Our army, marshalled by Bhishma, is insufficient, whereas theirs, marshalled by Bhima, is sufficient.\\n\",\n+      \"\\n\",\n+      \"Chapter 1 Verse 11 \u2014 Therefore, do all of you, stationed in your respective positions in the several divisions of the army, protect Bhishma alone.\\n\",\n+      \"\\n\",\n+      \"Chapter 1 Verse 12 \u2014 His glorious grandsire, the oldest of the Kauravas, roared like a lion to cheer Duryodhana and blew his conch.\\n\",\n+      \"\\n\",\n+      \"Metadata:\\n\",\n+      \"{'row_start': 6, 'row_end': 11, 'filename': 'gita.csv', 'chapters': [1, 1, 1, 1, 1, 1], 'verses': [7, 8, 9, 10, 11, 12]}\\n\",\n+      \"------------------------------\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"import pandas as pd\\n\",\n+    \"import io\\n\",\n+    \"from modules.document_processor import DocumentProcessor\\n\",\n+    \" \\n\",\n+    \"with open(\\\"gita.csv\\\", \\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n+    \"    csv_buffer = io.StringIO(f.read())\\n\",\n+    \"\\n\",\n+    \"processor = DocumentProcessor()\\n\",\n+    \"final_docs, original_df = processor.process_csv_to_chunks(\\n\",\n+    \"    file_path=csv_buffer,          # now it's actual CSV content\\n\",\n+    \"    content_column=\\\"translation\\\",\\n\",\n+    \"    filename=\\\"gita.csv\\\"            # required if file_path is not a path\\n\",\n+    \")\\n\",\n+    \"\\n\",\n+    \"print(pd.read_csv(\\\"gita.csv\\\", nrows=0).columns.tolist())\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"# --- 3. Inspect the Results ---\\n\",\n+    \"print(f\\\"Original CSV had {len(original_df)} rows.\\\")\\n\",\n+    \"print(f\\\"Processed into {len(final_docs)} final document chunks.\\\\n\\\")\\n\",\n+    \"\\n\",\n+    \"# Print the first chunk to see the result\\n\",\n+    \"print(\\\"--- Example of the First Chunk ---\\\")\\n\",\n+    \"print(f\\\"Content:\\\\n{final_docs[0].page_content}\\\")\\n\",\n+    \"print(f\\\"\\\\nMetadata:\\\\n{final_docs[0].metadata}\\\")\\n\",\n+    \"print(\\\"-\\\" * 30)\\n\",\n+    \"\\n\",\n+    \"# Print the second chunk to see another example\\n\",\n+    \"if len(final_docs) > 1:\\n\",\n+    \"    print(\\\"\\\\n--- Example of the Second Chunk ---\\\")\\n\",\n+    \"    print(f\\\"Content:\\\\n{final_docs[1].page_content}\\\")\\n\",\n+    \"    print(f\\\"\\\\nMetadata:\\\\n{final_docs[1].metadata}\\\")\\n\",\n+    \"    print(\\\"-\\\" * 30)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 7,\n+   \"id\": \"e6bc9b57\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"Split blog post into 185 sub-documents.\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"from langchain_text_splitters import RecursiveCharacterTextSplitter\\n\",\n+    \"\\n\",\n+    \"text_splitter = RecursiveCharacterTextSplitter(\\n\",\n+    \"    chunk_size=1000,  # chunk size (characters)\\n\",\n+    \"    chunk_overlap=200,  # chunk overlap (characters)\\n\",\n+    \"    add_start_index=True,  # track index in original document\\n\",\n+    \")\\n\",\n+    \"all_splits = text_splitter.split_documents(final_docs)\\n\",\n+    \"\\n\",\n+    \"print(f\\\"Split blog post into {len(all_splits)} sub-documents.\\\")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 8,\n+   \"id\": \"4d5a45a2\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \" \\n\",\n+    \"class Config:\\n\",\n+    \"    \\\"\\\"\\\"Configuration settings for the document processor.\\\"\\\"\\\"\\n\",\n+    \"    CHUNK_SIZE = 1000  # The target size for each final text chunk in characters.\\n\",\n+    \"    CHUNK_OVERLAP = 150 # Number of characters to overlap between chunks.\\n\",\n+    \"    GROUP_SIZE = 5     # Number of CSV rows to group together before splitting.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"1f574b00\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": []\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 9,\n+   \"id\": \"f4ecde6f\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stderr\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"C:\\\\Users\\\\parth\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_19320\\\\3056763568.py:14: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\\n\",\n+      \"  embeddings = HuggingFaceEmbeddings(\\n\"\n+     ]\n+    },\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"Successfully imported Chroma and initialized HuggingFace embeddings.\\n\",\n+      \"Successfully created Chroma vector store instance with HuggingFace embeddings.\\n\",\n+      \"Successfully added documents to the vector store.\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"import os\\n\",\n+    \"from langchain_chroma import Chroma\\n\",\n+    \"# from langchain_openai import OpenAIEmbeddings # <- REMOVE THIS\\n\",\n+    \"from langchain_community.embeddings import HuggingFaceEmbeddings # <- ADD THIS\\n\",\n+    \"\\n\",\n+    \"# 1. Initialize your new embedding function\\n\",\n+    \"# embeddings = OpenAIEmbeddings() # <- REPLACE THIS\\n\",\n+    \"\\n\",\n+    \"# Use a popular, lightweight model from Hugging Face\\n\",\n+    \"# The first time you run this, it will download the model which may take a minute.\\n\",\n+    \"model_name = \\\"sentence-transformers/all-MiniLM-L6-v2\\\"\\n\",\n+    \"model_kwargs = {'device': 'cpu'} # Use 'cuda' for GPU\\n\",\n+    \"encode_kwargs = {'normalize_embeddings': False}\\n\",\n+    \"embeddings = HuggingFaceEmbeddings(\\n\",\n+    \"    model_name=model_name,\\n\",\n+    \"    model_kwargs=model_kwargs,\\n\",\n+    \"    encode_kwargs=encode_kwargs\\n\",\n+    \")\\n\",\n+    \"\\n\",\n+    \"print(\\\"Successfully imported Chroma and initialized HuggingFace embeddings.\\\")\\n\",\n+    \"\\n\",\n+    \"# 2. Your code to create or load the vector store\\n\",\n+    \"# THIS PART REMAINS EXACTLY THE SAME\\n\",\n+    \"vector_store = Chroma(\\n\",\n+    \"    collection_name=\\\"example_collection\\\",\\n\",\n+    \"    embedding_function=embeddings,\\n\",\n+    \"    persist_directory=\\\"./chroma_huggingface_db\\\", # Changed directory to avoid conflicts\\n\",\n+    \")\\n\",\n+    \"\\n\",\n+    \"print(\\\"Successfully created Chroma vector store instance with HuggingFace embeddings.\\\")\\n\",\n+    \"\\n\",\n+    \"# 3. Add some documents to verify it's working\\n\",\n+    \"vector_store.add_texts(\\n\",\n+    \"    texts=[\\\"This is a test document about ChromaDB.\\\", \\\"LangChain helps build LLM applications.\\\"],\\n\",\n+    \"    metadatas=[{\\\"source\\\": \\\"test1\\\"}, {\\\"source\\\": \\\"test2\\\"}],\\n\",\n+    \"    ids=[\\\"doc1\\\", \\\"doc2\\\"]\\n\",\n+    \")\\n\",\n+    \"\\n\",\n+    \"print(\\\"Successfully added documents to the vector store.\\\")\\n\",\n+    \"\\n\",\n+    \" \"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 32,\n+   \"id\": \"241f3fd4\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"Yes. Certain times of day\u2014especially early morning (dawn) and twilight\u2014are regarded as most auspicious for yoga practice, and regular, sustained practice over time is essential for progress.\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"import os\\n\",\n+    \"from langchain_groq import ChatGroq\\n\",\n+    \"# Assume 'vector_store' and 'query' are already defined from the retrieval step\\n\",\n+    \"# vector_store = Chroma(...)\\n\",\n+    \"\\n\",\n+    \"# --- Groq Integration ---\\n\",\n+    \"\\n\",\n+    \"# Make sure your Groq API key is set\\n\",\n+    \"# os.environ[\\\"GROQ_API_KEY\\\"] = \\\"gsk_...\\\"\\n\",\n+    \"query = \\\"Is there any significance of time in the attainment of Yoga?\\\"\\n\",\n+    \"retrieved_docs = vector_store.similarity_search(query, k=5)\\n\",\n+    \"\\n\",\n+    \"# 1. Define the Groq LLM you want to use for generation\\n\",\n+    \"# Llama3 8b is a great, fast choice available on Groq\\n\",\n+    \"llm = ChatGroq(model_name=\\\"openai/gpt-oss-20b\\\")\\n\",\n+    \"\\n\",\n+    \"# 2. Create a prompt template (this part is unchanged)\\n\",\n+    \"prompt_template = \\\"\\\"\\\"\\n\",\n+    \"\\\"You are a helpful assistant. Use the provided context from the source\\\"\\n\",\n+    \"             \\\"the user's question accurately. \\\"\\n\",\n+    \"             \\\"Do not include verse text in your answer. The answer is is ashort as possible.\\n\",\n+    \"Context:\\n\",\n+    \"{context}\\n\",\n+    \"\\n\",\n+    \"Question: {question}\\n\",\n+    \"\\\"\\\"\\\"\\n\",\n+    \"\\n\",\n+    \"# 3. Format the retrieved documents into a single context string\\n\",\n+    \"context_string = \\\"\\\\n\\\\n\\\".join([doc.page_content for doc in retrieved_docs])\\n\",\n+    \"\\n\",\n+    \"# 4. Fill the prompt with the context and question\\n\",\n+    \"formatted_prompt = prompt_template.format(\\n\",\n+    \"    context=context_string,\\n\",\n+    \"    question=query\\n\",\n+    \")\\n\",\n+    \"\\n\",\n+    \"# 5. Call the Groq LLM to get the fast, final answer\\n\",\n+    \"final_answer = llm.invoke(formatted_prompt)\\n\",\n+    \"\\n\",\n+    \" \\n\",\n+    \"print(final_answer.content)\\n\",\n+    \" \"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"faf9db49\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": []\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 11,\n+   \"id\": \"1f4027d8\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"Chroma + HuggingFaceEmbeddings ready.\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"# Notebook cell: setup embeddings and Chroma (no OpenAI)\\n\",\n+    \"from modules.config import Config\\n\",\n+    \"from langchain_huggingface.embeddings import HuggingFaceEmbeddings\\n\",\n+    \"from langchain_chroma import Chroma\\n\",\n+    \"\\n\",\n+    \"# HuggingFace embeddings (same as Neo4j pipeline)\\n\",\n+    \"embeddings = HuggingFaceEmbeddings(\\n\",\n+    \"    model_name=Config.EMBEDDING_MODEL,\\n\",\n+    \"    model_kwargs={'device': Config.EMBEDDING_DEVICE},\\n\",\n+    \"    encode_kwargs={'normalize_embeddings': True},\\n\",\n+    \")\\n\",\n+    \"\\n\",\n+    \"vector_store = Chroma(\\n\",\n+    \"    collection_name=\\\"gita_idx\\\",\\n\",\n+    \"    embedding_function=embeddings,\\n\",\n+    \"    persist_directory=\\\"./chroma_huggingface_db\\\",\\n\",\n+    \")\\n\",\n+    \"print(\\\"Chroma + HuggingFaceEmbeddings ready.\\\")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 12,\n+   \"id\": \"9ddfba8f\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"ename\": \"ValueError\",\n+     \"evalue\": \"Expected metadata value to be a str, int, float, bool, SparseVector, or None, got [1, 1, 1, 1, 1, 1] which is a list in upsert.\\n\\nTry filtering complex metadata from the document using langchain_community.vectorstores.utils.filter_complex_metadata.\",\n+     \"output_type\": \"error\",\n+     \"traceback\": [\n+      \"\\u001b[1;31m---------------------------------------------------------------------------\\u001b[0m\",\n+      \"\\u001b[1;31mValueError\\u001b[0m                                Traceback (most recent call last)\",\n+      \"File \\u001b[1;32m~\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\langchain_chroma\\\\vectorstores.py:647\\u001b[0m, in \\u001b[0;36mChroma.add_texts\\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\\u001b[0m\\n\\u001b[0;32m    646\\u001b[0m \\u001b[38;5;28;01mtry\\u001b[39;00m:\\n\\u001b[1;32m--> 647\\u001b[0m     \\u001b[38;5;28;43mself\\u001b[39;49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43m_collection\\u001b[49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43mupsert\\u001b[49m\\u001b[43m(\\u001b[49m\\n\\u001b[0;32m    648\\u001b[0m \\u001b[43m        \\u001b[49m\\u001b[43mmetadatas\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43mmetadatas\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m  \\u001b[49m\\u001b[38;5;66;43;03m# type: ignore[arg-type]\\u001b[39;49;00m\\n\\u001b[0;32m    649\\u001b[0m \\u001b[43m        \\u001b[49m\\u001b[43membeddings\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43membeddings_with_metadatas\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m  \\u001b[49m\\u001b[38;5;66;43;03m# type: ignore[arg-type]\\u001b[39;49;00m\\n\\u001b[0;32m    650\\u001b[0m \\u001b[43m        \\u001b[49m\\u001b[43mdocuments\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43mtexts_with_metadatas\\u001b[49m\\u001b[43m,\\u001b[49m\\n\\u001b[0;32m    651\\u001b[0m \\u001b[43m        \\u001b[49m\\u001b[43mids\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43mids_with_metadata\\u001b[49m\\u001b[43m,\\u001b[49m\\n\\u001b[0;32m    652\\u001b[0m \\u001b[43m    \\u001b[49m\\u001b[43m)\\u001b[49m\\n\\u001b[0;32m    653\\u001b[0m \\u001b[38;5;28;01mexcept\\u001b[39;00m \\u001b[38;5;167;01mValueError\\u001b[39;00m \\u001b[38;5;28;01mas\\u001b[39;00m e:\\n\",\n+      \"File \\u001b[1;32m~\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\chromadb\\\\api\\\\models\\\\Collection.py:442\\u001b[0m, in \\u001b[0;36mCollection.upsert\\u001b[1;34m(self, ids, embeddings, metadatas, documents, images, uris)\\u001b[0m\\n\\u001b[0;32m    431\\u001b[0m \\u001b[38;5;250m\\u001b[39m\\u001b[38;5;124;03m\\\"\\\"\\\"Update the embeddings, metadatas or documents for provided ids, or create them if they don't exist.\\u001b[39;00m\\n\\u001b[0;32m    432\\u001b[0m \\n\\u001b[0;32m    433\\u001b[0m \\u001b[38;5;124;03mArgs:\\u001b[39;00m\\n\\u001b[1;32m   (...)\\u001b[0m\\n\\u001b[0;32m    440\\u001b[0m \\u001b[38;5;124;03m    None\\u001b[39;00m\\n\\u001b[0;32m    441\\u001b[0m \\u001b[38;5;124;03m\\\"\\\"\\\"\\u001b[39;00m\\n\\u001b[1;32m--> 442\\u001b[0m upsert_request \\u001b[38;5;241m=\\u001b[39m \\u001b[38;5;28;43mself\\u001b[39;49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43m_validate_and_prepare_upsert_request\\u001b[49m\\u001b[43m(\\u001b[49m\\n\\u001b[0;32m    443\\u001b[0m \\u001b[43m    \\u001b[49m\\u001b[43mids\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43mids\\u001b[49m\\u001b[43m,\\u001b[49m\\n\\u001b[0;32m    444\\u001b[0m \\u001b[43m    \\u001b[49m\\u001b[43membeddings\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43membeddings\\u001b[49m\\u001b[43m,\\u001b[49m\\n\\u001b[0;32m    445\\u001b[0m \\u001b[43m    \\u001b[49m\\u001b[43mmetadatas\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43mmetadatas\\u001b[49m\\u001b[43m,\\u001b[49m\\n\\u001b[0;32m    446\\u001b[0m \\u001b[43m    \\u001b[49m\\u001b[43mdocuments\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43mdocuments\\u001b[49m\\u001b[43m,\\u001b[49m\\n\\u001b[0;32m    447\\u001b[0m \\u001b[43m    \\u001b[49m\\u001b[43mimages\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43mimages\\u001b[49m\\u001b[43m,\\u001b[49m\\n\\u001b[0;32m    448\\u001b[0m \\u001b[43m    \\u001b[49m\\u001b[43muris\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43muris\\u001b[49m\\u001b[43m,\\u001b[49m\\n\\u001b[0;32m    449\\u001b[0m \\u001b[43m\\u001b[49m\\u001b[43m)\\u001b[49m\\n\\u001b[0;32m    451\\u001b[0m \\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39m_client\\u001b[38;5;241m.\\u001b[39m_upsert(\\n\\u001b[0;32m    452\\u001b[0m     collection_id\\u001b[38;5;241m=\\u001b[39m\\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39mid,\\n\\u001b[0;32m    453\\u001b[0m     ids\\u001b[38;5;241m=\\u001b[39mupsert_request[\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mids\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m],\\n\\u001b[1;32m   (...)\\u001b[0m\\n\\u001b[0;32m    459\\u001b[0m     database\\u001b[38;5;241m=\\u001b[39m\\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39mdatabase,\\n\\u001b[0;32m    460\\u001b[0m )\\n\",\n+      \"File \\u001b[1;32m~\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\chromadb\\\\api\\\\models\\\\CollectionCommon.py:95\\u001b[0m, in \\u001b[0;36mvalidation_context.<locals>.decorator.<locals>.wrapper\\u001b[1;34m(self, *args, **kwargs)\\u001b[0m\\n\\u001b[0;32m     94\\u001b[0m \\u001b[38;5;28;01mtry\\u001b[39;00m:\\n\\u001b[1;32m---> 95\\u001b[0m     \\u001b[38;5;28;01mreturn\\u001b[39;00m \\u001b[43mfunc\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[38;5;28;43mself\\u001b[39;49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[43margs\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[43mkwargs\\u001b[49m\\u001b[43m)\\u001b[49m\\n\\u001b[0;32m     96\\u001b[0m \\u001b[38;5;28;01mexcept\\u001b[39;00m \\u001b[38;5;167;01mException\\u001b[39;00m \\u001b[38;5;28;01mas\\u001b[39;00m e:\\n\",\n+      \"File \\u001b[1;32m~\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\chromadb\\\\api\\\\models\\\\CollectionCommon.py:417\\u001b[0m, in \\u001b[0;36mCollectionCommon._validate_and_prepare_upsert_request\\u001b[1;34m(self, ids, embeddings, metadatas, documents, images, uris)\\u001b[0m\\n\\u001b[0;32m    416\\u001b[0m \\u001b[38;5;66;03m# Validate\\u001b[39;00m\\n\\u001b[1;32m--> 417\\u001b[0m \\u001b[43mvalidate_insert_record_set\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[43mrecord_set\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43mupsert_records\\u001b[49m\\u001b[43m)\\u001b[49m\\n\\u001b[0;32m    419\\u001b[0m \\u001b[38;5;66;03m# Prepare\\u001b[39;00m\\n\",\n+      \"File \\u001b[1;32m~\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\chromadb\\\\api\\\\types.py:317\\u001b[0m, in \\u001b[0;36mvalidate_insert_record_set\\u001b[1;34m(record_set)\\u001b[0m\\n\\u001b[0;32m    316\\u001b[0m \\u001b[38;5;28;01mif\\u001b[39;00m record_set[\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mmetadatas\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m] \\u001b[38;5;129;01mis\\u001b[39;00m \\u001b[38;5;129;01mnot\\u001b[39;00m \\u001b[38;5;28;01mNone\\u001b[39;00m:\\n\\u001b[1;32m--> 317\\u001b[0m     \\u001b[43mvalidate_metadatas\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[43mrecord_set\\u001b[49m\\u001b[43m[\\u001b[49m\\u001b[38;5;124;43m\\\"\\u001b[39;49m\\u001b[38;5;124;43mmetadatas\\u001b[39;49m\\u001b[38;5;124;43m\\\"\\u001b[39;49m\\u001b[43m]\\u001b[49m\\u001b[43m)\\u001b[49m\\n\",\n+      \"File \\u001b[1;32m~\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\chromadb\\\\api\\\\types.py:899\\u001b[0m, in \\u001b[0;36mvalidate_metadatas\\u001b[1;34m(metadatas)\\u001b[0m\\n\\u001b[0;32m    898\\u001b[0m \\u001b[38;5;28;01mfor\\u001b[39;00m metadata \\u001b[38;5;129;01min\\u001b[39;00m metadatas:\\n\\u001b[1;32m--> 899\\u001b[0m     \\u001b[43mvalidate_metadata\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[43mmetadata\\u001b[49m\\u001b[43m)\\u001b[49m\\n\\u001b[0;32m    900\\u001b[0m \\u001b[38;5;28;01mreturn\\u001b[39;00m metadatas\\n\",\n+      \"File \\u001b[1;32m~\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\chromadb\\\\api\\\\types.py:859\\u001b[0m, in \\u001b[0;36mvalidate_metadata\\u001b[1;34m(metadata)\\u001b[0m\\n\\u001b[0;32m    856\\u001b[0m     \\u001b[38;5;28;01melif\\u001b[39;00m \\u001b[38;5;129;01mnot\\u001b[39;00m \\u001b[38;5;28misinstance\\u001b[39m(value, \\u001b[38;5;28mbool\\u001b[39m) \\u001b[38;5;129;01mand\\u001b[39;00m \\u001b[38;5;129;01mnot\\u001b[39;00m \\u001b[38;5;28misinstance\\u001b[39m(\\n\\u001b[0;32m    857\\u001b[0m         value, (\\u001b[38;5;28mstr\\u001b[39m, \\u001b[38;5;28mint\\u001b[39m, \\u001b[38;5;28mfloat\\u001b[39m, \\u001b[38;5;28mtype\\u001b[39m(\\u001b[38;5;28;01mNone\\u001b[39;00m))\\n\\u001b[0;32m    858\\u001b[0m     ):\\n\\u001b[1;32m--> 859\\u001b[0m         \\u001b[38;5;28;01mraise\\u001b[39;00m \\u001b[38;5;167;01mValueError\\u001b[39;00m(\\n\\u001b[0;32m    860\\u001b[0m             \\u001b[38;5;124mf\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mExpected metadata value to be a str, int, float, bool, SparseVector, or None, got \\u001b[39m\\u001b[38;5;132;01m{\\u001b[39;00mvalue\\u001b[38;5;132;01m}\\u001b[39;00m\\u001b[38;5;124m which is a \\u001b[39m\\u001b[38;5;132;01m{\\u001b[39;00m\\u001b[38;5;28mtype\\u001b[39m(value)\\u001b[38;5;241m.\\u001b[39m\\u001b[38;5;18m__name__\\u001b[39m\\u001b[38;5;132;01m}\\u001b[39;00m\\u001b[38;5;124m\\\"\\u001b[39m\\n\\u001b[0;32m    861\\u001b[0m         )\\n\\u001b[0;32m    862\\u001b[0m \\u001b[38;5;28;01mreturn\\u001b[39;00m metadata\\n\",\n+      \"\\u001b[1;31mValueError\\u001b[0m: Expected metadata value to be a str, int, float, bool, SparseVector, or None, got [1, 1, 1, 1, 1, 1] which is a list in upsert.\",\n+      \"\\nThe above exception was the direct cause of the following exception:\\n\",\n+      \"\\u001b[1;31mValueError\\u001b[0m                                Traceback (most recent call last)\",\n+      \"Cell \\u001b[1;32mIn[12], line 13\\u001b[0m\\n\\u001b[0;32m      6\\u001b[0m final_docs, original_df \\u001b[38;5;241m=\\u001b[39m processor\\u001b[38;5;241m.\\u001b[39mprocess_csv_to_chunks(\\n\\u001b[0;32m      7\\u001b[0m     file_path\\u001b[38;5;241m=\\u001b[39mcsv_path,\\n\\u001b[0;32m      8\\u001b[0m     content_column\\u001b[38;5;241m=\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mtranslation\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m,\\n\\u001b[0;32m      9\\u001b[0m     filename\\u001b[38;5;241m=\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mgita.csv\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\n\\u001b[0;32m     10\\u001b[0m )\\n\\u001b[0;32m     12\\u001b[0m \\u001b[38;5;66;03m# Populate Chroma with the same docs\\u001b[39;00m\\n\\u001b[1;32m---> 13\\u001b[0m \\u001b[43mvector_store\\u001b[49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43madd_documents\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[43mfinal_docs\\u001b[49m\\u001b[43m)\\u001b[49m\\n\\u001b[0;32m     14\\u001b[0m \\u001b[38;5;28mprint\\u001b[39m(\\u001b[38;5;124mf\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mChroma populated with \\u001b[39m\\u001b[38;5;132;01m{\\u001b[39;00m\\u001b[38;5;28mlen\\u001b[39m(final_docs)\\u001b[38;5;132;01m}\\u001b[39;00m\\u001b[38;5;124m chunks from \\u001b[39m\\u001b[38;5;132;01m{\\u001b[39;00mcsv_path\\u001b[38;5;132;01m}\\u001b[39;00m\\u001b[38;5;124m.\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m)\\n\",\n+      \"File \\u001b[1;32m~\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\langchain_core\\\\vectorstores\\\\base.py:279\\u001b[0m, in \\u001b[0;36mVectorStore.add_documents\\u001b[1;34m(self, documents, **kwargs)\\u001b[0m\\n\\u001b[0;32m    277\\u001b[0m     texts \\u001b[38;5;241m=\\u001b[39m [doc\\u001b[38;5;241m.\\u001b[39mpage_content \\u001b[38;5;28;01mfor\\u001b[39;00m doc \\u001b[38;5;129;01min\\u001b[39;00m documents]\\n\\u001b[0;32m    278\\u001b[0m     metadatas \\u001b[38;5;241m=\\u001b[39m [doc\\u001b[38;5;241m.\\u001b[39mmetadata \\u001b[38;5;28;01mfor\\u001b[39;00m doc \\u001b[38;5;129;01min\\u001b[39;00m documents]\\n\\u001b[1;32m--> 279\\u001b[0m     \\u001b[38;5;28;01mreturn\\u001b[39;00m \\u001b[38;5;28;43mself\\u001b[39;49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43madd_texts\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[43mtexts\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[43mmetadatas\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[43mkwargs\\u001b[49m\\u001b[43m)\\u001b[49m\\n\\u001b[0;32m    280\\u001b[0m msg \\u001b[38;5;241m=\\u001b[39m (\\n\\u001b[0;32m    281\\u001b[0m     \\u001b[38;5;124mf\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124m`add_documents` and `add_texts` has not been implemented \\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\n\\u001b[0;32m    282\\u001b[0m     \\u001b[38;5;124mf\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mfor \\u001b[39m\\u001b[38;5;132;01m{\\u001b[39;00m\\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39m\\u001b[38;5;18m__class__\\u001b[39m\\u001b[38;5;241m.\\u001b[39m\\u001b[38;5;18m__name__\\u001b[39m\\u001b[38;5;132;01m}\\u001b[39;00m\\u001b[38;5;124m \\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\n\\u001b[0;32m    283\\u001b[0m )\\n\\u001b[0;32m    284\\u001b[0m \\u001b[38;5;28;01mraise\\u001b[39;00m \\u001b[38;5;167;01mNotImplementedError\\u001b[39;00m(msg)\\n\",\n+      \"File \\u001b[1;32m~\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\langchain_chroma\\\\vectorstores.py:659\\u001b[0m, in \\u001b[0;36mChroma.add_texts\\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\\u001b[0m\\n\\u001b[0;32m    654\\u001b[0m         \\u001b[38;5;28;01mif\\u001b[39;00m \\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mExpected metadata value to be\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m \\u001b[38;5;129;01min\\u001b[39;00m \\u001b[38;5;28mstr\\u001b[39m(e):\\n\\u001b[0;32m    655\\u001b[0m             msg \\u001b[38;5;241m=\\u001b[39m (\\n\\u001b[0;32m    656\\u001b[0m                 \\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mTry filtering complex metadata from the document using \\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\n\\u001b[0;32m    657\\u001b[0m                 \\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mlangchain_community.vectorstores.utils.filter_complex_metadata.\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\n\\u001b[0;32m    658\\u001b[0m             )\\n\\u001b[1;32m--> 659\\u001b[0m             \\u001b[38;5;28;01mraise\\u001b[39;00m \\u001b[38;5;167;01mValueError\\u001b[39;00m(e\\u001b[38;5;241m.\\u001b[39margs[\\u001b[38;5;241m0\\u001b[39m] \\u001b[38;5;241m+\\u001b[39m \\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;130;01m\\\\n\\u001b[39;00m\\u001b[38;5;130;01m\\\\n\\u001b[39;00m\\u001b[38;5;124m\\\"\\u001b[39m \\u001b[38;5;241m+\\u001b[39m msg) \\u001b[38;5;28;01mfrom\\u001b[39;00m\\u001b[38;5;250m \\u001b[39m\\u001b[38;5;21;01me\\u001b[39;00m\\n\\u001b[0;32m    660\\u001b[0m         \\u001b[38;5;28;01mraise\\u001b[39;00m e\\n\\u001b[0;32m    661\\u001b[0m \\u001b[38;5;28;01mif\\u001b[39;00m empty_ids:\\n\",\n+      \"\\u001b[1;31mValueError\\u001b[0m: Expected metadata value to be a str, int, float, bool, SparseVector, or None, got [1, 1, 1, 1, 1, 1] which is a list in upsert.\\n\\nTry filtering complex metadata from the document using langchain_community.vectorstores.utils.filter_complex_metadata.\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"# Notebook cell: build docs for Chroma (if not already built in this session)\\n\",\n+    \"from modules.document_processor import DocumentProcessor\\n\",\n+    \"\\n\",\n+    \"csv_path = \\\"gita.csv\\\"  # adjust path if needed\\n\",\n+    \"processor = DocumentProcessor()\\n\",\n+    \"final_docs, original_df = processor.process_csv_to_chunks(\\n\",\n+    \"    file_path=csv_path,\\n\",\n+    \"    content_column=\\\"translation\\\",\\n\",\n+    \"    filename=\\\"gita.csv\\\"\\n\",\n+    \")\\n\",\n+    \"\\n\",\n+    \"# Populate Chroma with the same docs\\n\",\n+    \"vector_store.add_documents(final_docs)\\n\",\n+    \"print(f\\\"Chroma populated with {len(final_docs)} chunks from {csv_path}.\\\")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"eeb2139d\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# Notebook cell: compare function\\n\",\n+    \"from modules.llm_chain import LLMChain\\n\",\n+    \"from modules.neo4j_manager import Neo4jManager\\n\",\n+    \"from modules.config import Config\\n\",\n+    \"\\n\",\n+    \"def compare_results(query: str, filename: str, top_k: int = 5):\\n\",\n+    \"    llm = LLMChain()\\n\",\n+    \"\\n\",\n+    \"    # 1) Rewrite once for fairness (same rule as your app)\\n\",\n+    \"    rw = llm.rewrite_query(query, chat_history=[], source_name=filename, fallback_on_error=True)\\n\",\n+    \"    rewritten = rw.get(\\\"rewritten_query\\\", query)\\n\",\n+    \"\\n\",\n+    \"    # 2) Neo4j retrieval (connect to existing index for this file)\\n\",\n+    \"    idx_name = Config.get_index_name(filename)\\n\",\n+    \"    neo = Neo4jManager()                            # uses the same HF embeddings as Chroma\\n\",\n+    \"    neo.connect_to_existing_index(idx_name)         # index should already exist (created via app)\\n\",\n+    \"    neo_docs = neo.retrieve_with_filename_filter(rewritten, filename, top_k=top_k)\\n\",\n+    \"    neo_answer = llm.graph_qa_chain(\\n\",\n+    \"        question=query, docs=neo_docs, source_name=filename, memory_context=None, fallback_on_error=True\\n\",\n+    \"    )\\n\",\n+    \"\\n\",\n+    \"    # 3) Chroma retrieval with the SAME rewritten query and SAME LLM\\n\",\n+    \"    chroma_docs = vector_store.similarity_search(rewritten, k=top_k)\\n\",\n+    \"    chroma_answer = llm.graph_qa_chain(\\n\",\n+    \"        question=query, docs=chroma_docs, source_name=filename, memory_context=None, fallback_on_error=True\\n\",\n+    \"    )\\n\",\n+    \"\\n\",\n+    \"    # 4) Print side-by-side summaries\\n\",\n+    \"    print(\\\"=== Query ===\\\")\\n\",\n+    \"    print(\\\"Original: \\\", query)\\n\",\n+    \"    print(\\\"Rewritten:\\\", rewritten)\\n\",\n+    \"    print()\\n\",\n+    \"\\n\",\n+    \"    print(\\\"=== Neo4j Answer ===\\\")\\n\",\n+    \"    print(neo_answer.get(\\\"text\\\", \\\"\\\"))\\n\",\n+    \"    print(\\\"Refs:\\\", neo_answer.get(\\\"refrence\\\", []))\\n\",\n+    \"    print()\\n\",\n+    \"\\n\",\n+    \"    print(\\\"=== Chroma Answer ===\\\")\\n\",\n+    \"    print(chroma_answer.get(\\\"text\\\", \\\"\\\"))\\n\",\n+    \"    print(\\\"Refs:\\\", chroma_answer.get(\\\"refrence\\\", []))\\n\",\n+    \"    print()\\n\",\n+    \"\\n\",\n+    \"    # Optional: top-1 snippet previews to see retrieved context difference\\n\",\n+    \"    if neo_docs:\\n\",\n+    \"        print(\\\"Neo4j top-1 snippet:\\\", neo_docs[0].page_content[:250].replace(\\\"\\\\n\\\", \\\" \\\"), \\\"| score:\\\", neo_docs[0].metadata.get(\\\"score\\\"))\\n\",\n+    \"    if chroma_docs:\\n\",\n+    \"        print(\\\"Chroma top-1 snippet:\\\", chroma_docs[0].page_content[:250].replace(\\\"\\\\n\\\", \\\" \\\"))\\n\",\n+    \"\\n\",\n+    \"# Example usage:\\n\",\n+    \"# compare_results(\\\"What is karma yoga?\\\", filename=\\\"gita.csv\\\", top_k=5)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"d5728302\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# Create the Neo4j vector index from the same final_docs (one-time)\\n\",\n+    \"nm = Neo4jManager()\\n\",\n+    \"nm.create_vector_store(final_docs, Config.get_index_name(\\\"gita.csv\\\"))\\n\",\n+    \"nm.create_file_relationships(\\\"gita.csv\\\")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"b0519509\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": []\n+  }\n+ ],\n+ \"metadata\": {\n+  \"kernelspec\": {\n+   \"display_name\": \"Python 3\",\n+   \"language\": \"python\",\n+   \"name\": \"python3\"\n+  },\n+  \"language_info\": {\n+   \"codemirror_mode\": {\n+    \"name\": \"ipython\",\n+    \"version\": 3\n+   },\n+   \"file_extension\": \".py\",\n+   \"mimetype\": \"text/x-python\",\n+   \"name\": \"python\",\n+   \"nbconvert_exporter\": \"python\",\n+   \"pygments_lexer\": \"ipython3\",\n+   \"version\": \"3.12.9\"\n+  }\n+ },\n+ \"nbformat\": 4,\n+ \"nbformat_minor\": 5\n+}",
      "patch_lines": [
        "@@ -0,0 +1,550 @@\n",
        "+{\n",
        "+ \"cells\": [\n",
        "+  {\n",
        "+   \"cell_type\": \"code\",\n",
        "+   \"execution_count\": 2,\n",
        "+   \"id\": \"1d2e74b1\",\n",
        "+   \"metadata\": {},\n",
        "+   \"outputs\": [\n",
        "+    {\n",
        "+     \"name\": \"stdout\",\n",
        "+     \"output_type\": \"stream\",\n",
        "+     \"text\": [\n",
        "+      \"Note: you may need to restart the kernel to use updated packages.\\n\"\n",
        "+     ]\n",
        "+    },\n",
        "+    {\n",
        "+     \"name\": \"stderr\",\n",
        "+     \"output_type\": \"stream\",\n",
        "+     \"text\": [\n",
        "+      \"\\n\",\n",
        "+      \"[notice] A new release of pip is available: 25.0.1 -> 25.2\\n\",\n",
        "+      \"[notice] To update, run: python.exe -m pip install --upgrade pip\\n\"\n",
        "+     ]\n",
        "+    }\n",
        "+   ],\n",
        "+   \"source\": [\n",
        "+    \"pip install -qU langchain[groq] langchain-chroma\"\n",
        "+   ]\n",
        "+  },\n",
        "+  {\n",
        "+   \"cell_type\": \"code\",\n",
        "+   \"execution_count\": 3,\n",
        "+   \"id\": \"a7522473\",\n",
        "+   \"metadata\": {},\n",
        "+   \"outputs\": [\n",
        "+    {\n",
        "+     \"name\": \"stdout\",\n",
        "+     \"output_type\": \"stream\",\n",
        "+     \"text\": [\n",
        "+      \"Defaulting to user installation because normal site-packages is not writeable\\n\",\n",
        "+      \"Requirement already satisfied: opentelemetry-api in c:\\\\users\\\\parth\\\\appdata\\\\roaming\\\\python\\\\python312\\\\site-packages (1.37.0)\\n\",\n",
        "+      \"Requirement already satisfied: langchain-chroma in c:\\\\users\\\\parth\\\\appdata\\\\roaming\\\\python\\\\python312\\\\site-packages (0.2.6)\\n\",\n",
        "+      \"Note: you may need to restart the kernel to use updated packages.\\n\"\n",
        "+     ]\n",
        "+    },\n",
        "+    {\n",
        "+     \"name\": \"stderr\",\n",
        "+     \"output_type\": \"stream\",\n",
        "+     \"text\": [\n",
        "+      \"ERROR: Could not find a version that satisfies the requirement chromadbpip (from versions: none)\\n\",\n",
        "+      \"\\n\",\n",
        "+      \"[notice] A new release of pip is available: 25.0.1 -> 25.2\\n\",\n",
        "+      \"[notice] To update, run: python.exe -m pip install --upgrade pip\\n\",\n",
        "+      \"ERROR: No matching distribution found for chromadbpip\\n\"\n",
        "+     ]\n",
        "+    }\n",
        "+   ],\n",
        "+   \"source\": [\n",
        "+    \"pip install --upgrade opentelemetry-api langchain-chroma chromadbpip install -U opentelemetry-api opentelemetry-sdk\"\n",
        "+   ]\n",
        "+  },\n",
        "+  {\n",
        "+   \"cell_type\": \"code\",\n",
        "+   \"execution_count\": 4,\n",
        "+   \"id\": \"04678c32\",\n",
        "+   \"metadata\": {},\n",
        "+   \"outputs\": [],\n",
        "+   \"source\": [\n",
        "+    \"import getpass\\n\",\n",
        "+    \"import os\\n\",\n",
        "+    \"import pandas as pd\\n\",\n",
        "+    \"import io\\n\",\n",
        "+    \"from modules.document_processor import DocumentProcessor\\n\"\n",
        "+   ]\n",
        "+  },\n",
        "+  {\n",
        "+   \"cell_type\": \"code\",\n",
        "+   \"execution_count\": 5,\n",
        "+   \"id\": \"42221159\",\n",
        "+   \"metadata\": {},\n",
        "+   \"outputs\": [],\n",
        "+   \"source\": [\n",
        "+    \"\\n\",\n",
        "+    \"if not os.environ.get(\\\"GROQ_API_KEY\\\"):\\n\",\n",
        "+    \"  os.environ[\\\"GROQ_API_KEY\\\"] = getpass.getpass(\\\"Enter API key for Groq: \\\")\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"from langchain.chat_models import init_chat_model\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"model = init_chat_model(\\\"llama3-8b-8192\\\", model_provider=\\\"groq\\\")\"\n",
        "+   ]\n",
        "+  },\n",
        "+  {\n",
        "+   \"cell_type\": \"code\",\n",
        "+   \"execution_count\": 6,\n",
        "+   \"id\": \"c8ddef6d\",\n",
        "+   \"metadata\": {},\n",
        "+   \"outputs\": [\n",
        "+    {\n",
        "+     \"name\": \"stdout\",\n",
        "+     \"output_type\": \"stream\",\n",
        "+     \"text\": [\n",
        "+      \"['chapter', 'verse', 'speaker', 'sanskrit', 'translation', 'question']\\n\",\n",
        "+      \"Original CSV had 700 rows.\\n\",\n",
        "+      \"Processed into 117 final document chunks.\\n\",\n",
        "+      \"\\n\",\n",
        "+      \"--- Example of the First Chunk ---\\n\",\n",
        "+      \"Content:\\n\",\n",
        "+      \"Chapter 1 Verse 1 \u2014 Dhritarashtra said, \\\"What did my people and the sons of Pandu do when they had assembled together, eager for battle, on the holy plain of Kurukshetra, O Sanjaya?\\\"\\n\",\n",
        "+      \"\\n\",\n",
        "+      \"Chapter 1 Verse 2 \u2014 Sanjaya said: Having seen the army of the Pandavas drawn up in battle array, King Duryodhana approached his teacher, Drona, and spoke these words.\\n\",\n",
        "+      \"\\n\",\n",
        "+      \"Chapter 1 Verse 3 \u2014 Behold, O Teacher! This mighty army of the sons of Pandu, arrayed by the son of Drupada, thy wise disciple.\\n\",\n",
        "+      \"\\n\",\n",
        "+      \"Chapter 1 Verse 4 \u2014 Here are heroes, mighty archers, equal in battle to Bhima and Arjuna, Yuyudhana (Satyaki), Virata, and Drupada\u2014all mighty warriors.\\n\",\n",
        "+      \"\\n\",\n",
        "+      \"Chapter 1 Verse 5 \u2014 Dhrishtaketu, Chekitana, the valiant king of Kasi, Purujit, Kuntibhoja, and Saibya\u2014the best of men.\\n\",\n",
        "+      \"\\n\",\n",
        "+      \"Chapter 1 Verse 6 \u2014 The strong Yudhamanyu and the brave Uttamaujas, the son of Subhadra (Abhimanyu, the son of Subhadra and Arjuna), and the sons of Draupadi, all of them great charioteers (great heroes).\\\"\\n\",\n",
        "+      \"\\n\",\n",
        "+      \"Metadata:\\n\",\n",
        "+      \"{'row_start': 0, 'row_end': 5, 'filename': 'gita.csv', 'chapters': [1, 1, 1, 1, 1, 1], 'verses': [1, 2, 3, 4, 5, 6]}\\n\",\n",
        "+      \"------------------------------\\n\",\n",
        "+      \"\\n\",\n",
        "+      \"--- Example of the Second Chunk ---\\n\",\n",
        "+      \"Content:\\n\",\n",
        "+      \"Chapter 1 Verse 7 \u2014 Know also, O best among the twice-born! the names of those who are the most distinguished amongst ourselves, the leaders of my army; these I name to you for your information.\\n\",\n",
        "+      \"\\n\",\n",
        "+      \"Chapter 1 Verse 8 \u2014 \\\"Thou thyself, Bhishma, Karna, Kripa, the victorious in war, Asvatthama, Vikarna, and Bhurisrava, the son of Somadatta\u2014all these are ready for battle.\\\"\\n\",\n",
        "+      \"\\n\",\n",
        "+      \"Chapter 1 Verse 9 \u2014 And also many other heroes, ready to give up their lives for my sake, armed with various weapons and missiles, all well-skilled in battle.\\n\",\n",
        "+      \"\\n\",\n",
        "+      \"Chapter 1 Verse 10 \u2014 Our army, marshalled by Bhishma, is insufficient, whereas theirs, marshalled by Bhima, is sufficient.\\n\",\n",
        "+      \"\\n\",\n",
        "+      \"Chapter 1 Verse 11 \u2014 Therefore, do all of you, stationed in your respective positions in the several divisions of the army, protect Bhishma alone.\\n\",\n",
        "+      \"\\n\",\n",
        "+      \"Chapter 1 Verse 12 \u2014 His glorious grandsire, the oldest of the Kauravas, roared like a lion to cheer Duryodhana and blew his conch.\\n\",\n",
        "+      \"\\n\",\n",
        "+      \"Metadata:\\n\",\n",
        "+      \"{'row_start': 6, 'row_end': 11, 'filename': 'gita.csv', 'chapters': [1, 1, 1, 1, 1, 1], 'verses': [7, 8, 9, 10, 11, 12]}\\n\",\n",
        "+      \"------------------------------\\n\"\n",
        "+     ]\n",
        "+    }\n",
        "+   ],\n",
        "+   \"source\": [\n",
        "+    \"import pandas as pd\\n\",\n",
        "+    \"import io\\n\",\n",
        "+    \"from modules.document_processor import DocumentProcessor\\n\",\n",
        "+    \" \\n\",\n",
        "+    \"with open(\\\"gita.csv\\\", \\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n",
        "+    \"    csv_buffer = io.StringIO(f.read())\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"processor = DocumentProcessor()\\n\",\n",
        "+    \"final_docs, original_df = processor.process_csv_to_chunks(\\n\",\n",
        "+    \"    file_path=csv_buffer,          # now it's actual CSV content\\n\",\n",
        "+    \"    content_column=\\\"translation\\\",\\n\",\n",
        "+    \"    filename=\\\"gita.csv\\\"            # required if file_path is not a path\\n\",\n",
        "+    \")\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"print(pd.read_csv(\\\"gita.csv\\\", nrows=0).columns.tolist())\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"# --- 3. Inspect the Results ---\\n\",\n",
        "+    \"print(f\\\"Original CSV had {len(original_df)} rows.\\\")\\n\",\n",
        "+    \"print(f\\\"Processed into {len(final_docs)} final document chunks.\\\\n\\\")\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"# Print the first chunk to see the result\\n\",\n",
        "+    \"print(\\\"--- Example of the First Chunk ---\\\")\\n\",\n",
        "+    \"print(f\\\"Content:\\\\n{final_docs[0].page_content}\\\")\\n\",\n",
        "+    \"print(f\\\"\\\\nMetadata:\\\\n{final_docs[0].metadata}\\\")\\n\",\n",
        "+    \"print(\\\"-\\\" * 30)\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"# Print the second chunk to see another example\\n\",\n",
        "+    \"if len(final_docs) > 1:\\n\",\n",
        "+    \"    print(\\\"\\\\n--- Example of the Second Chunk ---\\\")\\n\",\n",
        "+    \"    print(f\\\"Content:\\\\n{final_docs[1].page_content}\\\")\\n\",\n",
        "+    \"    print(f\\\"\\\\nMetadata:\\\\n{final_docs[1].metadata}\\\")\\n\",\n",
        "+    \"    print(\\\"-\\\" * 30)\"\n",
        "+   ]\n",
        "+  },\n",
        "+  {\n",
        "+   \"cell_type\": \"code\",\n",
        "+   \"execution_count\": 7,\n",
        "+   \"id\": \"e6bc9b57\",\n",
        "+   \"metadata\": {},\n",
        "+   \"outputs\": [\n",
        "+    {\n",
        "+     \"name\": \"stdout\",\n",
        "+     \"output_type\": \"stream\",\n",
        "+     \"text\": [\n",
        "+      \"Split blog post into 185 sub-documents.\\n\"\n",
        "+     ]\n",
        "+    }\n",
        "+   ],\n",
        "+   \"source\": [\n",
        "+    \"from langchain_text_splitters import RecursiveCharacterTextSplitter\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"text_splitter = RecursiveCharacterTextSplitter(\\n\",\n",
        "+    \"    chunk_size=1000,  # chunk size (characters)\\n\",\n",
        "+    \"    chunk_overlap=200,  # chunk overlap (characters)\\n\",\n",
        "+    \"    add_start_index=True,  # track index in original document\\n\",\n",
        "+    \")\\n\",\n",
        "+    \"all_splits = text_splitter.split_documents(final_docs)\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"print(f\\\"Split blog post into {len(all_splits)} sub-documents.\\\")\"\n",
        "+   ]\n",
        "+  },\n",
        "+  {\n",
        "+   \"cell_type\": \"code\",\n",
        "+   \"execution_count\": 8,\n",
        "+   \"id\": \"4d5a45a2\",\n",
        "+   \"metadata\": {},\n",
        "+   \"outputs\": [],\n",
        "+   \"source\": [\n",
        "+    \" \\n\",\n",
        "+    \"class Config:\\n\",\n",
        "+    \"    \\\"\\\"\\\"Configuration settings for the document processor.\\\"\\\"\\\"\\n\",\n",
        "+    \"    CHUNK_SIZE = 1000  # The target size for each final text chunk in characters.\\n\",\n",
        "+    \"    CHUNK_OVERLAP = 150 # Number of characters to overlap between chunks.\\n\",\n",
        "+    \"    GROUP_SIZE = 5     # Number of CSV rows to group together before splitting.\"\n",
        "+   ]\n",
        "+  },\n",
        "+  {\n",
        "+   \"cell_type\": \"code\",\n",
        "+   \"execution_count\": null,\n",
        "+   \"id\": \"1f574b00\",\n",
        "+   \"metadata\": {},\n",
        "+   \"outputs\": [],\n",
        "+   \"source\": []\n",
        "+  },\n",
        "+  {\n",
        "+   \"cell_type\": \"code\",\n",
        "+   \"execution_count\": 9,\n",
        "+   \"id\": \"f4ecde6f\",\n",
        "+   \"metadata\": {},\n",
        "+   \"outputs\": [\n",
        "+    {\n",
        "+     \"name\": \"stderr\",\n",
        "+     \"output_type\": \"stream\",\n",
        "+     \"text\": [\n",
        "+      \"C:\\\\Users\\\\parth\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_19320\\\\3056763568.py:14: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\\n\",\n",
        "+      \"  embeddings = HuggingFaceEmbeddings(\\n\"\n",
        "+     ]\n",
        "+    },\n",
        "+    {\n",
        "+     \"name\": \"stdout\",\n",
        "+     \"output_type\": \"stream\",\n",
        "+     \"text\": [\n",
        "+      \"Successfully imported Chroma and initialized HuggingFace embeddings.\\n\",\n",
        "+      \"Successfully created Chroma vector store instance with HuggingFace embeddings.\\n\",\n",
        "+      \"Successfully added documents to the vector store.\\n\"\n",
        "+     ]\n",
        "+    }\n",
        "+   ],\n",
        "+   \"source\": [\n",
        "+    \"import os\\n\",\n",
        "+    \"from langchain_chroma import Chroma\\n\",\n",
        "+    \"# from langchain_openai import OpenAIEmbeddings # <- REMOVE THIS\\n\",\n",
        "+    \"from langchain_community.embeddings import HuggingFaceEmbeddings # <- ADD THIS\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"# 1. Initialize your new embedding function\\n\",\n",
        "+    \"# embeddings = OpenAIEmbeddings() # <- REPLACE THIS\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"# Use a popular, lightweight model from Hugging Face\\n\",\n",
        "+    \"# The first time you run this, it will download the model which may take a minute.\\n\",\n",
        "+    \"model_name = \\\"sentence-transformers/all-MiniLM-L6-v2\\\"\\n\",\n",
        "+    \"model_kwargs = {'device': 'cpu'} # Use 'cuda' for GPU\\n\",\n",
        "+    \"encode_kwargs = {'normalize_embeddings': False}\\n\",\n",
        "+    \"embeddings = HuggingFaceEmbeddings(\\n\",\n",
        "+    \"    model_name=model_name,\\n\",\n",
        "+    \"    model_kwargs=model_kwargs,\\n\",\n",
        "+    \"    encode_kwargs=encode_kwargs\\n\",\n",
        "+    \")\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"print(\\\"Successfully imported Chroma and initialized HuggingFace embeddings.\\\")\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"# 2. Your code to create or load the vector store\\n\",\n",
        "+    \"# THIS PART REMAINS EXACTLY THE SAME\\n\",\n",
        "+    \"vector_store = Chroma(\\n\",\n",
        "+    \"    collection_name=\\\"example_collection\\\",\\n\",\n",
        "+    \"    embedding_function=embeddings,\\n\",\n",
        "+    \"    persist_directory=\\\"./chroma_huggingface_db\\\", # Changed directory to avoid conflicts\\n\",\n",
        "+    \")\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"print(\\\"Successfully created Chroma vector store instance with HuggingFace embeddings.\\\")\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"# 3. Add some documents to verify it's working\\n\",\n",
        "+    \"vector_store.add_texts(\\n\",\n",
        "+    \"    texts=[\\\"This is a test document about ChromaDB.\\\", \\\"LangChain helps build LLM applications.\\\"],\\n\",\n",
        "+    \"    metadatas=[{\\\"source\\\": \\\"test1\\\"}, {\\\"source\\\": \\\"test2\\\"}],\\n\",\n",
        "+    \"    ids=[\\\"doc1\\\", \\\"doc2\\\"]\\n\",\n",
        "+    \")\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"print(\\\"Successfully added documents to the vector store.\\\")\\n\",\n",
        "+    \"\\n\",\n",
        "+    \" \"\n",
        "+   ]\n",
        "+  },\n",
        "+  {\n",
        "+   \"cell_type\": \"code\",\n",
        "+   \"execution_count\": 32,\n",
        "+   \"id\": \"241f3fd4\",\n",
        "+   \"metadata\": {},\n",
        "+   \"outputs\": [\n",
        "+    {\n",
        "+     \"name\": \"stdout\",\n",
        "+     \"output_type\": \"stream\",\n",
        "+     \"text\": [\n",
        "+      \"Yes. Certain times of day\u2014especially early morning (dawn) and twilight\u2014are regarded as most auspicious for yoga practice, and regular, sustained practice over time is essential for progress.\\n\"\n",
        "+     ]\n",
        "+    }\n",
        "+   ],\n",
        "+   \"source\": [\n",
        "+    \"import os\\n\",\n",
        "+    \"from langchain_groq import ChatGroq\\n\",\n",
        "+    \"# Assume 'vector_store' and 'query' are already defined from the retrieval step\\n\",\n",
        "+    \"# vector_store = Chroma(...)\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"# --- Groq Integration ---\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"# Make sure your Groq API key is set\\n\",\n",
        "+    \"# os.environ[\\\"GROQ_API_KEY\\\"] = \\\"gsk_...\\\"\\n\",\n",
        "+    \"query = \\\"Is there any significance of time in the attainment of Yoga?\\\"\\n\",\n",
        "+    \"retrieved_docs = vector_store.similarity_search(query, k=5)\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"# 1. Define the Groq LLM you want to use for generation\\n\",\n",
        "+    \"# Llama3 8b is a great, fast choice available on Groq\\n\",\n",
        "+    \"llm = ChatGroq(model_name=\\\"openai/gpt-oss-20b\\\")\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"# 2. Create a prompt template (this part is unchanged)\\n\",\n",
        "+    \"prompt_template = \\\"\\\"\\\"\\n\",\n",
        "+    \"\\\"You are a helpful assistant. Use the provided context from the source\\\"\\n\",\n",
        "+    \"             \\\"the user's question accurately. \\\"\\n\",\n",
        "+    \"             \\\"Do not include verse text in your answer. The answer is is ashort as possible.\\n\",\n",
        "+    \"Context:\\n\",\n",
        "+    \"{context}\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"Question: {question}\\n\",\n",
        "+    \"\\\"\\\"\\\"\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"# 3. Format the retrieved documents into a single context string\\n\",\n",
        "+    \"context_string = \\\"\\\\n\\\\n\\\".join([doc.page_content for doc in retrieved_docs])\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"# 4. Fill the prompt with the context and question\\n\",\n",
        "+    \"formatted_prompt = prompt_template.format(\\n\",\n",
        "+    \"    context=context_string,\\n\",\n",
        "+    \"    question=query\\n\",\n",
        "+    \")\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"# 5. Call the Groq LLM to get the fast, final answer\\n\",\n",
        "+    \"final_answer = llm.invoke(formatted_prompt)\\n\",\n",
        "+    \"\\n\",\n",
        "+    \" \\n\",\n",
        "+    \"print(final_answer.content)\\n\",\n",
        "+    \" \"\n",
        "+   ]\n",
        "+  },\n",
        "+  {\n",
        "+   \"cell_type\": \"code\",\n",
        "+   \"execution_count\": null,\n",
        "+   \"id\": \"faf9db49\",\n",
        "+   \"metadata\": {},\n",
        "+   \"outputs\": [],\n",
        "+   \"source\": []\n",
        "+  },\n",
        "+  {\n",
        "+   \"cell_type\": \"code\",\n",
        "+   \"execution_count\": 11,\n",
        "+   \"id\": \"1f4027d8\",\n",
        "+   \"metadata\": {},\n",
        "+   \"outputs\": [\n",
        "+    {\n",
        "+     \"name\": \"stdout\",\n",
        "+     \"output_type\": \"stream\",\n",
        "+     \"text\": [\n",
        "+      \"Chroma + HuggingFaceEmbeddings ready.\\n\"\n",
        "+     ]\n",
        "+    }\n",
        "+   ],\n",
        "+   \"source\": [\n",
        "+    \"# Notebook cell: setup embeddings and Chroma (no OpenAI)\\n\",\n",
        "+    \"from modules.config import Config\\n\",\n",
        "+    \"from langchain_huggingface.embeddings import HuggingFaceEmbeddings\\n\",\n",
        "+    \"from langchain_chroma import Chroma\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"# HuggingFace embeddings (same as Neo4j pipeline)\\n\",\n",
        "+    \"embeddings = HuggingFaceEmbeddings(\\n\",\n",
        "+    \"    model_name=Config.EMBEDDING_MODEL,\\n\",\n",
        "+    \"    model_kwargs={'device': Config.EMBEDDING_DEVICE},\\n\",\n",
        "+    \"    encode_kwargs={'normalize_embeddings': True},\\n\",\n",
        "+    \")\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"vector_store = Chroma(\\n\",\n",
        "+    \"    collection_name=\\\"gita_idx\\\",\\n\",\n",
        "+    \"    embedding_function=embeddings,\\n\",\n",
        "+    \"    persist_directory=\\\"./chroma_huggingface_db\\\",\\n\",\n",
        "+    \")\\n\",\n",
        "+    \"print(\\\"Chroma + HuggingFaceEmbeddings ready.\\\")\"\n",
        "+   ]\n",
        "+  },\n",
        "+  {\n",
        "+   \"cell_type\": \"code\",\n",
        "+   \"execution_count\": 12,\n",
        "+   \"id\": \"9ddfba8f\",\n",
        "+   \"metadata\": {},\n",
        "+   \"outputs\": [\n",
        "+    {\n",
        "+     \"ename\": \"ValueError\",\n",
        "+     \"evalue\": \"Expected metadata value to be a str, int, float, bool, SparseVector, or None, got [1, 1, 1, 1, 1, 1] which is a list in upsert.\\n\\nTry filtering complex metadata from the document using langchain_community.vectorstores.utils.filter_complex_metadata.\",\n",
        "+     \"output_type\": \"error\",\n",
        "+     \"traceback\": [\n",
        "+      \"\\u001b[1;31m---------------------------------------------------------------------------\\u001b[0m\",\n",
        "+      \"\\u001b[1;31mValueError\\u001b[0m                                Traceback (most recent call last)\",\n",
        "+      \"File \\u001b[1;32m~\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\langchain_chroma\\\\vectorstores.py:647\\u001b[0m, in \\u001b[0;36mChroma.add_texts\\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\\u001b[0m\\n\\u001b[0;32m    646\\u001b[0m \\u001b[38;5;28;01mtry\\u001b[39;00m:\\n\\u001b[1;32m--> 647\\u001b[0m     \\u001b[38;5;28;43mself\\u001b[39;49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43m_collection\\u001b[49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43mupsert\\u001b[49m\\u001b[43m(\\u001b[49m\\n\\u001b[0;32m    648\\u001b[0m \\u001b[43m        \\u001b[49m\\u001b[43mmetadatas\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43mmetadatas\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m  \\u001b[49m\\u001b[38;5;66;43;03m# type: ignore[arg-type]\\u001b[39;49;00m\\n\\u001b[0;32m    649\\u001b[0m \\u001b[43m        \\u001b[49m\\u001b[43membeddings\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43membeddings_with_metadatas\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m  \\u001b[49m\\u001b[38;5;66;43;03m# type: ignore[arg-type]\\u001b[39;49;00m\\n\\u001b[0;32m    650\\u001b[0m \\u001b[43m        \\u001b[49m\\u001b[43mdocuments\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43mtexts_with_metadatas\\u001b[49m\\u001b[43m,\\u001b[49m\\n\\u001b[0;32m    651\\u001b[0m \\u001b[43m        \\u001b[49m\\u001b[43mids\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43mids_with_metadata\\u001b[49m\\u001b[43m,\\u001b[49m\\n\\u001b[0;32m    652\\u001b[0m \\u001b[43m    \\u001b[49m\\u001b[43m)\\u001b[49m\\n\\u001b[0;32m    653\\u001b[0m \\u001b[38;5;28;01mexcept\\u001b[39;00m \\u001b[38;5;167;01mValueError\\u001b[39;00m \\u001b[38;5;28;01mas\\u001b[39;00m e:\\n\",\n",
        "+      \"File \\u001b[1;32m~\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\chromadb\\\\api\\\\models\\\\Collection.py:442\\u001b[0m, in \\u001b[0;36mCollection.upsert\\u001b[1;34m(self, ids, embeddings, metadatas, documents, images, uris)\\u001b[0m\\n\\u001b[0;32m    431\\u001b[0m \\u001b[38;5;250m\\u001b[39m\\u001b[38;5;124;03m\\\"\\\"\\\"Update the embeddings, metadatas or documents for provided ids, or create them if they don't exist.\\u001b[39;00m\\n\\u001b[0;32m    432\\u001b[0m \\n\\u001b[0;32m    433\\u001b[0m \\u001b[38;5;124;03mArgs:\\u001b[39;00m\\n\\u001b[1;32m   (...)\\u001b[0m\\n\\u001b[0;32m    440\\u001b[0m \\u001b[38;5;124;03m    None\\u001b[39;00m\\n\\u001b[0;32m    441\\u001b[0m \\u001b[38;5;124;03m\\\"\\\"\\\"\\u001b[39;00m\\n\\u001b[1;32m--> 442\\u001b[0m upsert_request \\u001b[38;5;241m=\\u001b[39m \\u001b[38;5;28;43mself\\u001b[39;49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43m_validate_and_prepare_upsert_request\\u001b[49m\\u001b[43m(\\u001b[49m\\n\\u001b[0;32m    443\\u001b[0m \\u001b[43m    \\u001b[49m\\u001b[43mids\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43mids\\u001b[49m\\u001b[43m,\\u001b[49m\\n\\u001b[0;32m    444\\u001b[0m \\u001b[43m    \\u001b[49m\\u001b[43membeddings\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43membeddings\\u001b[49m\\u001b[43m,\\u001b[49m\\n\\u001b[0;32m    445\\u001b[0m \\u001b[43m    \\u001b[49m\\u001b[43mmetadatas\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43mmetadatas\\u001b[49m\\u001b[43m,\\u001b[49m\\n\\u001b[0;32m    446\\u001b[0m \\u001b[43m    \\u001b[49m\\u001b[43mdocuments\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43mdocuments\\u001b[49m\\u001b[43m,\\u001b[49m\\n\\u001b[0;32m    447\\u001b[0m \\u001b[43m    \\u001b[49m\\u001b[43mimages\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43mimages\\u001b[49m\\u001b[43m,\\u001b[49m\\n\\u001b[0;32m    448\\u001b[0m \\u001b[43m    \\u001b[49m\\u001b[43muris\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43muris\\u001b[49m\\u001b[43m,\\u001b[49m\\n\\u001b[0;32m    449\\u001b[0m \\u001b[43m\\u001b[49m\\u001b[43m)\\u001b[49m\\n\\u001b[0;32m    451\\u001b[0m \\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39m_client\\u001b[38;5;241m.\\u001b[39m_upsert(\\n\\u001b[0;32m    452\\u001b[0m     collection_id\\u001b[38;5;241m=\\u001b[39m\\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39mid,\\n\\u001b[0;32m    453\\u001b[0m     ids\\u001b[38;5;241m=\\u001b[39mupsert_request[\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mids\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m],\\n\\u001b[1;32m   (...)\\u001b[0m\\n\\u001b[0;32m    459\\u001b[0m     database\\u001b[38;5;241m=\\u001b[39m\\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39mdatabase,\\n\\u001b[0;32m    460\\u001b[0m )\\n\",\n",
        "+      \"File \\u001b[1;32m~\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\chromadb\\\\api\\\\models\\\\CollectionCommon.py:95\\u001b[0m, in \\u001b[0;36mvalidation_context.<locals>.decorator.<locals>.wrapper\\u001b[1;34m(self, *args, **kwargs)\\u001b[0m\\n\\u001b[0;32m     94\\u001b[0m \\u001b[38;5;28;01mtry\\u001b[39;00m:\\n\\u001b[1;32m---> 95\\u001b[0m     \\u001b[38;5;28;01mreturn\\u001b[39;00m \\u001b[43mfunc\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[38;5;28;43mself\\u001b[39;49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[43margs\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[43mkwargs\\u001b[49m\\u001b[43m)\\u001b[49m\\n\\u001b[0;32m     96\\u001b[0m \\u001b[38;5;28;01mexcept\\u001b[39;00m \\u001b[38;5;167;01mException\\u001b[39;00m \\u001b[38;5;28;01mas\\u001b[39;00m e:\\n\",\n",
        "+      \"File \\u001b[1;32m~\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\chromadb\\\\api\\\\models\\\\CollectionCommon.py:417\\u001b[0m, in \\u001b[0;36mCollectionCommon._validate_and_prepare_upsert_request\\u001b[1;34m(self, ids, embeddings, metadatas, documents, images, uris)\\u001b[0m\\n\\u001b[0;32m    416\\u001b[0m \\u001b[38;5;66;03m# Validate\\u001b[39;00m\\n\\u001b[1;32m--> 417\\u001b[0m \\u001b[43mvalidate_insert_record_set\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[43mrecord_set\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43mupsert_records\\u001b[49m\\u001b[43m)\\u001b[49m\\n\\u001b[0;32m    419\\u001b[0m \\u001b[38;5;66;03m# Prepare\\u001b[39;00m\\n\",\n",
        "+      \"File \\u001b[1;32m~\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\chromadb\\\\api\\\\types.py:317\\u001b[0m, in \\u001b[0;36mvalidate_insert_record_set\\u001b[1;34m(record_set)\\u001b[0m\\n\\u001b[0;32m    316\\u001b[0m \\u001b[38;5;28;01mif\\u001b[39;00m record_set[\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mmetadatas\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m] \\u001b[38;5;129;01mis\\u001b[39;00m \\u001b[38;5;129;01mnot\\u001b[39;00m \\u001b[38;5;28;01mNone\\u001b[39;00m:\\n\\u001b[1;32m--> 317\\u001b[0m     \\u001b[43mvalidate_metadatas\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[43mrecord_set\\u001b[49m\\u001b[43m[\\u001b[49m\\u001b[38;5;124;43m\\\"\\u001b[39;49m\\u001b[38;5;124;43mmetadatas\\u001b[39;49m\\u001b[38;5;124;43m\\\"\\u001b[39;49m\\u001b[43m]\\u001b[49m\\u001b[43m)\\u001b[49m\\n\",\n",
        "+      \"File \\u001b[1;32m~\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\chromadb\\\\api\\\\types.py:899\\u001b[0m, in \\u001b[0;36mvalidate_metadatas\\u001b[1;34m(metadatas)\\u001b[0m\\n\\u001b[0;32m    898\\u001b[0m \\u001b[38;5;28;01mfor\\u001b[39;00m metadata \\u001b[38;5;129;01min\\u001b[39;00m metadatas:\\n\\u001b[1;32m--> 899\\u001b[0m     \\u001b[43mvalidate_metadata\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[43mmetadata\\u001b[49m\\u001b[43m)\\u001b[49m\\n\\u001b[0;32m    900\\u001b[0m \\u001b[38;5;28;01mreturn\\u001b[39;00m metadatas\\n\",\n",
        "+      \"File \\u001b[1;32m~\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\chromadb\\\\api\\\\types.py:859\\u001b[0m, in \\u001b[0;36mvalidate_metadata\\u001b[1;34m(metadata)\\u001b[0m\\n\\u001b[0;32m    856\\u001b[0m     \\u001b[38;5;28;01melif\\u001b[39;00m \\u001b[38;5;129;01mnot\\u001b[39;00m \\u001b[38;5;28misinstance\\u001b[39m(value, \\u001b[38;5;28mbool\\u001b[39m) \\u001b[38;5;129;01mand\\u001b[39;00m \\u001b[38;5;129;01mnot\\u001b[39;00m \\u001b[38;5;28misinstance\\u001b[39m(\\n\\u001b[0;32m    857\\u001b[0m         value, (\\u001b[38;5;28mstr\\u001b[39m, \\u001b[38;5;28mint\\u001b[39m, \\u001b[38;5;28mfloat\\u001b[39m, \\u001b[38;5;28mtype\\u001b[39m(\\u001b[38;5;28;01mNone\\u001b[39;00m))\\n\\u001b[0;32m    858\\u001b[0m     ):\\n\\u001b[1;32m--> 859\\u001b[0m         \\u001b[38;5;28;01mraise\\u001b[39;00m \\u001b[38;5;167;01mValueError\\u001b[39;00m(\\n\\u001b[0;32m    860\\u001b[0m             \\u001b[38;5;124mf\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mExpected metadata value to be a str, int, float, bool, SparseVector, or None, got \\u001b[39m\\u001b[38;5;132;01m{\\u001b[39;00mvalue\\u001b[38;5;132;01m}\\u001b[39;00m\\u001b[38;5;124m which is a \\u001b[39m\\u001b[38;5;132;01m{\\u001b[39;00m\\u001b[38;5;28mtype\\u001b[39m(value)\\u001b[38;5;241m.\\u001b[39m\\u001b[38;5;18m__name__\\u001b[39m\\u001b[38;5;132;01m}\\u001b[39;00m\\u001b[38;5;124m\\\"\\u001b[39m\\n\\u001b[0;32m    861\\u001b[0m         )\\n\\u001b[0;32m    862\\u001b[0m \\u001b[38;5;28;01mreturn\\u001b[39;00m metadata\\n\",\n",
        "+      \"\\u001b[1;31mValueError\\u001b[0m: Expected metadata value to be a str, int, float, bool, SparseVector, or None, got [1, 1, 1, 1, 1, 1] which is a list in upsert.\",\n",
        "+      \"\\nThe above exception was the direct cause of the following exception:\\n\",\n",
        "+      \"\\u001b[1;31mValueError\\u001b[0m                                Traceback (most recent call last)\",\n",
        "+      \"Cell \\u001b[1;32mIn[12], line 13\\u001b[0m\\n\\u001b[0;32m      6\\u001b[0m final_docs, original_df \\u001b[38;5;241m=\\u001b[39m processor\\u001b[38;5;241m.\\u001b[39mprocess_csv_to_chunks(\\n\\u001b[0;32m      7\\u001b[0m     file_path\\u001b[38;5;241m=\\u001b[39mcsv_path,\\n\\u001b[0;32m      8\\u001b[0m     content_column\\u001b[38;5;241m=\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mtranslation\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m,\\n\\u001b[0;32m      9\\u001b[0m     filename\\u001b[38;5;241m=\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mgita.csv\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\n\\u001b[0;32m     10\\u001b[0m )\\n\\u001b[0;32m     12\\u001b[0m \\u001b[38;5;66;03m# Populate Chroma with the same docs\\u001b[39;00m\\n\\u001b[1;32m---> 13\\u001b[0m \\u001b[43mvector_store\\u001b[49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43madd_documents\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[43mfinal_docs\\u001b[49m\\u001b[43m)\\u001b[49m\\n\\u001b[0;32m     14\\u001b[0m \\u001b[38;5;28mprint\\u001b[39m(\\u001b[38;5;124mf\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mChroma populated with \\u001b[39m\\u001b[38;5;132;01m{\\u001b[39;00m\\u001b[38;5;28mlen\\u001b[39m(final_docs)\\u001b[38;5;132;01m}\\u001b[39;00m\\u001b[38;5;124m chunks from \\u001b[39m\\u001b[38;5;132;01m{\\u001b[39;00mcsv_path\\u001b[38;5;132;01m}\\u001b[39;00m\\u001b[38;5;124m.\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m)\\n\",\n",
        "+      \"File \\u001b[1;32m~\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\langchain_core\\\\vectorstores\\\\base.py:279\\u001b[0m, in \\u001b[0;36mVectorStore.add_documents\\u001b[1;34m(self, documents, **kwargs)\\u001b[0m\\n\\u001b[0;32m    277\\u001b[0m     texts \\u001b[38;5;241m=\\u001b[39m [doc\\u001b[38;5;241m.\\u001b[39mpage_content \\u001b[38;5;28;01mfor\\u001b[39;00m doc \\u001b[38;5;129;01min\\u001b[39;00m documents]\\n\\u001b[0;32m    278\\u001b[0m     metadatas \\u001b[38;5;241m=\\u001b[39m [doc\\u001b[38;5;241m.\\u001b[39mmetadata \\u001b[38;5;28;01mfor\\u001b[39;00m doc \\u001b[38;5;129;01min\\u001b[39;00m documents]\\n\\u001b[1;32m--> 279\\u001b[0m     \\u001b[38;5;28;01mreturn\\u001b[39;00m \\u001b[38;5;28;43mself\\u001b[39;49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43madd_texts\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[43mtexts\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[43mmetadatas\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[43mkwargs\\u001b[49m\\u001b[43m)\\u001b[49m\\n\\u001b[0;32m    280\\u001b[0m msg \\u001b[38;5;241m=\\u001b[39m (\\n\\u001b[0;32m    281\\u001b[0m     \\u001b[38;5;124mf\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124m`add_documents` and `add_texts` has not been implemented \\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\n\\u001b[0;32m    282\\u001b[0m     \\u001b[38;5;124mf\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mfor \\u001b[39m\\u001b[38;5;132;01m{\\u001b[39;00m\\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39m\\u001b[38;5;18m__class__\\u001b[39m\\u001b[38;5;241m.\\u001b[39m\\u001b[38;5;18m__name__\\u001b[39m\\u001b[38;5;132;01m}\\u001b[39;00m\\u001b[38;5;124m \\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\n\\u001b[0;32m    283\\u001b[0m )\\n\\u001b[0;32m    284\\u001b[0m \\u001b[38;5;28;01mraise\\u001b[39;00m \\u001b[38;5;167;01mNotImplementedError\\u001b[39;00m(msg)\\n\",\n",
        "+      \"File \\u001b[1;32m~\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\langchain_chroma\\\\vectorstores.py:659\\u001b[0m, in \\u001b[0;36mChroma.add_texts\\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\\u001b[0m\\n\\u001b[0;32m    654\\u001b[0m         \\u001b[38;5;28;01mif\\u001b[39;00m \\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mExpected metadata value to be\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m \\u001b[38;5;129;01min\\u001b[39;00m \\u001b[38;5;28mstr\\u001b[39m(e):\\n\\u001b[0;32m    655\\u001b[0m             msg \\u001b[38;5;241m=\\u001b[39m (\\n\\u001b[0;32m    656\\u001b[0m                 \\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mTry filtering complex metadata from the document using \\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\n\\u001b[0;32m    657\\u001b[0m                 \\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mlangchain_community.vectorstores.utils.filter_complex_metadata.\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\n\\u001b[0;32m    658\\u001b[0m             )\\n\\u001b[1;32m--> 659\\u001b[0m             \\u001b[38;5;28;01mraise\\u001b[39;00m \\u001b[38;5;167;01mValueError\\u001b[39;00m(e\\u001b[38;5;241m.\\u001b[39margs[\\u001b[38;5;241m0\\u001b[39m] \\u001b[38;5;241m+\\u001b[39m \\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;130;01m\\\\n\\u001b[39;00m\\u001b[38;5;130;01m\\\\n\\u001b[39;00m\\u001b[38;5;124m\\\"\\u001b[39m \\u001b[38;5;241m+\\u001b[39m msg) \\u001b[38;5;28;01mfrom\\u001b[39;00m\\u001b[38;5;250m \\u001b[39m\\u001b[38;5;21;01me\\u001b[39;00m\\n\\u001b[0;32m    660\\u001b[0m         \\u001b[38;5;28;01mraise\\u001b[39;00m e\\n\\u001b[0;32m    661\\u001b[0m \\u001b[38;5;28;01mif\\u001b[39;00m empty_ids:\\n\",\n",
        "+      \"\\u001b[1;31mValueError\\u001b[0m: Expected metadata value to be a str, int, float, bool, SparseVector, or None, got [1, 1, 1, 1, 1, 1] which is a list in upsert.\\n\\nTry filtering complex metadata from the document using langchain_community.vectorstores.utils.filter_complex_metadata.\"\n",
        "+     ]\n",
        "+    }\n",
        "+   ],\n",
        "+   \"source\": [\n",
        "+    \"# Notebook cell: build docs for Chroma (if not already built in this session)\\n\",\n",
        "+    \"from modules.document_processor import DocumentProcessor\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"csv_path = \\\"gita.csv\\\"  # adjust path if needed\\n\",\n",
        "+    \"processor = DocumentProcessor()\\n\",\n",
        "+    \"final_docs, original_df = processor.process_csv_to_chunks(\\n\",\n",
        "+    \"    file_path=csv_path,\\n\",\n",
        "+    \"    content_column=\\\"translation\\\",\\n\",\n",
        "+    \"    filename=\\\"gita.csv\\\"\\n\",\n",
        "+    \")\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"# Populate Chroma with the same docs\\n\",\n",
        "+    \"vector_store.add_documents(final_docs)\\n\",\n",
        "+    \"print(f\\\"Chroma populated with {len(final_docs)} chunks from {csv_path}.\\\")\"\n",
        "+   ]\n",
        "+  },\n",
        "+  {\n",
        "+   \"cell_type\": \"code\",\n",
        "+   \"execution_count\": null,\n",
        "+   \"id\": \"eeb2139d\",\n",
        "+   \"metadata\": {},\n",
        "+   \"outputs\": [],\n",
        "+   \"source\": [\n",
        "+    \"# Notebook cell: compare function\\n\",\n",
        "+    \"from modules.llm_chain import LLMChain\\n\",\n",
        "+    \"from modules.neo4j_manager import Neo4jManager\\n\",\n",
        "+    \"from modules.config import Config\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"def compare_results(query: str, filename: str, top_k: int = 5):\\n\",\n",
        "+    \"    llm = LLMChain()\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"    # 1) Rewrite once for fairness (same rule as your app)\\n\",\n",
        "+    \"    rw = llm.rewrite_query(query, chat_history=[], source_name=filename, fallback_on_error=True)\\n\",\n",
        "+    \"    rewritten = rw.get(\\\"rewritten_query\\\", query)\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"    # 2) Neo4j retrieval (connect to existing index for this file)\\n\",\n",
        "+    \"    idx_name = Config.get_index_name(filename)\\n\",\n",
        "+    \"    neo = Neo4jManager()                            # uses the same HF embeddings as Chroma\\n\",\n",
        "+    \"    neo.connect_to_existing_index(idx_name)         # index should already exist (created via app)\\n\",\n",
        "+    \"    neo_docs = neo.retrieve_with_filename_filter(rewritten, filename, top_k=top_k)\\n\",\n",
        "+    \"    neo_answer = llm.graph_qa_chain(\\n\",\n",
        "+    \"        question=query, docs=neo_docs, source_name=filename, memory_context=None, fallback_on_error=True\\n\",\n",
        "+    \"    )\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"    # 3) Chroma retrieval with the SAME rewritten query and SAME LLM\\n\",\n",
        "+    \"    chroma_docs = vector_store.similarity_search(rewritten, k=top_k)\\n\",\n",
        "+    \"    chroma_answer = llm.graph_qa_chain(\\n\",\n",
        "+    \"        question=query, docs=chroma_docs, source_name=filename, memory_context=None, fallback_on_error=True\\n\",\n",
        "+    \"    )\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"    # 4) Print side-by-side summaries\\n\",\n",
        "+    \"    print(\\\"=== Query ===\\\")\\n\",\n",
        "+    \"    print(\\\"Original: \\\", query)\\n\",\n",
        "+    \"    print(\\\"Rewritten:\\\", rewritten)\\n\",\n",
        "+    \"    print()\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"    print(\\\"=== Neo4j Answer ===\\\")\\n\",\n",
        "+    \"    print(neo_answer.get(\\\"text\\\", \\\"\\\"))\\n\",\n",
        "+    \"    print(\\\"Refs:\\\", neo_answer.get(\\\"refrence\\\", []))\\n\",\n",
        "+    \"    print()\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"    print(\\\"=== Chroma Answer ===\\\")\\n\",\n",
        "+    \"    print(chroma_answer.get(\\\"text\\\", \\\"\\\"))\\n\",\n",
        "+    \"    print(\\\"Refs:\\\", chroma_answer.get(\\\"refrence\\\", []))\\n\",\n",
        "+    \"    print()\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"    # Optional: top-1 snippet previews to see retrieved context difference\\n\",\n",
        "+    \"    if neo_docs:\\n\",\n",
        "+    \"        print(\\\"Neo4j top-1 snippet:\\\", neo_docs[0].page_content[:250].replace(\\\"\\\\n\\\", \\\" \\\"), \\\"| score:\\\", neo_docs[0].metadata.get(\\\"score\\\"))\\n\",\n",
        "+    \"    if chroma_docs:\\n\",\n",
        "+    \"        print(\\\"Chroma top-1 snippet:\\\", chroma_docs[0].page_content[:250].replace(\\\"\\\\n\\\", \\\" \\\"))\\n\",\n",
        "+    \"\\n\",\n",
        "+    \"# Example usage:\\n\",\n",
        "+    \"# compare_results(\\\"What is karma yoga?\\\", filename=\\\"gita.csv\\\", top_k=5)\"\n",
        "+   ]\n",
        "+  },\n",
        "+  {\n",
        "+   \"cell_type\": \"code\",\n",
        "+   \"execution_count\": null,\n",
        "+   \"id\": \"d5728302\",\n",
        "+   \"metadata\": {},\n",
        "+   \"outputs\": [],\n",
        "+   \"source\": [\n",
        "+    \"# Create the Neo4j vector index from the same final_docs (one-time)\\n\",\n",
        "+    \"nm = Neo4jManager()\\n\",\n",
        "+    \"nm.create_vector_store(final_docs, Config.get_index_name(\\\"gita.csv\\\"))\\n\",\n",
        "+    \"nm.create_file_relationships(\\\"gita.csv\\\")\"\n",
        "+   ]\n",
        "+  },\n",
        "+  {\n",
        "+   \"cell_type\": \"code\",\n",
        "+   \"execution_count\": null,\n",
        "+   \"id\": \"b0519509\",\n",
        "+   \"metadata\": {},\n",
        "+   \"outputs\": [],\n",
        "+   \"source\": []\n",
        "+  }\n",
        "+ ],\n",
        "+ \"metadata\": {\n",
        "+  \"kernelspec\": {\n",
        "+   \"display_name\": \"Python 3\",\n",
        "+   \"language\": \"python\",\n",
        "+   \"name\": \"python3\"\n",
        "+  },\n",
        "+  \"language_info\": {\n",
        "+   \"codemirror_mode\": {\n",
        "+    \"name\": \"ipython\",\n",
        "+    \"version\": 3\n",
        "+   },\n",
        "+   \"file_extension\": \".py\",\n",
        "+   \"mimetype\": \"text/x-python\",\n",
        "+   \"name\": \"python\",\n",
        "+   \"nbconvert_exporter\": \"python\",\n",
        "+   \"pygments_lexer\": \"ipython3\",\n",
        "+   \"version\": \"3.12.9\"\n",
        "+  }\n",
        "+ },\n",
        "+ \"nbformat\": 4,\n",
        "+ \"nbformat_minor\": 5\n",
        "+}\n"
      ]
    }
  ]
}
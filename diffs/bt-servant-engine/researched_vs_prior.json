{
  "project": "Research Data/bt-servant-engine",
  "repo": "unfoldingWord/bt-servant-engine",
  "prior_commit": "64e8ae1443de945f9d01633685d106326b3c558b",
  "researched_commit": "231d4f2d2c062c04a233a788fd99f487ba4866bf",
  "compare_url": "https://github.com/unfoldingWord/bt-servant-engine/compare/64e8ae1443de945f9d01633685d106326b3c558b...231d4f2d2c062c04a233a788fd99f487ba4866bf",
  "ahead_by": 1,
  "behind_by": 0,
  "changed_files": [
    {
      "path": "AGENTS.md",
      "status": "modified",
      "additions": 17,
      "deletions": 1,
      "patch": "@@ -24,6 +24,14 @@\n - Python 3.12+, 4-space indentation, UTF-8 files.\n - Naming: `snake_case` for functions/vars, `PascalCase` for classes, `UPPER_SNAKE` for constants.\n - Keep functions small and single-purpose; prefer explicit returns.\n+- Node handlers in `brain.py` should stay lean (target \u226460 lines). When they start\n+  to sprawl, extract helper functions so the orchestrators remain readable.\n+- Agentic strength now supports `normal`, `low`, and `very_low`. Use\n+  `_resolve_agentic_strength` + `_model_for_agentic_strength` to pick models instead of\n+  hardcoding string comparisons.\n+- When the user asks for an assessment or approach before coding, pause and confirm the\n+  plan in the conversation before modifying files. Only move to implementation after the\n+  user signs off.\n - Docstrings for public functions; keep comments minimal and useful.\n - Tools: `ruff` for style, `pylint` for code hygiene, `mypy` for typing.\n \n@@ -255,11 +263,19 @@ Recommended workflow\n ### Passage Selection (DRY)\n \n - Use the shared helper in `brain.py` to parse and normalize user queries that refer to Bible passages:\n-  - `_resolve_selection_for_single_book(query: str, query_lang: str) -> tuple[canonical_book | None, ranges | None, error | None]`\n+  - `_resolve_selection_for_single_book(query: str, query_lang: str, focus_hint: str | None = None) -> tuple[canonical_book | None, ranges | None, error | None]`\n   - It handles: translation to English for parsing, extraction via the selection prompt, the \"chapters X\u2013Y\" heuristic, canonicalization (single book), and range building (including whole\u2011book sentinel handling).\n - Do NOT duplicate selection parsing/normalization inside individual handlers. Call this helper and handle the `error` case by returning an intent\u2011specific message.\n - For labeling output headers, always use `utils/bsb.label_ranges(...)` to build a canonical reference string. It already special\u2011cases whole\u2011book selections to avoid odd labels like \"Book 1\u201110000\".\n \n+## GitHub CLI Usage\n+\n+- The development workstation has `gh` (GitHub CLI) installed. Prefer `gh pr create` to open pull requests directly from the terminal.\n+- When a PR is ready:\n+  1. Ensure the branch is pushed (`git push origin <branch>`).\n+  2. Run `gh pr create` with the appropriate base, title, and body, or use `gh pr create --fill` after preparing the template.\n+  3. Confirm the PR URL and share it with reviewers.\n+\n ### Passage Summary Intent\n - Extraction and scope:\n   - Supports a single canonical book per request. Disallow cross\u2011book selections.",
      "patch_lines": [
        "@@ -24,6 +24,14 @@\n",
        " - Python 3.12+, 4-space indentation, UTF-8 files.\n",
        " - Naming: `snake_case` for functions/vars, `PascalCase` for classes, `UPPER_SNAKE` for constants.\n",
        " - Keep functions small and single-purpose; prefer explicit returns.\n",
        "+- Node handlers in `brain.py` should stay lean (target \u226460 lines). When they start\n",
        "+  to sprawl, extract helper functions so the orchestrators remain readable.\n",
        "+- Agentic strength now supports `normal`, `low`, and `very_low`. Use\n",
        "+  `_resolve_agentic_strength` + `_model_for_agentic_strength` to pick models instead of\n",
        "+  hardcoding string comparisons.\n",
        "+- When the user asks for an assessment or approach before coding, pause and confirm the\n",
        "+  plan in the conversation before modifying files. Only move to implementation after the\n",
        "+  user signs off.\n",
        " - Docstrings for public functions; keep comments minimal and useful.\n",
        " - Tools: `ruff` for style, `pylint` for code hygiene, `mypy` for typing.\n",
        " \n",
        "@@ -255,11 +263,19 @@ Recommended workflow\n",
        " ### Passage Selection (DRY)\n",
        " \n",
        " - Use the shared helper in `brain.py` to parse and normalize user queries that refer to Bible passages:\n",
        "-  - `_resolve_selection_for_single_book(query: str, query_lang: str) -> tuple[canonical_book | None, ranges | None, error | None]`\n",
        "+  - `_resolve_selection_for_single_book(query: str, query_lang: str, focus_hint: str | None = None) -> tuple[canonical_book | None, ranges | None, error | None]`\n",
        "   - It handles: translation to English for parsing, extraction via the selection prompt, the \"chapters X\u2013Y\" heuristic, canonicalization (single book), and range building (including whole\u2011book sentinel handling).\n",
        " - Do NOT duplicate selection parsing/normalization inside individual handlers. Call this helper and handle the `error` case by returning an intent\u2011specific message.\n",
        " - For labeling output headers, always use `utils/bsb.label_ranges(...)` to build a canonical reference string. It already special\u2011cases whole\u2011book selections to avoid odd labels like \"Book 1\u201110000\".\n",
        " \n",
        "+## GitHub CLI Usage\n",
        "+\n",
        "+- The development workstation has `gh` (GitHub CLI) installed. Prefer `gh pr create` to open pull requests directly from the terminal.\n",
        "+- When a PR is ready:\n",
        "+  1. Ensure the branch is pushed (`git push origin <branch>`).\n",
        "+  2. Run `gh pr create` with the appropriate base, title, and body, or use `gh pr create --fill` after preparing the template.\n",
        "+  3. Confirm the PR URL and share it with reviewers.\n",
        "+\n",
        " ### Passage Summary Intent\n",
        " - Extraction and scope:\n",
        "   - Supports a single canonical book per request. Disallow cross\u2011book selections.\n"
      ]
    },
    {
      "path": "README.md",
      "status": "modified",
      "additions": 3,
      "deletions": 2,
      "patch": "@@ -127,17 +127,18 @@ uvicorn bt_servant:app --reload\n | `OPENAI_PRICING_JSON`  | (Optional) Per-million token pricing to compute costs in perf reports |\n | `ENABLE_ADMIN_AUTH`    | (Optional) When `true`, protect admin endpoints with a token   |\n | `ADMIN_API_TOKEN`      | (Optional) Token value accepted for admin endpoints            |\n+| `AGENTIC_STRENGTH`     | (Optional) Default LLM agentic strength (`normal`, `low`, `very_low`). Default is `low`.    |\n \n Other acceptable values for log level: critical, error, warning, and debug\n \n ---\n \n ## Pricing Defaults\n \n-- The engine computes token and cost totals in its performance reports. By default, it includes pricing for `gpt-4o` and voice flows (`gpt-4o-transcribe`, `gpt-4o-mini-tts`) if no environment override is provided.\n+- The engine computes token and cost totals in its performance reports. By default, it includes pricing for `gpt-4o`, `gpt-4o-mini`, and voice flows (`gpt-4o-transcribe`, `gpt-4o-mini-tts`) if no environment override is provided.\n \n ```\n-OPENAI_PRICING_JSON='{\"gpt-4o\": {\"input_per_million\": 2.5, \"output_per_million\": 10.0, \"cached_input\": 1.25}, \"gpt-4o-transcribe\": {\"input_per_million\": 2.5, \"output_per_million\": 10.0, \"audio_input_per_million\": 6.0}, \"gpt-4o-mini-tts\": {\"input_per_million\": 0.6, \"audio_output_per_million\": 12.0}}'\n+OPENAI_PRICING_JSON='{\"gpt-4o\": {\"input_per_million\": 2.5, \"output_per_million\": 10.0, \"cached_input\": 1.25}, \"gpt-4o-mini\": {\"input_per_million\": 0.15, \"output_per_million\": 0.6}, \"gpt-4o-transcribe\": {\"input_per_million\": 2.5, \"output_per_million\": 10.0, \"audio_input_per_million\": 6.0}, \"gpt-4o-mini-tts\": {\"input_per_million\": 0.6, \"audio_output_per_million\": 12.0}}'\n ```\n \n - To override (e.g., if pricing changes), set `OPENAI_PRICING_JSON` in your environment or `.env` to a JSON object mapping model names to per\u2011million rates. Recognized keys per model:",
      "patch_lines": [
        "@@ -127,17 +127,18 @@ uvicorn bt_servant:app --reload\n",
        " | `OPENAI_PRICING_JSON`  | (Optional) Per-million token pricing to compute costs in perf reports |\n",
        " | `ENABLE_ADMIN_AUTH`    | (Optional) When `true`, protect admin endpoints with a token   |\n",
        " | `ADMIN_API_TOKEN`      | (Optional) Token value accepted for admin endpoints            |\n",
        "+| `AGENTIC_STRENGTH`     | (Optional) Default LLM agentic strength (`normal`, `low`, `very_low`). Default is `low`.    |\n",
        " \n",
        " Other acceptable values for log level: critical, error, warning, and debug\n",
        " \n",
        " ---\n",
        " \n",
        " ## Pricing Defaults\n",
        " \n",
        "-- The engine computes token and cost totals in its performance reports. By default, it includes pricing for `gpt-4o` and voice flows (`gpt-4o-transcribe`, `gpt-4o-mini-tts`) if no environment override is provided.\n",
        "+- The engine computes token and cost totals in its performance reports. By default, it includes pricing for `gpt-4o`, `gpt-4o-mini`, and voice flows (`gpt-4o-transcribe`, `gpt-4o-mini-tts`) if no environment override is provided.\n",
        " \n",
        " ```\n",
        "-OPENAI_PRICING_JSON='{\"gpt-4o\": {\"input_per_million\": 2.5, \"output_per_million\": 10.0, \"cached_input\": 1.25}, \"gpt-4o-transcribe\": {\"input_per_million\": 2.5, \"output_per_million\": 10.0, \"audio_input_per_million\": 6.0}, \"gpt-4o-mini-tts\": {\"input_per_million\": 0.6, \"audio_output_per_million\": 12.0}}'\n",
        "+OPENAI_PRICING_JSON='{\"gpt-4o\": {\"input_per_million\": 2.5, \"output_per_million\": 10.0, \"cached_input\": 1.25}, \"gpt-4o-mini\": {\"input_per_million\": 0.15, \"output_per_million\": 0.6}, \"gpt-4o-transcribe\": {\"input_per_million\": 2.5, \"output_per_million\": 10.0, \"audio_input_per_million\": 6.0}, \"gpt-4o-mini-tts\": {\"input_per_million\": 0.6, \"audio_output_per_million\": 12.0}}'\n",
        " ```\n",
        " \n",
        " - To override (e.g., if pricing changes), set `OPENAI_PRICING_JSON` in your environment or `.env` to a JSON object mapping model names to per\u2011million rates. Recognized keys per model:\n"
      ]
    },
    {
      "path": "brain.py",
      "status": "modified",
      "additions": 637,
      "deletions": 167,
      "patch": "@@ -12,7 +12,7 @@\n import operator\n from pathlib import Path\n import re\n-from typing import Annotated, Dict, List, cast, Any, Optional\n+from typing import Annotated, Dict, Iterable, List, cast, Any, Optional\n from collections.abc import Hashable\n from enum import Enum\n from typing_extensions import TypedDict\n@@ -43,6 +43,7 @@\n     is_first_interaction,\n     set_first_interaction,\n     set_user_response_language,\n+    set_user_agentic_strength,\n )\n \n # (Moved dynamic feature messaging and related prompts below IntentType)\n@@ -206,6 +207,9 @@\n \n # Instructions\n \n+- Obey any developer instructions that precede the user message. They may narrow the\n+  focus to a specific clause of the user's text. When instructed, ignore unrelated\n+  books or requests outside that clause.\n - Only choose from these canonical book names (exact match):\n   {books}\n - Accept a variety of phrasings (e.g., \"John 3:16\", \"Jn 3:16\u201318\", \"1 John 2:1-3\", \"Psalm 1\", \"Song of Songs 2\").\n@@ -235,6 +239,9 @@\n - \"John and Mark 1:1\" -> choose Mark 1:1 (explicit qualifier picks Mark over first mention).\n - \"summarize 3 John\" -> choose book \"3 John\" with no chapters/verses (whole book selection).\n - \"summarize John 3\" -> choose book \"John\" with start_chapter=3 (whole chapter if no verses).\n+- Developer hint: \"Focus only on the portion asking for translation helps.\"\n+  User message: \"I want to listen to John 1:1, and I also want help translating Gal 1:3-4.\"\n+  -> choose Galatians 1:3-4 (ignore the listening request entirely).\n \"\"\"\n \n \n@@ -361,6 +368,11 @@\n     The user wants to change the language in which the system responds. They might ask for responses in \n     Spanish, French, Arabic, etc.\n   </intent>\n+  <intent name=\"set-agentic-strength\">\n+    The user wants to adjust how assertive the assistant should be when answering. They might say \"set my agentic\n+    strength to low\" or \"use normal agentic strength\". Only use this when they clearly request one of the supported\n+    strength levels (normal, low, or very low).\n+  </intent>\n   <intent name=\"retrieve-system-information\">\n     The user wants information about the BT Servant system itself \u2014 how it works, where it gets data, uptime, \n     example questions, supported languages, features, or current system configuration (like the documents currently \n@@ -537,6 +549,10 @@\n     <message>Can you reply to me in French from now on?</message>\n     <intent>set-response-language</intent>\n   </example>\n+  <example>\n+    <message>Set my agentic strength to low.</message>\n+    <intent>set-agentic-strength</intent>\n+  </example>\n   <example>\n     <message>Where does BT Servant get its information from?</message>\n     <intent>retrieve-system-information</intent>\n@@ -719,6 +735,68 @@ class ResponseLanguage(BaseModel):\n \"\"\"\n \n \n+ALLOWED_AGENTIC_STRENGTH = {\"normal\", \"low\", \"very_low\"}\n+\n+\n+class AgenticStrengthChoice(str, Enum):\n+    \"\"\"Accepted agentic strength options for controllable responses.\"\"\"\n+\n+    NORMAL = \"normal\"\n+    LOW = \"low\"\n+    VERY_LOW = \"very_low\"\n+    UNKNOWN = \"unknown\"\n+\n+\n+class AgenticStrengthSetting(BaseModel):\n+    \"\"\"Schema for parsing agentic strength adjustments from the user.\"\"\"\n+\n+    strength: AgenticStrengthChoice\n+\n+\n+SET_AGENTIC_STRENGTH_AGENT_SYSTEM_PROMPT = \"\"\"\n+Task: Determine whether the user is asking to adjust the agentic strength setting. The allowed values are \"normal\",\n+\"low\", and \"very_low\". Use the conversation context and latest message to infer the requested level.\n+\n+Output schema: { \"strength\": <normal|low|very_low|unknown> }\n+\n+Rules:\n+- If the user clearly requests \"normal\", \"low\", or \"very low\", return that value.\n+- If the intent is ambiguous or references any other option, return \"unknown\".\n+- Do not include explanations or additional text. Only produce a JSON object that matches the schema.\n+\"\"\"\n+\n+\n+def _resolve_agentic_strength(state: BrainState) -> str:\n+    \"\"\"Return the effective agentic strength, honoring user overrides when set.\"\"\"\n+    candidate = cast(Optional[str], state.get(\"agentic_strength\") or state.get(\"user_agentic_strength\"))\n+    if isinstance(candidate, str):\n+        lowered = candidate.lower()\n+        if lowered in ALLOWED_AGENTIC_STRENGTH:\n+            return lowered\n+\n+    configured = getattr(config, \"AGENTIC_STRENGTH\", \"normal\")\n+    if isinstance(configured, str):\n+        configured_lower = configured.lower()\n+        if configured_lower in ALLOWED_AGENTIC_STRENGTH:\n+            return configured_lower\n+    return \"normal\"\n+\n+\n+def _model_for_agentic_strength(\n+    agentic_strength: str,\n+    *,\n+    allow_low: bool,\n+    allow_very_low: bool,\n+) -> str:\n+    \"\"\"Return GPT model name based on strength and allowed downgrades.\"\"\"\n+    allowed: set[str] = set()\n+    if allow_low:\n+        allowed.add(\"low\")\n+    if allow_very_low:\n+        allowed.add(\"very_low\")\n+    return \"gpt-4o-mini\" if agentic_strength in allowed else \"gpt-4o\"\n+\n+\n class MessageLanguage(BaseModel):\n     \"\"\"Model for parsing/validating the detected language of a message.\"\"\"\n     language: Language\n@@ -790,6 +868,254 @@ def _reconstruct_structured_text(resp_item: dict | str, localize_to: Optional[st\n     return str(body)\n \n \n+TranslationRange = tuple[int, int | None, int | None, int | None]\n+\n+\n+def _partition_response_items(responses: Iterable[dict]) -> tuple[list[dict], list[dict]]:\n+    \"\"\"Split responses into scripture-protected and normal sets.\"\"\"\n+    protected: list[dict] = []\n+    normal: list[dict] = []\n+    for item in responses:\n+        if _is_protected_response_item(item):\n+            protected.append(item)\n+        else:\n+            normal.append(item)\n+    return protected, normal\n+\n+\n+def _normalize_single_response(item: dict) -> dict | str:\n+    \"\"\"Return a representation suitable for translation when no combine is needed.\"\"\"\n+    body = cast(dict | str, item.get(\"response\"))\n+    if isinstance(body, str):\n+        return body\n+    return item\n+\n+\n+def _build_translation_queue(\n+    state: BrainState,\n+    protected_items: list[dict],\n+    normal_items: list[dict],\n+) -> list[dict | str]:\n+    \"\"\"Assemble responses in the order they should be translated or localized.\"\"\"\n+    queue: list[dict | str] = list(protected_items)\n+    non_combinable: list[dict] = [i for i in normal_items if i.get(\"suppress_combining\")]\n+    combinable: list[dict] = [i for i in normal_items if not i.get(\"suppress_combining\")]\n+\n+    for item in non_combinable:\n+        queue.append(_normalize_single_response(item))\n+\n+    if not combinable:\n+        return queue\n+    if len(combinable) == 1:\n+        queue.append(_normalize_single_response(combinable[0]))\n+        return queue\n+    queue.append(\n+        combine_responses(\n+            state[\"user_chat_history\"],\n+            state[\"user_query\"],\n+            combinable,\n+        )\n+    )\n+    return queue\n+\n+\n+def _resolve_target_language(\n+    state: BrainState,\n+    responses_for_translation: list[dict | str],\n+) -> tuple[Optional[str], Optional[list[str]]]:\n+    \"\"\"Determine the target language or build a pass-through fallback.\"\"\"\n+    user_language = cast(Optional[str], state.get(\"user_response_language\"))\n+    if user_language:\n+        return user_language, None\n+\n+    target_language = cast(str, state.get(\"query_language\"))\n+    if target_language != LANGUAGE_UNKNOWN:\n+        return target_language, None\n+\n+    logger.warning('target language unknown. bailing out.')\n+    passthrough_texts: list[str] = [\n+        _reconstruct_structured_text(resp_item=resp, localize_to=None)\n+        for resp in responses_for_translation\n+    ]\n+    notice = (\n+        \"You haven't set your desired response language and I wasn't able to determine the language of your \"\n+        \"original message in order to match it. You can set your desired response language at any time by \"\n+        \"saying: Set my response language to Spanish, or Indonesian, or any of the supported languages: \"\n+        f\"{', '.join(supported_language_map.keys())}.\"\n+    )\n+    passthrough_texts.append(notice)\n+    return None, passthrough_texts\n+\n+\n+def _translate_or_localize_response(\n+    resp: dict | str,\n+    target_language: str,\n+    agentic_strength: str,\n+) -> str:\n+    \"\"\"Translate free-form text or localize structured scripture outputs.\"\"\"\n+    if isinstance(resp, str):\n+        sample = _sample_for_language_detection(resp)\n+        detected_lang = detect_language(sample, agentic_strength=agentic_strength) if sample else target_language\n+        if detected_lang != target_language:\n+            logger.info('preparing to translate to %s', target_language)\n+            return translate_text(\n+                response_text=resp,\n+                target_language=target_language,\n+                agentic_strength=agentic_strength,\n+            )\n+        logger.info('chunk translation not required. using chunk as is.')\n+        return resp\n+\n+    body = cast(dict | str, resp.get(\"response\"))\n+    if isinstance(body, dict) and isinstance(body.get(\"segments\"), list):\n+        item_lang = cast(Optional[str], body.get(\"content_language\"))\n+        header_is_translated = bool(body.get(\"header_is_translated\"))\n+        localize_to = None if header_is_translated else (item_lang or target_language)\n+        return _reconstruct_structured_text(resp_item=resp, localize_to=localize_to)\n+    return str(body)\n+\n+\n+def _compact_translation_help_entries(entries: list[dict]) -> list[dict]:\n+    \"\"\"Reduce translation help entries to essentials for the LLM payload.\"\"\"\n+    compact: list[dict] = []\n+    for entry in entries:\n+        verse_text = cast(str, entry.get(\"ult_verse_text\") or \"\")\n+        notes: list[dict] = []\n+        for note in cast(list[dict], entry.get(\"notes\") or []):\n+            note_text = cast(str, note.get(\"note\") or \"\")\n+            if not note_text:\n+                continue\n+            compact_note: dict[str, str] = {\"note\": note_text}\n+            quote = cast(Optional[str], note.get(\"orig_language_quote\"))\n+            if quote:\n+                compact_note[\"orig_language_quote\"] = quote\n+            notes.append(compact_note)\n+        compact.append(\n+            {\n+                \"reference\": entry.get(\"reference\"),\n+                \"verse_text\": verse_text,\n+                \"notes\": notes,\n+            }\n+        )\n+    return compact\n+\n+\n+def _prepare_translation_helps(\n+    state: BrainState,\n+    th_root: Path,\n+    bsb_root: Path,\n+    *,\n+    selection_focus_hint: str | None = None,\n+) -> tuple[Optional[str], Optional[list[TranslationRange]], Optional[list[dict]], Optional[str]]:\n+    \"\"\"Resolve canonical selection, enforce limits, and load raw help entries.\"\"\"\n+    canonical_book, ranges, err = _resolve_selection_for_single_book(\n+        state[\"transformed_query\"],\n+        state[\"query_language\"],\n+        focus_hint=selection_focus_hint,\n+    )\n+    if err:\n+        return None, None, None, err\n+    assert canonical_book is not None and ranges is not None\n+\n+    missing_books = set(get_missing_th_books(th_root))\n+    if canonical_book in missing_books:\n+        return (\n+            None,\n+            None,\n+            None,\n+            (\n+                \"Translation helps for \"\n+                f\"{BSB_BOOK_MAP[canonical_book]['ref_abbr']} are not available yet. \"\n+                \"Currently missing books: \"\n+                f\"{', '.join(sorted(BSB_BOOK_MAP[b]['ref_abbr'] for b in missing_books))}. \"\n+                \"Would you like translation help for one of the supported books instead?\"\n+            ),\n+        )\n+\n+    verse_count = len(select_verses(bsb_root, canonical_book, ranges))\n+    if verse_count > config.TRANSLATION_HELPS_VERSE_LIMIT:\n+        return (\n+            None,\n+            None,\n+            None,\n+            (\n+                \"I can only provide translate help for \"\n+                f\"{config.TRANSLATION_HELPS_VERSE_LIMIT} verses at a time. \"\n+                \"Your selection \"\n+                f\"{label_ranges(canonical_book, ranges)} includes {verse_count} verses. \"\n+                \"Please narrow the range (e.g., a chapter or a shorter span).\"\n+            ),\n+        )\n+\n+    limited_ranges = clamp_ranges_by_verse_limit(\n+        bsb_root,\n+        canonical_book,\n+        ranges,\n+        max_verses=config.TRANSLATION_HELPS_VERSE_LIMIT,\n+    )\n+    if not limited_ranges:\n+        return (\n+            None,\n+            None,\n+            None,\n+            \"I couldn't identify verses for that selection in the BSB index. Please try another reference.\",\n+        )\n+\n+    raw_helps = select_translation_helps(th_root, canonical_book, limited_ranges)\n+    logger.info(\"[translation-helps] selected %d help entries\", len(raw_helps))\n+    if not raw_helps:\n+        return (\n+            None,\n+            None,\n+            None,\n+            \"I couldn't locate translation helps for that selection. Please check the reference and try again.\",\n+        )\n+    return canonical_book, list(limited_ranges), raw_helps, None\n+\n+\n+def _build_translation_helps_context(\n+    canonical_book: str,\n+    ranges: list[TranslationRange],\n+    raw_helps: list[dict],\n+) -> tuple[str, dict[str, Any]]:\n+    \"\"\"Return the reference label and compact JSON context for the LLM.\"\"\"\n+    ref_label = label_ranges(canonical_book, ranges)\n+    context_obj = {\n+        \"reference_label\": ref_label,\n+        \"selection\": {\n+            \"book\": canonical_book,\n+            \"ranges\": [\n+                {\n+                    \"start_chapter\": sc,\n+                    \"start_verse\": sv,\n+                    \"end_chapter\": ec,\n+                    \"end_verse\": ev,\n+                }\n+                for (sc, sv, ec, ev) in ranges\n+            ],\n+        },\n+        \"translation_helps\": _compact_translation_help_entries(raw_helps),\n+    }\n+    return ref_label, context_obj\n+\n+\n+def _build_translation_helps_messages(ref_label: str, context_obj: dict[str, object]) -> list[EasyInputMessageParam]:\n+    \"\"\"Construct the LLM messages for the translation helps prompt.\"\"\"\n+    payload = json.dumps(context_obj, ensure_ascii=False)\n+    return [\n+        {\"role\": \"developer\", \"content\": \"Focus only on the portion of the user's message that asked for translation helps. Ignore any other requests or book references in the message.\"},\n+        {\"role\": \"developer\", \"content\": f\"Selection: {ref_label}\"},\n+        {\"role\": \"developer\", \"content\": \"Use the JSON context below strictly:\"},\n+        {\"role\": \"developer\", \"content\": payload},\n+        {\n+            \"role\": \"user\",\n+            \"content\": (\n+                \"Using the provided context, explain the translation challenges and give actionable guidance for this selection.\"\n+            ),\n+        },\n+    ]\n+\n+\n class IntentType(str, Enum):\n     \"\"\"Enumeration of all supported user intents in the graph.\"\"\"\n     GET_BIBLE_TRANSLATION_ASSISTANCE = \"get-bible-translation-assistance\"\n@@ -803,6 +1129,7 @@ class IntentType(str, Enum):\n     PERFORM_UNSUPPORTED_FUNCTION = \"perform-unsupported-function\"\n     RETRIEVE_SYSTEM_INFORMATION = \"retrieve-system-information\"\n     SET_RESPONSE_LANGUAGE = \"set-response-language\"\n+    SET_AGENTIC_STRENGTH = \"set-agentic-strength\"\n     CONVERSE_WITH_BT_SERVANT = 'converse-with-bt-servant'\n \n \n@@ -820,6 +1147,8 @@ class BrainState(TypedDict, total=False):\n     perf_trace_id: str\n     query_language: str\n     user_response_language: str\n+    agentic_strength: str\n+    user_agentic_strength: str\n     transformed_query: str\n     docs: List[Dict[str, str]]\n     collection_used: str\n@@ -831,6 +1160,7 @@ class BrainState(TypedDict, total=False):\n     passage_selection: list[dict]\n     # Delivery hint for bt_servant to send a voice message instead of text\n     send_voice_message: bool\n+    voice_message_text: str\n \n \n # Centralized capability registry and builders for feature help/boilerplate\n@@ -914,6 +1244,15 @@ def _capabilities() -> List[Capability]:\n             ],\n             \"include_in_boilerplate\": True,\n         },\n+        {\n+            \"intent\": IntentType.SET_AGENTIC_STRENGTH,\n+            \"label\": \"Adjust agentic strength\",\n+            \"description\": \"Tune how assertive the assistant should be (normal, low, or very low).\",\n+            \"examples\": [\n+                \"Set my agentic strength to low.\",\n+            ],\n+            \"include_in_boilerplate\": False,\n+        },\n         {\n             \"intent\": IntentType.SET_RESPONSE_LANGUAGE,\n             \"label\": \"Set response language\",\n@@ -1138,6 +1477,97 @@ def set_response_language(state: Any) -> dict:\n     }\n \n \n+def set_agentic_strength(state: Any) -> dict:\n+    \"\"\"Detect and persist the user's preferred agentic strength.\"\"\"\n+    s = cast(BrainState, state)\n+    chat_input: list[EasyInputMessageParam] = [\n+        {\n+            \"role\": \"user\",\n+            \"content\": f\"Past conversation: {json.dumps(s['user_chat_history'])}\",\n+        },\n+        {\n+            \"role\": \"user\",\n+            \"content\": f\"the user's most recent message: {s['user_query']}\",\n+        },\n+        {\n+            \"role\": \"user\",\n+            \"content\": \"Is the user asking to set the agentic strength to normal, low, or very low?\",\n+        },\n+    ]\n+\n+    try:\n+        response = open_ai_client.responses.parse(\n+            model=\"gpt-4o\",\n+            instructions=SET_AGENTIC_STRENGTH_AGENT_SYSTEM_PROMPT,\n+            input=cast(Any, chat_input),\n+            text_format=AgenticStrengthSetting,\n+            temperature=0,\n+            store=False,\n+        )\n+        usage = getattr(response, \"usage\", None)\n+        if usage is not None:\n+            add_tokens(\n+                getattr(usage, \"input_tokens\", None),\n+                getattr(usage, \"output_tokens\", None),\n+                getattr(usage, \"total_tokens\", None)\n+                or (\n+                    (getattr(usage, \"input_tokens\", None) or 0)\n+                    + (getattr(usage, \"output_tokens\", None) or 0)\n+                ),\n+                model=\"gpt-4o\",\n+                cached_input_tokens=_extract_cached_input_tokens(usage),\n+            )\n+        parsed = cast(AgenticStrengthSetting | None, response.output_parsed)\n+    except OpenAIError:\n+        logger.error(\"[agentic-strength] OpenAI request failed while parsing user preference.\", exc_info=True)\n+        parsed = None\n+    except Exception:  # pylint: disable=broad-except\n+        logger.error(\"[agentic-strength] Unexpected failure while parsing agentic strength.\", exc_info=True)\n+        parsed = None\n+\n+    if not parsed or parsed.strength == AgenticStrengthChoice.UNKNOWN:\n+        msg = (\n+            \"I can set the agentic strength to normal, low, or very low. Please specify one of those options \"\n+            \"so I can update it.\"\n+        )\n+        return {\n+            \"responses\": [\n+                {\"intent\": IntentType.SET_AGENTIC_STRENGTH, \"response\": msg}\n+            ]\n+        }\n+\n+    desired = parsed.strength.value\n+    user_id: str = s[\"user_id\"]\n+    try:\n+        set_user_agentic_strength(user_id, desired)\n+    except ValueError:\n+        logger.warning(\"[agentic-strength] Attempted to set invalid value '%s' for user %s\", desired, user_id)\n+        msg = (\n+            \"That setting isn't supported. I can only use normal, low, or very low for agentic strength.\"\n+        )\n+        return {\n+            \"responses\": [\n+                {\"intent\": IntentType.SET_AGENTIC_STRENGTH, \"response\": msg}\n+            ]\n+        }\n+\n+    friendly = {\n+        \"normal\": \"Normal\",\n+        \"low\": \"Low\",\n+        \"very_low\": \"Very Low\",\n+    }.get(desired, desired.capitalize())\n+    response_text = (\n+        f\"Agentic strength set to {friendly.lower()}. I'll use the {friendly} setting from now on.\"\n+    )\n+    return {\n+        \"responses\": [\n+            {\"intent\": IntentType.SET_AGENTIC_STRENGTH, \"response\": response_text}\n+        ],\n+        \"agentic_strength\": desired,\n+        \"user_agentic_strength\": desired,\n+    }\n+\n+\n def combine_responses(chat_history, latest_user_message, responses) -> str:\n     \"\"\"Ask OpenAI to synthesize multiple node responses into one coherent text.\n \n@@ -1180,83 +1610,56 @@ def combine_responses(chat_history, latest_user_message, responses) -> str:\n \n \n def translate_responses(state: Any) -> dict:\n-    \"\"\"Translate the response(s) into the user's desired language if needed.\n-\n-    Scripture-protected responses are not combined via LLM and are not machine-\n-    translated. If present, they are passed through (with optional localized\n-    headers) and the remaining responses are combined and translated as needed.\n-    \"\"\"\n+    \"\"\"Translate or localize responses into the user's desired language.\"\"\"\n     s = cast(BrainState, state)\n-    uncombined = list(s[\"responses\"])\n-\n-    protected_items: list[dict] = [i for i in uncombined if _is_protected_response_item(i)]\n-    normal_items: list[dict] = [i for i in uncombined if not _is_protected_response_item(i)]\n+    raw_responses = [\n+        resp for resp in cast(list[dict], s[\"responses\"]) if not resp.get(\"suppress_text_delivery\")\n+    ]\n+    if not raw_responses:\n+        if bool(s.get(\"send_voice_message\")):\n+            logger.info(\"[translate] skipping text translation because delivery is voice-only\")\n+            return {\"translated_responses\": []}\n+        raise ValueError(\"no responses to translate. something bad happened. bailing out.\")\n \n-    responses_for_translation: list[dict | str] = list(protected_items)\n-    # Combine normal items (if any) using LLM synthesizer into a single string and append\n-    if normal_items:\n-        responses_for_translation.append(\n-            combine_responses(s[\"user_chat_history\"], s[\"user_query\"], normal_items)\n-        )\n+    protected_items, normal_items = _partition_response_items(raw_responses)\n+    responses_for_translation = _build_translation_queue(s, protected_items, normal_items)\n     if not responses_for_translation:\n+        if bool(s.get(\"send_voice_message\")):\n+            logger.info(\"[translate] no text responses after queue assembly; voice-only delivery\")\n+            return {\"translated_responses\": []}\n         raise ValueError(\"no responses to translate. something bad happened. bailing out.\")\n \n-    if s[\"user_response_language\"]:\n-        target_language = s[\"user_response_language\"]\n-    else:\n-        target_language = s[\"query_language\"]\n-        if target_language == LANGUAGE_UNKNOWN:\n-            logger.warning('target language unknown. bailing out.')\n-            # Build pass-through texts for current responses, then append notice\n-            passthrough_texts: list[str] = [\n-                _reconstruct_structured_text(resp_item=resp, localize_to=None)\n-                for resp in responses_for_translation\n-            ]\n-            passthrough_texts.append(\n-                \"You haven't set your desired response language and I wasn't able to determine the language of your \"\n-                \"original message in order to match it. You can set your desired response language at any time by \"\n-                \"saying: Set my response language to Spanish, or Indonesian, or any of the supported languages: \"\n-                f\"{', '.join(supported_language_map.keys())}.\"\n-            )\n-            return {\"translated_responses\": passthrough_texts}\n-\n-    translated_responses: list[str] = []\n-    for resp in responses_for_translation:\n-        if isinstance(resp, str):\n-            if detect_language(resp) != target_language:\n-                logger.info('preparing to translate to %s', target_language)\n-                translated_responses.append(translate_text(response_text=resp, target_language=target_language))\n-            else:\n-                logger.info('chunk translation not required. using chunk as is.')\n-                translated_responses.append(resp)\n-            continue\n-\n-        # Structured/metadata response\n-        body = cast(dict | str, resp.get(\"response\"))\n-        if isinstance(body, dict) and isinstance(body.get(\"segments\"), list):\n-            item_lang = cast(Optional[str], body.get(\"content_language\"))\n-            header_is_translated = bool(body.get(\"header_is_translated\"))\n-            # If the header was already translated by the producer, do not localize again.\n-            # Otherwise, localize the canonical header to the passage's content language when known,\n-            # falling back to the target UI language.\n-            localize_to = None if header_is_translated else (item_lang or target_language)\n-            final_text2 = _reconstruct_structured_text(resp_item=resp, localize_to=localize_to)\n-            translated_responses.append(final_text2)\n-            continue\n+    target_language, passthrough = _resolve_target_language(s, responses_for_translation)\n+    if passthrough is not None:\n+        return {\"translated_responses\": passthrough}\n+    assert target_language is not None\n \n-        # Unknown structured shape: fall back to string conversion\n-        translated_responses.append(str(body))\n-    return {\n-        \"translated_responses\": translated_responses\n-    }\n+    agentic_strength = _resolve_agentic_strength(s)\n+    translated_responses = [\n+        _translate_or_localize_response(resp, target_language, agentic_strength)\n+        for resp in responses_for_translation\n+    ]\n+    return {\"translated_responses\": translated_responses}\n \n \n-def translate_text(response_text: str, target_language: str) -> str:\n+def translate_text(\n+    response_text: str,\n+    target_language: str,\n+    *,\n+    agentic_strength: Optional[str] = None,\n+) -> str:\n     \"\"\"Translate a single text into the target ISO 639-1 language code.\n \n     Returns a plain string. If the OpenAI SDK returns a structured content\n     list or None, normalize it to a string.\n     \"\"\"\n+    resolved_strength = (\n+        agentic_strength if agentic_strength in ALLOWED_AGENTIC_STRENGTH else None\n+    )\n+    if resolved_strength is None:\n+        configured = getattr(config, \"AGENTIC_STRENGTH\", \"normal\")\n+        resolved_strength = configured if configured in ALLOWED_AGENTIC_STRENGTH else \"normal\"\n+    model_name = _model_for_agentic_strength(resolved_strength, allow_low=False, allow_very_low=True)\n     chat_messages = cast(List[ChatCompletionMessageParam], [\n         {\n             \"role\": \"system\",\n@@ -1271,7 +1674,7 @@ def translate_text(response_text: str, target_language: str) -> str:\n         },\n     ])\n     completion = open_ai_client.chat.completions.create(\n-        model=\"gpt-4o\",\n+        model=model_name,\n         messages=chat_messages,\n     )\n     usage = getattr(completion, \"usage\", None)\n@@ -1280,7 +1683,7 @@ def translate_text(response_text: str, target_language: str) -> str:\n         ot = getattr(usage, \"completion_tokens\", None)\n         tt = getattr(usage, \"total_tokens\", None)\n         cit = _extract_cached_input_tokens(usage)\n-        add_tokens(it, ot, tt, model=\"gpt-4o\", cached_input_tokens=cit)\n+        add_tokens(it, ot, tt, model=model_name, cached_input_tokens=cit)\n     content = completion.choices[0].message.content\n     if isinstance(content, list):\n         text = \"\".join(part.get(\"text\", \"\") if isinstance(part, dict) else \"\" for part in content)\n@@ -1292,7 +1695,7 @@ def translate_text(response_text: str, target_language: str) -> str:\n     return cast(str, text)\n \n \n-def detect_language(text) -> str:\n+def detect_language(text: str, *, agentic_strength: Optional[str] = None) -> str:  # pylint: disable=too-many-locals\n     \"\"\"Detect ISO 639-1 language code of the given text via OpenAI.\n \n     Uses a domain-aware prompt with deterministic decoding and a light\n@@ -1305,6 +1708,11 @@ def detect_language(text) -> str:\n             \"content\": f\"text: {text}\",\n         },\n     ]\n+    strength_source = agentic_strength if agentic_strength is not None else getattr(config, \"AGENTIC_STRENGTH\", \"normal\")\n+    strength = str(strength_source).lower()\n+    if strength not in ALLOWED_AGENTIC_STRENGTH:\n+        strength = \"normal\"\n+    model_name = _model_for_agentic_strength(strength, allow_low=True, allow_very_low=True)\n     response = open_ai_client.responses.parse(\n         model=\"gpt-4o\",\n         instructions=DETECT_LANGUAGE_AGENT_SYSTEM_PROMPT,\n@@ -1321,7 +1729,7 @@ def detect_language(text) -> str:\n         if tt is None and (it is not None or ot is not None):\n             tt = (it or 0) + (ot or 0)\n         cit = _extract_cached_input_tokens(usage)\n-        add_tokens(it, ot, tt, model=\"gpt-4o\", cached_input_tokens=cit)\n+        add_tokens(it, ot, tt, model=model_name, cached_input_tokens=cit)\n     message_language = cast(MessageLanguage | None, response.output_parsed)\n     predicted = message_language.language.value if message_language else \"en\"\n     logger.info(\"language detection (model): %s\", predicted)\n@@ -1356,7 +1764,8 @@ def determine_query_language(state: Any) -> dict:\n     \"\"\"Determine the language of the user's original query and set collection order.\"\"\"\n     s = cast(BrainState, state)\n     query = s[\"user_query\"]\n-    query_language = detect_language(query)\n+    agentic_strength = _resolve_agentic_strength(s)\n+    query_language = detect_language(query, agentic_strength=agentic_strength)\n     logger.info(\"language code %s detected by gpt-4o.\", query_language)\n     stack_rank_collections = [\n         \"knowledgebase\",\n@@ -1505,6 +1914,10 @@ def query_open_ai(state: Any) -> dict:\n                 \"role\": \"developer\",\n                 \"content\": rag_context_message\n             },\n+            {\n+                \"role\": \"developer\",\n+                \"content\": \"Focus only on the portion of the user's message requesting general Bible translation assistance. Ignore unrelated requests or passages mentioned elsewhere in the message.\",\n+            },\n             {\n                 \"role\": \"developer\",\n                 \"content\": chat_history_context_message\n@@ -1514,8 +1927,10 @@ def query_open_ai(state: Any) -> dict:\n                 \"content\": query\n             }\n         ])\n+        agentic_strength = _resolve_agentic_strength(s)\n+        model_name = _model_for_agentic_strength(agentic_strength, allow_low=False, allow_very_low=True)\n         response = open_ai_client.responses.create(\n-            model=\"gpt-4o\",\n+            model=model_name,\n             instructions=FINAL_RESPONSE_AGENT_SYSTEM_PROMPT,\n             input=cast(Any, messages)\n         )\n@@ -1527,7 +1942,7 @@ def query_open_ai(state: Any) -> dict:\n             if tt is None and (it is not None or ot is not None):\n                 tt = (it or 0) + (ot or 0)\n             cit = _extract_cached_input_tokens(usage)\n-            add_tokens(it, ot, tt, model=\"gpt-4o\", cached_input_tokens=cit)\n+            add_tokens(it, ot, tt, model=model_name, cached_input_tokens=cit)\n         bt_servant_response = response.output_text\n         logger.info('response from openai: %s', bt_servant_response)\n         logger.debug(\"%d characters returned from openAI\", len(bt_servant_response))\n@@ -1649,6 +2064,10 @@ def _query_collection(name: str) -> list[dict[str, str]]:\n             \"role\": \"developer\",\n             \"content\": f\"FIA context resources: {context_payload}\",\n         },\n+        {\n+            \"role\": \"developer\",\n+            \"content\": \"Focus only on the portion of the user's message that requests FIA guidance. Ignore any other requests or book references in the message.\",\n+        },\n         {\n             \"role\": \"developer\",\n             \"content\": f\"Use this conversation history if helpful: {json.dumps(chat_history)}\",\n@@ -1660,8 +2079,10 @@ def _query_collection(name: str) -> list[dict[str, str]]:\n     ])\n \n     try:\n+        agentic_strength = _resolve_agentic_strength(s)\n+        model_name = _model_for_agentic_strength(agentic_strength, allow_low=True, allow_very_low=True)\n         response = open_ai_client.responses.create(\n-            model=\"gpt-4o\",\n+            model=model_name,\n             instructions=CONSULT_FIA_RESOURCES_SYSTEM_PROMPT,\n             input=cast(Any, messages),\n         )\n@@ -1673,7 +2094,7 @@ def _query_collection(name: str) -> list[dict[str, str]]:\n             if tt is None and (it is not None or ot is not None):\n                 tt = (it or 0) + (ot or 0)\n             cit = _extract_cached_input_tokens(usage)\n-            add_tokens(it, ot, tt, model=\"gpt-4o\", cached_input_tokens=cit)\n+            add_tokens(it, ot, tt, model=model_name, cached_input_tokens=cit)\n \n         fia_response = response.output_text\n         logger.info(\"[consult-fia] response from openai: %s\", fia_response)\n@@ -1795,7 +2216,11 @@ def _pack_items(items: list[str], max_len: int) -> list[str]:\n \n def needs_chunking(state: BrainState) -> str:\n     \"\"\"Return next node key if chunking is required, otherwise finish.\"\"\"\n-    first_response = state[\"translated_responses\"][0]\n+    responses = state[\"translated_responses\"]\n+    if not responses:\n+        logger.info(\"[chunk-check] no text responses to send; skipping chunking\")\n+        return END\n+    first_response = responses[0]\n     if len(first_response) > config.MAX_META_TEXT_LENGTH:\n         logger.warning('message to big: %d chars. preparing to chunk.', len(first_response))\n         return \"chunk_message_node\"\n@@ -1826,6 +2251,8 @@ def process_intents(state: Any) -> List[Hashable]:  # pylint: disable=too-many-b\n         nodes_to_traverse.append(\"handle_listen_to_scripture_node\")\n     if IntentType.SET_RESPONSE_LANGUAGE in user_intents:\n         nodes_to_traverse.append(\"set_response_language_node\")\n+    if IntentType.SET_AGENTIC_STRENGTH in user_intents:\n+        nodes_to_traverse.append(\"set_agentic_strength_node\")\n     if IntentType.PERFORM_UNSUPPORTED_FUNCTION in user_intents:\n         nodes_to_traverse.append(\"handle_unsupported_function_node\")\n     if IntentType.RETRIEVE_SYSTEM_INFORMATION in user_intents:\n@@ -2005,13 +2432,17 @@ def _choose_primary_book(text: str, candidates: list[str]) -> str | None:\n def _resolve_selection_for_single_book(\n     query: str,\n     query_lang: str,\n+    focus_hint: str | None = None,\n ) -> tuple[str | None, list[tuple[int, int | None, int | None, int | None]] | None, str | None]:\n     # pylint: disable=too-many-return-statements, too-many-branches\n     \"\"\"Parse and normalize a user query into a single canonical book and ranges.\n \n     Returns a tuple of (canonical_book, ranges, error_message). On success, the\n     error_message is None. On failure, canonical_book and ranges are None and\n     error_message contains a user-friendly explanation.\n+\n+    If ``focus_hint`` is provided, it is sent as a developer message to steer the\n+    selection model toward the clause relevant to the current intent.\n     \"\"\"\n     logger.info(\"[selection-helper] start; query_lang=%s; query=%s\", query_lang, query)\n \n@@ -2028,8 +2459,15 @@ def _resolve_selection_for_single_book(\n     system_prompt = PASSAGE_SELECTION_AGENT_SYSTEM_PROMPT.format(books=books)\n     selection_messages: list[EasyInputMessageParam] = cast(List[EasyInputMessageParam], [\n         {\"role\": \"system\", \"content\": system_prompt},\n-        {\"role\": \"user\", \"content\": parse_input},\n     ])\n+    if focus_hint:\n+        logger.info(\"[selection-helper] applying focus hint: %s\", focus_hint)\n+        selection_messages.append(\n+            cast(EasyInputMessageParam, {\"role\": \"developer\", \"content\": focus_hint})\n+        )\n+    selection_messages.append(\n+        cast(EasyInputMessageParam, {\"role\": \"user\", \"content\": parse_input})\n+    )\n     logger.info(\"[selection-helper] extracting passage selection via LLM\")\n     selection_resp = open_ai_client.responses.parse(\n         model=\"gpt-4o\",\n@@ -2155,7 +2593,11 @@ def handle_get_passage_summary(state: Any) -> dict:\n     query_lang = s[\"query_language\"]\n     logger.info(\"[passage-summary] start; query_lang=%s; query=%s\", query_lang, query)\n \n-    canonical_book, ranges, err = _resolve_selection_for_single_book(query, query_lang)\n+    canonical_book, ranges, err = _resolve_selection_for_single_book(\n+        query,\n+        query_lang,\n+        focus_hint=\"Focus only on the portion of the user's message that asked for a passage summary. Ignore any other requests or book references in the message.\",\n+    )\n     if err:\n         return {\"responses\": [{\"intent\": IntentType.GET_PASSAGE_SUMMARY, \"response\": err}]}\n     assert canonical_book is not None and ranges is not None\n@@ -2203,13 +2645,16 @@ def handle_get_passage_summary(state: Any) -> dict:\n \n     # Summarize using LLM with strict system prompt\n     sum_messages: list[EasyInputMessageParam] = [\n+        {\"role\": \"developer\", \"content\": \"Focus only on summarizing the portion of the user's message that asked for a passage summary. Ignore any other requests or book references in the message.\"},\n         {\"role\": \"developer\", \"content\": f\"Passage reference: {ref_label}\"},\n         {\"role\": \"developer\", \"content\": f\"Passage verses (use only this content):\\n{joined}\"},\n         {\"role\": \"user\", \"content\": \"Provide a concise, faithful summary of the passage above.\"},\n     ]\n+    agentic_strength = _resolve_agentic_strength(s)\n+    model_name = _model_for_agentic_strength(agentic_strength, allow_low=True, allow_very_low=True)\n     logger.info(\"[passage-summary] summarizing %d verses\", len(verses))\n     summary_resp = open_ai_client.responses.create(\n-        model=\"gpt-4o\",\n+        model=model_name,\n         instructions=PASSAGE_SUMMARY_AGENT_SYSTEM_PROMPT,\n         input=cast(Any, sum_messages),\n         store=False,\n@@ -2222,13 +2667,21 @@ def handle_get_passage_summary(state: Any) -> dict:\n         if tt is None and (it is not None or ot is not None):\n             tt = (it or 0) + (ot or 0)\n         cit = _extract_cached_input_tokens(usage)\n-        add_tokens(it, ot, tt, model=\"gpt-4o\", cached_input_tokens=cit)\n+        add_tokens(it, ot, tt, model=model_name, cached_input_tokens=cit)\n     summary_text = summary_resp.output_text\n     logger.info(\"[passage-summary] summary generated (len=%d)\", len(summary_text) if summary_text else 0)\n \n     response_text = f\"Summary of {ref_label}:\\n\\n{summary_text}\"\n     logger.info(\"[passage-summary] done\")\n-    return {\"responses\": [{\"intent\": IntentType.GET_PASSAGE_SUMMARY, \"response\": response_text}]}\n+    return {\n+        \"responses\": [\n+            {\n+                \"intent\": IntentType.GET_PASSAGE_SUMMARY,\n+                \"response\": response_text,\n+                \"suppress_combining\": True,\n+            }\n+        ]\n+    }\n \n \n def handle_get_passage_keywords(state: Any) -> dict:\n@@ -2244,7 +2697,11 @@ def handle_get_passage_keywords(state: Any) -> dict:\n     query_lang = s[\"query_language\"]\n     logger.info(\"[passage-keywords] start; query_lang=%s; query=%s\", query_lang, query)\n \n-    canonical_book, ranges, err = _resolve_selection_for_single_book(query, query_lang)\n+    canonical_book, ranges, err = _resolve_selection_for_single_book(\n+        query,\n+        query_lang,\n+        focus_hint=\"Focus only on the portion of the user's message that asked for passage keywords. Ignore any other requests or book references in the message.\",\n+    )\n     if err:\n         return {\"responses\": [{\"intent\": IntentType.GET_PASSAGE_KEYWORDS, \"response\": err}]}\n     assert canonical_book is not None and ranges is not None\n@@ -2267,7 +2724,15 @@ def handle_get_passage_keywords(state: Any) -> dict:\n     body = \", \".join(keywords)\n     response_text = header + body\n     logger.info(\"[passage-keywords] done\")\n-    return {\"responses\": [{\"intent\": IntentType.GET_PASSAGE_KEYWORDS, \"response\": response_text}]}\n+    return {\n+        \"responses\": [\n+            {\n+                \"intent\": IntentType.GET_PASSAGE_KEYWORDS,\n+                \"response\": response_text,\n+                \"suppress_combining\": True,\n+            }\n+        ]\n+    }\n \n \n TRANSLATION_HELPS_AGENT_SYSTEM_PROMPT = \"\"\"\n@@ -2295,91 +2760,34 @@ def handle_get_passage_keywords(state: Any) -> dict:\n \n \n def handle_get_translation_helps(state: Any) -> dict:\n-    \"\"\"Handle get-translation-helps: extract refs, load helps, and guide.\n-\n-    - Parse and validate a single-book selection via the shared helper.\n-    - Load per-verse translation helps from sources/translation_helps.\n-    - Provide a structured JSON context to the LLM and return a guidance response.\n-    \"\"\"\n+    \"\"\"Generate focused translation helps guidance for a selected passage.\"\"\"\n     s = cast(BrainState, state)\n     query = s[\"transformed_query\"]\n     query_lang = s[\"query_language\"]\n     logger.info(\"[translation-helps] start; query_lang=%s; query=%s\", query_lang, query)\n \n-    canonical_book, ranges, err = _resolve_selection_for_single_book(query, query_lang)\n-    if err:\n-        return {\"responses\": [{\"intent\": IntentType.GET_TRANSLATION_HELPS, \"response\": err}]}\n-    assert canonical_book is not None and ranges is not None\n-\n     th_root = Path(\"sources\") / \"translation_helps\"\n-    logger.info(\"[translation-helps] loading helps from %s\", th_root)\n-    # Special-case: book entirely missing from translation helps dataset\n-    missing_books = set(get_missing_th_books(th_root))\n-    if canonical_book in missing_books:\n-        abbrs = sorted(BSB_BOOK_MAP[b][\"ref_abbr\"] for b in missing_books)\n-        requested_abbr = BSB_BOOK_MAP[canonical_book][\"ref_abbr\"]\n-        msg = (\n-            f\"Translation helps for {requested_abbr} are not available yet. \"\n-            f\"Currently missing books: {', '.join(abbrs)}. \"\n-            \"Would you like translation help for one of the supported books instead?\"\n-        )\n-        return {\"responses\": [{\"intent\": IntentType.GET_TRANSLATION_HELPS, \"response\": msg}]}\n-    # Count total verses first; if above limit, return a user-facing error instead of truncating\n     bsb_root = Path(\"sources\") / \"bible_data\" / \"en\" / \"bsb\"\n-    verse_count = len(select_verses(bsb_root, canonical_book, ranges))\n-    if verse_count > config.TRANSLATION_HELPS_VERSE_LIMIT:\n-        ref_label_over = label_ranges(canonical_book, ranges)\n-        msg = (\n-            f\"I can only provide translate help for {config.TRANSLATION_HELPS_VERSE_LIMIT} verses at a time. \"\n-            f\"Your selection {ref_label_over} includes {verse_count} verses. Please narrow the range (e.g., a chapter or a shorter span).\"\n-        )\n-        return {\"responses\": [{\"intent\": IntentType.GET_TRANSLATION_HELPS, \"response\": msg}]}\n-    # Enforce verse-count limit to control context/token size\n-    limited_ranges = clamp_ranges_by_verse_limit(\n+    logger.info(\"[translation-helps] loading helps from %s\", th_root)\n+\n+    canonical_book, ranges, raw_helps, err = _prepare_translation_helps(\n+        s,\n+        th_root,\n         bsb_root,\n-        canonical_book,\n-        ranges,\n-        max_verses=config.TRANSLATION_HELPS_VERSE_LIMIT,\n+        selection_focus_hint=\"Focus only on the portion of the user's message that asked for translation helps. Ignore any other requests or book references in the message.\",\n     )\n-    if not limited_ranges:\n-        msg = \"I couldn't identify verses for that selection in the BSB index. Please try another reference.\"\n-        return {\"responses\": [{\"intent\": IntentType.GET_TRANSLATION_HELPS, \"response\": msg}]}\n-    helps = select_translation_helps(th_root, canonical_book, limited_ranges)\n-    logger.info(\"[translation-helps] selected %d help entries\", len(helps))\n-    if not helps:\n-        msg = (\n-            \"I couldn't locate translation helps for that selection. Please check the reference and try again.\"\n-        )\n-        return {\"responses\": [{\"intent\": IntentType.GET_TRANSLATION_HELPS, \"response\": msg}]}\n-\n-    ref_label = label_ranges(canonical_book, limited_ranges)\n-    context_obj = {\n-        \"reference_label\": ref_label,\n-        \"selection\": {\n-            \"book\": canonical_book,\n-            \"ranges\": [\n-                {\n-                    \"start_chapter\": sc,\n-                    \"start_verse\": sv,\n-                    \"end_chapter\": ec,\n-                    \"end_verse\": ev,\n-                }\n-                for (sc, sv, ec, ev) in limited_ranges\n-            ],\n-        },\n-        \"translation_helps\": helps,\n-    }\n+    if err:\n+        return {\"responses\": [{\"intent\": IntentType.GET_TRANSLATION_HELPS, \"response\": err}]}\n+    assert canonical_book is not None and ranges is not None and raw_helps is not None\n \n-    messages: list[EasyInputMessageParam] = [\n-        {\"role\": \"developer\", \"content\": f\"Selection: {ref_label}\"},\n-        {\"role\": \"developer\", \"content\": \"Use the JSON context below strictly:\"},\n-        {\"role\": \"developer\", \"content\": json.dumps(context_obj, ensure_ascii=False)},\n-        {\"role\": \"user\", \"content\": \"Using the provided context, explain the translation challenges and give actionable guidance for this selection.\"},\n-    ]\n+    ref_label, context_obj = _build_translation_helps_context(canonical_book, ranges, raw_helps)\n+    messages = _build_translation_helps_messages(ref_label, context_obj)\n \n-    logger.info(\"[translation-helps] invoking LLM with %d helps\", len(helps))\n+    logger.info(\"[translation-helps] invoking LLM with %d helps\", len(raw_helps))\n+    agentic_strength = _resolve_agentic_strength(s)\n+    model_name = _model_for_agentic_strength(agentic_strength, allow_low=True, allow_very_low=True)\n     resp = open_ai_client.responses.create(\n-        model=\"gpt-4o\",\n+        model=model_name,\n         instructions=TRANSLATION_HELPS_AGENT_SYSTEM_PROMPT,\n         input=cast(Any, messages),\n         store=False,\n@@ -2392,11 +2800,19 @@ def handle_get_translation_helps(state: Any) -> dict:\n         if tt is None and (it is not None or ot is not None):\n             tt = (it or 0) + (ot or 0)\n         cit = _extract_cached_input_tokens(usage)\n-        add_tokens(it, ot, tt, model=\"gpt-4o\", cached_input_tokens=cit)\n-    text = resp.output_text\n+        add_tokens(it, ot, tt, model=model_name, cached_input_tokens=cit)\n+\n     header = f\"Translation helps for {ref_label}\\n\\n\"\n-    response_text = header + (text or \"\")\n-    return {\"responses\": [{\"intent\": IntentType.GET_TRANSLATION_HELPS, \"response\": response_text}]}\n+    response_text = header + (resp.output_text or \"\")\n+    return {\n+        \"responses\": [\n+            {\n+                \"intent\": IntentType.GET_TRANSLATION_HELPS,\n+                \"response\": response_text,\n+                \"suppress_combining\": True,\n+            }\n+        ]\n+    }\n \n \n def handle_retrieve_scripture(state: Any) -> dict:  # pylint: disable=too-many-branches,too-many-return-statements\n@@ -2419,9 +2835,14 @@ def handle_retrieve_scripture(state: Any) -> dict:  # pylint: disable=too-many-b\n     query = s[\"transformed_query\"]\n     query_lang = s[\"query_language\"]\n     logger.info(\"[retrieve-scripture] start; query_lang=%s; query=%s\", query_lang, query)\n+    agentic_strength = _resolve_agentic_strength(s)\n \n     # 1) Parse passage selection\n-    canonical_book, ranges, err = _resolve_selection_for_single_book(query, query_lang)\n+    canonical_book, ranges, err = _resolve_selection_for_single_book(\n+        query,\n+        query_lang,\n+        focus_hint=\"Focus only on the portion of the user's message that asked to retrieve or listen to scripture. Ignore any other requests or book references in the message.\",\n+    )\n     if err:\n         return {\"responses\": [{\"intent\": IntentType.RETRIEVE_SCRIPTURE, \"response\": err}]}\n     assert canonical_book is not None and ranges is not None\n@@ -2571,11 +2992,20 @@ def _norm_ws(s: str) -> str:\n             else:\n                 # As a last resort, translate the canonical book name with the LLM.\n                 translated_book = translate_text(\n-                    response_text=canonical_book, target_language=desired_target\n+                    response_text=canonical_book,\n+                    target_language=desired_target,\n+                    agentic_strength=agentic_strength,\n                 )\n         # Translate each verse text and join into a flowing paragraph\n         translated_lines: list[str] = [\n-            _norm_ws(translate_text(response_text=str(txt), target_language=desired_target)) for _ref, txt in verses\n+            _norm_ws(\n+                translate_text(\n+                    response_text=str(txt),\n+                    target_language=desired_target,\n+                    agentic_strength=agentic_strength,\n+                )\n+            )\n+            for _ref, txt in verses\n         ]\n         translated_body = \" \".join(translated_lines)\n         response_obj = {\n@@ -2615,6 +3045,15 @@ def handle_listen_to_scripture(state: Any) -> dict:\n     \"\"\"\n     out = handle_retrieve_scripture(state)\n     out[\"send_voice_message\"] = True\n+    responses = cast(list[dict], out.get(\"responses\", []))\n+    if responses:\n+        # Reconstruct scripture text for voice playback using the structured response.\n+        out[\"voice_message_text\"] = _reconstruct_structured_text(\n+            resp_item=responses[0],\n+            localize_to=None,\n+        )\n+        for resp in responses:\n+            resp[\"suppress_text_delivery\"] = True\n     return out\n \n def handle_translate_scripture(state: Any) -> dict:  # pylint: disable=too-many-branches,too-many-return-statements\n@@ -2629,10 +3068,15 @@ def handle_translate_scripture(state: Any) -> dict:  # pylint: disable=too-many-\n     query = s[\"transformed_query\"]\n     query_lang = s[\"query_language\"]\n     logger.info(\"[translate-scripture] start; query_lang=%s; query=%s\", query_lang, query)\n+    agentic_strength = _resolve_agentic_strength(s)\n \n     # First, validate the passage selection so we can surface selection errors\n     # (e.g., unsupported book like \"Enoch\") before language guidance.\n-    canonical_book, ranges, err = _resolve_selection_for_single_book(query, query_lang)\n+    canonical_book, ranges, err = _resolve_selection_for_single_book(\n+        query,\n+        query_lang,\n+        focus_hint=\"Focus only on the portion of the user's message that asked to translate scripture. Ignore any other requests or book references in the message.\",\n+    )\n     if err:\n         return {\"responses\": [{\"intent\": IntentType.TRANSLATE_SCRIPTURE, \"response\": err}]}\n     assert canonical_book is not None and ranges is not None\n@@ -2802,8 +3246,9 @@ def _norm_ws(s: str) -> str:\n             {\"role\": \"developer\", \"content\": \"passage body (translate; preserve newlines):\"},\n             {\"role\": \"developer\", \"content\": body_src},\n         ]\n+        model_name = _model_for_agentic_strength(agentic_strength, allow_low=False, allow_very_low=True)\n         resp = open_ai_client.responses.parse(\n-            model=\"gpt-4o\",\n+            model=model_name,\n             instructions=TRANSLATE_PASSAGE_AGENT_SYSTEM_PROMPT,\n             input=cast(Any, messages),\n             text_format=TranslatedPassage,\n@@ -2818,7 +3263,7 @@ def _norm_ws(s: str) -> str:\n             if tt is None and (it is not None or ot is not None):\n                 tt = (it or 0) + (ot or 0)\n             cit = _extract_cached_input_tokens(usage)\n-            add_tokens(it, ot, tt, model=\"gpt-4o\", cached_input_tokens=cit)\n+            add_tokens(it, ot, tt, model=model_name, cached_input_tokens=cit)\n         translated = cast(TranslatedPassage | None, resp.output_parsed)\n     except OpenAIError:\n         logger.warning(\"[translate-scripture] structured parse failed due to OpenAI error; falling back.\", exc_info=True)\n@@ -2828,8 +3273,18 @@ def _norm_ws(s: str) -> str:\n         translated = None\n \n     if translated is None:\n-        translated_body = _norm_ws(translate_text(response_text=body_src, target_language=cast(str, target_code)))\n-        translated_book = translate_text(response_text=canonical_book, target_language=cast(str, target_code))\n+        translated_body = _norm_ws(\n+            translate_text(\n+                response_text=body_src,\n+                target_language=cast(str, target_code),\n+                agentic_strength=agentic_strength,\n+            )\n+        )\n+        translated_book = translate_text(\n+            response_text=canonical_book,\n+            target_language=cast(str, target_code),\n+            agentic_strength=agentic_strength,\n+        )\n         response_obj = {\n             \"suppress_translation\": True,\n             \"content_language\": cast(str, target_code),\n@@ -2875,6 +3330,7 @@ def _make_state_graph(schema: Any) -> StateGraph[BrainState]:\n     builder.add_node(\"preprocess_user_query_node\", wrap_node_with_timing(preprocess_user_query, \"preprocess_user_query_node\"))\n     builder.add_node(\"determine_intents_node\", wrap_node_with_timing(determine_intents, \"determine_intents_node\"))\n     builder.add_node(\"set_response_language_node\", wrap_node_with_timing(set_response_language, \"set_response_language_node\"))\n+    builder.add_node(\"set_agentic_strength_node\", wrap_node_with_timing(set_agentic_strength, \"set_agentic_strength_node\"))\n     builder.add_node(\"query_vector_db_node\", wrap_node_with_timing(query_vector_db, \"query_vector_db_node\"))\n     builder.add_node(\"query_open_ai_node\", wrap_node_with_timing(query_open_ai, \"query_open_ai_node\"))\n     builder.add_node(\"consult_fia_resources_node\", wrap_node_with_timing(consult_fia_resources, \"consult_fia_resources_node\"))\n@@ -2900,6 +3356,7 @@ def _make_state_graph(schema: Any) -> StateGraph[BrainState]:\n     )\n     builder.add_edge(\"query_vector_db_node\", \"query_open_ai_node\")\n     builder.add_edge(\"set_response_language_node\", \"translate_responses_node\")\n+    builder.add_edge(\"set_agentic_strength_node\", \"translate_responses_node\")\n     # After chunking, finish. Do not loop back to translate, which can recreate\n     # the long message and trigger an infinite chunk cycle.\n \n@@ -2922,3 +3379,16 @@ def _make_state_graph(schema: Any) -> StateGraph[BrainState]:\n     builder.set_finish_point(\"chunk_message_node\")\n \n     return builder.compile()\n+LANG_DETECTION_SAMPLE_CHARS = 100\n+\n+\n+def _sample_for_language_detection(text: str) -> str:\n+    \"\"\"Return a short prefix ending at a whitespace boundary for detection.\"\"\"\n+    trimmed = text.lstrip()\n+    if len(trimmed) <= LANG_DETECTION_SAMPLE_CHARS:\n+        return trimmed\n+    snippet = trimmed[:LANG_DETECTION_SAMPLE_CHARS]\n+    parts = snippet.rsplit(maxsplit=1)\n+    if len(parts) > 1 and parts[0]:\n+        return parts[0]\n+    return snippet",
      "patch_lines": [
        "@@ -12,7 +12,7 @@\n",
        " import operator\n",
        " from pathlib import Path\n",
        " import re\n",
        "-from typing import Annotated, Dict, List, cast, Any, Optional\n",
        "+from typing import Annotated, Dict, Iterable, List, cast, Any, Optional\n",
        " from collections.abc import Hashable\n",
        " from enum import Enum\n",
        " from typing_extensions import TypedDict\n",
        "@@ -43,6 +43,7 @@\n",
        "     is_first_interaction,\n",
        "     set_first_interaction,\n",
        "     set_user_response_language,\n",
        "+    set_user_agentic_strength,\n",
        " )\n",
        " \n",
        " # (Moved dynamic feature messaging and related prompts below IntentType)\n",
        "@@ -206,6 +207,9 @@\n",
        " \n",
        " # Instructions\n",
        " \n",
        "+- Obey any developer instructions that precede the user message. They may narrow the\n",
        "+  focus to a specific clause of the user's text. When instructed, ignore unrelated\n",
        "+  books or requests outside that clause.\n",
        " - Only choose from these canonical book names (exact match):\n",
        "   {books}\n",
        " - Accept a variety of phrasings (e.g., \"John 3:16\", \"Jn 3:16\u201318\", \"1 John 2:1-3\", \"Psalm 1\", \"Song of Songs 2\").\n",
        "@@ -235,6 +239,9 @@\n",
        " - \"John and Mark 1:1\" -> choose Mark 1:1 (explicit qualifier picks Mark over first mention).\n",
        " - \"summarize 3 John\" -> choose book \"3 John\" with no chapters/verses (whole book selection).\n",
        " - \"summarize John 3\" -> choose book \"John\" with start_chapter=3 (whole chapter if no verses).\n",
        "+- Developer hint: \"Focus only on the portion asking for translation helps.\"\n",
        "+  User message: \"I want to listen to John 1:1, and I also want help translating Gal 1:3-4.\"\n",
        "+  -> choose Galatians 1:3-4 (ignore the listening request entirely).\n",
        " \"\"\"\n",
        " \n",
        " \n",
        "@@ -361,6 +368,11 @@\n",
        "     The user wants to change the language in which the system responds. They might ask for responses in \n",
        "     Spanish, French, Arabic, etc.\n",
        "   </intent>\n",
        "+  <intent name=\"set-agentic-strength\">\n",
        "+    The user wants to adjust how assertive the assistant should be when answering. They might say \"set my agentic\n",
        "+    strength to low\" or \"use normal agentic strength\". Only use this when they clearly request one of the supported\n",
        "+    strength levels (normal, low, or very low).\n",
        "+  </intent>\n",
        "   <intent name=\"retrieve-system-information\">\n",
        "     The user wants information about the BT Servant system itself \u2014 how it works, where it gets data, uptime, \n",
        "     example questions, supported languages, features, or current system configuration (like the documents currently \n",
        "@@ -537,6 +549,10 @@\n",
        "     <message>Can you reply to me in French from now on?</message>\n",
        "     <intent>set-response-language</intent>\n",
        "   </example>\n",
        "+  <example>\n",
        "+    <message>Set my agentic strength to low.</message>\n",
        "+    <intent>set-agentic-strength</intent>\n",
        "+  </example>\n",
        "   <example>\n",
        "     <message>Where does BT Servant get its information from?</message>\n",
        "     <intent>retrieve-system-information</intent>\n",
        "@@ -719,6 +735,68 @@ class ResponseLanguage(BaseModel):\n",
        " \"\"\"\n",
        " \n",
        " \n",
        "+ALLOWED_AGENTIC_STRENGTH = {\"normal\", \"low\", \"very_low\"}\n",
        "+\n",
        "+\n",
        "+class AgenticStrengthChoice(str, Enum):\n",
        "+    \"\"\"Accepted agentic strength options for controllable responses.\"\"\"\n",
        "+\n",
        "+    NORMAL = \"normal\"\n",
        "+    LOW = \"low\"\n",
        "+    VERY_LOW = \"very_low\"\n",
        "+    UNKNOWN = \"unknown\"\n",
        "+\n",
        "+\n",
        "+class AgenticStrengthSetting(BaseModel):\n",
        "+    \"\"\"Schema for parsing agentic strength adjustments from the user.\"\"\"\n",
        "+\n",
        "+    strength: AgenticStrengthChoice\n",
        "+\n",
        "+\n",
        "+SET_AGENTIC_STRENGTH_AGENT_SYSTEM_PROMPT = \"\"\"\n",
        "+Task: Determine whether the user is asking to adjust the agentic strength setting. The allowed values are \"normal\",\n",
        "+\"low\", and \"very_low\". Use the conversation context and latest message to infer the requested level.\n",
        "+\n",
        "+Output schema: { \"strength\": <normal|low|very_low|unknown> }\n",
        "+\n",
        "+Rules:\n",
        "+- If the user clearly requests \"normal\", \"low\", or \"very low\", return that value.\n",
        "+- If the intent is ambiguous or references any other option, return \"unknown\".\n",
        "+- Do not include explanations or additional text. Only produce a JSON object that matches the schema.\n",
        "+\"\"\"\n",
        "+\n",
        "+\n",
        "+def _resolve_agentic_strength(state: BrainState) -> str:\n",
        "+    \"\"\"Return the effective agentic strength, honoring user overrides when set.\"\"\"\n",
        "+    candidate = cast(Optional[str], state.get(\"agentic_strength\") or state.get(\"user_agentic_strength\"))\n",
        "+    if isinstance(candidate, str):\n",
        "+        lowered = candidate.lower()\n",
        "+        if lowered in ALLOWED_AGENTIC_STRENGTH:\n",
        "+            return lowered\n",
        "+\n",
        "+    configured = getattr(config, \"AGENTIC_STRENGTH\", \"normal\")\n",
        "+    if isinstance(configured, str):\n",
        "+        configured_lower = configured.lower()\n",
        "+        if configured_lower in ALLOWED_AGENTIC_STRENGTH:\n",
        "+            return configured_lower\n",
        "+    return \"normal\"\n",
        "+\n",
        "+\n",
        "+def _model_for_agentic_strength(\n",
        "+    agentic_strength: str,\n",
        "+    *,\n",
        "+    allow_low: bool,\n",
        "+    allow_very_low: bool,\n",
        "+) -> str:\n",
        "+    \"\"\"Return GPT model name based on strength and allowed downgrades.\"\"\"\n",
        "+    allowed: set[str] = set()\n",
        "+    if allow_low:\n",
        "+        allowed.add(\"low\")\n",
        "+    if allow_very_low:\n",
        "+        allowed.add(\"very_low\")\n",
        "+    return \"gpt-4o-mini\" if agentic_strength in allowed else \"gpt-4o\"\n",
        "+\n",
        "+\n",
        " class MessageLanguage(BaseModel):\n",
        "     \"\"\"Model for parsing/validating the detected language of a message.\"\"\"\n",
        "     language: Language\n",
        "@@ -790,6 +868,254 @@ def _reconstruct_structured_text(resp_item: dict | str, localize_to: Optional[st\n",
        "     return str(body)\n",
        " \n",
        " \n",
        "+TranslationRange = tuple[int, int | None, int | None, int | None]\n",
        "+\n",
        "+\n",
        "+def _partition_response_items(responses: Iterable[dict]) -> tuple[list[dict], list[dict]]:\n",
        "+    \"\"\"Split responses into scripture-protected and normal sets.\"\"\"\n",
        "+    protected: list[dict] = []\n",
        "+    normal: list[dict] = []\n",
        "+    for item in responses:\n",
        "+        if _is_protected_response_item(item):\n",
        "+            protected.append(item)\n",
        "+        else:\n",
        "+            normal.append(item)\n",
        "+    return protected, normal\n",
        "+\n",
        "+\n",
        "+def _normalize_single_response(item: dict) -> dict | str:\n",
        "+    \"\"\"Return a representation suitable for translation when no combine is needed.\"\"\"\n",
        "+    body = cast(dict | str, item.get(\"response\"))\n",
        "+    if isinstance(body, str):\n",
        "+        return body\n",
        "+    return item\n",
        "+\n",
        "+\n",
        "+def _build_translation_queue(\n",
        "+    state: BrainState,\n",
        "+    protected_items: list[dict],\n",
        "+    normal_items: list[dict],\n",
        "+) -> list[dict | str]:\n",
        "+    \"\"\"Assemble responses in the order they should be translated or localized.\"\"\"\n",
        "+    queue: list[dict | str] = list(protected_items)\n",
        "+    non_combinable: list[dict] = [i for i in normal_items if i.get(\"suppress_combining\")]\n",
        "+    combinable: list[dict] = [i for i in normal_items if not i.get(\"suppress_combining\")]\n",
        "+\n",
        "+    for item in non_combinable:\n",
        "+        queue.append(_normalize_single_response(item))\n",
        "+\n",
        "+    if not combinable:\n",
        "+        return queue\n",
        "+    if len(combinable) == 1:\n",
        "+        queue.append(_normalize_single_response(combinable[0]))\n",
        "+        return queue\n",
        "+    queue.append(\n",
        "+        combine_responses(\n",
        "+            state[\"user_chat_history\"],\n",
        "+            state[\"user_query\"],\n",
        "+            combinable,\n",
        "+        )\n",
        "+    )\n",
        "+    return queue\n",
        "+\n",
        "+\n",
        "+def _resolve_target_language(\n",
        "+    state: BrainState,\n",
        "+    responses_for_translation: list[dict | str],\n",
        "+) -> tuple[Optional[str], Optional[list[str]]]:\n",
        "+    \"\"\"Determine the target language or build a pass-through fallback.\"\"\"\n",
        "+    user_language = cast(Optional[str], state.get(\"user_response_language\"))\n",
        "+    if user_language:\n",
        "+        return user_language, None\n",
        "+\n",
        "+    target_language = cast(str, state.get(\"query_language\"))\n",
        "+    if target_language != LANGUAGE_UNKNOWN:\n",
        "+        return target_language, None\n",
        "+\n",
        "+    logger.warning('target language unknown. bailing out.')\n",
        "+    passthrough_texts: list[str] = [\n",
        "+        _reconstruct_structured_text(resp_item=resp, localize_to=None)\n",
        "+        for resp in responses_for_translation\n",
        "+    ]\n",
        "+    notice = (\n",
        "+        \"You haven't set your desired response language and I wasn't able to determine the language of your \"\n",
        "+        \"original message in order to match it. You can set your desired response language at any time by \"\n",
        "+        \"saying: Set my response language to Spanish, or Indonesian, or any of the supported languages: \"\n",
        "+        f\"{', '.join(supported_language_map.keys())}.\"\n",
        "+    )\n",
        "+    passthrough_texts.append(notice)\n",
        "+    return None, passthrough_texts\n",
        "+\n",
        "+\n",
        "+def _translate_or_localize_response(\n",
        "+    resp: dict | str,\n",
        "+    target_language: str,\n",
        "+    agentic_strength: str,\n",
        "+) -> str:\n",
        "+    \"\"\"Translate free-form text or localize structured scripture outputs.\"\"\"\n",
        "+    if isinstance(resp, str):\n",
        "+        sample = _sample_for_language_detection(resp)\n",
        "+        detected_lang = detect_language(sample, agentic_strength=agentic_strength) if sample else target_language\n",
        "+        if detected_lang != target_language:\n",
        "+            logger.info('preparing to translate to %s', target_language)\n",
        "+            return translate_text(\n",
        "+                response_text=resp,\n",
        "+                target_language=target_language,\n",
        "+                agentic_strength=agentic_strength,\n",
        "+            )\n",
        "+        logger.info('chunk translation not required. using chunk as is.')\n",
        "+        return resp\n",
        "+\n",
        "+    body = cast(dict | str, resp.get(\"response\"))\n",
        "+    if isinstance(body, dict) and isinstance(body.get(\"segments\"), list):\n",
        "+        item_lang = cast(Optional[str], body.get(\"content_language\"))\n",
        "+        header_is_translated = bool(body.get(\"header_is_translated\"))\n",
        "+        localize_to = None if header_is_translated else (item_lang or target_language)\n",
        "+        return _reconstruct_structured_text(resp_item=resp, localize_to=localize_to)\n",
        "+    return str(body)\n",
        "+\n",
        "+\n",
        "+def _compact_translation_help_entries(entries: list[dict]) -> list[dict]:\n",
        "+    \"\"\"Reduce translation help entries to essentials for the LLM payload.\"\"\"\n",
        "+    compact: list[dict] = []\n",
        "+    for entry in entries:\n",
        "+        verse_text = cast(str, entry.get(\"ult_verse_text\") or \"\")\n",
        "+        notes: list[dict] = []\n",
        "+        for note in cast(list[dict], entry.get(\"notes\") or []):\n",
        "+            note_text = cast(str, note.get(\"note\") or \"\")\n",
        "+            if not note_text:\n",
        "+                continue\n",
        "+            compact_note: dict[str, str] = {\"note\": note_text}\n",
        "+            quote = cast(Optional[str], note.get(\"orig_language_quote\"))\n",
        "+            if quote:\n",
        "+                compact_note[\"orig_language_quote\"] = quote\n",
        "+            notes.append(compact_note)\n",
        "+        compact.append(\n",
        "+            {\n",
        "+                \"reference\": entry.get(\"reference\"),\n",
        "+                \"verse_text\": verse_text,\n",
        "+                \"notes\": notes,\n",
        "+            }\n",
        "+        )\n",
        "+    return compact\n",
        "+\n",
        "+\n",
        "+def _prepare_translation_helps(\n",
        "+    state: BrainState,\n",
        "+    th_root: Path,\n",
        "+    bsb_root: Path,\n",
        "+    *,\n",
        "+    selection_focus_hint: str | None = None,\n",
        "+) -> tuple[Optional[str], Optional[list[TranslationRange]], Optional[list[dict]], Optional[str]]:\n",
        "+    \"\"\"Resolve canonical selection, enforce limits, and load raw help entries.\"\"\"\n",
        "+    canonical_book, ranges, err = _resolve_selection_for_single_book(\n",
        "+        state[\"transformed_query\"],\n",
        "+        state[\"query_language\"],\n",
        "+        focus_hint=selection_focus_hint,\n",
        "+    )\n",
        "+    if err:\n",
        "+        return None, None, None, err\n",
        "+    assert canonical_book is not None and ranges is not None\n",
        "+\n",
        "+    missing_books = set(get_missing_th_books(th_root))\n",
        "+    if canonical_book in missing_books:\n",
        "+        return (\n",
        "+            None,\n",
        "+            None,\n",
        "+            None,\n",
        "+            (\n",
        "+                \"Translation helps for \"\n",
        "+                f\"{BSB_BOOK_MAP[canonical_book]['ref_abbr']} are not available yet. \"\n",
        "+                \"Currently missing books: \"\n",
        "+                f\"{', '.join(sorted(BSB_BOOK_MAP[b]['ref_abbr'] for b in missing_books))}. \"\n",
        "+                \"Would you like translation help for one of the supported books instead?\"\n",
        "+            ),\n",
        "+        )\n",
        "+\n",
        "+    verse_count = len(select_verses(bsb_root, canonical_book, ranges))\n",
        "+    if verse_count > config.TRANSLATION_HELPS_VERSE_LIMIT:\n",
        "+        return (\n",
        "+            None,\n",
        "+            None,\n",
        "+            None,\n",
        "+            (\n",
        "+                \"I can only provide translate help for \"\n",
        "+                f\"{config.TRANSLATION_HELPS_VERSE_LIMIT} verses at a time. \"\n",
        "+                \"Your selection \"\n",
        "+                f\"{label_ranges(canonical_book, ranges)} includes {verse_count} verses. \"\n",
        "+                \"Please narrow the range (e.g., a chapter or a shorter span).\"\n",
        "+            ),\n",
        "+        )\n",
        "+\n",
        "+    limited_ranges = clamp_ranges_by_verse_limit(\n",
        "+        bsb_root,\n",
        "+        canonical_book,\n",
        "+        ranges,\n",
        "+        max_verses=config.TRANSLATION_HELPS_VERSE_LIMIT,\n",
        "+    )\n",
        "+    if not limited_ranges:\n",
        "+        return (\n",
        "+            None,\n",
        "+            None,\n",
        "+            None,\n",
        "+            \"I couldn't identify verses for that selection in the BSB index. Please try another reference.\",\n",
        "+        )\n",
        "+\n",
        "+    raw_helps = select_translation_helps(th_root, canonical_book, limited_ranges)\n",
        "+    logger.info(\"[translation-helps] selected %d help entries\", len(raw_helps))\n",
        "+    if not raw_helps:\n",
        "+        return (\n",
        "+            None,\n",
        "+            None,\n",
        "+            None,\n",
        "+            \"I couldn't locate translation helps for that selection. Please check the reference and try again.\",\n",
        "+        )\n",
        "+    return canonical_book, list(limited_ranges), raw_helps, None\n",
        "+\n",
        "+\n",
        "+def _build_translation_helps_context(\n",
        "+    canonical_book: str,\n",
        "+    ranges: list[TranslationRange],\n",
        "+    raw_helps: list[dict],\n",
        "+) -> tuple[str, dict[str, Any]]:\n",
        "+    \"\"\"Return the reference label and compact JSON context for the LLM.\"\"\"\n",
        "+    ref_label = label_ranges(canonical_book, ranges)\n",
        "+    context_obj = {\n",
        "+        \"reference_label\": ref_label,\n",
        "+        \"selection\": {\n",
        "+            \"book\": canonical_book,\n",
        "+            \"ranges\": [\n",
        "+                {\n",
        "+                    \"start_chapter\": sc,\n",
        "+                    \"start_verse\": sv,\n",
        "+                    \"end_chapter\": ec,\n",
        "+                    \"end_verse\": ev,\n",
        "+                }\n",
        "+                for (sc, sv, ec, ev) in ranges\n",
        "+            ],\n",
        "+        },\n",
        "+        \"translation_helps\": _compact_translation_help_entries(raw_helps),\n",
        "+    }\n",
        "+    return ref_label, context_obj\n",
        "+\n",
        "+\n",
        "+def _build_translation_helps_messages(ref_label: str, context_obj: dict[str, object]) -> list[EasyInputMessageParam]:\n",
        "+    \"\"\"Construct the LLM messages for the translation helps prompt.\"\"\"\n",
        "+    payload = json.dumps(context_obj, ensure_ascii=False)\n",
        "+    return [\n",
        "+        {\"role\": \"developer\", \"content\": \"Focus only on the portion of the user's message that asked for translation helps. Ignore any other requests or book references in the message.\"},\n",
        "+        {\"role\": \"developer\", \"content\": f\"Selection: {ref_label}\"},\n",
        "+        {\"role\": \"developer\", \"content\": \"Use the JSON context below strictly:\"},\n",
        "+        {\"role\": \"developer\", \"content\": payload},\n",
        "+        {\n",
        "+            \"role\": \"user\",\n",
        "+            \"content\": (\n",
        "+                \"Using the provided context, explain the translation challenges and give actionable guidance for this selection.\"\n",
        "+            ),\n",
        "+        },\n",
        "+    ]\n",
        "+\n",
        "+\n",
        " class IntentType(str, Enum):\n",
        "     \"\"\"Enumeration of all supported user intents in the graph.\"\"\"\n",
        "     GET_BIBLE_TRANSLATION_ASSISTANCE = \"get-bible-translation-assistance\"\n",
        "@@ -803,6 +1129,7 @@ class IntentType(str, Enum):\n",
        "     PERFORM_UNSUPPORTED_FUNCTION = \"perform-unsupported-function\"\n",
        "     RETRIEVE_SYSTEM_INFORMATION = \"retrieve-system-information\"\n",
        "     SET_RESPONSE_LANGUAGE = \"set-response-language\"\n",
        "+    SET_AGENTIC_STRENGTH = \"set-agentic-strength\"\n",
        "     CONVERSE_WITH_BT_SERVANT = 'converse-with-bt-servant'\n",
        " \n",
        " \n",
        "@@ -820,6 +1147,8 @@ class BrainState(TypedDict, total=False):\n",
        "     perf_trace_id: str\n",
        "     query_language: str\n",
        "     user_response_language: str\n",
        "+    agentic_strength: str\n",
        "+    user_agentic_strength: str\n",
        "     transformed_query: str\n",
        "     docs: List[Dict[str, str]]\n",
        "     collection_used: str\n",
        "@@ -831,6 +1160,7 @@ class BrainState(TypedDict, total=False):\n",
        "     passage_selection: list[dict]\n",
        "     # Delivery hint for bt_servant to send a voice message instead of text\n",
        "     send_voice_message: bool\n",
        "+    voice_message_text: str\n",
        " \n",
        " \n",
        " # Centralized capability registry and builders for feature help/boilerplate\n",
        "@@ -914,6 +1244,15 @@ def _capabilities() -> List[Capability]:\n",
        "             ],\n",
        "             \"include_in_boilerplate\": True,\n",
        "         },\n",
        "+        {\n",
        "+            \"intent\": IntentType.SET_AGENTIC_STRENGTH,\n",
        "+            \"label\": \"Adjust agentic strength\",\n",
        "+            \"description\": \"Tune how assertive the assistant should be (normal, low, or very low).\",\n",
        "+            \"examples\": [\n",
        "+                \"Set my agentic strength to low.\",\n",
        "+            ],\n",
        "+            \"include_in_boilerplate\": False,\n",
        "+        },\n",
        "         {\n",
        "             \"intent\": IntentType.SET_RESPONSE_LANGUAGE,\n",
        "             \"label\": \"Set response language\",\n",
        "@@ -1138,6 +1477,97 @@ def set_response_language(state: Any) -> dict:\n",
        "     }\n",
        " \n",
        " \n",
        "+def set_agentic_strength(state: Any) -> dict:\n",
        "+    \"\"\"Detect and persist the user's preferred agentic strength.\"\"\"\n",
        "+    s = cast(BrainState, state)\n",
        "+    chat_input: list[EasyInputMessageParam] = [\n",
        "+        {\n",
        "+            \"role\": \"user\",\n",
        "+            \"content\": f\"Past conversation: {json.dumps(s['user_chat_history'])}\",\n",
        "+        },\n",
        "+        {\n",
        "+            \"role\": \"user\",\n",
        "+            \"content\": f\"the user's most recent message: {s['user_query']}\",\n",
        "+        },\n",
        "+        {\n",
        "+            \"role\": \"user\",\n",
        "+            \"content\": \"Is the user asking to set the agentic strength to normal, low, or very low?\",\n",
        "+        },\n",
        "+    ]\n",
        "+\n",
        "+    try:\n",
        "+        response = open_ai_client.responses.parse(\n",
        "+            model=\"gpt-4o\",\n",
        "+            instructions=SET_AGENTIC_STRENGTH_AGENT_SYSTEM_PROMPT,\n",
        "+            input=cast(Any, chat_input),\n",
        "+            text_format=AgenticStrengthSetting,\n",
        "+            temperature=0,\n",
        "+            store=False,\n",
        "+        )\n",
        "+        usage = getattr(response, \"usage\", None)\n",
        "+        if usage is not None:\n",
        "+            add_tokens(\n",
        "+                getattr(usage, \"input_tokens\", None),\n",
        "+                getattr(usage, \"output_tokens\", None),\n",
        "+                getattr(usage, \"total_tokens\", None)\n",
        "+                or (\n",
        "+                    (getattr(usage, \"input_tokens\", None) or 0)\n",
        "+                    + (getattr(usage, \"output_tokens\", None) or 0)\n",
        "+                ),\n",
        "+                model=\"gpt-4o\",\n",
        "+                cached_input_tokens=_extract_cached_input_tokens(usage),\n",
        "+            )\n",
        "+        parsed = cast(AgenticStrengthSetting | None, response.output_parsed)\n",
        "+    except OpenAIError:\n",
        "+        logger.error(\"[agentic-strength] OpenAI request failed while parsing user preference.\", exc_info=True)\n",
        "+        parsed = None\n",
        "+    except Exception:  # pylint: disable=broad-except\n",
        "+        logger.error(\"[agentic-strength] Unexpected failure while parsing agentic strength.\", exc_info=True)\n",
        "+        parsed = None\n",
        "+\n",
        "+    if not parsed or parsed.strength == AgenticStrengthChoice.UNKNOWN:\n",
        "+        msg = (\n",
        "+            \"I can set the agentic strength to normal, low, or very low. Please specify one of those options \"\n",
        "+            \"so I can update it.\"\n",
        "+        )\n",
        "+        return {\n",
        "+            \"responses\": [\n",
        "+                {\"intent\": IntentType.SET_AGENTIC_STRENGTH, \"response\": msg}\n",
        "+            ]\n",
        "+        }\n",
        "+\n",
        "+    desired = parsed.strength.value\n",
        "+    user_id: str = s[\"user_id\"]\n",
        "+    try:\n",
        "+        set_user_agentic_strength(user_id, desired)\n",
        "+    except ValueError:\n",
        "+        logger.warning(\"[agentic-strength] Attempted to set invalid value '%s' for user %s\", desired, user_id)\n",
        "+        msg = (\n",
        "+            \"That setting isn't supported. I can only use normal, low, or very low for agentic strength.\"\n",
        "+        )\n",
        "+        return {\n",
        "+            \"responses\": [\n",
        "+                {\"intent\": IntentType.SET_AGENTIC_STRENGTH, \"response\": msg}\n",
        "+            ]\n",
        "+        }\n",
        "+\n",
        "+    friendly = {\n",
        "+        \"normal\": \"Normal\",\n",
        "+        \"low\": \"Low\",\n",
        "+        \"very_low\": \"Very Low\",\n",
        "+    }.get(desired, desired.capitalize())\n",
        "+    response_text = (\n",
        "+        f\"Agentic strength set to {friendly.lower()}. I'll use the {friendly} setting from now on.\"\n",
        "+    )\n",
        "+    return {\n",
        "+        \"responses\": [\n",
        "+            {\"intent\": IntentType.SET_AGENTIC_STRENGTH, \"response\": response_text}\n",
        "+        ],\n",
        "+        \"agentic_strength\": desired,\n",
        "+        \"user_agentic_strength\": desired,\n",
        "+    }\n",
        "+\n",
        "+\n",
        " def combine_responses(chat_history, latest_user_message, responses) -> str:\n",
        "     \"\"\"Ask OpenAI to synthesize multiple node responses into one coherent text.\n",
        " \n",
        "@@ -1180,83 +1610,56 @@ def combine_responses(chat_history, latest_user_message, responses) -> str:\n",
        " \n",
        " \n",
        " def translate_responses(state: Any) -> dict:\n",
        "-    \"\"\"Translate the response(s) into the user's desired language if needed.\n",
        "-\n",
        "-    Scripture-protected responses are not combined via LLM and are not machine-\n",
        "-    translated. If present, they are passed through (with optional localized\n",
        "-    headers) and the remaining responses are combined and translated as needed.\n",
        "-    \"\"\"\n",
        "+    \"\"\"Translate or localize responses into the user's desired language.\"\"\"\n",
        "     s = cast(BrainState, state)\n",
        "-    uncombined = list(s[\"responses\"])\n",
        "-\n",
        "-    protected_items: list[dict] = [i for i in uncombined if _is_protected_response_item(i)]\n",
        "-    normal_items: list[dict] = [i for i in uncombined if not _is_protected_response_item(i)]\n",
        "+    raw_responses = [\n",
        "+        resp for resp in cast(list[dict], s[\"responses\"]) if not resp.get(\"suppress_text_delivery\")\n",
        "+    ]\n",
        "+    if not raw_responses:\n",
        "+        if bool(s.get(\"send_voice_message\")):\n",
        "+            logger.info(\"[translate] skipping text translation because delivery is voice-only\")\n",
        "+            return {\"translated_responses\": []}\n",
        "+        raise ValueError(\"no responses to translate. something bad happened. bailing out.\")\n",
        " \n",
        "-    responses_for_translation: list[dict | str] = list(protected_items)\n",
        "-    # Combine normal items (if any) using LLM synthesizer into a single string and append\n",
        "-    if normal_items:\n",
        "-        responses_for_translation.append(\n",
        "-            combine_responses(s[\"user_chat_history\"], s[\"user_query\"], normal_items)\n",
        "-        )\n",
        "+    protected_items, normal_items = _partition_response_items(raw_responses)\n",
        "+    responses_for_translation = _build_translation_queue(s, protected_items, normal_items)\n",
        "     if not responses_for_translation:\n",
        "+        if bool(s.get(\"send_voice_message\")):\n",
        "+            logger.info(\"[translate] no text responses after queue assembly; voice-only delivery\")\n",
        "+            return {\"translated_responses\": []}\n",
        "         raise ValueError(\"no responses to translate. something bad happened. bailing out.\")\n",
        " \n",
        "-    if s[\"user_response_language\"]:\n",
        "-        target_language = s[\"user_response_language\"]\n",
        "-    else:\n",
        "-        target_language = s[\"query_language\"]\n",
        "-        if target_language == LANGUAGE_UNKNOWN:\n",
        "-            logger.warning('target language unknown. bailing out.')\n",
        "-            # Build pass-through texts for current responses, then append notice\n",
        "-            passthrough_texts: list[str] = [\n",
        "-                _reconstruct_structured_text(resp_item=resp, localize_to=None)\n",
        "-                for resp in responses_for_translation\n",
        "-            ]\n",
        "-            passthrough_texts.append(\n",
        "-                \"You haven't set your desired response language and I wasn't able to determine the language of your \"\n",
        "-                \"original message in order to match it. You can set your desired response language at any time by \"\n",
        "-                \"saying: Set my response language to Spanish, or Indonesian, or any of the supported languages: \"\n",
        "-                f\"{', '.join(supported_language_map.keys())}.\"\n",
        "-            )\n",
        "-            return {\"translated_responses\": passthrough_texts}\n",
        "-\n",
        "-    translated_responses: list[str] = []\n",
        "-    for resp in responses_for_translation:\n",
        "-        if isinstance(resp, str):\n",
        "-            if detect_language(resp) != target_language:\n",
        "-                logger.info('preparing to translate to %s', target_language)\n",
        "-                translated_responses.append(translate_text(response_text=resp, target_language=target_language))\n",
        "-            else:\n",
        "-                logger.info('chunk translation not required. using chunk as is.')\n",
        "-                translated_responses.append(resp)\n",
        "-            continue\n",
        "-\n",
        "-        # Structured/metadata response\n",
        "-        body = cast(dict | str, resp.get(\"response\"))\n",
        "-        if isinstance(body, dict) and isinstance(body.get(\"segments\"), list):\n",
        "-            item_lang = cast(Optional[str], body.get(\"content_language\"))\n",
        "-            header_is_translated = bool(body.get(\"header_is_translated\"))\n",
        "-            # If the header was already translated by the producer, do not localize again.\n",
        "-            # Otherwise, localize the canonical header to the passage's content language when known,\n",
        "-            # falling back to the target UI language.\n",
        "-            localize_to = None if header_is_translated else (item_lang or target_language)\n",
        "-            final_text2 = _reconstruct_structured_text(resp_item=resp, localize_to=localize_to)\n",
        "-            translated_responses.append(final_text2)\n",
        "-            continue\n",
        "+    target_language, passthrough = _resolve_target_language(s, responses_for_translation)\n",
        "+    if passthrough is not None:\n",
        "+        return {\"translated_responses\": passthrough}\n",
        "+    assert target_language is not None\n",
        " \n",
        "-        # Unknown structured shape: fall back to string conversion\n",
        "-        translated_responses.append(str(body))\n",
        "-    return {\n",
        "-        \"translated_responses\": translated_responses\n",
        "-    }\n",
        "+    agentic_strength = _resolve_agentic_strength(s)\n",
        "+    translated_responses = [\n",
        "+        _translate_or_localize_response(resp, target_language, agentic_strength)\n",
        "+        for resp in responses_for_translation\n",
        "+    ]\n",
        "+    return {\"translated_responses\": translated_responses}\n",
        " \n",
        " \n",
        "-def translate_text(response_text: str, target_language: str) -> str:\n",
        "+def translate_text(\n",
        "+    response_text: str,\n",
        "+    target_language: str,\n",
        "+    *,\n",
        "+    agentic_strength: Optional[str] = None,\n",
        "+) -> str:\n",
        "     \"\"\"Translate a single text into the target ISO 639-1 language code.\n",
        " \n",
        "     Returns a plain string. If the OpenAI SDK returns a structured content\n",
        "     list or None, normalize it to a string.\n",
        "     \"\"\"\n",
        "+    resolved_strength = (\n",
        "+        agentic_strength if agentic_strength in ALLOWED_AGENTIC_STRENGTH else None\n",
        "+    )\n",
        "+    if resolved_strength is None:\n",
        "+        configured = getattr(config, \"AGENTIC_STRENGTH\", \"normal\")\n",
        "+        resolved_strength = configured if configured in ALLOWED_AGENTIC_STRENGTH else \"normal\"\n",
        "+    model_name = _model_for_agentic_strength(resolved_strength, allow_low=False, allow_very_low=True)\n",
        "     chat_messages = cast(List[ChatCompletionMessageParam], [\n",
        "         {\n",
        "             \"role\": \"system\",\n",
        "@@ -1271,7 +1674,7 @@ def translate_text(response_text: str, target_language: str) -> str:\n",
        "         },\n",
        "     ])\n",
        "     completion = open_ai_client.chat.completions.create(\n",
        "-        model=\"gpt-4o\",\n",
        "+        model=model_name,\n",
        "         messages=chat_messages,\n",
        "     )\n",
        "     usage = getattr(completion, \"usage\", None)\n",
        "@@ -1280,7 +1683,7 @@ def translate_text(response_text: str, target_language: str) -> str:\n",
        "         ot = getattr(usage, \"completion_tokens\", None)\n",
        "         tt = getattr(usage, \"total_tokens\", None)\n",
        "         cit = _extract_cached_input_tokens(usage)\n",
        "-        add_tokens(it, ot, tt, model=\"gpt-4o\", cached_input_tokens=cit)\n",
        "+        add_tokens(it, ot, tt, model=model_name, cached_input_tokens=cit)\n",
        "     content = completion.choices[0].message.content\n",
        "     if isinstance(content, list):\n",
        "         text = \"\".join(part.get(\"text\", \"\") if isinstance(part, dict) else \"\" for part in content)\n",
        "@@ -1292,7 +1695,7 @@ def translate_text(response_text: str, target_language: str) -> str:\n",
        "     return cast(str, text)\n",
        " \n",
        " \n",
        "-def detect_language(text) -> str:\n",
        "+def detect_language(text: str, *, agentic_strength: Optional[str] = None) -> str:  # pylint: disable=too-many-locals\n",
        "     \"\"\"Detect ISO 639-1 language code of the given text via OpenAI.\n",
        " \n",
        "     Uses a domain-aware prompt with deterministic decoding and a light\n",
        "@@ -1305,6 +1708,11 @@ def detect_language(text) -> str:\n",
        "             \"content\": f\"text: {text}\",\n",
        "         },\n",
        "     ]\n",
        "+    strength_source = agentic_strength if agentic_strength is not None else getattr(config, \"AGENTIC_STRENGTH\", \"normal\")\n",
        "+    strength = str(strength_source).lower()\n",
        "+    if strength not in ALLOWED_AGENTIC_STRENGTH:\n",
        "+        strength = \"normal\"\n",
        "+    model_name = _model_for_agentic_strength(strength, allow_low=True, allow_very_low=True)\n",
        "     response = open_ai_client.responses.parse(\n",
        "         model=\"gpt-4o\",\n",
        "         instructions=DETECT_LANGUAGE_AGENT_SYSTEM_PROMPT,\n",
        "@@ -1321,7 +1729,7 @@ def detect_language(text) -> str:\n",
        "         if tt is None and (it is not None or ot is not None):\n",
        "             tt = (it or 0) + (ot or 0)\n",
        "         cit = _extract_cached_input_tokens(usage)\n",
        "-        add_tokens(it, ot, tt, model=\"gpt-4o\", cached_input_tokens=cit)\n",
        "+        add_tokens(it, ot, tt, model=model_name, cached_input_tokens=cit)\n",
        "     message_language = cast(MessageLanguage | None, response.output_parsed)\n",
        "     predicted = message_language.language.value if message_language else \"en\"\n",
        "     logger.info(\"language detection (model): %s\", predicted)\n",
        "@@ -1356,7 +1764,8 @@ def determine_query_language(state: Any) -> dict:\n",
        "     \"\"\"Determine the language of the user's original query and set collection order.\"\"\"\n",
        "     s = cast(BrainState, state)\n",
        "     query = s[\"user_query\"]\n",
        "-    query_language = detect_language(query)\n",
        "+    agentic_strength = _resolve_agentic_strength(s)\n",
        "+    query_language = detect_language(query, agentic_strength=agentic_strength)\n",
        "     logger.info(\"language code %s detected by gpt-4o.\", query_language)\n",
        "     stack_rank_collections = [\n",
        "         \"knowledgebase\",\n",
        "@@ -1505,6 +1914,10 @@ def query_open_ai(state: Any) -> dict:\n",
        "                 \"role\": \"developer\",\n",
        "                 \"content\": rag_context_message\n",
        "             },\n",
        "+            {\n",
        "+                \"role\": \"developer\",\n",
        "+                \"content\": \"Focus only on the portion of the user's message requesting general Bible translation assistance. Ignore unrelated requests or passages mentioned elsewhere in the message.\",\n",
        "+            },\n",
        "             {\n",
        "                 \"role\": \"developer\",\n",
        "                 \"content\": chat_history_context_message\n",
        "@@ -1514,8 +1927,10 @@ def query_open_ai(state: Any) -> dict:\n",
        "                 \"content\": query\n",
        "             }\n",
        "         ])\n",
        "+        agentic_strength = _resolve_agentic_strength(s)\n",
        "+        model_name = _model_for_agentic_strength(agentic_strength, allow_low=False, allow_very_low=True)\n",
        "         response = open_ai_client.responses.create(\n",
        "-            model=\"gpt-4o\",\n",
        "+            model=model_name,\n",
        "             instructions=FINAL_RESPONSE_AGENT_SYSTEM_PROMPT,\n",
        "             input=cast(Any, messages)\n",
        "         )\n",
        "@@ -1527,7 +1942,7 @@ def query_open_ai(state: Any) -> dict:\n",
        "             if tt is None and (it is not None or ot is not None):\n",
        "                 tt = (it or 0) + (ot or 0)\n",
        "             cit = _extract_cached_input_tokens(usage)\n",
        "-            add_tokens(it, ot, tt, model=\"gpt-4o\", cached_input_tokens=cit)\n",
        "+            add_tokens(it, ot, tt, model=model_name, cached_input_tokens=cit)\n",
        "         bt_servant_response = response.output_text\n",
        "         logger.info('response from openai: %s', bt_servant_response)\n",
        "         logger.debug(\"%d characters returned from openAI\", len(bt_servant_response))\n",
        "@@ -1649,6 +2064,10 @@ def _query_collection(name: str) -> list[dict[str, str]]:\n",
        "             \"role\": \"developer\",\n",
        "             \"content\": f\"FIA context resources: {context_payload}\",\n",
        "         },\n",
        "+        {\n",
        "+            \"role\": \"developer\",\n",
        "+            \"content\": \"Focus only on the portion of the user's message that requests FIA guidance. Ignore any other requests or book references in the message.\",\n",
        "+        },\n",
        "         {\n",
        "             \"role\": \"developer\",\n",
        "             \"content\": f\"Use this conversation history if helpful: {json.dumps(chat_history)}\",\n",
        "@@ -1660,8 +2079,10 @@ def _query_collection(name: str) -> list[dict[str, str]]:\n",
        "     ])\n",
        " \n",
        "     try:\n",
        "+        agentic_strength = _resolve_agentic_strength(s)\n",
        "+        model_name = _model_for_agentic_strength(agentic_strength, allow_low=True, allow_very_low=True)\n",
        "         response = open_ai_client.responses.create(\n",
        "-            model=\"gpt-4o\",\n",
        "+            model=model_name,\n",
        "             instructions=CONSULT_FIA_RESOURCES_SYSTEM_PROMPT,\n",
        "             input=cast(Any, messages),\n",
        "         )\n",
        "@@ -1673,7 +2094,7 @@ def _query_collection(name: str) -> list[dict[str, str]]:\n",
        "             if tt is None and (it is not None or ot is not None):\n",
        "                 tt = (it or 0) + (ot or 0)\n",
        "             cit = _extract_cached_input_tokens(usage)\n",
        "-            add_tokens(it, ot, tt, model=\"gpt-4o\", cached_input_tokens=cit)\n",
        "+            add_tokens(it, ot, tt, model=model_name, cached_input_tokens=cit)\n",
        " \n",
        "         fia_response = response.output_text\n",
        "         logger.info(\"[consult-fia] response from openai: %s\", fia_response)\n",
        "@@ -1795,7 +2216,11 @@ def _pack_items(items: list[str], max_len: int) -> list[str]:\n",
        " \n",
        " def needs_chunking(state: BrainState) -> str:\n",
        "     \"\"\"Return next node key if chunking is required, otherwise finish.\"\"\"\n",
        "-    first_response = state[\"translated_responses\"][0]\n",
        "+    responses = state[\"translated_responses\"]\n",
        "+    if not responses:\n",
        "+        logger.info(\"[chunk-check] no text responses to send; skipping chunking\")\n",
        "+        return END\n",
        "+    first_response = responses[0]\n",
        "     if len(first_response) > config.MAX_META_TEXT_LENGTH:\n",
        "         logger.warning('message to big: %d chars. preparing to chunk.', len(first_response))\n",
        "         return \"chunk_message_node\"\n",
        "@@ -1826,6 +2251,8 @@ def process_intents(state: Any) -> List[Hashable]:  # pylint: disable=too-many-b\n",
        "         nodes_to_traverse.append(\"handle_listen_to_scripture_node\")\n",
        "     if IntentType.SET_RESPONSE_LANGUAGE in user_intents:\n",
        "         nodes_to_traverse.append(\"set_response_language_node\")\n",
        "+    if IntentType.SET_AGENTIC_STRENGTH in user_intents:\n",
        "+        nodes_to_traverse.append(\"set_agentic_strength_node\")\n",
        "     if IntentType.PERFORM_UNSUPPORTED_FUNCTION in user_intents:\n",
        "         nodes_to_traverse.append(\"handle_unsupported_function_node\")\n",
        "     if IntentType.RETRIEVE_SYSTEM_INFORMATION in user_intents:\n",
        "@@ -2005,13 +2432,17 @@ def _choose_primary_book(text: str, candidates: list[str]) -> str | None:\n",
        " def _resolve_selection_for_single_book(\n",
        "     query: str,\n",
        "     query_lang: str,\n",
        "+    focus_hint: str | None = None,\n",
        " ) -> tuple[str | None, list[tuple[int, int | None, int | None, int | None]] | None, str | None]:\n",
        "     # pylint: disable=too-many-return-statements, too-many-branches\n",
        "     \"\"\"Parse and normalize a user query into a single canonical book and ranges.\n",
        " \n",
        "     Returns a tuple of (canonical_book, ranges, error_message). On success, the\n",
        "     error_message is None. On failure, canonical_book and ranges are None and\n",
        "     error_message contains a user-friendly explanation.\n",
        "+\n",
        "+    If ``focus_hint`` is provided, it is sent as a developer message to steer the\n",
        "+    selection model toward the clause relevant to the current intent.\n",
        "     \"\"\"\n",
        "     logger.info(\"[selection-helper] start; query_lang=%s; query=%s\", query_lang, query)\n",
        " \n",
        "@@ -2028,8 +2459,15 @@ def _resolve_selection_for_single_book(\n",
        "     system_prompt = PASSAGE_SELECTION_AGENT_SYSTEM_PROMPT.format(books=books)\n",
        "     selection_messages: list[EasyInputMessageParam] = cast(List[EasyInputMessageParam], [\n",
        "         {\"role\": \"system\", \"content\": system_prompt},\n",
        "-        {\"role\": \"user\", \"content\": parse_input},\n",
        "     ])\n",
        "+    if focus_hint:\n",
        "+        logger.info(\"[selection-helper] applying focus hint: %s\", focus_hint)\n",
        "+        selection_messages.append(\n",
        "+            cast(EasyInputMessageParam, {\"role\": \"developer\", \"content\": focus_hint})\n",
        "+        )\n",
        "+    selection_messages.append(\n",
        "+        cast(EasyInputMessageParam, {\"role\": \"user\", \"content\": parse_input})\n",
        "+    )\n",
        "     logger.info(\"[selection-helper] extracting passage selection via LLM\")\n",
        "     selection_resp = open_ai_client.responses.parse(\n",
        "         model=\"gpt-4o\",\n",
        "@@ -2155,7 +2593,11 @@ def handle_get_passage_summary(state: Any) -> dict:\n",
        "     query_lang = s[\"query_language\"]\n",
        "     logger.info(\"[passage-summary] start; query_lang=%s; query=%s\", query_lang, query)\n",
        " \n",
        "-    canonical_book, ranges, err = _resolve_selection_for_single_book(query, query_lang)\n",
        "+    canonical_book, ranges, err = _resolve_selection_for_single_book(\n",
        "+        query,\n",
        "+        query_lang,\n",
        "+        focus_hint=\"Focus only on the portion of the user's message that asked for a passage summary. Ignore any other requests or book references in the message.\",\n",
        "+    )\n",
        "     if err:\n",
        "         return {\"responses\": [{\"intent\": IntentType.GET_PASSAGE_SUMMARY, \"response\": err}]}\n",
        "     assert canonical_book is not None and ranges is not None\n",
        "@@ -2203,13 +2645,16 @@ def handle_get_passage_summary(state: Any) -> dict:\n",
        " \n",
        "     # Summarize using LLM with strict system prompt\n",
        "     sum_messages: list[EasyInputMessageParam] = [\n",
        "+        {\"role\": \"developer\", \"content\": \"Focus only on summarizing the portion of the user's message that asked for a passage summary. Ignore any other requests or book references in the message.\"},\n",
        "         {\"role\": \"developer\", \"content\": f\"Passage reference: {ref_label}\"},\n",
        "         {\"role\": \"developer\", \"content\": f\"Passage verses (use only this content):\\n{joined}\"},\n",
        "         {\"role\": \"user\", \"content\": \"Provide a concise, faithful summary of the passage above.\"},\n",
        "     ]\n",
        "+    agentic_strength = _resolve_agentic_strength(s)\n",
        "+    model_name = _model_for_agentic_strength(agentic_strength, allow_low=True, allow_very_low=True)\n",
        "     logger.info(\"[passage-summary] summarizing %d verses\", len(verses))\n",
        "     summary_resp = open_ai_client.responses.create(\n",
        "-        model=\"gpt-4o\",\n",
        "+        model=model_name,\n",
        "         instructions=PASSAGE_SUMMARY_AGENT_SYSTEM_PROMPT,\n",
        "         input=cast(Any, sum_messages),\n",
        "         store=False,\n",
        "@@ -2222,13 +2667,21 @@ def handle_get_passage_summary(state: Any) -> dict:\n",
        "         if tt is None and (it is not None or ot is not None):\n",
        "             tt = (it or 0) + (ot or 0)\n",
        "         cit = _extract_cached_input_tokens(usage)\n",
        "-        add_tokens(it, ot, tt, model=\"gpt-4o\", cached_input_tokens=cit)\n",
        "+        add_tokens(it, ot, tt, model=model_name, cached_input_tokens=cit)\n",
        "     summary_text = summary_resp.output_text\n",
        "     logger.info(\"[passage-summary] summary generated (len=%d)\", len(summary_text) if summary_text else 0)\n",
        " \n",
        "     response_text = f\"Summary of {ref_label}:\\n\\n{summary_text}\"\n",
        "     logger.info(\"[passage-summary] done\")\n",
        "-    return {\"responses\": [{\"intent\": IntentType.GET_PASSAGE_SUMMARY, \"response\": response_text}]}\n",
        "+    return {\n",
        "+        \"responses\": [\n",
        "+            {\n",
        "+                \"intent\": IntentType.GET_PASSAGE_SUMMARY,\n",
        "+                \"response\": response_text,\n",
        "+                \"suppress_combining\": True,\n",
        "+            }\n",
        "+        ]\n",
        "+    }\n",
        " \n",
        " \n",
        " def handle_get_passage_keywords(state: Any) -> dict:\n",
        "@@ -2244,7 +2697,11 @@ def handle_get_passage_keywords(state: Any) -> dict:\n",
        "     query_lang = s[\"query_language\"]\n",
        "     logger.info(\"[passage-keywords] start; query_lang=%s; query=%s\", query_lang, query)\n",
        " \n",
        "-    canonical_book, ranges, err = _resolve_selection_for_single_book(query, query_lang)\n",
        "+    canonical_book, ranges, err = _resolve_selection_for_single_book(\n",
        "+        query,\n",
        "+        query_lang,\n",
        "+        focus_hint=\"Focus only on the portion of the user's message that asked for passage keywords. Ignore any other requests or book references in the message.\",\n",
        "+    )\n",
        "     if err:\n",
        "         return {\"responses\": [{\"intent\": IntentType.GET_PASSAGE_KEYWORDS, \"response\": err}]}\n",
        "     assert canonical_book is not None and ranges is not None\n",
        "@@ -2267,7 +2724,15 @@ def handle_get_passage_keywords(state: Any) -> dict:\n",
        "     body = \", \".join(keywords)\n",
        "     response_text = header + body\n",
        "     logger.info(\"[passage-keywords] done\")\n",
        "-    return {\"responses\": [{\"intent\": IntentType.GET_PASSAGE_KEYWORDS, \"response\": response_text}]}\n",
        "+    return {\n",
        "+        \"responses\": [\n",
        "+            {\n",
        "+                \"intent\": IntentType.GET_PASSAGE_KEYWORDS,\n",
        "+                \"response\": response_text,\n",
        "+                \"suppress_combining\": True,\n",
        "+            }\n",
        "+        ]\n",
        "+    }\n",
        " \n",
        " \n",
        " TRANSLATION_HELPS_AGENT_SYSTEM_PROMPT = \"\"\"\n",
        "@@ -2295,91 +2760,34 @@ def handle_get_passage_keywords(state: Any) -> dict:\n",
        " \n",
        " \n",
        " def handle_get_translation_helps(state: Any) -> dict:\n",
        "-    \"\"\"Handle get-translation-helps: extract refs, load helps, and guide.\n",
        "-\n",
        "-    - Parse and validate a single-book selection via the shared helper.\n",
        "-    - Load per-verse translation helps from sources/translation_helps.\n",
        "-    - Provide a structured JSON context to the LLM and return a guidance response.\n",
        "-    \"\"\"\n",
        "+    \"\"\"Generate focused translation helps guidance for a selected passage.\"\"\"\n",
        "     s = cast(BrainState, state)\n",
        "     query = s[\"transformed_query\"]\n",
        "     query_lang = s[\"query_language\"]\n",
        "     logger.info(\"[translation-helps] start; query_lang=%s; query=%s\", query_lang, query)\n",
        " \n",
        "-    canonical_book, ranges, err = _resolve_selection_for_single_book(query, query_lang)\n",
        "-    if err:\n",
        "-        return {\"responses\": [{\"intent\": IntentType.GET_TRANSLATION_HELPS, \"response\": err}]}\n",
        "-    assert canonical_book is not None and ranges is not None\n",
        "-\n",
        "     th_root = Path(\"sources\") / \"translation_helps\"\n",
        "-    logger.info(\"[translation-helps] loading helps from %s\", th_root)\n",
        "-    # Special-case: book entirely missing from translation helps dataset\n",
        "-    missing_books = set(get_missing_th_books(th_root))\n",
        "-    if canonical_book in missing_books:\n",
        "-        abbrs = sorted(BSB_BOOK_MAP[b][\"ref_abbr\"] for b in missing_books)\n",
        "-        requested_abbr = BSB_BOOK_MAP[canonical_book][\"ref_abbr\"]\n",
        "-        msg = (\n",
        "-            f\"Translation helps for {requested_abbr} are not available yet. \"\n",
        "-            f\"Currently missing books: {', '.join(abbrs)}. \"\n",
        "-            \"Would you like translation help for one of the supported books instead?\"\n",
        "-        )\n",
        "-        return {\"responses\": [{\"intent\": IntentType.GET_TRANSLATION_HELPS, \"response\": msg}]}\n",
        "-    # Count total verses first; if above limit, return a user-facing error instead of truncating\n",
        "     bsb_root = Path(\"sources\") / \"bible_data\" / \"en\" / \"bsb\"\n",
        "-    verse_count = len(select_verses(bsb_root, canonical_book, ranges))\n",
        "-    if verse_count > config.TRANSLATION_HELPS_VERSE_LIMIT:\n",
        "-        ref_label_over = label_ranges(canonical_book, ranges)\n",
        "-        msg = (\n",
        "-            f\"I can only provide translate help for {config.TRANSLATION_HELPS_VERSE_LIMIT} verses at a time. \"\n",
        "-            f\"Your selection {ref_label_over} includes {verse_count} verses. Please narrow the range (e.g., a chapter or a shorter span).\"\n",
        "-        )\n",
        "-        return {\"responses\": [{\"intent\": IntentType.GET_TRANSLATION_HELPS, \"response\": msg}]}\n",
        "-    # Enforce verse-count limit to control context/token size\n",
        "-    limited_ranges = clamp_ranges_by_verse_limit(\n",
        "+    logger.info(\"[translation-helps] loading helps from %s\", th_root)\n",
        "+\n",
        "+    canonical_book, ranges, raw_helps, err = _prepare_translation_helps(\n",
        "+        s,\n",
        "+        th_root,\n",
        "         bsb_root,\n",
        "-        canonical_book,\n",
        "-        ranges,\n",
        "-        max_verses=config.TRANSLATION_HELPS_VERSE_LIMIT,\n",
        "+        selection_focus_hint=\"Focus only on the portion of the user's message that asked for translation helps. Ignore any other requests or book references in the message.\",\n",
        "     )\n",
        "-    if not limited_ranges:\n",
        "-        msg = \"I couldn't identify verses for that selection in the BSB index. Please try another reference.\"\n",
        "-        return {\"responses\": [{\"intent\": IntentType.GET_TRANSLATION_HELPS, \"response\": msg}]}\n",
        "-    helps = select_translation_helps(th_root, canonical_book, limited_ranges)\n",
        "-    logger.info(\"[translation-helps] selected %d help entries\", len(helps))\n",
        "-    if not helps:\n",
        "-        msg = (\n",
        "-            \"I couldn't locate translation helps for that selection. Please check the reference and try again.\"\n",
        "-        )\n",
        "-        return {\"responses\": [{\"intent\": IntentType.GET_TRANSLATION_HELPS, \"response\": msg}]}\n",
        "-\n",
        "-    ref_label = label_ranges(canonical_book, limited_ranges)\n",
        "-    context_obj = {\n",
        "-        \"reference_label\": ref_label,\n",
        "-        \"selection\": {\n",
        "-            \"book\": canonical_book,\n",
        "-            \"ranges\": [\n",
        "-                {\n",
        "-                    \"start_chapter\": sc,\n",
        "-                    \"start_verse\": sv,\n",
        "-                    \"end_chapter\": ec,\n",
        "-                    \"end_verse\": ev,\n",
        "-                }\n",
        "-                for (sc, sv, ec, ev) in limited_ranges\n",
        "-            ],\n",
        "-        },\n",
        "-        \"translation_helps\": helps,\n",
        "-    }\n",
        "+    if err:\n",
        "+        return {\"responses\": [{\"intent\": IntentType.GET_TRANSLATION_HELPS, \"response\": err}]}\n",
        "+    assert canonical_book is not None and ranges is not None and raw_helps is not None\n",
        " \n",
        "-    messages: list[EasyInputMessageParam] = [\n",
        "-        {\"role\": \"developer\", \"content\": f\"Selection: {ref_label}\"},\n",
        "-        {\"role\": \"developer\", \"content\": \"Use the JSON context below strictly:\"},\n",
        "-        {\"role\": \"developer\", \"content\": json.dumps(context_obj, ensure_ascii=False)},\n",
        "-        {\"role\": \"user\", \"content\": \"Using the provided context, explain the translation challenges and give actionable guidance for this selection.\"},\n",
        "-    ]\n",
        "+    ref_label, context_obj = _build_translation_helps_context(canonical_book, ranges, raw_helps)\n",
        "+    messages = _build_translation_helps_messages(ref_label, context_obj)\n",
        " \n",
        "-    logger.info(\"[translation-helps] invoking LLM with %d helps\", len(helps))\n",
        "+    logger.info(\"[translation-helps] invoking LLM with %d helps\", len(raw_helps))\n",
        "+    agentic_strength = _resolve_agentic_strength(s)\n",
        "+    model_name = _model_for_agentic_strength(agentic_strength, allow_low=True, allow_very_low=True)\n",
        "     resp = open_ai_client.responses.create(\n",
        "-        model=\"gpt-4o\",\n",
        "+        model=model_name,\n",
        "         instructions=TRANSLATION_HELPS_AGENT_SYSTEM_PROMPT,\n",
        "         input=cast(Any, messages),\n",
        "         store=False,\n",
        "@@ -2392,11 +2800,19 @@ def handle_get_translation_helps(state: Any) -> dict:\n",
        "         if tt is None and (it is not None or ot is not None):\n",
        "             tt = (it or 0) + (ot or 0)\n",
        "         cit = _extract_cached_input_tokens(usage)\n",
        "-        add_tokens(it, ot, tt, model=\"gpt-4o\", cached_input_tokens=cit)\n",
        "-    text = resp.output_text\n",
        "+        add_tokens(it, ot, tt, model=model_name, cached_input_tokens=cit)\n",
        "+\n",
        "     header = f\"Translation helps for {ref_label}\\n\\n\"\n",
        "-    response_text = header + (text or \"\")\n",
        "-    return {\"responses\": [{\"intent\": IntentType.GET_TRANSLATION_HELPS, \"response\": response_text}]}\n",
        "+    response_text = header + (resp.output_text or \"\")\n",
        "+    return {\n",
        "+        \"responses\": [\n",
        "+            {\n",
        "+                \"intent\": IntentType.GET_TRANSLATION_HELPS,\n",
        "+                \"response\": response_text,\n",
        "+                \"suppress_combining\": True,\n",
        "+            }\n",
        "+        ]\n",
        "+    }\n",
        " \n",
        " \n",
        " def handle_retrieve_scripture(state: Any) -> dict:  # pylint: disable=too-many-branches,too-many-return-statements\n",
        "@@ -2419,9 +2835,14 @@ def handle_retrieve_scripture(state: Any) -> dict:  # pylint: disable=too-many-b\n",
        "     query = s[\"transformed_query\"]\n",
        "     query_lang = s[\"query_language\"]\n",
        "     logger.info(\"[retrieve-scripture] start; query_lang=%s; query=%s\", query_lang, query)\n",
        "+    agentic_strength = _resolve_agentic_strength(s)\n",
        " \n",
        "     # 1) Parse passage selection\n",
        "-    canonical_book, ranges, err = _resolve_selection_for_single_book(query, query_lang)\n",
        "+    canonical_book, ranges, err = _resolve_selection_for_single_book(\n",
        "+        query,\n",
        "+        query_lang,\n",
        "+        focus_hint=\"Focus only on the portion of the user's message that asked to retrieve or listen to scripture. Ignore any other requests or book references in the message.\",\n",
        "+    )\n",
        "     if err:\n",
        "         return {\"responses\": [{\"intent\": IntentType.RETRIEVE_SCRIPTURE, \"response\": err}]}\n",
        "     assert canonical_book is not None and ranges is not None\n",
        "@@ -2571,11 +2992,20 @@ def _norm_ws(s: str) -> str:\n",
        "             else:\n",
        "                 # As a last resort, translate the canonical book name with the LLM.\n",
        "                 translated_book = translate_text(\n",
        "-                    response_text=canonical_book, target_language=desired_target\n",
        "+                    response_text=canonical_book,\n",
        "+                    target_language=desired_target,\n",
        "+                    agentic_strength=agentic_strength,\n",
        "                 )\n",
        "         # Translate each verse text and join into a flowing paragraph\n",
        "         translated_lines: list[str] = [\n",
        "-            _norm_ws(translate_text(response_text=str(txt), target_language=desired_target)) for _ref, txt in verses\n",
        "+            _norm_ws(\n",
        "+                translate_text(\n",
        "+                    response_text=str(txt),\n",
        "+                    target_language=desired_target,\n",
        "+                    agentic_strength=agentic_strength,\n",
        "+                )\n",
        "+            )\n",
        "+            for _ref, txt in verses\n",
        "         ]\n",
        "         translated_body = \" \".join(translated_lines)\n",
        "         response_obj = {\n",
        "@@ -2615,6 +3045,15 @@ def handle_listen_to_scripture(state: Any) -> dict:\n",
        "     \"\"\"\n",
        "     out = handle_retrieve_scripture(state)\n",
        "     out[\"send_voice_message\"] = True\n",
        "+    responses = cast(list[dict], out.get(\"responses\", []))\n",
        "+    if responses:\n",
        "+        # Reconstruct scripture text for voice playback using the structured response.\n",
        "+        out[\"voice_message_text\"] = _reconstruct_structured_text(\n",
        "+            resp_item=responses[0],\n",
        "+            localize_to=None,\n",
        "+        )\n",
        "+        for resp in responses:\n",
        "+            resp[\"suppress_text_delivery\"] = True\n",
        "     return out\n",
        " \n",
        " def handle_translate_scripture(state: Any) -> dict:  # pylint: disable=too-many-branches,too-many-return-statements\n",
        "@@ -2629,10 +3068,15 @@ def handle_translate_scripture(state: Any) -> dict:  # pylint: disable=too-many-\n",
        "     query = s[\"transformed_query\"]\n",
        "     query_lang = s[\"query_language\"]\n",
        "     logger.info(\"[translate-scripture] start; query_lang=%s; query=%s\", query_lang, query)\n",
        "+    agentic_strength = _resolve_agentic_strength(s)\n",
        " \n",
        "     # First, validate the passage selection so we can surface selection errors\n",
        "     # (e.g., unsupported book like \"Enoch\") before language guidance.\n",
        "-    canonical_book, ranges, err = _resolve_selection_for_single_book(query, query_lang)\n",
        "+    canonical_book, ranges, err = _resolve_selection_for_single_book(\n",
        "+        query,\n",
        "+        query_lang,\n",
        "+        focus_hint=\"Focus only on the portion of the user's message that asked to translate scripture. Ignore any other requests or book references in the message.\",\n",
        "+    )\n",
        "     if err:\n",
        "         return {\"responses\": [{\"intent\": IntentType.TRANSLATE_SCRIPTURE, \"response\": err}]}\n",
        "     assert canonical_book is not None and ranges is not None\n",
        "@@ -2802,8 +3246,9 @@ def _norm_ws(s: str) -> str:\n",
        "             {\"role\": \"developer\", \"content\": \"passage body (translate; preserve newlines):\"},\n",
        "             {\"role\": \"developer\", \"content\": body_src},\n",
        "         ]\n",
        "+        model_name = _model_for_agentic_strength(agentic_strength, allow_low=False, allow_very_low=True)\n",
        "         resp = open_ai_client.responses.parse(\n",
        "-            model=\"gpt-4o\",\n",
        "+            model=model_name,\n",
        "             instructions=TRANSLATE_PASSAGE_AGENT_SYSTEM_PROMPT,\n",
        "             input=cast(Any, messages),\n",
        "             text_format=TranslatedPassage,\n",
        "@@ -2818,7 +3263,7 @@ def _norm_ws(s: str) -> str:\n",
        "             if tt is None and (it is not None or ot is not None):\n",
        "                 tt = (it or 0) + (ot or 0)\n",
        "             cit = _extract_cached_input_tokens(usage)\n",
        "-            add_tokens(it, ot, tt, model=\"gpt-4o\", cached_input_tokens=cit)\n",
        "+            add_tokens(it, ot, tt, model=model_name, cached_input_tokens=cit)\n",
        "         translated = cast(TranslatedPassage | None, resp.output_parsed)\n",
        "     except OpenAIError:\n",
        "         logger.warning(\"[translate-scripture] structured parse failed due to OpenAI error; falling back.\", exc_info=True)\n",
        "@@ -2828,8 +3273,18 @@ def _norm_ws(s: str) -> str:\n",
        "         translated = None\n",
        " \n",
        "     if translated is None:\n",
        "-        translated_body = _norm_ws(translate_text(response_text=body_src, target_language=cast(str, target_code)))\n",
        "-        translated_book = translate_text(response_text=canonical_book, target_language=cast(str, target_code))\n",
        "+        translated_body = _norm_ws(\n",
        "+            translate_text(\n",
        "+                response_text=body_src,\n",
        "+                target_language=cast(str, target_code),\n",
        "+                agentic_strength=agentic_strength,\n",
        "+            )\n",
        "+        )\n",
        "+        translated_book = translate_text(\n",
        "+            response_text=canonical_book,\n",
        "+            target_language=cast(str, target_code),\n",
        "+            agentic_strength=agentic_strength,\n",
        "+        )\n",
        "         response_obj = {\n",
        "             \"suppress_translation\": True,\n",
        "             \"content_language\": cast(str, target_code),\n",
        "@@ -2875,6 +3330,7 @@ def _make_state_graph(schema: Any) -> StateGraph[BrainState]:\n",
        "     builder.add_node(\"preprocess_user_query_node\", wrap_node_with_timing(preprocess_user_query, \"preprocess_user_query_node\"))\n",
        "     builder.add_node(\"determine_intents_node\", wrap_node_with_timing(determine_intents, \"determine_intents_node\"))\n",
        "     builder.add_node(\"set_response_language_node\", wrap_node_with_timing(set_response_language, \"set_response_language_node\"))\n",
        "+    builder.add_node(\"set_agentic_strength_node\", wrap_node_with_timing(set_agentic_strength, \"set_agentic_strength_node\"))\n",
        "     builder.add_node(\"query_vector_db_node\", wrap_node_with_timing(query_vector_db, \"query_vector_db_node\"))\n",
        "     builder.add_node(\"query_open_ai_node\", wrap_node_with_timing(query_open_ai, \"query_open_ai_node\"))\n",
        "     builder.add_node(\"consult_fia_resources_node\", wrap_node_with_timing(consult_fia_resources, \"consult_fia_resources_node\"))\n",
        "@@ -2900,6 +3356,7 @@ def _make_state_graph(schema: Any) -> StateGraph[BrainState]:\n",
        "     )\n",
        "     builder.add_edge(\"query_vector_db_node\", \"query_open_ai_node\")\n",
        "     builder.add_edge(\"set_response_language_node\", \"translate_responses_node\")\n",
        "+    builder.add_edge(\"set_agentic_strength_node\", \"translate_responses_node\")\n",
        "     # After chunking, finish. Do not loop back to translate, which can recreate\n",
        "     # the long message and trigger an infinite chunk cycle.\n",
        " \n",
        "@@ -2922,3 +3379,16 @@ def _make_state_graph(schema: Any) -> StateGraph[BrainState]:\n",
        "     builder.set_finish_point(\"chunk_message_node\")\n",
        " \n",
        "     return builder.compile()\n",
        "+LANG_DETECTION_SAMPLE_CHARS = 100\n",
        "+\n",
        "+\n",
        "+def _sample_for_language_detection(text: str) -> str:\n",
        "+    \"\"\"Return a short prefix ending at a whitespace boundary for detection.\"\"\"\n",
        "+    trimmed = text.lstrip()\n",
        "+    if len(trimmed) <= LANG_DETECTION_SAMPLE_CHARS:\n",
        "+        return trimmed\n",
        "+    snippet = trimmed[:LANG_DETECTION_SAMPLE_CHARS]\n",
        "+    parts = snippet.rsplit(maxsplit=1)\n",
        "+    if len(parts) > 1 and parts[0]:\n",
        "+        return parts[0]\n",
        "+    return snippet\n"
      ]
    },
    {
      "path": "bt_servant.py",
      "status": "modified",
      "additions": 42,
      "deletions": 8,
      "patch": "@@ -33,6 +33,7 @@\n     get_user_chat_history,\n     update_user_chat_history,\n     get_user_response_language,\n+    get_user_agentic_strength,\n     get_or_create_chroma_collection,\n     create_chroma_collection,\n     delete_chroma_collection,\n@@ -91,6 +92,16 @@ async def _lifespan(_: FastAPI):\n _merge_task_cancel_flags: dict[str, asyncio.Event] = {}\n \n \n+def _compute_agentic_strengths(user_id: str) -> tuple[str, Optional[str]]:\n+    \"\"\"Return effective agentic strength and stored user preference (if any).\"\"\"\n+    user_strength = get_user_agentic_strength(user_id=user_id)\n+    system_strength = str(config.AGENTIC_STRENGTH).lower()\n+    if system_strength not in {\"normal\", \"low\", \"very_low\"}:\n+        system_strength = \"normal\"\n+    effective = user_strength or system_strength\n+    return effective, user_strength\n+\n+\n # Conservative cap for total input tokens per embeddings request.\n # OpenAI currently enforces ~300k tokens per request. Keep a safety margin.\n _EMBEDDING_MAX_TOKENS_PER_REQUEST = int(os.environ.get(\"OPENAI_EMBED_MAX_TOKENS_PER_REQUEST\", \"290000\"))\n@@ -1009,7 +1020,7 @@ def _on_done(_: asyncio.Task, start: float = _msg_t0, trace_id: str = user_messa\n         return JSONResponse(status_code=status.HTTP_400_BAD_REQUEST, content={\"error\": \"Invalid JSON\"})\n \n \n-async def process_message(user_message: UserMessage):  # pylint: disable=too-many-branches\n+async def process_message(user_message: UserMessage):  # pylint: disable=too-many-branches,too-many-locals,too-many-statements\n     \"\"\"Serialize user processing per user id and send responses back.\"\"\"\n     async with user_locks[user_message.user_id]:\n         start_time = time.time()\n@@ -1036,24 +1047,47 @@ async def process_message(user_message: UserMessage):  # pylint: disable=too-man\n \n                 loop = asyncio.get_event_loop()\n                 assert brain is not None  # mypy: brain set during startup or lazily above\n-                result = await loop.run_in_executor(None, brain.invoke, {\n+                effective_agentic_strength, user_agentic_strength = _compute_agentic_strengths(\n+                    user_message.user_id\n+                )\n+\n+                brain_payload: dict[str, Any] = {\n                     \"user_id\": user_message.user_id,\n                     \"user_query\": text,\n                     \"user_chat_history\": get_user_chat_history(user_id=user_message.user_id),\n                     \"user_response_language\": get_user_response_language(user_id=user_message.user_id),\n+                    \"agentic_strength\": effective_agentic_strength,\n                     # Attach perf trace id for cross-thread node timing\n                     \"perf_trace_id\": user_message.message_id,\n-                })\n-                responses = result[\"translated_responses\"]\n+                }\n+                if user_agentic_strength is not None:\n+                    brain_payload[\"user_agentic_strength\"] = user_agentic_strength\n+\n+                result = await loop.run_in_executor(None, brain.invoke, brain_payload)\n+                responses = list(result[\"translated_responses\"])\n                 full_response_text = \"\\n\\n\".join(responses).rstrip()\n                 send_voice = bool(result.get(\"send_voice_message\")) or user_message.message_type == \"audio\"\n+                voice_text = result.get(\"voice_message_text\")\n+\n                 if send_voice:\n-                    await send_voice_message(user_id=user_message.user_id, text=full_response_text)\n-                else:\n+                    voice_payload = voice_text or full_response_text\n+                    if voice_payload:\n+                        await send_voice_message(user_id=user_message.user_id, text=voice_payload)\n+\n+                should_send_text = True\n+                if send_voice and voice_text is None and user_message.message_type == \"audio\":\n+                    # Preserve legacy behavior for audio conversations without explicit voice payloads\n+                    should_send_text = False\n+\n+                if should_send_text and responses:\n                     response_count = len(responses)\n+                    formatted_responses = list(responses)\n                     if response_count > 1:\n-                        responses = [f'({i}/{response_count}) {r}' for i, r in enumerate(responses, start=1)]\n-                    for response in responses:\n+                        formatted_responses = [\n+                            f\"({i}/{response_count}) {r}\"\n+                            for i, r in enumerate(formatted_responses, start=1)\n+                        ]\n+                    for response in formatted_responses:\n                         logger.info(\"Response from bt_servant: %s\", response)\n                         try:\n                             await send_text_message(user_id=user_message.user_id, text=response)",
      "patch_lines": [
        "@@ -33,6 +33,7 @@\n",
        "     get_user_chat_history,\n",
        "     update_user_chat_history,\n",
        "     get_user_response_language,\n",
        "+    get_user_agentic_strength,\n",
        "     get_or_create_chroma_collection,\n",
        "     create_chroma_collection,\n",
        "     delete_chroma_collection,\n",
        "@@ -91,6 +92,16 @@ async def _lifespan(_: FastAPI):\n",
        " _merge_task_cancel_flags: dict[str, asyncio.Event] = {}\n",
        " \n",
        " \n",
        "+def _compute_agentic_strengths(user_id: str) -> tuple[str, Optional[str]]:\n",
        "+    \"\"\"Return effective agentic strength and stored user preference (if any).\"\"\"\n",
        "+    user_strength = get_user_agentic_strength(user_id=user_id)\n",
        "+    system_strength = str(config.AGENTIC_STRENGTH).lower()\n",
        "+    if system_strength not in {\"normal\", \"low\", \"very_low\"}:\n",
        "+        system_strength = \"normal\"\n",
        "+    effective = user_strength or system_strength\n",
        "+    return effective, user_strength\n",
        "+\n",
        "+\n",
        " # Conservative cap for total input tokens per embeddings request.\n",
        " # OpenAI currently enforces ~300k tokens per request. Keep a safety margin.\n",
        " _EMBEDDING_MAX_TOKENS_PER_REQUEST = int(os.environ.get(\"OPENAI_EMBED_MAX_TOKENS_PER_REQUEST\", \"290000\"))\n",
        "@@ -1009,7 +1020,7 @@ def _on_done(_: asyncio.Task, start: float = _msg_t0, trace_id: str = user_messa\n",
        "         return JSONResponse(status_code=status.HTTP_400_BAD_REQUEST, content={\"error\": \"Invalid JSON\"})\n",
        " \n",
        " \n",
        "-async def process_message(user_message: UserMessage):  # pylint: disable=too-many-branches\n",
        "+async def process_message(user_message: UserMessage):  # pylint: disable=too-many-branches,too-many-locals,too-many-statements\n",
        "     \"\"\"Serialize user processing per user id and send responses back.\"\"\"\n",
        "     async with user_locks[user_message.user_id]:\n",
        "         start_time = time.time()\n",
        "@@ -1036,24 +1047,47 @@ async def process_message(user_message: UserMessage):  # pylint: disable=too-man\n",
        " \n",
        "                 loop = asyncio.get_event_loop()\n",
        "                 assert brain is not None  # mypy: brain set during startup or lazily above\n",
        "-                result = await loop.run_in_executor(None, brain.invoke, {\n",
        "+                effective_agentic_strength, user_agentic_strength = _compute_agentic_strengths(\n",
        "+                    user_message.user_id\n",
        "+                )\n",
        "+\n",
        "+                brain_payload: dict[str, Any] = {\n",
        "                     \"user_id\": user_message.user_id,\n",
        "                     \"user_query\": text,\n",
        "                     \"user_chat_history\": get_user_chat_history(user_id=user_message.user_id),\n",
        "                     \"user_response_language\": get_user_response_language(user_id=user_message.user_id),\n",
        "+                    \"agentic_strength\": effective_agentic_strength,\n",
        "                     # Attach perf trace id for cross-thread node timing\n",
        "                     \"perf_trace_id\": user_message.message_id,\n",
        "-                })\n",
        "-                responses = result[\"translated_responses\"]\n",
        "+                }\n",
        "+                if user_agentic_strength is not None:\n",
        "+                    brain_payload[\"user_agentic_strength\"] = user_agentic_strength\n",
        "+\n",
        "+                result = await loop.run_in_executor(None, brain.invoke, brain_payload)\n",
        "+                responses = list(result[\"translated_responses\"])\n",
        "                 full_response_text = \"\\n\\n\".join(responses).rstrip()\n",
        "                 send_voice = bool(result.get(\"send_voice_message\")) or user_message.message_type == \"audio\"\n",
        "+                voice_text = result.get(\"voice_message_text\")\n",
        "+\n",
        "                 if send_voice:\n",
        "-                    await send_voice_message(user_id=user_message.user_id, text=full_response_text)\n",
        "-                else:\n",
        "+                    voice_payload = voice_text or full_response_text\n",
        "+                    if voice_payload:\n",
        "+                        await send_voice_message(user_id=user_message.user_id, text=voice_payload)\n",
        "+\n",
        "+                should_send_text = True\n",
        "+                if send_voice and voice_text is None and user_message.message_type == \"audio\":\n",
        "+                    # Preserve legacy behavior for audio conversations without explicit voice payloads\n",
        "+                    should_send_text = False\n",
        "+\n",
        "+                if should_send_text and responses:\n",
        "                     response_count = len(responses)\n",
        "+                    formatted_responses = list(responses)\n",
        "                     if response_count > 1:\n",
        "-                        responses = [f'({i}/{response_count}) {r}' for i, r in enumerate(responses, start=1)]\n",
        "-                    for response in responses:\n",
        "+                        formatted_responses = [\n",
        "+                            f\"({i}/{response_count}) {r}\"\n",
        "+                            for i, r in enumerate(formatted_responses, start=1)\n",
        "+                        ]\n",
        "+                    for response in formatted_responses:\n",
        "                         logger.info(\"Response from bt_servant: %s\", response)\n",
        "                         try:\n",
        "                             await send_text_message(user_id=user_message.user_id, text=response)\n"
      ]
    },
    {
      "path": "config.py",
      "status": "modified",
      "additions": 6,
      "deletions": 1,
      "patch": "@@ -4,6 +4,7 @@\n \"\"\"\n import os\n from pathlib import Path\n+from typing import Literal\n \n from pydantic import Field\n from pydantic_settings import BaseSettings, SettingsConfigDict\n@@ -27,7 +28,7 @@ class Config(BaseSettings):\n     BT_SERVANT_LOG_LEVEL: str = Field(default=\"info\")\n     MAX_META_TEXT_LENGTH: int = Field(default=4096)\n     # Max verses to include in get-translation-helps context to control token usage\n-    TRANSLATION_HELPS_VERSE_LIMIT: int = Field(default=10)\n+    TRANSLATION_HELPS_VERSE_LIMIT: int = Field(default=5)\n     # Max verses allowed for retrieve-scripture (prevents huge selections like an entire book)\n     RETRIEVE_SCRIPTURE_VERSE_LIMIT: int = Field(default=120)\n     # Max verses allowed for translate-scripture (avoid very large translations)\n@@ -38,6 +39,8 @@ class Config(BaseSettings):\n     HEALTHCHECK_API_TOKEN: str | None = Field(default=None)\n     # Enable admin auth for protected endpoints (default False for local/dev tests)\n     ENABLE_ADMIN_AUTH: bool = Field(default=True)\n+    # Tuning knob for LLM creativity/agency (normal | low)\n+    AGENTIC_STRENGTH: Literal[\"normal\", \"low\", \"very_low\"] = Field(default=\"low\")\n \n     # Optional with default value\n     DATA_DIR: Path = Field(default=Path(\"/data\"))\n@@ -47,6 +50,8 @@ class Config(BaseSettings):\n             '{'\n             '\"gpt-4o\": {\"input_per_million\": 2.5, '\n             '\"output_per_million\": 10.0, \"cached_input\": 1.25}, '\n+            '\"gpt-4o-mini\": {\"input_per_million\": 0.15, '\n+            '\"output_per_million\": 0.6}, '\n             '\"gpt-4o-transcribe\": {\"input_per_million\": 2.5, '\n             '\"output_per_million\": 10.0, '\n             '\"audio_input_per_million\": 6.0}, '",
      "patch_lines": [
        "@@ -4,6 +4,7 @@\n",
        " \"\"\"\n",
        " import os\n",
        " from pathlib import Path\n",
        "+from typing import Literal\n",
        " \n",
        " from pydantic import Field\n",
        " from pydantic_settings import BaseSettings, SettingsConfigDict\n",
        "@@ -27,7 +28,7 @@ class Config(BaseSettings):\n",
        "     BT_SERVANT_LOG_LEVEL: str = Field(default=\"info\")\n",
        "     MAX_META_TEXT_LENGTH: int = Field(default=4096)\n",
        "     # Max verses to include in get-translation-helps context to control token usage\n",
        "-    TRANSLATION_HELPS_VERSE_LIMIT: int = Field(default=10)\n",
        "+    TRANSLATION_HELPS_VERSE_LIMIT: int = Field(default=5)\n",
        "     # Max verses allowed for retrieve-scripture (prevents huge selections like an entire book)\n",
        "     RETRIEVE_SCRIPTURE_VERSE_LIMIT: int = Field(default=120)\n",
        "     # Max verses allowed for translate-scripture (avoid very large translations)\n",
        "@@ -38,6 +39,8 @@ class Config(BaseSettings):\n",
        "     HEALTHCHECK_API_TOKEN: str | None = Field(default=None)\n",
        "     # Enable admin auth for protected endpoints (default False for local/dev tests)\n",
        "     ENABLE_ADMIN_AUTH: bool = Field(default=True)\n",
        "+    # Tuning knob for LLM creativity/agency (normal | low)\n",
        "+    AGENTIC_STRENGTH: Literal[\"normal\", \"low\", \"very_low\"] = Field(default=\"low\")\n",
        " \n",
        "     # Optional with default value\n",
        "     DATA_DIR: Path = Field(default=Path(\"/data\"))\n",
        "@@ -47,6 +50,8 @@ class Config(BaseSettings):\n",
        "             '{'\n",
        "             '\"gpt-4o\": {\"input_per_million\": 2.5, '\n",
        "             '\"output_per_million\": 10.0, \"cached_input\": 1.25}, '\n",
        "+            '\"gpt-4o-mini\": {\"input_per_million\": 0.15, '\n",
        "+            '\"output_per_million\": 0.6}, '\n",
        "             '\"gpt-4o-transcribe\": {\"input_per_million\": 2.5, '\n",
        "             '\"output_per_million\": 10.0, '\n",
        "             '\"audio_input_per_million\": 6.0}, '\n"
      ]
    },
    {
      "path": "db/__init__.py",
      "status": "modified",
      "additions": 4,
      "deletions": 0,
      "patch": "@@ -23,6 +23,8 @@\n     update_user_chat_history,\n     get_user_response_language,\n     set_user_response_language,\n+    get_user_agentic_strength,\n+    set_user_agentic_strength,\n     set_first_interaction,\n     is_first_interaction\n )\n@@ -31,6 +33,8 @@\n     \"update_user_chat_history\",\n     \"get_user_response_language\",\n     \"set_user_response_language\",\n+    \"get_user_agentic_strength\",\n+    \"set_user_agentic_strength\",\n     \"set_first_interaction\",\n     \"is_first_interaction\",\n     \"get_or_create_chroma_collection\",",
      "patch_lines": [
        "@@ -23,6 +23,8 @@\n",
        "     update_user_chat_history,\n",
        "     get_user_response_language,\n",
        "     set_user_response_language,\n",
        "+    get_user_agentic_strength,\n",
        "+    set_user_agentic_strength,\n",
        "     set_first_interaction,\n",
        "     is_first_interaction\n",
        " )\n",
        "@@ -31,6 +33,8 @@\n",
        "     \"update_user_chat_history\",\n",
        "     \"get_user_response_language\",\n",
        "     \"set_user_response_language\",\n",
        "+    \"get_user_agentic_strength\",\n",
        "+    \"set_user_agentic_strength\",\n",
        "     \"set_first_interaction\",\n",
        "     \"is_first_interaction\",\n",
        "     \"get_or_create_chroma_collection\",\n"
      ]
    },
    {
      "path": "db/user.py",
      "status": "modified",
      "additions": 31,
      "deletions": 0,
      "patch": "@@ -12,6 +12,7 @@\n \n # NOTE: consider moving to configuration if needed.\n CHAT_HISTORY_MAX = 5\n+VALID_AGENTIC_STRENGTH = {\"normal\", \"low\", \"very_low\"}\n \n \n def get_user_chat_history(user_id: str) -> List[Dict[str, str]]:\n@@ -67,6 +68,36 @@ def set_user_response_language(user_id: str, language: str) -> None:\n     db.upsert(updated, cond)\n \n \n+def get_user_agentic_strength(user_id: str) -> Optional[str]:\n+    \"\"\"Get the user's preferred agentic strength, or None if not set.\"\"\"\n+    q = Query()\n+    cond = cast(QueryLike, q.user_id == user_id)\n+    raw = get_user_db().table(\"users\").get(cond)\n+    result = cast(Optional[Dict[str, Any]], raw)\n+    strength = result.get(\"agentic_strength\") if result else None\n+    if isinstance(strength, str) and strength in VALID_AGENTIC_STRENGTH:\n+        return strength\n+    return None\n+\n+\n+def set_user_agentic_strength(user_id: str, strength: str) -> None:\n+    \"\"\"Persist the user's preferred agentic strength level.\"\"\"\n+    normalized = strength.strip().lower()\n+    if normalized not in VALID_AGENTIC_STRENGTH:\n+        raise ValueError(f\"invalid agentic strength: {strength}\")\n+\n+    q = Query()\n+    db = get_user_db().table(\"users\")\n+    cond = cast(QueryLike, q.user_id == user_id)\n+    existing_raw = db.get(cond)\n+    existing = cast(Optional[Dict[str, Any]], existing_raw)\n+    updated: Dict[str, Any] = (\n+        existing.copy() if isinstance(existing, dict) else {\"user_id\": user_id}\n+    )\n+    updated[\"agentic_strength\"] = normalized\n+    db.upsert(updated, cond)\n+\n+\n def set_first_interaction(user_id: str, is_first: bool) -> None:\n     \"\"\"Set whether this is the user's first interaction.\"\"\"\n     q = Query()",
      "patch_lines": [
        "@@ -12,6 +12,7 @@\n",
        " \n",
        " # NOTE: consider moving to configuration if needed.\n",
        " CHAT_HISTORY_MAX = 5\n",
        "+VALID_AGENTIC_STRENGTH = {\"normal\", \"low\", \"very_low\"}\n",
        " \n",
        " \n",
        " def get_user_chat_history(user_id: str) -> List[Dict[str, str]]:\n",
        "@@ -67,6 +68,36 @@ def set_user_response_language(user_id: str, language: str) -> None:\n",
        "     db.upsert(updated, cond)\n",
        " \n",
        " \n",
        "+def get_user_agentic_strength(user_id: str) -> Optional[str]:\n",
        "+    \"\"\"Get the user's preferred agentic strength, or None if not set.\"\"\"\n",
        "+    q = Query()\n",
        "+    cond = cast(QueryLike, q.user_id == user_id)\n",
        "+    raw = get_user_db().table(\"users\").get(cond)\n",
        "+    result = cast(Optional[Dict[str, Any]], raw)\n",
        "+    strength = result.get(\"agentic_strength\") if result else None\n",
        "+    if isinstance(strength, str) and strength in VALID_AGENTIC_STRENGTH:\n",
        "+        return strength\n",
        "+    return None\n",
        "+\n",
        "+\n",
        "+def set_user_agentic_strength(user_id: str, strength: str) -> None:\n",
        "+    \"\"\"Persist the user's preferred agentic strength level.\"\"\"\n",
        "+    normalized = strength.strip().lower()\n",
        "+    if normalized not in VALID_AGENTIC_STRENGTH:\n",
        "+        raise ValueError(f\"invalid agentic strength: {strength}\")\n",
        "+\n",
        "+    q = Query()\n",
        "+    db = get_user_db().table(\"users\")\n",
        "+    cond = cast(QueryLike, q.user_id == user_id)\n",
        "+    existing_raw = db.get(cond)\n",
        "+    existing = cast(Optional[Dict[str, Any]], existing_raw)\n",
        "+    updated: Dict[str, Any] = (\n",
        "+        existing.copy() if isinstance(existing, dict) else {\"user_id\": user_id}\n",
        "+    )\n",
        "+    updated[\"agentic_strength\"] = normalized\n",
        "+    db.upsert(updated, cond)\n",
        "+\n",
        "+\n",
        " def set_first_interaction(user_id: str, is_first: bool) -> None:\n",
        "     \"\"\"Set whether this is the user's first interaction.\"\"\"\n",
        "     q = Query()\n"
      ]
    },
    {
      "path": "env.example",
      "status": "modified",
      "additions": 3,
      "deletions": 0,
      "patch": "@@ -32,3 +32,6 @@ META_SANDBOX_PHONE_NUMBER=14438677777\n ENABLE_ADMIN_AUTH=False\n ADMIN_API_TOKEN=change-me-for-admin\n HEALTHCHECK_API_TOKEN=change-me-for-health\n+\n+# Agentic creativity tuning for LLM calls (normal | low)\n+AGENTIC_STRENGTH=normal",
      "patch_lines": [
        "@@ -32,3 +32,6 @@ META_SANDBOX_PHONE_NUMBER=14438677777\n",
        " ENABLE_ADMIN_AUTH=False\n",
        " ADMIN_API_TOKEN=change-me-for-admin\n",
        " HEALTHCHECK_API_TOKEN=change-me-for-health\n",
        "+\n",
        "+# Agentic creativity tuning for LLM calls (normal | low)\n",
        "+AGENTIC_STRENGTH=normal\n"
      ]
    },
    {
      "path": "tests/test_agentic_strength.py",
      "status": "added",
      "additions": 82,
      "deletions": 0,
      "patch": "@@ -0,0 +1,82 @@\n+\"\"\"Tests for agentic strength preference handling.\"\"\"\n+\n+from __future__ import annotations\n+\n+from typing import Any, cast\n+\n+import pytest\n+\n+import brain\n+\n+\n+class _StubResponse:  # pylint: disable=too-few-public-methods\n+    \"\"\"Simple container matching the subset of Response fields we use.\"\"\"\n+\n+    def __init__(self, parsed: brain.AgenticStrengthSetting) -> None:\n+        self.output_parsed = parsed\n+        self.usage = None\n+\n+\n+def test_set_agentic_strength_persists_choice(monkeypatch: pytest.MonkeyPatch) -> None:\n+    \"\"\"When the user asks for a valid level, it is stored and echoed.\"\"\"\n+\n+    parsed = brain.AgenticStrengthSetting(strength=brain.AgenticStrengthChoice.LOW)\n+    monkeypatch.setattr(\n+        brain.open_ai_client.responses,\n+        \"parse\",\n+        lambda **_kwargs: _StubResponse(parsed),\n+    )\n+\n+    captured: dict[str, Any] = {}\n+\n+    def _fake_set(user_id: str, strength: str) -> None:  # noqa: D401\n+        captured[\"user_id\"] = user_id\n+        captured[\"strength\"] = strength\n+\n+    monkeypatch.setattr(brain, \"set_user_agentic_strength\", _fake_set)\n+\n+    state: dict[str, Any] = {\n+        \"user_id\": \"user-123\",\n+        \"user_query\": \"set my agentic strength to low\",\n+        \"user_chat_history\": [],\n+    }\n+\n+    out = brain.set_agentic_strength(cast(Any, state))\n+\n+    assert captured == {\"user_id\": \"user-123\", \"strength\": \"low\"}\n+    assert out.get(\"agentic_strength\") == \"low\"\n+    assert out.get(\"user_agentic_strength\") == \"low\"\n+    response_text = out[\"responses\"][0][\"response\"]\n+    assert \"agentic strength set to\" in response_text.lower()\n+    assert \"low\" in response_text.lower()\n+\n+\n+def test_set_agentic_strength_handles_unknown(monkeypatch: pytest.MonkeyPatch) -> None:\n+    \"\"\"If the LLM can't find a valid level, prompt the user again.\"\"\"\n+\n+    parsed = brain.AgenticStrengthSetting(strength=brain.AgenticStrengthChoice.UNKNOWN)\n+    monkeypatch.setattr(\n+        brain.open_ai_client.responses,\n+        \"parse\",\n+        lambda **_kwargs: _StubResponse(parsed),\n+    )\n+\n+    called = False\n+\n+    def _fail_set(*_args: Any, **_kwargs: Any) -> None:  # noqa: ANN401\n+        nonlocal called\n+        called = True\n+\n+    monkeypatch.setattr(brain, \"set_user_agentic_strength\", _fail_set)\n+\n+    state: dict[str, Any] = {\n+        \"user_id\": \"user-456\",\n+        \"user_query\": \"make it stronger\",\n+        \"user_chat_history\": [],\n+    }\n+\n+    out = brain.set_agentic_strength(cast(Any, state))\n+\n+    assert not called, \"should not persist an unknown preference\"\n+    assert \"normal, low, or very low\" in out[\"responses\"][0][\"response\"]\n+    assert \"agentic_strength\" not in out",
      "patch_lines": [
        "@@ -0,0 +1,82 @@\n",
        "+\"\"\"Tests for agentic strength preference handling.\"\"\"\n",
        "+\n",
        "+from __future__ import annotations\n",
        "+\n",
        "+from typing import Any, cast\n",
        "+\n",
        "+import pytest\n",
        "+\n",
        "+import brain\n",
        "+\n",
        "+\n",
        "+class _StubResponse:  # pylint: disable=too-few-public-methods\n",
        "+    \"\"\"Simple container matching the subset of Response fields we use.\"\"\"\n",
        "+\n",
        "+    def __init__(self, parsed: brain.AgenticStrengthSetting) -> None:\n",
        "+        self.output_parsed = parsed\n",
        "+        self.usage = None\n",
        "+\n",
        "+\n",
        "+def test_set_agentic_strength_persists_choice(monkeypatch: pytest.MonkeyPatch) -> None:\n",
        "+    \"\"\"When the user asks for a valid level, it is stored and echoed.\"\"\"\n",
        "+\n",
        "+    parsed = brain.AgenticStrengthSetting(strength=brain.AgenticStrengthChoice.LOW)\n",
        "+    monkeypatch.setattr(\n",
        "+        brain.open_ai_client.responses,\n",
        "+        \"parse\",\n",
        "+        lambda **_kwargs: _StubResponse(parsed),\n",
        "+    )\n",
        "+\n",
        "+    captured: dict[str, Any] = {}\n",
        "+\n",
        "+    def _fake_set(user_id: str, strength: str) -> None:  # noqa: D401\n",
        "+        captured[\"user_id\"] = user_id\n",
        "+        captured[\"strength\"] = strength\n",
        "+\n",
        "+    monkeypatch.setattr(brain, \"set_user_agentic_strength\", _fake_set)\n",
        "+\n",
        "+    state: dict[str, Any] = {\n",
        "+        \"user_id\": \"user-123\",\n",
        "+        \"user_query\": \"set my agentic strength to low\",\n",
        "+        \"user_chat_history\": [],\n",
        "+    }\n",
        "+\n",
        "+    out = brain.set_agentic_strength(cast(Any, state))\n",
        "+\n",
        "+    assert captured == {\"user_id\": \"user-123\", \"strength\": \"low\"}\n",
        "+    assert out.get(\"agentic_strength\") == \"low\"\n",
        "+    assert out.get(\"user_agentic_strength\") == \"low\"\n",
        "+    response_text = out[\"responses\"][0][\"response\"]\n",
        "+    assert \"agentic strength set to\" in response_text.lower()\n",
        "+    assert \"low\" in response_text.lower()\n",
        "+\n",
        "+\n",
        "+def test_set_agentic_strength_handles_unknown(monkeypatch: pytest.MonkeyPatch) -> None:\n",
        "+    \"\"\"If the LLM can't find a valid level, prompt the user again.\"\"\"\n",
        "+\n",
        "+    parsed = brain.AgenticStrengthSetting(strength=brain.AgenticStrengthChoice.UNKNOWN)\n",
        "+    monkeypatch.setattr(\n",
        "+        brain.open_ai_client.responses,\n",
        "+        \"parse\",\n",
        "+        lambda **_kwargs: _StubResponse(parsed),\n",
        "+    )\n",
        "+\n",
        "+    called = False\n",
        "+\n",
        "+    def _fail_set(*_args: Any, **_kwargs: Any) -> None:  # noqa: ANN401\n",
        "+        nonlocal called\n",
        "+        called = True\n",
        "+\n",
        "+    monkeypatch.setattr(brain, \"set_user_agentic_strength\", _fail_set)\n",
        "+\n",
        "+    state: dict[str, Any] = {\n",
        "+        \"user_id\": \"user-456\",\n",
        "+        \"user_query\": \"make it stronger\",\n",
        "+        \"user_chat_history\": [],\n",
        "+    }\n",
        "+\n",
        "+    out = brain.set_agentic_strength(cast(Any, state))\n",
        "+\n",
        "+    assert not called, \"should not persist an unknown preference\"\n",
        "+    assert \"normal, low, or very low\" in out[\"responses\"][0][\"response\"]\n",
        "+    assert \"agentic_strength\" not in out\n"
      ]
    },
    {
      "path": "tests/test_consult_fia_resources.py",
      "status": "modified",
      "additions": 40,
      "deletions": 0,
      "patch": "@@ -47,6 +47,7 @@ class _FakeResponse:  # pylint: disable=too-few-public-methods\n \n     def _fake_create(**kwargs: Any) -> Any:  # noqa: ANN401\n         captured_messages[\"input\"] = kwargs[\"input\"]\n+        captured_messages[\"model\"] = kwargs.get(\"model\")\n         return _FakeResponse()\n \n     monkeypatch.setattr(brain, \"get_chroma_collection\", _fake_get_collection)\n@@ -58,19 +59,58 @@ def _fake_create(**kwargs: Any) -> Any:  # noqa: ANN401\n         \"user_chat_history\": [],\n         \"user_response_language\": \"es\",\n         \"query_language\": \"es\",\n+        \"agentic_strength\": \"low\",\n     }\n \n     out = brain.consult_fia_resources(cast(Any, state))\n \n     assert out[\"responses\"][0][\"intent\"] == brain.IntentType.CONSULT_FIA_RESOURCES\n     assert out.get(\"collection_used\") == \"en_fia_resources\"\n \n+    assert captured_messages.get(\"model\") == \"gpt-4o-mini\"\n+\n     # Ensure the FIA manual and vector doc both reached the prompt payload\n     payload = captured_messages[\"input\"][0][\"content\"]\n     assert \"FIA manual reference text\" in payload\n     assert \"English FIA doc\" in payload\n \n \n+def test_consult_fia_resources_uses_normal_model(monkeypatch: pytest.MonkeyPatch) -> None:\n+    \"\"\"Defaults to gpt-4o when agentic strength remains normal.\"\"\"\n+\n+    class _FakeResponse:  # pylint: disable=too-few-public-methods\n+        usage = None\n+        output_text = \"fia guidance\"\n+\n+    captured: dict[str, Any] = {}\n+\n+    def _fake_create(**kwargs: Any) -> Any:  # noqa: ANN401\n+        captured[\"model\"] = kwargs.get(\"model\")\n+        return _FakeResponse()\n+\n+    def _fake_get_collection(_name: str) -> _FakeCollection:\n+        return _FakeCollection([\n+            (\"Localized\", 0.2, {\"name\": \"Localized\", \"source\": \"fia/local.md\"}),\n+        ])\n+\n+    monkeypatch.setattr(brain, \"get_chroma_collection\", _fake_get_collection)\n+    monkeypatch.setattr(brain.open_ai_client.responses, \"create\", _fake_create)\n+    monkeypatch.setattr(brain, \"FIA_REFERENCE_CONTENT\", \"reference body\")\n+\n+    state: dict[str, Any] = {\n+        \"transformed_query\": \"Explain FIA step 3\",\n+        \"user_chat_history\": [],\n+        \"user_response_language\": \"en\",\n+        \"query_language\": \"en\",\n+        \"agentic_strength\": \"normal\",\n+    }\n+\n+    out = brain.consult_fia_resources(cast(Any, state))\n+\n+    assert out[\"responses\"], \"expected consult FIA response\"\n+    assert captured.get(\"model\") == \"gpt-4o\"\n+\n+\n def test_consult_fia_resources_handles_missing_context(monkeypatch: pytest.MonkeyPatch) -> None:\n     \"\"\"Returns a helpful fallback when no FIA resources are available.\"\"\"\n ",
      "patch_lines": [
        "@@ -47,6 +47,7 @@ class _FakeResponse:  # pylint: disable=too-few-public-methods\n",
        " \n",
        "     def _fake_create(**kwargs: Any) -> Any:  # noqa: ANN401\n",
        "         captured_messages[\"input\"] = kwargs[\"input\"]\n",
        "+        captured_messages[\"model\"] = kwargs.get(\"model\")\n",
        "         return _FakeResponse()\n",
        " \n",
        "     monkeypatch.setattr(brain, \"get_chroma_collection\", _fake_get_collection)\n",
        "@@ -58,19 +59,58 @@ def _fake_create(**kwargs: Any) -> Any:  # noqa: ANN401\n",
        "         \"user_chat_history\": [],\n",
        "         \"user_response_language\": \"es\",\n",
        "         \"query_language\": \"es\",\n",
        "+        \"agentic_strength\": \"low\",\n",
        "     }\n",
        " \n",
        "     out = brain.consult_fia_resources(cast(Any, state))\n",
        " \n",
        "     assert out[\"responses\"][0][\"intent\"] == brain.IntentType.CONSULT_FIA_RESOURCES\n",
        "     assert out.get(\"collection_used\") == \"en_fia_resources\"\n",
        " \n",
        "+    assert captured_messages.get(\"model\") == \"gpt-4o-mini\"\n",
        "+\n",
        "     # Ensure the FIA manual and vector doc both reached the prompt payload\n",
        "     payload = captured_messages[\"input\"][0][\"content\"]\n",
        "     assert \"FIA manual reference text\" in payload\n",
        "     assert \"English FIA doc\" in payload\n",
        " \n",
        " \n",
        "+def test_consult_fia_resources_uses_normal_model(monkeypatch: pytest.MonkeyPatch) -> None:\n",
        "+    \"\"\"Defaults to gpt-4o when agentic strength remains normal.\"\"\"\n",
        "+\n",
        "+    class _FakeResponse:  # pylint: disable=too-few-public-methods\n",
        "+        usage = None\n",
        "+        output_text = \"fia guidance\"\n",
        "+\n",
        "+    captured: dict[str, Any] = {}\n",
        "+\n",
        "+    def _fake_create(**kwargs: Any) -> Any:  # noqa: ANN401\n",
        "+        captured[\"model\"] = kwargs.get(\"model\")\n",
        "+        return _FakeResponse()\n",
        "+\n",
        "+    def _fake_get_collection(_name: str) -> _FakeCollection:\n",
        "+        return _FakeCollection([\n",
        "+            (\"Localized\", 0.2, {\"name\": \"Localized\", \"source\": \"fia/local.md\"}),\n",
        "+        ])\n",
        "+\n",
        "+    monkeypatch.setattr(brain, \"get_chroma_collection\", _fake_get_collection)\n",
        "+    monkeypatch.setattr(brain.open_ai_client.responses, \"create\", _fake_create)\n",
        "+    monkeypatch.setattr(brain, \"FIA_REFERENCE_CONTENT\", \"reference body\")\n",
        "+\n",
        "+    state: dict[str, Any] = {\n",
        "+        \"transformed_query\": \"Explain FIA step 3\",\n",
        "+        \"user_chat_history\": [],\n",
        "+        \"user_response_language\": \"en\",\n",
        "+        \"query_language\": \"en\",\n",
        "+        \"agentic_strength\": \"normal\",\n",
        "+    }\n",
        "+\n",
        "+    out = brain.consult_fia_resources(cast(Any, state))\n",
        "+\n",
        "+    assert out[\"responses\"], \"expected consult FIA response\"\n",
        "+    assert captured.get(\"model\") == \"gpt-4o\"\n",
        "+\n",
        "+\n",
        " def test_consult_fia_resources_handles_missing_context(monkeypatch: pytest.MonkeyPatch) -> None:\n",
        "     \"\"\"Returns a helpful fallback when no FIA resources are available.\"\"\"\n",
        " \n"
      ]
    },
    {
      "path": "tests/test_perf_coverage_llm_calls.py",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "patch": "@@ -154,7 +154,7 @@ def _fake_chat_create(**_kwargs: Any) -> Any:  # noqa: ARG001\n         return _FakeChat()\n \n     # Force translation by reporting response language != target language\n-    monkeypatch.setattr(brain, \"detect_language\", lambda _t: \"en\")\n+    monkeypatch.setattr(brain, \"detect_language\", lambda _t, **_k: \"en\")\n     monkeypatch.setattr(brain.open_ai_client.responses, \"create\", _fake_resp_create)\n     monkeypatch.setattr(brain.open_ai_client.chat.completions, \"create\", _fake_chat_create)\n ",
      "patch_lines": [
        "@@ -154,7 +154,7 @@ def _fake_chat_create(**_kwargs: Any) -> Any:  # noqa: ARG001\n",
        "         return _FakeChat()\n",
        " \n",
        "     # Force translation by reporting response language != target language\n",
        "-    monkeypatch.setattr(brain, \"detect_language\", lambda _t: \"en\")\n",
        "+    monkeypatch.setattr(brain, \"detect_language\", lambda _t, **_k: \"en\")\n",
        "     monkeypatch.setattr(brain.open_ai_client.responses, \"create\", _fake_resp_create)\n",
        "     monkeypatch.setattr(brain.open_ai_client.chat.completions, \"create\", _fake_chat_create)\n",
        " \n"
      ]
    }
  ]
}
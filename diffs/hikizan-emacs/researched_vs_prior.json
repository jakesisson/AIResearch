{
  "project": "Research Data/hikizan-emacs",
  "repo": "shskwmt/hikizan-emacs",
  "prior_commit": "57b593c832e1d1941d81895d9c0a91b196eaccb4",
  "researched_commit": "84365f48d52394fe39ed9ad9e8174a26ba1b9a8f",
  "compare_url": "https://github.com/shskwmt/hikizan-emacs/compare/57b593c832e1d1941d81895d9c0a91b196eaccb4...84365f48d52394fe39ed9ad9e8174a26ba1b9a8f",
  "ahead_by": 1,
  "behind_by": 0,
  "changed_files": [
    {
      "path": "python/emacsagent.py",
      "status": "modified",
      "additions": 329,
      "deletions": 55,
      "patch": "@@ -1,4 +1,3 @@\n-# import packages\n import os\n import subprocess\n import tempfile\n@@ -15,6 +14,7 @@\n from langchain_core.tools import tool\n from langchain_google_genai import ChatGoogleGenerativeAI\n from langgraph.graph import END, StateGraph\n+from langgraph.checkpoint.memory import MemorySaver\n from langgraph.graph.message import add_messages\n \n # --- Constants ---\n@@ -23,6 +23,8 @@\n MODEL_NAME = \"gemini-2.5-flash\"\n LLM_TEMPERATURE = 0\n TIMEOUT = 60\n+MEMORY_WINDOW_SIZE = 10  # Keep last 10 messages in active memory\n+MAX_EXECUTION_LOG_SIZE = 50  # Keep last 50 execution steps\n SYSTEM_PROMPT = \"\"\"\n ** Role\n You are a large language model living in Emacs, a powerful coding assistant.\n@@ -116,13 +118,108 @@\n Keep your feedback constructive and actionable.\n \"\"\"\n \n+def fix_gemini_conversation_flow(messages: Sequence[BaseMessage]) -> Sequence[BaseMessage]:\n+    \"\"\"\n+    Fix conversation flow to meet Gemini's requirements:\n+    - Function calls must come immediately after user messages or function responses\n+    - No consecutive AI messages\n+    - Proper alternating pattern\n+    \"\"\"\n+    if not messages:\n+        return messages\n+\n+    fixed_messages = []\n+    i = 0\n+\n+    while i < len(messages):\n+        current_msg = messages[i]\n+\n+        # Always keep system messages\n+        if isinstance(current_msg, SystemMessage):\n+            fixed_messages.append(current_msg)\n+            i += 1\n+            continue\n+\n+        # Handle AI messages\n+        if isinstance(current_msg, AIMessage):\n+            # Check if this AI message has tool calls\n+            has_tool_calls = hasattr(current_msg, 'tool_calls') and current_msg.tool_calls\n+\n+            # If AI message has tool calls, it must follow a user message or tool message\n+            if has_tool_calls:\n+                # Check if previous message is appropriate\n+                if fixed_messages:\n+                    prev_msg = fixed_messages[-1]\n+                    if not isinstance(prev_msg, (HumanMessage, ToolMessage, SystemMessage)):\n+                        # Insert a dummy user acknowledgment\n+                        fixed_messages.append(HumanMessage(content=\"Continue with the task.\"))\n+\n+                fixed_messages.append(current_msg)\n+            else:\n+                # Regular AI message without tool calls\n+                # Check if we have consecutive AI messages\n+                if fixed_messages and isinstance(fixed_messages[-1], AIMessage):\n+                    # Merge consecutive AI messages\n+                    prev_content = fixed_messages[-1].content or \"\"\n+                    current_content = current_msg.content or \"\"\n+                    merged_content = f\"{prev_content}\\n\\n{current_content}\".strip()\n+                    fixed_messages[-1] = AIMessage(content=merged_content)\n+                else:\n+                    fixed_messages.append(current_msg)\n+\n+        # Handle other message types normally\n+        elif isinstance(current_msg, (HumanMessage, ToolMessage)):\n+            fixed_messages.append(current_msg)\n+\n+        i += 1\n+\n+    return fixed_messages\n+\n+def validate_gemini_conversation(messages: Sequence[BaseMessage]) -> bool:\n+    \"\"\"Validate that the conversation follows Gemini's expected pattern.\"\"\"\n+    for i in range(1, len(messages)):\n+        current_msg = messages[i]\n+        prev_msg = messages[i-1]\n+\n+        # Check if AI message with tool calls follows appropriate message\n+        if isinstance(current_msg, AIMessage) and hasattr(current_msg, 'tool_calls') and current_msg.tool_calls:\n+            if not isinstance(prev_msg, (HumanMessage, ToolMessage, SystemMessage)):\n+                return False\n+\n+    return True\n+\n+def trim_messages_window(messages: Sequence[BaseMessage], window_size: int = MEMORY_WINDOW_SIZE) -> Sequence[BaseMessage]:\n+    \"\"\"\n+    Trim messages to keep only the most recent ones within the window size.\n+    Always preserve the system message if it exists.\n+    \"\"\"\n+    if len(messages) <= window_size:\n+        return messages\n+\n+    # Check if first message is system message\n+    if messages and isinstance(messages[0], SystemMessage):\n+        # Keep system message + last (window_size - 1) messages\n+        return [messages[0]] + list(messages[-(window_size-1):])\n+    else:\n+        # Just keep last window_size messages\n+        return list(messages[-window_size:])\n+\n+def get_message_memory_size(messages: Sequence[BaseMessage]) -> int:\n+    \"\"\"Calculate approximate memory usage of messages in characters.\"\"\"\n+    total_chars = 0\n+    for msg in messages:\n+        if hasattr(msg, 'content') and msg.content:\n+            total_chars += len(str(msg.content))\n+    return total_chars\n+\n \n # --- State Definition ---\n \n class AgentState(TypedDict):\n     \"\"\"The state of the agent.\"\"\"\n     messages: Annotated[Sequence[BaseMessage], add_messages]\n     n_reflection: int\n+    total_tokens_used: int = 0\n \n \n # --- Tools ---\n@@ -321,16 +418,32 @@ def __init__(self):\n             google_api_key=GOOGLE_API_KEY,\n         )\n \n+        self.memory = MemorySaver()\n+\n         self.tools = [execute_elisp_code, read_file, write_to_file, list_files, grep, find_files]\n         self.tools_by_name = {tool.name: tool for tool in self.tools}\n-        self.graph = self._build_graph()\n+\n+        # Build graph with memory checkpointer\n+        self.graph = self._build_graph().compile(checkpointer=self.memory)\n+\n+        # Thread/session management\n+        self.thread_id = \"emacs_agent_session\"\n+        self.config = RunnableConfig(configurable={\"thread_id\": self.thread_id})\n+\n         self.system_prompt = SYSTEM_PROMPT\n         self.plan_prompt = PLAN_PROMPT\n         self.reflection_prompt = REFLECTION_PROMPT\n         self.conversation_history = [SystemMessage(content=self.system_prompt)]\n         self.current_node = None  # Track current node for message formatting\n         self.execution_log = []  # Track execution steps\n \n+        # Memory optimization settings\n+        self.memory_window_size = MEMORY_WINDOW_SIZE\n+        self.max_execution_log_size = MAX_EXECUTION_LOG_SIZE\n+\n+        # Initialize with system message\n+        self._initialize_memory()\n+\n     def _build_graph(self) -> StateGraph:\n         \"\"\"Builds and compiles the LangGraph execution graph.\"\"\"\n         workflow = StateGraph(AgentState)\n@@ -351,7 +464,25 @@ def _build_graph(self) -> StateGraph:\n             self.should_reflect,\n             {\"reflect\": \"llm\", \"end\": END},\n         )\n-        return workflow.compile()\n+        return workflow\n+\n+    def _initialize_memory(self):\n+        \"\"\"Initialize memory with system message if not already present.\"\"\"\n+        try:\n+            # Get the current state snapshot\n+            state = self.graph.get_state(self.config)\n+\n+            # Check if we have any existing state and messages\n+            if not state or not state.values.get(\"messages\"):\n+                # Initialize with system message\n+                initial_state = {\n+                    \"messages\": [SystemMessage(content=self.system_prompt)],\n+                    \"n_reflection\": 0,\n+                    \"total_tokens_used\": 0\n+                }\n+                self.graph.update_state(self.config, initial_state)\n+        except Exception as e:\n+            print(f\"Warning: Could not initialize memory state: {e}\")\n \n     def _log_execution_step(self, node_name: str, action: str, details: str = \"\"):\n         \"\"\"Log execution steps for better debugging.\"\"\"\n@@ -363,6 +494,12 @@ def _log_execution_step(self, node_name: str, action: str, details: str = \"\"):\n         }\n         self.execution_log.append(step)\n \n+        # Trim execution log if it gets too large\n+        if len(self.execution_log) > self.max_execution_log_size:\n+            # Keep only the most recent entries\n+            self.execution_log = self.execution_log[-self.max_execution_log_size:]\n+            print(f\"\\033[0;33m  (Trimmed execution log to {self.max_execution_log_size} entries)\\033[0m\")\n+\n         # Print execution step with enhanced formatting\n         timestamp_str = time.strftime(\"%H:%M:%S\", time.localtime(step[\"timestamp\"]))\n         print(f\"\\n\\033[1;35m[{timestamp_str}] {node_name.upper()} \u2192 {action}\\033[0m\")\n@@ -371,9 +508,33 @@ def _log_execution_step(self, node_name: str, action: str, details: str = \"\"):\n             truncated_details = details[:100] + \"...\" if len(details) > 100 else details\n             print(f\"\\033[0;35m  Details: {truncated_details}\\033[0m\")\n \n+    def _optimize_state_memory(self, state: AgentState) -> AgentState:\n+        \"\"\"Optimize state memory by trimming old messages.\"\"\"\n+        messages = state[\"messages\"]\n+\n+        # Calculate current memory usage\n+        current_memory = get_message_memory_size(messages)\n+\n+        if len(messages) > self.memory_window_size:\n+            trimmed_messages = trim_messages_window(messages, self.memory_window_size)\n+            optimized_memory = get_message_memory_size(trimmed_messages)\n+\n+            print(f\"\\033[0;33m  Memory optimization: {len(messages)} \u2192 {len(trimmed_messages)} messages \"\n+                  f\"({current_memory} \u2192 {optimized_memory} chars)\\033[0m\")\n+\n+            return {**state, \"messages\": trimmed_messages}\n+\n+        return state\n+\n     def should_continue(self, state: AgentState) -> str:\n         \"\"\"Determines whether the agent should continue or end.\"\"\"\n         last_msg = state[\"messages\"][-1]\n+\n+        # Validate conversation flow before continuing\n+        if not validate_gemini_conversation(state[\"messages\"]):\n+            print(f\"\\033[0;33m  Warning: Invalid conversation flow detected, fixing...\\033[0m\")\n+            # This will be handled in the next node call\n+\n         if hasattr(last_msg, \"tool_calls\") and last_msg.tool_calls:\n             return \"continue\"\n         return \"end\"\n@@ -383,17 +544,35 @@ def should_reflect(self, state: AgentState) -> str:\n         if state[\"n_reflection\"] > 3:\n             self._log_execution_step(\"DECISION\", \"Max reflections reached, ending execution\")\n             return \"end\"\n+\n+        # Validate conversation flow before continuing\n+        if not validate_gemini_conversation(state[\"messages\"]):\n+            print(f\"\\033[0;33m  Warning: Invalid conversation flow detected, fixing...\\033[0m\")\n+            # This will be handled in the next node call\n+\n         return \"reflect\"\n \n     def plan(self, state: AgentState):\n         \"\"\"Generates a plan to address the user's request.\"\"\"\n         self.current_node = \"plan\"\n         self._log_execution_step(\"PLAN\", \"Generating execution plan\")\n \n+        # Fix conversation flow before planning\n+        state_messages = fix_gemini_conversation_flow(state[\"messages\"])\n+        state = {**state, \"messages\": state_messages}\n+\n+        # Optimize memory before planning\n+        state = self._optimize_state_memory(state)\n+\n         plan_message = [\n             SystemMessage(content=self.plan_prompt),\n             *state[\"messages\"],\n         ]\n+\n+        # Trim plan messages if needed to avoid context overflow\n+        if len(plan_message) > self.memory_window_size + 1:  # +1 for plan prompt\n+            plan_message = [plan_message[0]] + list(plan_message[-(self.memory_window_size):])\n+\n         response = self.llm.invoke(plan_message)\n \n         # Log the generated plan\n@@ -407,6 +586,13 @@ def call_model(self, state: AgentState, config: RunnableConfig):\n         self.current_node = \"llm\"\n         self._log_execution_step(\"LLM\", \"Calling language model\")\n \n+        # Fix conversation flow before LLM call\n+        state_messages = fix_gemini_conversation_flow(state[\"messages\"])\n+        state = {**state, \"messages\": state_messages}\n+\n+        # Optimize memory before LLM call\n+        state = self._optimize_state_memory(state)\n+\n         model_with_tools = self.llm.bind_tools(self.tools)\n         response = model_with_tools.invoke(state[\"messages\"], config)\n \n@@ -453,56 +639,34 @@ def call_tool(self, state: AgentState):\n                 )\n         return {\"messages\": outputs}\n \n-    def validate_messages_for_gemini(self, messages):\n-        \"\"\"Ensure all messages are compatible with Gemini API\"\"\"\n-        validated_messages = []\n-\n-        for msg in messages:\n-            if isinstance(msg, SystemMessage):\n-                # Clean system message content\n-                content = msg.content.replace('\\\\\"', '\"').replace('\\\\n', '\\n')\n-                validated_messages.append(SystemMessage(content=content))\n-            elif isinstance(msg, HumanMessage):\n-                # Keep human messages as-is\n-                validated_messages.append(HumanMessage(content=str(msg.content)))\n-            elif isinstance(msg, AIMessage):\n-                # Simplify AI messages - remove tool_calls metadata\n-                if hasattr(msg, 'tool_calls') and msg.tool_calls:\n-                    # Create a simplified description of the tool call\n-                    tool_name = msg.tool_calls[0]['name']\n-                    tool_args = msg.tool_calls[0]['args']\n-                    simplified_content = f\"Called tool '{tool_name}' with arguments: {tool_args}\"\n-                    if msg.content:\n-                        simplified_content = f\"{msg.content}\\n\\n{simplified_content}\"\n-                    validated_messages.append(AIMessage(content=simplified_content))\n-                else:\n-                    # Keep regular AI messages\n-                    validated_messages.append(AIMessage(content=str(msg.content or \"\")))\n-            elif isinstance(msg, ToolMessage):\n-                # Convert tool messages to human messages for clarity\n-                tool_result_msg = HumanMessage(content=f\"Tool '{msg.name}' returned: {msg.content}\")\n-                validated_messages.append(tool_result_msg)\n-\n-        return validated_messages\n-\n     def reflect(self, state: AgentState):\n         \"\"\"Critiques the agent's last action and decides whether to continue.\"\"\"\n         self.current_node = \"reflection\"\n         reflection_count = state[\"n_reflection\"]\n         self._log_execution_step(\"REFLECTION\", f\"Reflecting on execution (attempt {reflection_count + 1})\")\n \n+        # Fix conversation flow before reflection\n+        state_messages = fix_gemini_conversation_flow(state[\"messages\"])\n+        state = {**state, \"messages\": state_messages}\n+\n+        # Optimize memory before reflection\n+        state = self._optimize_state_memory(state)\n+\n         # Get recent messages to avoid overwhelming the context\n         recent_messages = state[\"messages\"][-6:]  # Last 6 messages\n \n-        # Validate and simplify messages for Gemini\n-        validated_messages = self.validate_messages_for_gemini(recent_messages)\n+        try:\n+            # Use the conversation flow fixed messages directly\n+            reflection_message = [\n+                SystemMessage(content=self.reflection_prompt),\n+                *recent_messages,\n+            ]\n \n-        reflection_message = [\n-            SystemMessage(content=self.reflection_prompt),\n-            *validated_messages,\n-        ]\n+            # Validate conversation flow before continuing\n+            if not validate_gemini_conversation(reflection_message):\n+                print(f\"\\033[0;33m  Warning: Invalid conversation flow detected, fixing...\\033[0m\")\n \n-        try:\n+            # This will be handled in the next node call\n             response = self.llm.invoke(reflection_message)\n \n             if \"SUCCESS\" in response.content:\n@@ -526,30 +690,54 @@ def reflect(self, state: AgentState):\n                 \"n_reflection\": state[\"n_reflection\"] + 1\n             }\n \n-    def run(self, query: str):\n+    def run(self, query: str, thread_id: str = None):\n         \"\"\"Runs the agent from an initial user query with enhanced logging.\"\"\"\n+        # Use provided thread_id or default\n+        if thread_id:\n+            self.thread_id = thread_id\n+            self.config = RunnableConfig(configurable={\"thread_id\": thread_id})\n+\n         print(f\"\\n\\033[1;34mStarting execution for query: {query}\\033[0m\")\n         self.execution_log.clear()  # Clear previous execution log\n \n-        self.conversation_history.append(HumanMessage(content=query))\n-        initial_state = {\"messages\": self.conversation_history.copy(), \"n_reflection\": 0}\n-        initial_history_length = len(self.conversation_history)\n+        # Get current state from memory\n+        current_state = self.graph.get_state(self.config)\n+        if current_state and current_state.values:\n+            existing_messages = current_state.values.get(\"messages\", [])\n+            n_reflection = current_state.values.get(\"n_reflection\", 0)\n+        else:\n+            existing_messages = [SystemMessage(content=self.system_prompt)]\n+            n_reflection = 0\n+\n+        # Add new human message\n+        new_messages = existing_messages + [HumanMessage(content=query)]\n+        initial_state = {\"messages\": new_messages, \"n_reflection\": 0, \"total_tokens_used\": 0}\n+        initial_history_length = len(existing_messages)\n         final_messages = []\n \n+        # Validate and fix conversation flow before starting\n+        initial_state[\"messages\"] = fix_gemini_conversation_flow(initial_state[\"messages\"])\n+        if not validate_gemini_conversation(initial_state[\"messages\"]):\n+            print(f\"\\033[0;33m  Warning: Conversation flow still invalid after fixing\\033[0m\")\n+\n         try:\n             # Track execution flow with node names\n-            for event in self.graph.stream(initial_state, stream_mode=\"values\"):\n+            for event in self.graph.stream(initial_state, config=self.config, stream_mode=\"values\"):\n                 latest_message = event[\"messages\"][-1]\n                 final_messages = event[\"messages\"]\n \n                 # Only print non-human messages with node context\n                 if not isinstance(latest_message, HumanMessage):\n                     _format_and_print_message(latest_message, self.current_node)\n \n+            # Update conversation history for backward compatibility\n             if len(final_messages) > initial_history_length:\n                 new_messages = final_messages[initial_history_length:]\n                 self.conversation_history.extend(new_messages)\n \n+                # Trim conversation history to prevent unlimited growth\n+                self.conversation_history = trim_messages_window(self.conversation_history, self.memory_window_size * 2)\n+\n             print(f\"\\n\\033[1;34mExecution completed. Total steps: {len(self.execution_log)}\\033[0m\")\n \n         except Exception as e:\n@@ -558,19 +746,94 @@ def run(self, query: str):\n             print(\"The agent will continue to be available for new requests.\")\n \n     def clear_history(self):\n-        \"\"\"Clear the conversation histories\"\"\"\n+        \"\"\"Clear the conversation histories and memory\"\"\"\n         self.conversation_history = [SystemMessage(content=self.system_prompt)]\n         self.execution_log.clear()\n-        print(\"Cleared conversation histories and execution log\")\n+\n+        # Clear memory state\n+        try:\n+            # Reset memory state\n+            initial_state = {\n+                \"messages\": [SystemMessage(content=self.system_prompt)],\n+                \"n_reflection\": 0,\n+                \"total_tokens_used\": 0\n+            }\n+            self.graph.update_state(self.config, initial_state, as_node=\"__start__\")\n+            print(\"Cleared conversation histories, execution log, and memory state\")\n+        except Exception as e:\n+            print(f\"Cleared local history, but could not clear memory state: {e}\")\n+\n+    def debug_conversation_flow(self):\n+        \"\"\"Debug conversation flow issues.\"\"\"\n+        try:\n+            state = self.graph.get_state(self.config)\n+            if state and state.values:\n+                messages = state.values.get(\"messages\", [])\n+                print(f\"\\n=== Conversation Flow Debug ===\")\n+                for i, msg in enumerate(messages):\n+                    msg_type = type(msg).__name__\n+                    has_tools = hasattr(msg, 'tool_calls') and msg.tool_calls\n+                    tool_info = f\" [HAS_TOOLS: {len(msg.tool_calls)}]\" if has_tools else \"\"\n+                    content_preview = str(msg.content)[:50] + \"...\" if len(str(msg.content)) > 50 else str(msg.content)\n+                    print(f\"{i+1:2d}. {msg_type:12}{tool_info}: {content_preview}\")\n+\n+                # Validate flow\n+                is_valid = validate_gemini_conversation(messages)\n+                print(f\"\\nConversation flow valid: {is_valid}\")\n+\n+                if not is_valid:\n+                    print(\"Suggested fix:\")\n+                    fixed_messages = fix_gemini_conversation_flow(messages)\n+                    for i, msg in enumerate(fixed_messages):\n+                        msg_type = type(msg).__name__\n+                        has_tools = hasattr(msg, 'tool_calls') and msg.tool_calls\n+                        tool_info = f\" [HAS_TOOLS: {len(msg.tool_calls)}]\" if has_tools else \"\"\n+                        content_preview = str(msg.content)[:50] + \"...\" if len(str(msg.content)) > 50 else str(msg.content)\n+                        print(f\"  {i+1:2d}. {msg_type:12}{tool_info}: {content_preview}\")\n+\n+                print(\"==============================\\n\")\n+        except Exception as e:\n+            print(f\"Error debugging conversation flow: {e}\")\n+\n+    def get_memory_stats(self):\n+        \"\"\"Get memory usage statistics.\"\"\"\n+        try:\n+            state = self.graph.get_state(self.config)\n+            if state and state.values:\n+                messages = state.values.get(\"messages\", [])\n+                memory_size = get_message_memory_size(messages)\n+                return {\n+                    \"message_count\": len(messages),\n+                    \"memory_size_chars\": memory_size,\n+                    \"execution_log_size\": len(self.execution_log),\n+                    \"memory_window_size\": self.memory_window_size\n+                }\n+        except Exception as e:\n+            print(f\"Could not get memory stats: {e}\")\n+        return {\"error\": \"Could not retrieve memory statistics\"}\n \n     def show_history(self):\n         \"\"\"Display conversation history.\"\"\"\n-        print(\"\\n=== Conversation History ===\")\n-        for i, msg in enumerate(self.conversation_history):\n-            role = getattr(msg, \"role\", msg.__class__.__name__)\n-            content_preview = str(msg.content)[:100] + \"...\" if len(str(msg.content)) > 100 else str(msg.content)\n-            print(f\"{i+1:2d}. [{role:12}]: {content_preview}\")\n-        print(\"============================\\n\")\n+        try:\n+            # Try to get from memory first\n+            state = self.graph.get_state(self.config)\n+            if state and state.values:\n+                messages = state.values.get(\"messages\", [])\n+            else:\n+                messages = self.conversation_history\n+\n+            print(\"\\n=== Conversation History ===\")\n+            for i, msg in enumerate(messages):\n+                role = getattr(msg, \"role\", msg.__class__.__name__)\n+                content_preview = str(msg.content)[:100] + \"...\" if len(str(msg.content)) > 100 else str(msg.content)\n+                print(f\"{i+1:2d}. [{role:12}]: {content_preview}\")\n+\n+            # Show memory stats\n+            stats = self.get_memory_stats()\n+            print(f\"Memory: {stats.get('message_count', 0)} messages, {stats.get('memory_size_chars', 0)} chars\")\n+            print(\"============================\\n\")\n+        except Exception as e:\n+            print(f\"Error showing history: {e}\")\n \n     def show_execution_log(self):\n         \"\"\"Display the execution log for debugging.\"\"\"\n@@ -624,6 +887,7 @@ def main():\n     print(\"  - 'history': show conversation history\")\n     print(\"  - 'log': show execution log\")\n     print(\"  - 'stats': show execution statistics\")\n+    print(\"  - 'memory': show memory usage statistics\")\n \n     try:\n         agent = EmacsAgent()\n@@ -643,6 +907,16 @@ def main():\n             elif query.lower() == \"stats\":\n                 agent.show_stats()\n                 continue\n+            elif query.lower() == \"memory\":\n+                stats = agent.get_memory_stats()\n+                print(f\"\\n=== Memory Statistics ===\")\n+                for key, value in stats.items():\n+                    print(f\"{key}: {value}\")\n+                print(\"========================\\n\")\n+                continue\n+            elif query.lower() == \"debug\":\n+                agent.debug_conversation_flow()\n+                continue\n             agent.run(query)\n     except ValueError as e:\n         print(f\"\\n\\033[1;31mInitialization Error: {e}\\033[0m\")",
      "patch_lines": [
        "@@ -1,4 +1,3 @@\n",
        "-# import packages\n",
        " import os\n",
        " import subprocess\n",
        " import tempfile\n",
        "@@ -15,6 +14,7 @@\n",
        " from langchain_core.tools import tool\n",
        " from langchain_google_genai import ChatGoogleGenerativeAI\n",
        " from langgraph.graph import END, StateGraph\n",
        "+from langgraph.checkpoint.memory import MemorySaver\n",
        " from langgraph.graph.message import add_messages\n",
        " \n",
        " # --- Constants ---\n",
        "@@ -23,6 +23,8 @@\n",
        " MODEL_NAME = \"gemini-2.5-flash\"\n",
        " LLM_TEMPERATURE = 0\n",
        " TIMEOUT = 60\n",
        "+MEMORY_WINDOW_SIZE = 10  # Keep last 10 messages in active memory\n",
        "+MAX_EXECUTION_LOG_SIZE = 50  # Keep last 50 execution steps\n",
        " SYSTEM_PROMPT = \"\"\"\n",
        " ** Role\n",
        " You are a large language model living in Emacs, a powerful coding assistant.\n",
        "@@ -116,13 +118,108 @@\n",
        " Keep your feedback constructive and actionable.\n",
        " \"\"\"\n",
        " \n",
        "+def fix_gemini_conversation_flow(messages: Sequence[BaseMessage]) -> Sequence[BaseMessage]:\n",
        "+    \"\"\"\n",
        "+    Fix conversation flow to meet Gemini's requirements:\n",
        "+    - Function calls must come immediately after user messages or function responses\n",
        "+    - No consecutive AI messages\n",
        "+    - Proper alternating pattern\n",
        "+    \"\"\"\n",
        "+    if not messages:\n",
        "+        return messages\n",
        "+\n",
        "+    fixed_messages = []\n",
        "+    i = 0\n",
        "+\n",
        "+    while i < len(messages):\n",
        "+        current_msg = messages[i]\n",
        "+\n",
        "+        # Always keep system messages\n",
        "+        if isinstance(current_msg, SystemMessage):\n",
        "+            fixed_messages.append(current_msg)\n",
        "+            i += 1\n",
        "+            continue\n",
        "+\n",
        "+        # Handle AI messages\n",
        "+        if isinstance(current_msg, AIMessage):\n",
        "+            # Check if this AI message has tool calls\n",
        "+            has_tool_calls = hasattr(current_msg, 'tool_calls') and current_msg.tool_calls\n",
        "+\n",
        "+            # If AI message has tool calls, it must follow a user message or tool message\n",
        "+            if has_tool_calls:\n",
        "+                # Check if previous message is appropriate\n",
        "+                if fixed_messages:\n",
        "+                    prev_msg = fixed_messages[-1]\n",
        "+                    if not isinstance(prev_msg, (HumanMessage, ToolMessage, SystemMessage)):\n",
        "+                        # Insert a dummy user acknowledgment\n",
        "+                        fixed_messages.append(HumanMessage(content=\"Continue with the task.\"))\n",
        "+\n",
        "+                fixed_messages.append(current_msg)\n",
        "+            else:\n",
        "+                # Regular AI message without tool calls\n",
        "+                # Check if we have consecutive AI messages\n",
        "+                if fixed_messages and isinstance(fixed_messages[-1], AIMessage):\n",
        "+                    # Merge consecutive AI messages\n",
        "+                    prev_content = fixed_messages[-1].content or \"\"\n",
        "+                    current_content = current_msg.content or \"\"\n",
        "+                    merged_content = f\"{prev_content}\\n\\n{current_content}\".strip()\n",
        "+                    fixed_messages[-1] = AIMessage(content=merged_content)\n",
        "+                else:\n",
        "+                    fixed_messages.append(current_msg)\n",
        "+\n",
        "+        # Handle other message types normally\n",
        "+        elif isinstance(current_msg, (HumanMessage, ToolMessage)):\n",
        "+            fixed_messages.append(current_msg)\n",
        "+\n",
        "+        i += 1\n",
        "+\n",
        "+    return fixed_messages\n",
        "+\n",
        "+def validate_gemini_conversation(messages: Sequence[BaseMessage]) -> bool:\n",
        "+    \"\"\"Validate that the conversation follows Gemini's expected pattern.\"\"\"\n",
        "+    for i in range(1, len(messages)):\n",
        "+        current_msg = messages[i]\n",
        "+        prev_msg = messages[i-1]\n",
        "+\n",
        "+        # Check if AI message with tool calls follows appropriate message\n",
        "+        if isinstance(current_msg, AIMessage) and hasattr(current_msg, 'tool_calls') and current_msg.tool_calls:\n",
        "+            if not isinstance(prev_msg, (HumanMessage, ToolMessage, SystemMessage)):\n",
        "+                return False\n",
        "+\n",
        "+    return True\n",
        "+\n",
        "+def trim_messages_window(messages: Sequence[BaseMessage], window_size: int = MEMORY_WINDOW_SIZE) -> Sequence[BaseMessage]:\n",
        "+    \"\"\"\n",
        "+    Trim messages to keep only the most recent ones within the window size.\n",
        "+    Always preserve the system message if it exists.\n",
        "+    \"\"\"\n",
        "+    if len(messages) <= window_size:\n",
        "+        return messages\n",
        "+\n",
        "+    # Check if first message is system message\n",
        "+    if messages and isinstance(messages[0], SystemMessage):\n",
        "+        # Keep system message + last (window_size - 1) messages\n",
        "+        return [messages[0]] + list(messages[-(window_size-1):])\n",
        "+    else:\n",
        "+        # Just keep last window_size messages\n",
        "+        return list(messages[-window_size:])\n",
        "+\n",
        "+def get_message_memory_size(messages: Sequence[BaseMessage]) -> int:\n",
        "+    \"\"\"Calculate approximate memory usage of messages in characters.\"\"\"\n",
        "+    total_chars = 0\n",
        "+    for msg in messages:\n",
        "+        if hasattr(msg, 'content') and msg.content:\n",
        "+            total_chars += len(str(msg.content))\n",
        "+    return total_chars\n",
        "+\n",
        " \n",
        " # --- State Definition ---\n",
        " \n",
        " class AgentState(TypedDict):\n",
        "     \"\"\"The state of the agent.\"\"\"\n",
        "     messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "     n_reflection: int\n",
        "+    total_tokens_used: int = 0\n",
        " \n",
        " \n",
        " # --- Tools ---\n",
        "@@ -321,16 +418,32 @@ def __init__(self):\n",
        "             google_api_key=GOOGLE_API_KEY,\n",
        "         )\n",
        " \n",
        "+        self.memory = MemorySaver()\n",
        "+\n",
        "         self.tools = [execute_elisp_code, read_file, write_to_file, list_files, grep, find_files]\n",
        "         self.tools_by_name = {tool.name: tool for tool in self.tools}\n",
        "-        self.graph = self._build_graph()\n",
        "+\n",
        "+        # Build graph with memory checkpointer\n",
        "+        self.graph = self._build_graph().compile(checkpointer=self.memory)\n",
        "+\n",
        "+        # Thread/session management\n",
        "+        self.thread_id = \"emacs_agent_session\"\n",
        "+        self.config = RunnableConfig(configurable={\"thread_id\": self.thread_id})\n",
        "+\n",
        "         self.system_prompt = SYSTEM_PROMPT\n",
        "         self.plan_prompt = PLAN_PROMPT\n",
        "         self.reflection_prompt = REFLECTION_PROMPT\n",
        "         self.conversation_history = [SystemMessage(content=self.system_prompt)]\n",
        "         self.current_node = None  # Track current node for message formatting\n",
        "         self.execution_log = []  # Track execution steps\n",
        " \n",
        "+        # Memory optimization settings\n",
        "+        self.memory_window_size = MEMORY_WINDOW_SIZE\n",
        "+        self.max_execution_log_size = MAX_EXECUTION_LOG_SIZE\n",
        "+\n",
        "+        # Initialize with system message\n",
        "+        self._initialize_memory()\n",
        "+\n",
        "     def _build_graph(self) -> StateGraph:\n",
        "         \"\"\"Builds and compiles the LangGraph execution graph.\"\"\"\n",
        "         workflow = StateGraph(AgentState)\n",
        "@@ -351,7 +464,25 @@ def _build_graph(self) -> StateGraph:\n",
        "             self.should_reflect,\n",
        "             {\"reflect\": \"llm\", \"end\": END},\n",
        "         )\n",
        "-        return workflow.compile()\n",
        "+        return workflow\n",
        "+\n",
        "+    def _initialize_memory(self):\n",
        "+        \"\"\"Initialize memory with system message if not already present.\"\"\"\n",
        "+        try:\n",
        "+            # Get the current state snapshot\n",
        "+            state = self.graph.get_state(self.config)\n",
        "+\n",
        "+            # Check if we have any existing state and messages\n",
        "+            if not state or not state.values.get(\"messages\"):\n",
        "+                # Initialize with system message\n",
        "+                initial_state = {\n",
        "+                    \"messages\": [SystemMessage(content=self.system_prompt)],\n",
        "+                    \"n_reflection\": 0,\n",
        "+                    \"total_tokens_used\": 0\n",
        "+                }\n",
        "+                self.graph.update_state(self.config, initial_state)\n",
        "+        except Exception as e:\n",
        "+            print(f\"Warning: Could not initialize memory state: {e}\")\n",
        " \n",
        "     def _log_execution_step(self, node_name: str, action: str, details: str = \"\"):\n",
        "         \"\"\"Log execution steps for better debugging.\"\"\"\n",
        "@@ -363,6 +494,12 @@ def _log_execution_step(self, node_name: str, action: str, details: str = \"\"):\n",
        "         }\n",
        "         self.execution_log.append(step)\n",
        " \n",
        "+        # Trim execution log if it gets too large\n",
        "+        if len(self.execution_log) > self.max_execution_log_size:\n",
        "+            # Keep only the most recent entries\n",
        "+            self.execution_log = self.execution_log[-self.max_execution_log_size:]\n",
        "+            print(f\"\\033[0;33m  (Trimmed execution log to {self.max_execution_log_size} entries)\\033[0m\")\n",
        "+\n",
        "         # Print execution step with enhanced formatting\n",
        "         timestamp_str = time.strftime(\"%H:%M:%S\", time.localtime(step[\"timestamp\"]))\n",
        "         print(f\"\\n\\033[1;35m[{timestamp_str}] {node_name.upper()} \u2192 {action}\\033[0m\")\n",
        "@@ -371,9 +508,33 @@ def _log_execution_step(self, node_name: str, action: str, details: str = \"\"):\n",
        "             truncated_details = details[:100] + \"...\" if len(details) > 100 else details\n",
        "             print(f\"\\033[0;35m  Details: {truncated_details}\\033[0m\")\n",
        " \n",
        "+    def _optimize_state_memory(self, state: AgentState) -> AgentState:\n",
        "+        \"\"\"Optimize state memory by trimming old messages.\"\"\"\n",
        "+        messages = state[\"messages\"]\n",
        "+\n",
        "+        # Calculate current memory usage\n",
        "+        current_memory = get_message_memory_size(messages)\n",
        "+\n",
        "+        if len(messages) > self.memory_window_size:\n",
        "+            trimmed_messages = trim_messages_window(messages, self.memory_window_size)\n",
        "+            optimized_memory = get_message_memory_size(trimmed_messages)\n",
        "+\n",
        "+            print(f\"\\033[0;33m  Memory optimization: {len(messages)} \u2192 {len(trimmed_messages)} messages \"\n",
        "+                  f\"({current_memory} \u2192 {optimized_memory} chars)\\033[0m\")\n",
        "+\n",
        "+            return {**state, \"messages\": trimmed_messages}\n",
        "+\n",
        "+        return state\n",
        "+\n",
        "     def should_continue(self, state: AgentState) -> str:\n",
        "         \"\"\"Determines whether the agent should continue or end.\"\"\"\n",
        "         last_msg = state[\"messages\"][-1]\n",
        "+\n",
        "+        # Validate conversation flow before continuing\n",
        "+        if not validate_gemini_conversation(state[\"messages\"]):\n",
        "+            print(f\"\\033[0;33m  Warning: Invalid conversation flow detected, fixing...\\033[0m\")\n",
        "+            # This will be handled in the next node call\n",
        "+\n",
        "         if hasattr(last_msg, \"tool_calls\") and last_msg.tool_calls:\n",
        "             return \"continue\"\n",
        "         return \"end\"\n",
        "@@ -383,17 +544,35 @@ def should_reflect(self, state: AgentState) -> str:\n",
        "         if state[\"n_reflection\"] > 3:\n",
        "             self._log_execution_step(\"DECISION\", \"Max reflections reached, ending execution\")\n",
        "             return \"end\"\n",
        "+\n",
        "+        # Validate conversation flow before continuing\n",
        "+        if not validate_gemini_conversation(state[\"messages\"]):\n",
        "+            print(f\"\\033[0;33m  Warning: Invalid conversation flow detected, fixing...\\033[0m\")\n",
        "+            # This will be handled in the next node call\n",
        "+\n",
        "         return \"reflect\"\n",
        " \n",
        "     def plan(self, state: AgentState):\n",
        "         \"\"\"Generates a plan to address the user's request.\"\"\"\n",
        "         self.current_node = \"plan\"\n",
        "         self._log_execution_step(\"PLAN\", \"Generating execution plan\")\n",
        " \n",
        "+        # Fix conversation flow before planning\n",
        "+        state_messages = fix_gemini_conversation_flow(state[\"messages\"])\n",
        "+        state = {**state, \"messages\": state_messages}\n",
        "+\n",
        "+        # Optimize memory before planning\n",
        "+        state = self._optimize_state_memory(state)\n",
        "+\n",
        "         plan_message = [\n",
        "             SystemMessage(content=self.plan_prompt),\n",
        "             *state[\"messages\"],\n",
        "         ]\n",
        "+\n",
        "+        # Trim plan messages if needed to avoid context overflow\n",
        "+        if len(plan_message) > self.memory_window_size + 1:  # +1 for plan prompt\n",
        "+            plan_message = [plan_message[0]] + list(plan_message[-(self.memory_window_size):])\n",
        "+\n",
        "         response = self.llm.invoke(plan_message)\n",
        " \n",
        "         # Log the generated plan\n",
        "@@ -407,6 +586,13 @@ def call_model(self, state: AgentState, config: RunnableConfig):\n",
        "         self.current_node = \"llm\"\n",
        "         self._log_execution_step(\"LLM\", \"Calling language model\")\n",
        " \n",
        "+        # Fix conversation flow before LLM call\n",
        "+        state_messages = fix_gemini_conversation_flow(state[\"messages\"])\n",
        "+        state = {**state, \"messages\": state_messages}\n",
        "+\n",
        "+        # Optimize memory before LLM call\n",
        "+        state = self._optimize_state_memory(state)\n",
        "+\n",
        "         model_with_tools = self.llm.bind_tools(self.tools)\n",
        "         response = model_with_tools.invoke(state[\"messages\"], config)\n",
        " \n",
        "@@ -453,56 +639,34 @@ def call_tool(self, state: AgentState):\n",
        "                 )\n",
        "         return {\"messages\": outputs}\n",
        " \n",
        "-    def validate_messages_for_gemini(self, messages):\n",
        "-        \"\"\"Ensure all messages are compatible with Gemini API\"\"\"\n",
        "-        validated_messages = []\n",
        "-\n",
        "-        for msg in messages:\n",
        "-            if isinstance(msg, SystemMessage):\n",
        "-                # Clean system message content\n",
        "-                content = msg.content.replace('\\\\\"', '\"').replace('\\\\n', '\\n')\n",
        "-                validated_messages.append(SystemMessage(content=content))\n",
        "-            elif isinstance(msg, HumanMessage):\n",
        "-                # Keep human messages as-is\n",
        "-                validated_messages.append(HumanMessage(content=str(msg.content)))\n",
        "-            elif isinstance(msg, AIMessage):\n",
        "-                # Simplify AI messages - remove tool_calls metadata\n",
        "-                if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
        "-                    # Create a simplified description of the tool call\n",
        "-                    tool_name = msg.tool_calls[0]['name']\n",
        "-                    tool_args = msg.tool_calls[0]['args']\n",
        "-                    simplified_content = f\"Called tool '{tool_name}' with arguments: {tool_args}\"\n",
        "-                    if msg.content:\n",
        "-                        simplified_content = f\"{msg.content}\\n\\n{simplified_content}\"\n",
        "-                    validated_messages.append(AIMessage(content=simplified_content))\n",
        "-                else:\n",
        "-                    # Keep regular AI messages\n",
        "-                    validated_messages.append(AIMessage(content=str(msg.content or \"\")))\n",
        "-            elif isinstance(msg, ToolMessage):\n",
        "-                # Convert tool messages to human messages for clarity\n",
        "-                tool_result_msg = HumanMessage(content=f\"Tool '{msg.name}' returned: {msg.content}\")\n",
        "-                validated_messages.append(tool_result_msg)\n",
        "-\n",
        "-        return validated_messages\n",
        "-\n",
        "     def reflect(self, state: AgentState):\n",
        "         \"\"\"Critiques the agent's last action and decides whether to continue.\"\"\"\n",
        "         self.current_node = \"reflection\"\n",
        "         reflection_count = state[\"n_reflection\"]\n",
        "         self._log_execution_step(\"REFLECTION\", f\"Reflecting on execution (attempt {reflection_count + 1})\")\n",
        " \n",
        "+        # Fix conversation flow before reflection\n",
        "+        state_messages = fix_gemini_conversation_flow(state[\"messages\"])\n",
        "+        state = {**state, \"messages\": state_messages}\n",
        "+\n",
        "+        # Optimize memory before reflection\n",
        "+        state = self._optimize_state_memory(state)\n",
        "+\n",
        "         # Get recent messages to avoid overwhelming the context\n",
        "         recent_messages = state[\"messages\"][-6:]  # Last 6 messages\n",
        " \n",
        "-        # Validate and simplify messages for Gemini\n",
        "-        validated_messages = self.validate_messages_for_gemini(recent_messages)\n",
        "+        try:\n",
        "+            # Use the conversation flow fixed messages directly\n",
        "+            reflection_message = [\n",
        "+                SystemMessage(content=self.reflection_prompt),\n",
        "+                *recent_messages,\n",
        "+            ]\n",
        " \n",
        "-        reflection_message = [\n",
        "-            SystemMessage(content=self.reflection_prompt),\n",
        "-            *validated_messages,\n",
        "-        ]\n",
        "+            # Validate conversation flow before continuing\n",
        "+            if not validate_gemini_conversation(reflection_message):\n",
        "+                print(f\"\\033[0;33m  Warning: Invalid conversation flow detected, fixing...\\033[0m\")\n",
        " \n",
        "-        try:\n",
        "+            # This will be handled in the next node call\n",
        "             response = self.llm.invoke(reflection_message)\n",
        " \n",
        "             if \"SUCCESS\" in response.content:\n",
        "@@ -526,30 +690,54 @@ def reflect(self, state: AgentState):\n",
        "                 \"n_reflection\": state[\"n_reflection\"] + 1\n",
        "             }\n",
        " \n",
        "-    def run(self, query: str):\n",
        "+    def run(self, query: str, thread_id: str = None):\n",
        "         \"\"\"Runs the agent from an initial user query with enhanced logging.\"\"\"\n",
        "+        # Use provided thread_id or default\n",
        "+        if thread_id:\n",
        "+            self.thread_id = thread_id\n",
        "+            self.config = RunnableConfig(configurable={\"thread_id\": thread_id})\n",
        "+\n",
        "         print(f\"\\n\\033[1;34mStarting execution for query: {query}\\033[0m\")\n",
        "         self.execution_log.clear()  # Clear previous execution log\n",
        " \n",
        "-        self.conversation_history.append(HumanMessage(content=query))\n",
        "-        initial_state = {\"messages\": self.conversation_history.copy(), \"n_reflection\": 0}\n",
        "-        initial_history_length = len(self.conversation_history)\n",
        "+        # Get current state from memory\n",
        "+        current_state = self.graph.get_state(self.config)\n",
        "+        if current_state and current_state.values:\n",
        "+            existing_messages = current_state.values.get(\"messages\", [])\n",
        "+            n_reflection = current_state.values.get(\"n_reflection\", 0)\n",
        "+        else:\n",
        "+            existing_messages = [SystemMessage(content=self.system_prompt)]\n",
        "+            n_reflection = 0\n",
        "+\n",
        "+        # Add new human message\n",
        "+        new_messages = existing_messages + [HumanMessage(content=query)]\n",
        "+        initial_state = {\"messages\": new_messages, \"n_reflection\": 0, \"total_tokens_used\": 0}\n",
        "+        initial_history_length = len(existing_messages)\n",
        "         final_messages = []\n",
        " \n",
        "+        # Validate and fix conversation flow before starting\n",
        "+        initial_state[\"messages\"] = fix_gemini_conversation_flow(initial_state[\"messages\"])\n",
        "+        if not validate_gemini_conversation(initial_state[\"messages\"]):\n",
        "+            print(f\"\\033[0;33m  Warning: Conversation flow still invalid after fixing\\033[0m\")\n",
        "+\n",
        "         try:\n",
        "             # Track execution flow with node names\n",
        "-            for event in self.graph.stream(initial_state, stream_mode=\"values\"):\n",
        "+            for event in self.graph.stream(initial_state, config=self.config, stream_mode=\"values\"):\n",
        "                 latest_message = event[\"messages\"][-1]\n",
        "                 final_messages = event[\"messages\"]\n",
        " \n",
        "                 # Only print non-human messages with node context\n",
        "                 if not isinstance(latest_message, HumanMessage):\n",
        "                     _format_and_print_message(latest_message, self.current_node)\n",
        " \n",
        "+            # Update conversation history for backward compatibility\n",
        "             if len(final_messages) > initial_history_length:\n",
        "                 new_messages = final_messages[initial_history_length:]\n",
        "                 self.conversation_history.extend(new_messages)\n",
        " \n",
        "+                # Trim conversation history to prevent unlimited growth\n",
        "+                self.conversation_history = trim_messages_window(self.conversation_history, self.memory_window_size * 2)\n",
        "+\n",
        "             print(f\"\\n\\033[1;34mExecution completed. Total steps: {len(self.execution_log)}\\033[0m\")\n",
        " \n",
        "         except Exception as e:\n",
        "@@ -558,19 +746,94 @@ def run(self, query: str):\n",
        "             print(\"The agent will continue to be available for new requests.\")\n",
        " \n",
        "     def clear_history(self):\n",
        "-        \"\"\"Clear the conversation histories\"\"\"\n",
        "+        \"\"\"Clear the conversation histories and memory\"\"\"\n",
        "         self.conversation_history = [SystemMessage(content=self.system_prompt)]\n",
        "         self.execution_log.clear()\n",
        "-        print(\"Cleared conversation histories and execution log\")\n",
        "+\n",
        "+        # Clear memory state\n",
        "+        try:\n",
        "+            # Reset memory state\n",
        "+            initial_state = {\n",
        "+                \"messages\": [SystemMessage(content=self.system_prompt)],\n",
        "+                \"n_reflection\": 0,\n",
        "+                \"total_tokens_used\": 0\n",
        "+            }\n",
        "+            self.graph.update_state(self.config, initial_state, as_node=\"__start__\")\n",
        "+            print(\"Cleared conversation histories, execution log, and memory state\")\n",
        "+        except Exception as e:\n",
        "+            print(f\"Cleared local history, but could not clear memory state: {e}\")\n",
        "+\n",
        "+    def debug_conversation_flow(self):\n",
        "+        \"\"\"Debug conversation flow issues.\"\"\"\n",
        "+        try:\n",
        "+            state = self.graph.get_state(self.config)\n",
        "+            if state and state.values:\n",
        "+                messages = state.values.get(\"messages\", [])\n",
        "+                print(f\"\\n=== Conversation Flow Debug ===\")\n",
        "+                for i, msg in enumerate(messages):\n",
        "+                    msg_type = type(msg).__name__\n",
        "+                    has_tools = hasattr(msg, 'tool_calls') and msg.tool_calls\n",
        "+                    tool_info = f\" [HAS_TOOLS: {len(msg.tool_calls)}]\" if has_tools else \"\"\n",
        "+                    content_preview = str(msg.content)[:50] + \"...\" if len(str(msg.content)) > 50 else str(msg.content)\n",
        "+                    print(f\"{i+1:2d}. {msg_type:12}{tool_info}: {content_preview}\")\n",
        "+\n",
        "+                # Validate flow\n",
        "+                is_valid = validate_gemini_conversation(messages)\n",
        "+                print(f\"\\nConversation flow valid: {is_valid}\")\n",
        "+\n",
        "+                if not is_valid:\n",
        "+                    print(\"Suggested fix:\")\n",
        "+                    fixed_messages = fix_gemini_conversation_flow(messages)\n",
        "+                    for i, msg in enumerate(fixed_messages):\n",
        "+                        msg_type = type(msg).__name__\n",
        "+                        has_tools = hasattr(msg, 'tool_calls') and msg.tool_calls\n",
        "+                        tool_info = f\" [HAS_TOOLS: {len(msg.tool_calls)}]\" if has_tools else \"\"\n",
        "+                        content_preview = str(msg.content)[:50] + \"...\" if len(str(msg.content)) > 50 else str(msg.content)\n",
        "+                        print(f\"  {i+1:2d}. {msg_type:12}{tool_info}: {content_preview}\")\n",
        "+\n",
        "+                print(\"==============================\\n\")\n",
        "+        except Exception as e:\n",
        "+            print(f\"Error debugging conversation flow: {e}\")\n",
        "+\n",
        "+    def get_memory_stats(self):\n",
        "+        \"\"\"Get memory usage statistics.\"\"\"\n",
        "+        try:\n",
        "+            state = self.graph.get_state(self.config)\n",
        "+            if state and state.values:\n",
        "+                messages = state.values.get(\"messages\", [])\n",
        "+                memory_size = get_message_memory_size(messages)\n",
        "+                return {\n",
        "+                    \"message_count\": len(messages),\n",
        "+                    \"memory_size_chars\": memory_size,\n",
        "+                    \"execution_log_size\": len(self.execution_log),\n",
        "+                    \"memory_window_size\": self.memory_window_size\n",
        "+                }\n",
        "+        except Exception as e:\n",
        "+            print(f\"Could not get memory stats: {e}\")\n",
        "+        return {\"error\": \"Could not retrieve memory statistics\"}\n",
        " \n",
        "     def show_history(self):\n",
        "         \"\"\"Display conversation history.\"\"\"\n",
        "-        print(\"\\n=== Conversation History ===\")\n",
        "-        for i, msg in enumerate(self.conversation_history):\n",
        "-            role = getattr(msg, \"role\", msg.__class__.__name__)\n",
        "-            content_preview = str(msg.content)[:100] + \"...\" if len(str(msg.content)) > 100 else str(msg.content)\n",
        "-            print(f\"{i+1:2d}. [{role:12}]: {content_preview}\")\n",
        "-        print(\"============================\\n\")\n",
        "+        try:\n",
        "+            # Try to get from memory first\n",
        "+            state = self.graph.get_state(self.config)\n",
        "+            if state and state.values:\n",
        "+                messages = state.values.get(\"messages\", [])\n",
        "+            else:\n",
        "+                messages = self.conversation_history\n",
        "+\n",
        "+            print(\"\\n=== Conversation History ===\")\n",
        "+            for i, msg in enumerate(messages):\n",
        "+                role = getattr(msg, \"role\", msg.__class__.__name__)\n",
        "+                content_preview = str(msg.content)[:100] + \"...\" if len(str(msg.content)) > 100 else str(msg.content)\n",
        "+                print(f\"{i+1:2d}. [{role:12}]: {content_preview}\")\n",
        "+\n",
        "+            # Show memory stats\n",
        "+            stats = self.get_memory_stats()\n",
        "+            print(f\"Memory: {stats.get('message_count', 0)} messages, {stats.get('memory_size_chars', 0)} chars\")\n",
        "+            print(\"============================\\n\")\n",
        "+        except Exception as e:\n",
        "+            print(f\"Error showing history: {e}\")\n",
        " \n",
        "     def show_execution_log(self):\n",
        "         \"\"\"Display the execution log for debugging.\"\"\"\n",
        "@@ -624,6 +887,7 @@ def main():\n",
        "     print(\"  - 'history': show conversation history\")\n",
        "     print(\"  - 'log': show execution log\")\n",
        "     print(\"  - 'stats': show execution statistics\")\n",
        "+    print(\"  - 'memory': show memory usage statistics\")\n",
        " \n",
        "     try:\n",
        "         agent = EmacsAgent()\n",
        "@@ -643,6 +907,16 @@ def main():\n",
        "             elif query.lower() == \"stats\":\n",
        "                 agent.show_stats()\n",
        "                 continue\n",
        "+            elif query.lower() == \"memory\":\n",
        "+                stats = agent.get_memory_stats()\n",
        "+                print(f\"\\n=== Memory Statistics ===\")\n",
        "+                for key, value in stats.items():\n",
        "+                    print(f\"{key}: {value}\")\n",
        "+                print(\"========================\\n\")\n",
        "+                continue\n",
        "+            elif query.lower() == \"debug\":\n",
        "+                agent.debug_conversation_flow()\n",
        "+                continue\n",
        "             agent.run(query)\n",
        "     except ValueError as e:\n",
        "         print(f\"\\n\\033[1;31mInitialization Error: {e}\\033[0m\")\n"
      ]
    }
  ]
}
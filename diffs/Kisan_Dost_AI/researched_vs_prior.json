{
  "project": "Research Data/Kisan_Dost_AI",
  "repo": "TuberRishi/Kisan_Dost_AI",
  "prior_commit": "d4c2e4ce3a82de7a357034d0b09532ddfc56a4fe",
  "researched_commit": "55c162e2d02236f2d6a133c34f4ca3cb4a5d3824",
  "compare_url": "https://github.com/TuberRishi/Kisan_Dost_AI/compare/d4c2e4ce3a82de7a357034d0b09532ddfc56a4fe...55c162e2d02236f2d6a133c34f4ca3cb4a5d3824",
  "ahead_by": 1,
  "behind_by": 0,
  "changed_files": [
    {
      "path": "main.py",
      "status": "modified",
      "additions": 20,
      "deletions": 189,
      "patch": "@@ -1,38 +1,21 @@\n import os\n-import json\n-from typing import List, TypedDict, Literal\n+from typing import List, TypedDict\n from dotenv import load_dotenv\n-from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, ToolMessage\n-from langchain_core.prompts import ChatPromptTemplate, PromptTemplate, MessagesPlaceholder\n+from langchain_core.messages import BaseMessage, HumanMessage\n+from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n from langgraph.graph import StateGraph, END\n from langgraph.checkpoint.memory import MemorySaver\n-from fastapi import FastAPI, File, UploadFile, Form\n+from fastapi import FastAPI, Form\n from pydantic import BaseModel\n from fastapi.responses import HTMLResponse, FileResponse, JSONResponse\n-import soundfile as sf\n-import whisper\n import tempfile\n import traceback\n \n # --- DEFERRED IMPORT FOR LLM ---\n # from langchain_community.chat_models import ChatOllama\n \n # --- Import Tools and Prompts ---\n-from prompts import (\n-    AGRONOMIST_PERSONA, PROBLEM_SOLVER_PERSONA,\n-    PERSONA_ROUTER_PROMPT, QUERY_ROUTER_PROMPT,\n-    MEMORY_CONSOLIDATION_PROMPT\n-)\n-from tools.disease_diagnosis import get_image_diagnosis\n-from tools.knowledge_base import knowledge_base_search_tool\n-from tools.crop_recommender import get_crop_recommender_tool\n-from tools.soil_health_advisor import get_soil_health_advisor_tool\n-from tools.fertilizer_recommender import get_fertilizer_recommender_tool\n-from tools.long_term_memory import retrieve_relevant_memories_tool # Will be filtered out\n-from tools.ticket_system import create_support_ticket\n import tickets\n-from translation import detect_language, translate_text, enrich_query\n-\n # --- Load Environment Variables ---\n load_dotenv()\n \n@@ -42,8 +25,7 @@\n \n def get_agent_graph():\n     \"\"\"\n-    Creates and returns the agent graph, initializing the LLM on the first call.\n-    This uses a lazy-loading pattern to speed up server startup.\n+    Creates and returns a simplified agent graph.\n     \"\"\"\n     global llm, agent_graph\n     if agent_graph:\n@@ -55,148 +37,36 @@ def get_agent_graph():\n     llm = ChatOllama(model=\"qwen3:4b\", temperature=0)\n     print(\"Ollama LLM initialized.\", flush=True)\n \n-    # --- Tool Integration ---\n-    # Filter out disabled tools (like the memory tool)\n-    all_tools = [\n-        t for t in [\n-            # retrieve_relevant_memories_tool, # DISABLED\n-            knowledge_base_search_tool,\n-            get_crop_recommender_tool(llm),\n-            get_soil_health_advisor_tool(llm),\n-            get_fertilizer_recommender_tool(llm),\n-            create_support_ticket,\n-        ] if t is not None\n-    ]\n-\n     # --- Agent State Definition ---\n     class AgentState(TypedDict):\n         messages: List[BaseMessage]\n-        persona: str\n         user_id: str\n-        # The route will determine which path to take\n-        query_route: str\n-\n-    # --- Graph Node Definitions ---\n-\n-    def route_query_node(state: AgentState):\n-        \"\"\"Routes the query to either the general conversational path or the specific agricultural path.\"\"\"\n-        print(\"--- 0. ROUTING QUERY ---\", flush=True)\n-        user_query = state[\"messages\"][-1].content\n-        prompt = ChatPromptTemplate.from_template(QUERY_ROUTER_PROMPT)\n-        chain = prompt | llm\n-        route = chain.invoke({\"query\": user_query}).content.strip()\n-        print(f\"Query route selected: {route}\", flush=True)\n-        return {\"query_route\": route}\n-\n-    def general_response_node(state: AgentState):\n-        \"\"\"Generates a direct, conversational response for general queries.\"\"\"\n-        print(\"--- GENERAL RESPONSE ---\", flush=True)\n-        user_query = state[\"messages\"][-1].content\n-        # This prompt now establishes the correct identity and has the strict no-thinking rule.\n-        system_prompt = \"\"\"\\\n-You are 'Kisan Dost', a friendly and helpful AI assistant for farmers.\n-CRITICAL RULE: You MUST NOT include your reasoning, internal monologue, or thoughts in your final response. Only provide the clean, direct answer to the user. Do not use tags like <think> or <thought>.\\\n-\"\"\"\n-        prompt = ChatPromptTemplate.from_messages([\n-            (\"system\", system_prompt),\n-            (\"human\", \"{query}\")\n-        ])\n-        chain = prompt | llm\n-        response_message = chain.invoke({\"query\": user_query})\n-        return {\"messages\": [response_message]}\n \n-    def determine_persona_node(state: AgentState):\n-        print(\"--- 1. DETERMINING PERSONA ---\", flush=True)\n+    # --- Graph Node Definition ---\n+    def chatbot_node(state: AgentState):\n+        \"\"\"Generates a direct, conversational response.\"\"\"\n+        print(\"--- SIMPLE CHATBOT ---\", flush=True)\n         user_query = state[\"messages\"][-1].content\n-        prompt = ChatPromptTemplate.from_template(PERSONA_ROUTER_PROMPT)\n-        chain = prompt | llm\n-        persona = chain.invoke({\"query\": user_query}).content.strip()\n-        if \"Agronomist\" not in persona and \"Problem-Solver\" not in persona:\n-            persona = \"Problem-Solver\"\n-        print(f\"Persona Selected: {persona}\", flush=True)\n-        return {\"persona\": persona}\n-\n-    def agent_node(state: AgentState):\n-        print(f\"--- 2. AGENT STEP (Persona: {state['persona']}) ---\", flush=True)\n-        system_prompt = AGRONOMIST_PERSONA if state[\"persona\"] == \"Agronomist\" else PROBLEM_SOLVER_PERSONA\n-\n+        system_prompt = \"You are 'Kisan Dost', a friendly and helpful AI assistant for farmers.\"\n         prompt = ChatPromptTemplate.from_messages([\n             (\"system\", system_prompt),\n             MessagesPlaceholder(variable_name=\"messages\"),\n         ])\n-\n-        llm_with_tools = llm.bind_tools(all_tools)\n-        chain = prompt | llm_with_tools\n+        chain = prompt | llm\n         response_message = chain.invoke({\"messages\": state['messages']})\n-\n-        print(f\"Agent Response: {response_message}\", flush=True)\n         return {\"messages\": [response_message]}\n \n-    def tool_node(state: AgentState):\n-        print(\"--- 3. TOOL EXECUTION ---\", flush=True)\n-        tool_calls = state[\"messages\"][-1].tool_calls\n-        tool_messages = []\n-        for tool_call in tool_calls:\n-            selected_tool = next((t for t in all_tools if t.name == tool_call[\"name\"]), None)\n-            if not selected_tool:\n-                raise ValueError(f\"Tool '{tool_call['name']}' not found.\")\n-            result = selected_tool.invoke(tool_call[\"args\"])\n-            tool_messages.append(ToolMessage(content=str(result), tool_call_id=tool_call[\"id\"]))\n-        return {\"messages\": tool_messages}\n-\n-    def should_continue_agent(state: AgentState) -> Literal[\"tools\", \"__end__\"]:\n-        print(\"--- 4. ROUTING (AGENT) ---\", flush=True)\n-        if state[\"messages\"][-1].tool_calls:\n-            print(\"Decision: Use tool.\", flush=True)\n-            return \"tools\"\n-        print(\"Decision: End conversation.\", flush=True)\n-        return \"__end__\"\n-\n-    def should_route_to_specialist(state: AgentState) -> Literal[\"specialist_path\", \"general_path\"]:\n-        \"\"\"Decides whether to use the specialist agent or the general response node.\"\"\"\n-        if state.get(\"query_route\") == \"specific_agricultural\":\n-            return \"specialist_path\"\n-        return \"general_path\"\n-\n     # --- Graph Construction ---\n     graph_builder = StateGraph(AgentState)\n-    graph_builder.add_node(\"route_query\", route_query_node)\n-    graph_builder.add_node(\"general_response\", general_response_node)\n-    graph_builder.add_node(\"determine_persona\", determine_persona_node)\n-    graph_builder.add_node(\"agent\", agent_node)\n-    graph_builder.add_node(\"tools\", tool_node)\n-\n-    # New Entry Point\n-    graph_builder.set_entry_point(\"route_query\")\n-\n-    # Conditional Routing\n-    graph_builder.add_conditional_edges(\n-        \"route_query\",\n-        should_route_to_specialist,\n-        {\n-            \"specialist_path\": \"determine_persona\",\n-            \"general_path\": \"general_response\",\n-        }\n-    )\n-\n-    # Edges for the specialist path\n-    graph_builder.add_edge(\"determine_persona\", \"agent\")\n-    graph_builder.add_conditional_edges(\"agent\", should_continue_agent, {\"tools\": \"tools\", \"__end__\": END})\n-    graph_builder.add_edge(\"tools\", \"agent\")\n-\n-    # Edge for the general path\n-    graph_builder.add_edge(\"general_response\", END)\n+    graph_builder.add_node(\"chatbot\", chatbot_node)\n+    graph_builder.set_entry_point(\"chatbot\")\n+    graph_builder.add_edge(\"chatbot\", END)\n \n     memory = MemorySaver()\n     agent_graph = graph_builder.compile(checkpointer=memory)\n     print(\"Agent graph compiled.\", flush=True)\n     return agent_graph\n \n-# --- Memory Consolidation ---\n-def consolidate_memories(user_id: str, conversation_history: List[BaseMessage]):\n-    # DISABLED as per user request\n-    return\n-\n # --- FastAPI Application ---\n app = FastAPI(title=\"Kisan Dost AI API\", description=\"API for the AI-Powered Personal Farming Assistant\")\n \n@@ -205,67 +75,28 @@ async def startup_event():\n     get_agent_graph()\n \n @app.post(\"/chat\")\n-async def chat(session_id: str = Form(...), query: str = Form(\"\"), file: UploadFile = File(None)):\n+async def chat(session_id: str = Form(...), query: str = Form(...)):\n     try:\n         user_id = session_id\n         config = {\"configurable\": {\"thread_id\": user_id}}\n-        augmented_query = query\n-\n-        if file:\n-            print(f\"--- Received image file: {file.filename} ---\")\n-            image_bytes = await file.read()\n-            diagnosis_result = get_image_diagnosis(image_bytes)\n-\n-            if \"UNCLEAR\" in diagnosis_result:\n-                return JSONResponse(status_code=400, content={\"error\": \"It seems that the image is not a plant or is unclear. Please send a clearer plant image.\"})\n-            if \"Error:\" in diagnosis_result:\n-                return JSONResponse(status_code=500, content={\"error\": diagnosis_result})\n-\n-            image_analysis_prompt = f\"The user has uploaded an image. Here is the analysis:\\n\\n{diagnosis_result}\"\n-            augmented_query = f\"{query}\\n\\n{image_analysis_prompt}\" if query else image_analysis_prompt\n-\n-        if not augmented_query:\n-            return JSONResponse(status_code=400, content={\"error\": \"Please provide a query or an image.\"})\n \n-        original_lang = detect_language(augmented_query)\n-        print(f\"Detected language: {original_lang}\", flush=True)\n+        if not query:\n+            return JSONResponse(status_code=400, content={\"error\": \"Please provide a query.\"})\n \n-        lang_map = {\"ml\": \"Malayalam\"}\n-        source_lang_name = lang_map.get(original_lang, \"English\")\n-\n-        if original_lang != 'en':\n-            print(\"Translating to English...\", flush=True)\n-            processed_query = translate_text(llm, augmented_query, target_language=\"English\", source_language=source_lang_name)\n-        else:\n-            processed_query = augmented_query\n-\n-        # Enrichment is now part of the specialist agent path, not done upfront.\n-        user_message = HumanMessage(content=processed_query.strip())\n+        user_message = HumanMessage(content=query.strip())\n         print(\"Invoking agent graph...\", flush=True)\n \n         graph = get_agent_graph()\n         # The initial state requires all keys to be present.\n         initial_state = {\n             \"messages\": [user_message],\n-            \"user_id\": user_id,\n-            \"persona\": \"\",\n-            \"query_route\": \"\"\n+            \"user_id\": user_id\n         }\n         response = graph.invoke(initial_state, config=config)\n         print(\"Agent graph finished.\", flush=True)\n \n         final_history = response.get(\"messages\", [])\n-        if final_history:\n-            # Memory consolidation is disabled.\n-            pass\n-\n-        english_response = final_history[-1].content\n-        final_response = english_response\n-\n-        if original_lang != 'en':\n-            print(f\"Translating final answer back to '{original_lang}'...\", flush=True)\n-            target_lang_name = lang_map.get(original_lang, original_lang)\n-            final_response = translate_text(llm, english_response, target_language=target_lang_name, source_language=\"English\")\n+        final_response = final_history[-1].content if final_history else \"\"\n \n         return {\"response\": final_response}\n ",
      "patch_lines": [
        "@@ -1,38 +1,21 @@\n",
        " import os\n",
        "-import json\n",
        "-from typing import List, TypedDict, Literal\n",
        "+from typing import List, TypedDict\n",
        " from dotenv import load_dotenv\n",
        "-from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, ToolMessage\n",
        "-from langchain_core.prompts import ChatPromptTemplate, PromptTemplate, MessagesPlaceholder\n",
        "+from langchain_core.messages import BaseMessage, HumanMessage\n",
        "+from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        " from langgraph.graph import StateGraph, END\n",
        " from langgraph.checkpoint.memory import MemorySaver\n",
        "-from fastapi import FastAPI, File, UploadFile, Form\n",
        "+from fastapi import FastAPI, Form\n",
        " from pydantic import BaseModel\n",
        " from fastapi.responses import HTMLResponse, FileResponse, JSONResponse\n",
        "-import soundfile as sf\n",
        "-import whisper\n",
        " import tempfile\n",
        " import traceback\n",
        " \n",
        " # --- DEFERRED IMPORT FOR LLM ---\n",
        " # from langchain_community.chat_models import ChatOllama\n",
        " \n",
        " # --- Import Tools and Prompts ---\n",
        "-from prompts import (\n",
        "-    AGRONOMIST_PERSONA, PROBLEM_SOLVER_PERSONA,\n",
        "-    PERSONA_ROUTER_PROMPT, QUERY_ROUTER_PROMPT,\n",
        "-    MEMORY_CONSOLIDATION_PROMPT\n",
        "-)\n",
        "-from tools.disease_diagnosis import get_image_diagnosis\n",
        "-from tools.knowledge_base import knowledge_base_search_tool\n",
        "-from tools.crop_recommender import get_crop_recommender_tool\n",
        "-from tools.soil_health_advisor import get_soil_health_advisor_tool\n",
        "-from tools.fertilizer_recommender import get_fertilizer_recommender_tool\n",
        "-from tools.long_term_memory import retrieve_relevant_memories_tool # Will be filtered out\n",
        "-from tools.ticket_system import create_support_ticket\n",
        " import tickets\n",
        "-from translation import detect_language, translate_text, enrich_query\n",
        "-\n",
        " # --- Load Environment Variables ---\n",
        " load_dotenv()\n",
        " \n",
        "@@ -42,8 +25,7 @@\n",
        " \n",
        " def get_agent_graph():\n",
        "     \"\"\"\n",
        "-    Creates and returns the agent graph, initializing the LLM on the first call.\n",
        "-    This uses a lazy-loading pattern to speed up server startup.\n",
        "+    Creates and returns a simplified agent graph.\n",
        "     \"\"\"\n",
        "     global llm, agent_graph\n",
        "     if agent_graph:\n",
        "@@ -55,148 +37,36 @@ def get_agent_graph():\n",
        "     llm = ChatOllama(model=\"qwen3:4b\", temperature=0)\n",
        "     print(\"Ollama LLM initialized.\", flush=True)\n",
        " \n",
        "-    # --- Tool Integration ---\n",
        "-    # Filter out disabled tools (like the memory tool)\n",
        "-    all_tools = [\n",
        "-        t for t in [\n",
        "-            # retrieve_relevant_memories_tool, # DISABLED\n",
        "-            knowledge_base_search_tool,\n",
        "-            get_crop_recommender_tool(llm),\n",
        "-            get_soil_health_advisor_tool(llm),\n",
        "-            get_fertilizer_recommender_tool(llm),\n",
        "-            create_support_ticket,\n",
        "-        ] if t is not None\n",
        "-    ]\n",
        "-\n",
        "     # --- Agent State Definition ---\n",
        "     class AgentState(TypedDict):\n",
        "         messages: List[BaseMessage]\n",
        "-        persona: str\n",
        "         user_id: str\n",
        "-        # The route will determine which path to take\n",
        "-        query_route: str\n",
        "-\n",
        "-    # --- Graph Node Definitions ---\n",
        "-\n",
        "-    def route_query_node(state: AgentState):\n",
        "-        \"\"\"Routes the query to either the general conversational path or the specific agricultural path.\"\"\"\n",
        "-        print(\"--- 0. ROUTING QUERY ---\", flush=True)\n",
        "-        user_query = state[\"messages\"][-1].content\n",
        "-        prompt = ChatPromptTemplate.from_template(QUERY_ROUTER_PROMPT)\n",
        "-        chain = prompt | llm\n",
        "-        route = chain.invoke({\"query\": user_query}).content.strip()\n",
        "-        print(f\"Query route selected: {route}\", flush=True)\n",
        "-        return {\"query_route\": route}\n",
        "-\n",
        "-    def general_response_node(state: AgentState):\n",
        "-        \"\"\"Generates a direct, conversational response for general queries.\"\"\"\n",
        "-        print(\"--- GENERAL RESPONSE ---\", flush=True)\n",
        "-        user_query = state[\"messages\"][-1].content\n",
        "-        # This prompt now establishes the correct identity and has the strict no-thinking rule.\n",
        "-        system_prompt = \"\"\"\\\n",
        "-You are 'Kisan Dost', a friendly and helpful AI assistant for farmers.\n",
        "-CRITICAL RULE: You MUST NOT include your reasoning, internal monologue, or thoughts in your final response. Only provide the clean, direct answer to the user. Do not use tags like <think> or <thought>.\\\n",
        "-\"\"\"\n",
        "-        prompt = ChatPromptTemplate.from_messages([\n",
        "-            (\"system\", system_prompt),\n",
        "-            (\"human\", \"{query}\")\n",
        "-        ])\n",
        "-        chain = prompt | llm\n",
        "-        response_message = chain.invoke({\"query\": user_query})\n",
        "-        return {\"messages\": [response_message]}\n",
        " \n",
        "-    def determine_persona_node(state: AgentState):\n",
        "-        print(\"--- 1. DETERMINING PERSONA ---\", flush=True)\n",
        "+    # --- Graph Node Definition ---\n",
        "+    def chatbot_node(state: AgentState):\n",
        "+        \"\"\"Generates a direct, conversational response.\"\"\"\n",
        "+        print(\"--- SIMPLE CHATBOT ---\", flush=True)\n",
        "         user_query = state[\"messages\"][-1].content\n",
        "-        prompt = ChatPromptTemplate.from_template(PERSONA_ROUTER_PROMPT)\n",
        "-        chain = prompt | llm\n",
        "-        persona = chain.invoke({\"query\": user_query}).content.strip()\n",
        "-        if \"Agronomist\" not in persona and \"Problem-Solver\" not in persona:\n",
        "-            persona = \"Problem-Solver\"\n",
        "-        print(f\"Persona Selected: {persona}\", flush=True)\n",
        "-        return {\"persona\": persona}\n",
        "-\n",
        "-    def agent_node(state: AgentState):\n",
        "-        print(f\"--- 2. AGENT STEP (Persona: {state['persona']}) ---\", flush=True)\n",
        "-        system_prompt = AGRONOMIST_PERSONA if state[\"persona\"] == \"Agronomist\" else PROBLEM_SOLVER_PERSONA\n",
        "-\n",
        "+        system_prompt = \"You are 'Kisan Dost', a friendly and helpful AI assistant for farmers.\"\n",
        "         prompt = ChatPromptTemplate.from_messages([\n",
        "             (\"system\", system_prompt),\n",
        "             MessagesPlaceholder(variable_name=\"messages\"),\n",
        "         ])\n",
        "-\n",
        "-        llm_with_tools = llm.bind_tools(all_tools)\n",
        "-        chain = prompt | llm_with_tools\n",
        "+        chain = prompt | llm\n",
        "         response_message = chain.invoke({\"messages\": state['messages']})\n",
        "-\n",
        "-        print(f\"Agent Response: {response_message}\", flush=True)\n",
        "         return {\"messages\": [response_message]}\n",
        " \n",
        "-    def tool_node(state: AgentState):\n",
        "-        print(\"--- 3. TOOL EXECUTION ---\", flush=True)\n",
        "-        tool_calls = state[\"messages\"][-1].tool_calls\n",
        "-        tool_messages = []\n",
        "-        for tool_call in tool_calls:\n",
        "-            selected_tool = next((t for t in all_tools if t.name == tool_call[\"name\"]), None)\n",
        "-            if not selected_tool:\n",
        "-                raise ValueError(f\"Tool '{tool_call['name']}' not found.\")\n",
        "-            result = selected_tool.invoke(tool_call[\"args\"])\n",
        "-            tool_messages.append(ToolMessage(content=str(result), tool_call_id=tool_call[\"id\"]))\n",
        "-        return {\"messages\": tool_messages}\n",
        "-\n",
        "-    def should_continue_agent(state: AgentState) -> Literal[\"tools\", \"__end__\"]:\n",
        "-        print(\"--- 4. ROUTING (AGENT) ---\", flush=True)\n",
        "-        if state[\"messages\"][-1].tool_calls:\n",
        "-            print(\"Decision: Use tool.\", flush=True)\n",
        "-            return \"tools\"\n",
        "-        print(\"Decision: End conversation.\", flush=True)\n",
        "-        return \"__end__\"\n",
        "-\n",
        "-    def should_route_to_specialist(state: AgentState) -> Literal[\"specialist_path\", \"general_path\"]:\n",
        "-        \"\"\"Decides whether to use the specialist agent or the general response node.\"\"\"\n",
        "-        if state.get(\"query_route\") == \"specific_agricultural\":\n",
        "-            return \"specialist_path\"\n",
        "-        return \"general_path\"\n",
        "-\n",
        "     # --- Graph Construction ---\n",
        "     graph_builder = StateGraph(AgentState)\n",
        "-    graph_builder.add_node(\"route_query\", route_query_node)\n",
        "-    graph_builder.add_node(\"general_response\", general_response_node)\n",
        "-    graph_builder.add_node(\"determine_persona\", determine_persona_node)\n",
        "-    graph_builder.add_node(\"agent\", agent_node)\n",
        "-    graph_builder.add_node(\"tools\", tool_node)\n",
        "-\n",
        "-    # New Entry Point\n",
        "-    graph_builder.set_entry_point(\"route_query\")\n",
        "-\n",
        "-    # Conditional Routing\n",
        "-    graph_builder.add_conditional_edges(\n",
        "-        \"route_query\",\n",
        "-        should_route_to_specialist,\n",
        "-        {\n",
        "-            \"specialist_path\": \"determine_persona\",\n",
        "-            \"general_path\": \"general_response\",\n",
        "-        }\n",
        "-    )\n",
        "-\n",
        "-    # Edges for the specialist path\n",
        "-    graph_builder.add_edge(\"determine_persona\", \"agent\")\n",
        "-    graph_builder.add_conditional_edges(\"agent\", should_continue_agent, {\"tools\": \"tools\", \"__end__\": END})\n",
        "-    graph_builder.add_edge(\"tools\", \"agent\")\n",
        "-\n",
        "-    # Edge for the general path\n",
        "-    graph_builder.add_edge(\"general_response\", END)\n",
        "+    graph_builder.add_node(\"chatbot\", chatbot_node)\n",
        "+    graph_builder.set_entry_point(\"chatbot\")\n",
        "+    graph_builder.add_edge(\"chatbot\", END)\n",
        " \n",
        "     memory = MemorySaver()\n",
        "     agent_graph = graph_builder.compile(checkpointer=memory)\n",
        "     print(\"Agent graph compiled.\", flush=True)\n",
        "     return agent_graph\n",
        " \n",
        "-# --- Memory Consolidation ---\n",
        "-def consolidate_memories(user_id: str, conversation_history: List[BaseMessage]):\n",
        "-    # DISABLED as per user request\n",
        "-    return\n",
        "-\n",
        " # --- FastAPI Application ---\n",
        " app = FastAPI(title=\"Kisan Dost AI API\", description=\"API for the AI-Powered Personal Farming Assistant\")\n",
        " \n",
        "@@ -205,67 +75,28 @@ async def startup_event():\n",
        "     get_agent_graph()\n",
        " \n",
        " @app.post(\"/chat\")\n",
        "-async def chat(session_id: str = Form(...), query: str = Form(\"\"), file: UploadFile = File(None)):\n",
        "+async def chat(session_id: str = Form(...), query: str = Form(...)):\n",
        "     try:\n",
        "         user_id = session_id\n",
        "         config = {\"configurable\": {\"thread_id\": user_id}}\n",
        "-        augmented_query = query\n",
        "-\n",
        "-        if file:\n",
        "-            print(f\"--- Received image file: {file.filename} ---\")\n",
        "-            image_bytes = await file.read()\n",
        "-            diagnosis_result = get_image_diagnosis(image_bytes)\n",
        "-\n",
        "-            if \"UNCLEAR\" in diagnosis_result:\n",
        "-                return JSONResponse(status_code=400, content={\"error\": \"It seems that the image is not a plant or is unclear. Please send a clearer plant image.\"})\n",
        "-            if \"Error:\" in diagnosis_result:\n",
        "-                return JSONResponse(status_code=500, content={\"error\": diagnosis_result})\n",
        "-\n",
        "-            image_analysis_prompt = f\"The user has uploaded an image. Here is the analysis:\\n\\n{diagnosis_result}\"\n",
        "-            augmented_query = f\"{query}\\n\\n{image_analysis_prompt}\" if query else image_analysis_prompt\n",
        "-\n",
        "-        if not augmented_query:\n",
        "-            return JSONResponse(status_code=400, content={\"error\": \"Please provide a query or an image.\"})\n",
        " \n",
        "-        original_lang = detect_language(augmented_query)\n",
        "-        print(f\"Detected language: {original_lang}\", flush=True)\n",
        "+        if not query:\n",
        "+            return JSONResponse(status_code=400, content={\"error\": \"Please provide a query.\"})\n",
        " \n",
        "-        lang_map = {\"ml\": \"Malayalam\"}\n",
        "-        source_lang_name = lang_map.get(original_lang, \"English\")\n",
        "-\n",
        "-        if original_lang != 'en':\n",
        "-            print(\"Translating to English...\", flush=True)\n",
        "-            processed_query = translate_text(llm, augmented_query, target_language=\"English\", source_language=source_lang_name)\n",
        "-        else:\n",
        "-            processed_query = augmented_query\n",
        "-\n",
        "-        # Enrichment is now part of the specialist agent path, not done upfront.\n",
        "-        user_message = HumanMessage(content=processed_query.strip())\n",
        "+        user_message = HumanMessage(content=query.strip())\n",
        "         print(\"Invoking agent graph...\", flush=True)\n",
        " \n",
        "         graph = get_agent_graph()\n",
        "         # The initial state requires all keys to be present.\n",
        "         initial_state = {\n",
        "             \"messages\": [user_message],\n",
        "-            \"user_id\": user_id,\n",
        "-            \"persona\": \"\",\n",
        "-            \"query_route\": \"\"\n",
        "+            \"user_id\": user_id\n",
        "         }\n",
        "         response = graph.invoke(initial_state, config=config)\n",
        "         print(\"Agent graph finished.\", flush=True)\n",
        " \n",
        "         final_history = response.get(\"messages\", [])\n",
        "-        if final_history:\n",
        "-            # Memory consolidation is disabled.\n",
        "-            pass\n",
        "-\n",
        "-        english_response = final_history[-1].content\n",
        "-        final_response = english_response\n",
        "-\n",
        "-        if original_lang != 'en':\n",
        "-            print(f\"Translating final answer back to '{original_lang}'...\", flush=True)\n",
        "-            target_lang_name = lang_map.get(original_lang, original_lang)\n",
        "-            final_response = translate_text(llm, english_response, target_language=target_lang_name, source_language=\"English\")\n",
        "+        final_response = final_history[-1].content if final_history else \"\"\n",
        " \n",
        "         return {\"response\": final_response}\n",
        " \n"
      ]
    }
  ]
}
{
  "project": "Research Data/ecommerce-chat",
  "repo": "arthurmorais12/ecommerce-chat",
  "prior_commit": "bc7575e0392e83d44472161298df08c9baf08b83",
  "researched_commit": "3e924091a8e3713a712fb8bcc187696d02c5b3fb",
  "compare_url": "https://github.com/arthurmorais12/ecommerce-chat/compare/bc7575e0392e83d44472161298df08c9baf08b83...3e924091a8e3713a712fb8bcc187696d02c5b3fb",
  "ahead_by": 1,
  "behind_by": 0,
  "changed_files": [
    {
      "path": ".gitignore",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "patch": "@@ -208,4 +208,5 @@ __marimo__/\n \n .cursor/\n emb/\n-chat_memory.db\n\\ No newline at end of file\n+chat_memory.db\n+run_test_cli.py\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -208,4 +208,5 @@ __marimo__/\n",
        " \n",
        " .cursor/\n",
        " emb/\n",
        "-chat_memory.db\n",
        "\\ No newline at end of file\n",
        "+chat_memory.db\n",
        "+run_test_cli.py\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "backend/api/core/deps.py",
      "status": "added",
      "additions": 11,
      "deletions": 0,
      "patch": "@@ -0,0 +1,11 @@\n+from fastapi import HTTPException, Request\n+from services import Agent\n+\n+\n+def get_agent(request: Request) -> Agent:\n+    if not hasattr(request.app.state, \"agent\"):\n+        raise HTTPException(\n+            status_code=500,\n+            detail=\"Agent not initialized. Lifespan event handler might have failed.\",\n+        )\n+    return request.app.state.agent",
      "patch_lines": [
        "@@ -0,0 +1,11 @@\n",
        "+from fastapi import HTTPException, Request\n",
        "+from services import Agent\n",
        "+\n",
        "+\n",
        "+def get_agent(request: Request) -> Agent:\n",
        "+    if not hasattr(request.app.state, \"agent\"):\n",
        "+        raise HTTPException(\n",
        "+            status_code=500,\n",
        "+            detail=\"Agent not initialized. Lifespan event handler might have failed.\",\n",
        "+        )\n",
        "+    return request.app.state.agent\n"
      ]
    },
    {
      "path": "backend/api/endpoints/chat.py",
      "status": "added",
      "additions": 43,
      "deletions": 0,
      "patch": "@@ -0,0 +1,43 @@\n+import json\n+\n+from fastapi import APIRouter, Depends, HTTPException\n+from fastapi.responses import StreamingResponse\n+from services import Agent\n+\n+from api.core.deps import get_agent\n+from api.schemas.chat import ChatRequest\n+\n+router = APIRouter()\n+\n+\n+@router.post(\"/\")\n+async def chat(request: ChatRequest, agent: Agent = Depends(get_agent)):\n+    message = request.message.strip()\n+\n+    if not message:\n+        raise HTTPException(status_code=400, detail=\"Message is required\")\n+\n+    thread_id = request.user_id\n+\n+    async def event_stream():\n+        yield f\"data: {json.dumps({\"type\": \"start\"})}\\n\\n\"\n+\n+        try:\n+            async for chunk in agent.stream_message(message, thread_id):\n+                yield f\"data: {json.dumps(chunk)}\\n\\n\"\n+        except Exception as e:\n+            print(f\"Stream error: {e}\")\n+            error_response = {\"type\": \"error\", \"data\": f\"An error occurred: {str(e)}\"}\n+            yield f\"data: {json.dumps(error_response)}\\n\\n\"\n+        finally:\n+            yield f\"data: {json.dumps({\"type\": \"end\"})}\\n\\n\"\n+\n+    return StreamingResponse(\n+        event_stream(),\n+        media_type=\"text/event-stream\",\n+        headers={\n+            \"Cache-Control\": \"no-cache\",\n+            \"Connection\": \"keep-alive\",\n+            \"X-Accel-Buffering\": \"no\",\n+        },\n+    )",
      "patch_lines": [
        "@@ -0,0 +1,43 @@\n",
        "+import json\n",
        "+\n",
        "+from fastapi import APIRouter, Depends, HTTPException\n",
        "+from fastapi.responses import StreamingResponse\n",
        "+from services import Agent\n",
        "+\n",
        "+from api.core.deps import get_agent\n",
        "+from api.schemas.chat import ChatRequest\n",
        "+\n",
        "+router = APIRouter()\n",
        "+\n",
        "+\n",
        "+@router.post(\"/\")\n",
        "+async def chat(request: ChatRequest, agent: Agent = Depends(get_agent)):\n",
        "+    message = request.message.strip()\n",
        "+\n",
        "+    if not message:\n",
        "+        raise HTTPException(status_code=400, detail=\"Message is required\")\n",
        "+\n",
        "+    thread_id = request.user_id\n",
        "+\n",
        "+    async def event_stream():\n",
        "+        yield f\"data: {json.dumps({\"type\": \"start\"})}\\n\\n\"\n",
        "+\n",
        "+        try:\n",
        "+            async for chunk in agent.stream_message(message, thread_id):\n",
        "+                yield f\"data: {json.dumps(chunk)}\\n\\n\"\n",
        "+        except Exception as e:\n",
        "+            print(f\"Stream error: {e}\")\n",
        "+            error_response = {\"type\": \"error\", \"data\": f\"An error occurred: {str(e)}\"}\n",
        "+            yield f\"data: {json.dumps(error_response)}\\n\\n\"\n",
        "+        finally:\n",
        "+            yield f\"data: {json.dumps({\"type\": \"end\"})}\\n\\n\"\n",
        "+\n",
        "+    return StreamingResponse(\n",
        "+        event_stream(),\n",
        "+        media_type=\"text/event-stream\",\n",
        "+        headers={\n",
        "+            \"Cache-Control\": \"no-cache\",\n",
        "+            \"Connection\": \"keep-alive\",\n",
        "+            \"X-Accel-Buffering\": \"no\",\n",
        "+        },\n",
        "+    )\n"
      ]
    },
    {
      "path": "backend/api/schemas/chat.py",
      "status": "modified",
      "additions": 1,
      "deletions": 0,
      "patch": "@@ -3,6 +3,7 @@\n \n class ChatRequest(BaseModel):\n     message: str\n+    user_id: str\n \n     class Config:\n         from_attributes = True",
      "patch_lines": [
        "@@ -3,6 +3,7 @@\n",
        " \n",
        " class ChatRequest(BaseModel):\n",
        "     message: str\n",
        "+    user_id: str\n",
        " \n",
        "     class Config:\n",
        "         from_attributes = True\n"
      ]
    },
    {
      "path": "backend/main.py",
      "status": "modified",
      "additions": 7,
      "deletions": 0,
      "patch": "@@ -2,10 +2,12 @@\n from contextlib import asynccontextmanager\n from pathlib import Path\n \n+from api.endpoints import chat\n from api.schemas.health import HealthResponse\n from chroma.client import chroma_client\n from fastapi import FastAPI\n from fastapi.middleware.cors import CORSMiddleware\n+from services import Agent\n from services.ingest import load_pdf_documents\n \n # Configurar logging\n@@ -39,6 +41,9 @@ async def lifespan(app: FastAPI):\n         logger.error(f\"\u274c Erro ao carregar documentos: {e}\")\n         # N\u00e3o falhar a inicializa\u00e7\u00e3o, apenas logar o erro\n \n+    agent_instance = Agent()\n+    app.state.agent = agent_instance\n+\n     logger.info(\"\ud83c\udf89 API inicializada com sucesso!\")\n \n     yield  # API est\u00e1 rodando\n@@ -63,6 +68,8 @@ async def lifespan(app: FastAPI):\n     allow_headers=[\"*\"],\n )\n \n+app.include_router(chat.router, prefix=\"/chat\", tags=[\"Chat\"])\n+\n \n @app.get(\"/\")\n async def root():",
      "patch_lines": [
        "@@ -2,10 +2,12 @@\n",
        " from contextlib import asynccontextmanager\n",
        " from pathlib import Path\n",
        " \n",
        "+from api.endpoints import chat\n",
        " from api.schemas.health import HealthResponse\n",
        " from chroma.client import chroma_client\n",
        " from fastapi import FastAPI\n",
        " from fastapi.middleware.cors import CORSMiddleware\n",
        "+from services import Agent\n",
        " from services.ingest import load_pdf_documents\n",
        " \n",
        " # Configurar logging\n",
        "@@ -39,6 +41,9 @@ async def lifespan(app: FastAPI):\n",
        "         logger.error(f\"\u274c Erro ao carregar documentos: {e}\")\n",
        "         # N\u00e3o falhar a inicializa\u00e7\u00e3o, apenas logar o erro\n",
        " \n",
        "+    agent_instance = Agent()\n",
        "+    app.state.agent = agent_instance\n",
        "+\n",
        "     logger.info(\"\ud83c\udf89 API inicializada com sucesso!\")\n",
        " \n",
        "     yield  # API est\u00e1 rodando\n",
        "@@ -63,6 +68,8 @@ async def lifespan(app: FastAPI):\n",
        "     allow_headers=[\"*\"],\n",
        " )\n",
        " \n",
        "+app.include_router(chat.router, prefix=\"/chat\", tags=[\"Chat\"])\n",
        "+\n",
        " \n",
        " @app.get(\"/\")\n",
        " async def root():\n"
      ]
    },
    {
      "path": "backend/services/agent/agent.py",
      "status": "modified",
      "additions": 10,
      "deletions": 46,
      "patch": "@@ -1,7 +1,6 @@\n-import uuid\n-\n from dotenv import load_dotenv\n from langchain.chat_models import init_chat_model\n+from langchain_core.messages import AIMessageChunk\n from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n from langgraph.prebuilt import create_react_agent\n \n@@ -12,67 +11,32 @@\n \n \n class Agent:\n-    def __init__(self, thread_id: str = None):\n+    def __init__(self):\n         self.model = init_chat_model(\n             model=\"gemini-2.5-flash-lite-preview-06-17\", model_provider=\"google_genai\"\n         )\n-        self.db_path = \"chat_memory.db\"\n         self.tools = tools\n         self.prompt = VENDEDOR_PROMPT\n-        self.thread_id = thread_id or str(uuid.uuid4())\n-\n-    async def send_message(self, user_message: str) -> str:\n-        \"\"\"Envia mensagem e retorna resposta completa\"\"\"\n-        async with AsyncSqliteSaver.from_conn_string(self.db_path) as checkpointer:\n-            agent = create_react_agent(\n-                model=self.model,\n-                tools=self.tools,\n-                checkpointer=checkpointer,\n-                prompt=self.prompt,\n-            )\n-\n-            config = {\"configurable\": {\"thread_id\": self.thread_id}}\n-\n-            result = await agent.ainvoke(\n-                {\"messages\": [(\"human\", user_message)]}, config\n-            )\n-\n-            return result[\"messages\"][-1].content\n+        self.db_path = \"chat_memory.db\"\n \n-    async def stream_message(self, user_message: str):\n+    async def stream_message(self, user_message: str, thread_id: str):\n         \"\"\"Envia mensagem e faz streaming da resposta\"\"\"\n         async with AsyncSqliteSaver.from_conn_string(self.db_path) as checkpointer:\n-            agent = create_react_agent(\n+            agent_executable = create_react_agent(\n                 model=self.model,\n                 tools=self.tools,\n                 checkpointer=checkpointer,\n                 prompt=self.prompt,\n             )\n-\n-            config = {\"configurable\": {\"thread_id\": self.thread_id}}\n-\n+            config = {\"configurable\": {\"thread_id\": thread_id}}\n             try:\n-                full_response = \"\"\n-\n-                async for chunk, metadata in agent.astream(\n+                async for chunk, metadata in agent_executable.astream(\n                     {\"messages\": [(\"human\", user_message)]},\n                     config,\n                     stream_mode=\"messages\",\n                 ):\n-                    chunk_type = chunk.__class__.__name__\n-                    if chunk_type in [\"AIMessageChunk\", \"AIMessage\"]:\n-                        token_content = chunk.content\n-                        full_response += token_content\n-\n-                        yield {\"type\": \"content\", \"data\": token_content}\n-\n-                # completion message\n-                yield {\"type\": \"complete\", \"data\": full_response}\n+                    if isinstance(chunk, AIMessageChunk) and chunk.content:\n+                        yield {\"type\": \"content\", \"data\": chunk.content}\n             except Exception as e:\n-                print(f\"Error in streaming: {e}\")\n+                print(f\"Error during agent stream: {e}\")\n                 yield {\"type\": \"error\", \"data\": f\"Error processing message: {str(e)}\"}\n-\n-    @classmethod\n-    def build(cls, thread_id: str = None):\n-        \"\"\"Factory method para criar inst\u00e2ncia\"\"\"\n-        return cls(thread_id=thread_id)",
      "patch_lines": [
        "@@ -1,7 +1,6 @@\n",
        "-import uuid\n",
        "-\n",
        " from dotenv import load_dotenv\n",
        " from langchain.chat_models import init_chat_model\n",
        "+from langchain_core.messages import AIMessageChunk\n",
        " from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
        " from langgraph.prebuilt import create_react_agent\n",
        " \n",
        "@@ -12,67 +11,32 @@\n",
        " \n",
        " \n",
        " class Agent:\n",
        "-    def __init__(self, thread_id: str = None):\n",
        "+    def __init__(self):\n",
        "         self.model = init_chat_model(\n",
        "             model=\"gemini-2.5-flash-lite-preview-06-17\", model_provider=\"google_genai\"\n",
        "         )\n",
        "-        self.db_path = \"chat_memory.db\"\n",
        "         self.tools = tools\n",
        "         self.prompt = VENDEDOR_PROMPT\n",
        "-        self.thread_id = thread_id or str(uuid.uuid4())\n",
        "-\n",
        "-    async def send_message(self, user_message: str) -> str:\n",
        "-        \"\"\"Envia mensagem e retorna resposta completa\"\"\"\n",
        "-        async with AsyncSqliteSaver.from_conn_string(self.db_path) as checkpointer:\n",
        "-            agent = create_react_agent(\n",
        "-                model=self.model,\n",
        "-                tools=self.tools,\n",
        "-                checkpointer=checkpointer,\n",
        "-                prompt=self.prompt,\n",
        "-            )\n",
        "-\n",
        "-            config = {\"configurable\": {\"thread_id\": self.thread_id}}\n",
        "-\n",
        "-            result = await agent.ainvoke(\n",
        "-                {\"messages\": [(\"human\", user_message)]}, config\n",
        "-            )\n",
        "-\n",
        "-            return result[\"messages\"][-1].content\n",
        "+        self.db_path = \"chat_memory.db\"\n",
        " \n",
        "-    async def stream_message(self, user_message: str):\n",
        "+    async def stream_message(self, user_message: str, thread_id: str):\n",
        "         \"\"\"Envia mensagem e faz streaming da resposta\"\"\"\n",
        "         async with AsyncSqliteSaver.from_conn_string(self.db_path) as checkpointer:\n",
        "-            agent = create_react_agent(\n",
        "+            agent_executable = create_react_agent(\n",
        "                 model=self.model,\n",
        "                 tools=self.tools,\n",
        "                 checkpointer=checkpointer,\n",
        "                 prompt=self.prompt,\n",
        "             )\n",
        "-\n",
        "-            config = {\"configurable\": {\"thread_id\": self.thread_id}}\n",
        "-\n",
        "+            config = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "             try:\n",
        "-                full_response = \"\"\n",
        "-\n",
        "-                async for chunk, metadata in agent.astream(\n",
        "+                async for chunk, metadata in agent_executable.astream(\n",
        "                     {\"messages\": [(\"human\", user_message)]},\n",
        "                     config,\n",
        "                     stream_mode=\"messages\",\n",
        "                 ):\n",
        "-                    chunk_type = chunk.__class__.__name__\n",
        "-                    if chunk_type in [\"AIMessageChunk\", \"AIMessage\"]:\n",
        "-                        token_content = chunk.content\n",
        "-                        full_response += token_content\n",
        "-\n",
        "-                        yield {\"type\": \"content\", \"data\": token_content}\n",
        "-\n",
        "-                # completion message\n",
        "-                yield {\"type\": \"complete\", \"data\": full_response}\n",
        "+                    if isinstance(chunk, AIMessageChunk) and chunk.content:\n",
        "+                        yield {\"type\": \"content\", \"data\": chunk.content}\n",
        "             except Exception as e:\n",
        "-                print(f\"Error in streaming: {e}\")\n",
        "+                print(f\"Error during agent stream: {e}\")\n",
        "                 yield {\"type\": \"error\", \"data\": f\"Error processing message: {str(e)}\"}\n",
        "-\n",
        "-    @classmethod\n",
        "-    def build(cls, thread_id: str = None):\n",
        "-        \"\"\"Factory method para criar inst\u00e2ncia\"\"\"\n",
        "-        return cls(thread_id=thread_id)\n"
      ]
    }
  ]
}
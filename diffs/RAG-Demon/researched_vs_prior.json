{
  "project": "Research Data/RAG-Demon",
  "repo": "Les-Mills-Ai-Agent/RAG-Demon",
  "prior_commit": "de66a26cac71261a4803b93484248a9a5b0514aa",
  "researched_commit": "b7bca5a1310bdbfdd98023ad28e98954b063367c",
  "compare_url": "https://github.com/Les-Mills-Ai-Agent/RAG-Demon/compare/de66a26cac71261a4803b93484248a9a5b0514aa...b7bca5a1310bdbfdd98023ad28e98954b063367c",
  "ahead_by": 1,
  "behind_by": 0,
  "changed_files": [
    {
      "path": "chat_data/chat_history.json",
      "status": "modified",
      "additions": 10,
      "deletions": 0,
      "patch": "@@ -143,5 +143,15 @@\n     \"timestamp\": \"2025-07-24 18:23:48\",\n     \"question\": \"What kind of content does Les Mills offer?\",\n     \"response\": \"I don't know.\"\n+  },\n+  {\n+    \"timestamp\": \"2025-07-24 18:44:41\",\n+    \"question\": \"What kind of content does Les Mills offer?\",\n+    \"response\": \"I don't know.\"\n+  },\n+  {\n+    \"timestamp\": \"2025-07-24 18:46:10\",\n+    \"question\": \"What kind of content does Les Mills offer?\",\n+    \"response\": \"I don't know.\"\n   }\n ]\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -143,5 +143,15 @@\n",
        "     \"timestamp\": \"2025-07-24 18:23:48\",\n",
        "     \"question\": \"What kind of content does Les Mills offer?\",\n",
        "     \"response\": \"I don't know.\"\n",
        "+  },\n",
        "+  {\n",
        "+    \"timestamp\": \"2025-07-24 18:44:41\",\n",
        "+    \"question\": \"What kind of content does Les Mills offer?\",\n",
        "+    \"response\": \"I don't know.\"\n",
        "+  },\n",
        "+  {\n",
        "+    \"timestamp\": \"2025-07-24 18:46:10\",\n",
        "+    \"question\": \"What kind of content does Les Mills offer?\",\n",
        "+    \"response\": \"I don't know.\"\n",
        "   }\n",
        " ]\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "src/ragdemon/server.py",
      "status": "modified",
      "additions": 20,
      "deletions": 10,
      "patch": "@@ -45,27 +45,37 @@ class ChatRequest(BaseModel):\n @api.post(\"/api/chat\")\n async def chat(req: ChatRequest):\n     try:\n-        if not req.messages:\n-            raise HTTPException(status_code=400, detail=\"Message list is empty.\")\n-\n-        reply = None\n-        for step in graph.stream({\"messages\": req.messages}, stream_mode=\"values\", config=config):\n+        if not req.messages or not isinstance(req.messages, list):\n+            raise HTTPException(status_code=400, detail=\"Message list is empty or invalid.\")\n+\n+        # Just extract the latest message\n+        latest_msg = req.messages[-1]\n+\n+        # Start streaming into LangGraph with only the new message\n+        assistant_response = None\n+        for step in graph.stream(\n+            {\"messages\": [latest_msg]},\n+            stream_mode=\"values\",\n+            config=config\n+        ):\n             msg = step[\"messages\"][-1]\n             if getattr(msg, \"type\", None) in (\"ai\", \"assistant\"):\n-                reply = msg.content\n+                assistant_response = msg.content\n \n-        if reply is None:\n-            raise RuntimeError(\"No assistant reply generated\")\n+        if assistant_response is None:\n+            raise RuntimeError(\"No assistant message generated \u2014 check input or node config.\")\n \n-        return {\"reply\": reply}\n+        return {\"reply\": assistant_response}\n \n     except HTTPException:\n-        raise  # re-raise cleanly\n+        raise\n     except Exception as e:\n         print(f\"Internal Server Error: {type(e).__name__}: {e}\")\n         raise HTTPException(status_code=500, detail=\"An internal error occurred. Please try again later.\")\n \n \n+\n+\n # Dev server entry point\n if __name__ == \"__main__\":\n     import uvicorn",
      "patch_lines": [
        "@@ -45,27 +45,37 @@ class ChatRequest(BaseModel):\n",
        " @api.post(\"/api/chat\")\n",
        " async def chat(req: ChatRequest):\n",
        "     try:\n",
        "-        if not req.messages:\n",
        "-            raise HTTPException(status_code=400, detail=\"Message list is empty.\")\n",
        "-\n",
        "-        reply = None\n",
        "-        for step in graph.stream({\"messages\": req.messages}, stream_mode=\"values\", config=config):\n",
        "+        if not req.messages or not isinstance(req.messages, list):\n",
        "+            raise HTTPException(status_code=400, detail=\"Message list is empty or invalid.\")\n",
        "+\n",
        "+        # Just extract the latest message\n",
        "+        latest_msg = req.messages[-1]\n",
        "+\n",
        "+        # Start streaming into LangGraph with only the new message\n",
        "+        assistant_response = None\n",
        "+        for step in graph.stream(\n",
        "+            {\"messages\": [latest_msg]},\n",
        "+            stream_mode=\"values\",\n",
        "+            config=config\n",
        "+        ):\n",
        "             msg = step[\"messages\"][-1]\n",
        "             if getattr(msg, \"type\", None) in (\"ai\", \"assistant\"):\n",
        "-                reply = msg.content\n",
        "+                assistant_response = msg.content\n",
        " \n",
        "-        if reply is None:\n",
        "-            raise RuntimeError(\"No assistant reply generated\")\n",
        "+        if assistant_response is None:\n",
        "+            raise RuntimeError(\"No assistant message generated \u2014 check input or node config.\")\n",
        " \n",
        "-        return {\"reply\": reply}\n",
        "+        return {\"reply\": assistant_response}\n",
        " \n",
        "     except HTTPException:\n",
        "-        raise  # re-raise cleanly\n",
        "+        raise\n",
        "     except Exception as e:\n",
        "         print(f\"Internal Server Error: {type(e).__name__}: {e}\")\n",
        "         raise HTTPException(status_code=500, detail=\"An internal error occurred. Please try again later.\")\n",
        " \n",
        " \n",
        "+\n",
        "+\n",
        " # Dev server entry point\n",
        " if __name__ == \"__main__\":\n",
        "     import uvicorn\n"
      ]
    },
    {
      "path": "tests/test_server.py",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "patch": "@@ -29,5 +29,6 @@ async def test_chat_endpoint_with_empty_message():\n         response = await ac.post(\"http://localhost:8000/api/chat\", json={\"messages\": test_messages})\n \n     assert response.status_code == 400\n-    assert response.json()[\"detail\"] == \"Message list is empty.\"\n+    assert response.json()[\"detail\"] == \"Message list is empty or invalid.\"\n+\n ",
      "patch_lines": [
        "@@ -29,5 +29,6 @@ async def test_chat_endpoint_with_empty_message():\n",
        "         response = await ac.post(\"http://localhost:8000/api/chat\", json={\"messages\": test_messages})\n",
        " \n",
        "     assert response.status_code == 400\n",
        "-    assert response.json()[\"detail\"] == \"Message list is empty.\"\n",
        "+    assert response.json()[\"detail\"] == \"Message list is empty or invalid.\"\n",
        "+\n",
        " \n"
      ]
    }
  ]
}
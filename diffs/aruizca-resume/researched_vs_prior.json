{
  "project": "Research Data/aruizca-resume",
  "repo": "aruizca/aruizca-resume",
  "prior_commit": "efde30ea49726f13e830963f6d971117387bc2c8",
  "researched_commit": "8108951369a79d1ed04b08998697c87d5a6e3f9d",
  "compare_url": "https://github.com/aruizca/aruizca-resume/compare/efde30ea49726f13e830963f6d971117387bc2c8...8108951369a79d1ed04b08998697c87d5a6e3f9d",
  "ahead_by": 1,
  "behind_by": 0,
  "changed_files": [
    {
      "path": "packages/core/src/main/cover-letter/infrastructure/langchain/CoverLetterPromptRunner.ts",
      "status": "modified",
      "additions": 61,
      "deletions": 58,
      "patch": "@@ -1,8 +1,7 @@\n import { readFileSync } from 'fs';\n import { join } from 'path';\n import { PromptTemplate } from '@langchain/core/prompts';\n-import { ChatOpenAI } from '@langchain/openai';\n-import { ModelFactory } from '../../../shared';\n+import { LangchainPromptRunner, ModelFactory } from '../../../shared';\n import { JobOffer, ParsedLinkedInData } from '../../domain';\n \n export interface CoverLetterPromptRunner {\n@@ -11,74 +10,78 @@ export interface CoverLetterPromptRunner {\n   extractJobInfoFromHtml(html: string): Promise<JobOffer>;\n }\n \n+interface CoverLetterInput {\n+  jobPostingJson: string;\n+  resumeJson: string;\n+}\n+\n export class DefaultCoverLetterPromptRunner implements CoverLetterPromptRunner {\n-  private model: ChatOpenAI;\n-  private jsonPrompt: PromptTemplate;\n-  private jobExtractionPrompt: PromptTemplate;\n+  private coverLetterRunner: LangchainPromptRunner<CoverLetterInput, string>;\n+  private jobExtractionRunner: LangchainPromptRunner<string, JobOffer>;\n \n   constructor() {\n-    this.model = ModelFactory.createCoverLetterModel();\n-    this.jsonPrompt = this.createJsonPrompt();\n-    this.jobExtractionPrompt = this.createJobExtractionPrompt();\n+    // Initialize cover letter generation runner\n+    this.coverLetterRunner = new LangchainPromptRunner({\n+      modelFactory: () => ModelFactory.createCoverLetterModel(),\n+      promptFactory: () => this.createJsonPrompt(),\n+      inputTransformer: (input) => ({\n+        jobPostingJson: input.jobPostingJson,\n+        resumeJson: input.resumeJson\n+      }),\n+      outputTransformer: (result) => {\n+        // Handle both string and object responses\n+        return typeof result === 'string' ? result : (result.content || result);\n+      },\n+      outputParser: 'string',\n+      cacheConfig: {\n+        ttl: 8 * 60 * 60 * 1000 // 8 hours\n+      },\n+      operationName: 'Generate Cover Letter (LLM)'\n+    });\n+\n+    // Initialize job extraction runner  \n+    this.jobExtractionRunner = new LangchainPromptRunner({\n+      modelFactory: () => ModelFactory.createCoverLetterModel(),\n+      promptFactory: () => this.createJobExtractionPrompt(),\n+      inputTransformer: (html) => ({\n+        htmlContent: this.truncateHtml(html)\n+      }),\n+      outputTransformer: (result) => this.parseJobExtractionResult(result.content || result),\n+      outputParser: 'string',\n+      cacheConfig: {\n+        ttl: 8 * 60 * 60 * 1000 // 8 hours\n+      },\n+      operationName: 'Extract Job Info from HTML (LLM)'\n+    });\n   }\n \n   async run(jobOffer: JobOffer, userProfile: ParsedLinkedInData): Promise<string> {\n-    try {\n-      // Convert to JSON format for consistency\n-      const jobPostingJson = JSON.stringify(jobOffer, null, 2);\n-      const resumeJson = JSON.stringify(userProfile, null, 2);\n-      \n-      return await this.runWithJson(jobPostingJson, resumeJson);\n-    } catch (error) {\n-      throw new Error(`Failed to generate cover letter: ${error instanceof Error ? error.message : 'Unknown error'}`);\n-    }\n+    // Convert to JSON format for consistency\n+    const jobPostingJson = JSON.stringify(jobOffer, null, 2);\n+    const resumeJson = JSON.stringify(userProfile, null, 2);\n+    \n+    return await this.runWithJson(jobPostingJson, resumeJson);\n   }\n \n   async runWithJson(jobPostingJson: string, resumeJson: string): Promise<string> {\n-    try {\n-      console.log('\ud83e\udd16 Generating cover letter with JSON inputs...');\n-      \n-      // Execute the JSON prompt\n-      const result = await this.jsonPrompt.pipe(this.model).invoke({\n-        jobPostingJson,\n-        resumeJson\n-      });\n-      \n-      return result.content as string;\n-    } catch (error) {\n-      throw new Error(`Failed to generate cover letter with JSON: ${error instanceof Error ? error.message : 'Unknown error'}`);\n-    }\n+    return await this.coverLetterRunner.execute({ jobPostingJson, resumeJson });\n   }\n \n   async extractJobInfoFromHtml(html: string): Promise<JobOffer> {\n-    try {\n-      console.log('\ud83d\udd0d Extracting job information from HTML using LLM...');\n-      \n-      // Truncate HTML if it's too long (to avoid token limits)\n-      const truncatedHtml = this.truncateHtml(html);\n-      \n-      // Execute the job extraction prompt\n-      const result = await this.jobExtractionPrompt.pipe(this.model).invoke({\n-        htmlContent: truncatedHtml\n-      });\n-      \n-      // Parse the JSON response\n-      const extractedData = this.parseJobExtractionResult(result.content as string);\n-      \n-      return {\n-        url: '', // Will be set by the scraper\n-        title: extractedData.title || 'Unknown Title',\n-        company: extractedData.company || 'Unknown Company',\n-        description: extractedData.description || 'No description available',\n-        requirements: extractedData.requirements || [],\n-        responsibilities: extractedData.responsibilities || [],\n-        location: extractedData.location,\n-        salary: extractedData.salary,\n-        scrapedAt: new Date()\n-      };\n-    } catch (error) {\n-      throw new Error(`Failed to extract job information from HTML: ${error instanceof Error ? error.message : 'Unknown error'}`);\n-    }\n+    const extractedData = await this.jobExtractionRunner.execute(html);\n+    \n+    // Ensure required fields and set defaults\n+    return {\n+      url: '', // Will be set by the scraper\n+      title: extractedData.title || 'Unknown Title',\n+      company: extractedData.company || 'Unknown Company',\n+      description: extractedData.description || 'No description available',\n+      requirements: extractedData.requirements || [],\n+      responsibilities: extractedData.responsibilities || [],\n+      location: extractedData.location,\n+      salary: extractedData.salary,\n+      scrapedAt: new Date()\n+    };\n   }\n \n   private createJsonPrompt(): PromptTemplate {",
      "patch_lines": [
        "@@ -1,8 +1,7 @@\n",
        " import { readFileSync } from 'fs';\n",
        " import { join } from 'path';\n",
        " import { PromptTemplate } from '@langchain/core/prompts';\n",
        "-import { ChatOpenAI } from '@langchain/openai';\n",
        "-import { ModelFactory } from '../../../shared';\n",
        "+import { LangchainPromptRunner, ModelFactory } from '../../../shared';\n",
        " import { JobOffer, ParsedLinkedInData } from '../../domain';\n",
        " \n",
        " export interface CoverLetterPromptRunner {\n",
        "@@ -11,74 +10,78 @@ export interface CoverLetterPromptRunner {\n",
        "   extractJobInfoFromHtml(html: string): Promise<JobOffer>;\n",
        " }\n",
        " \n",
        "+interface CoverLetterInput {\n",
        "+  jobPostingJson: string;\n",
        "+  resumeJson: string;\n",
        "+}\n",
        "+\n",
        " export class DefaultCoverLetterPromptRunner implements CoverLetterPromptRunner {\n",
        "-  private model: ChatOpenAI;\n",
        "-  private jsonPrompt: PromptTemplate;\n",
        "-  private jobExtractionPrompt: PromptTemplate;\n",
        "+  private coverLetterRunner: LangchainPromptRunner<CoverLetterInput, string>;\n",
        "+  private jobExtractionRunner: LangchainPromptRunner<string, JobOffer>;\n",
        " \n",
        "   constructor() {\n",
        "-    this.model = ModelFactory.createCoverLetterModel();\n",
        "-    this.jsonPrompt = this.createJsonPrompt();\n",
        "-    this.jobExtractionPrompt = this.createJobExtractionPrompt();\n",
        "+    // Initialize cover letter generation runner\n",
        "+    this.coverLetterRunner = new LangchainPromptRunner({\n",
        "+      modelFactory: () => ModelFactory.createCoverLetterModel(),\n",
        "+      promptFactory: () => this.createJsonPrompt(),\n",
        "+      inputTransformer: (input) => ({\n",
        "+        jobPostingJson: input.jobPostingJson,\n",
        "+        resumeJson: input.resumeJson\n",
        "+      }),\n",
        "+      outputTransformer: (result) => {\n",
        "+        // Handle both string and object responses\n",
        "+        return typeof result === 'string' ? result : (result.content || result);\n",
        "+      },\n",
        "+      outputParser: 'string',\n",
        "+      cacheConfig: {\n",
        "+        ttl: 8 * 60 * 60 * 1000 // 8 hours\n",
        "+      },\n",
        "+      operationName: 'Generate Cover Letter (LLM)'\n",
        "+    });\n",
        "+\n",
        "+    // Initialize job extraction runner  \n",
        "+    this.jobExtractionRunner = new LangchainPromptRunner({\n",
        "+      modelFactory: () => ModelFactory.createCoverLetterModel(),\n",
        "+      promptFactory: () => this.createJobExtractionPrompt(),\n",
        "+      inputTransformer: (html) => ({\n",
        "+        htmlContent: this.truncateHtml(html)\n",
        "+      }),\n",
        "+      outputTransformer: (result) => this.parseJobExtractionResult(result.content || result),\n",
        "+      outputParser: 'string',\n",
        "+      cacheConfig: {\n",
        "+        ttl: 8 * 60 * 60 * 1000 // 8 hours\n",
        "+      },\n",
        "+      operationName: 'Extract Job Info from HTML (LLM)'\n",
        "+    });\n",
        "   }\n",
        " \n",
        "   async run(jobOffer: JobOffer, userProfile: ParsedLinkedInData): Promise<string> {\n",
        "-    try {\n",
        "-      // Convert to JSON format for consistency\n",
        "-      const jobPostingJson = JSON.stringify(jobOffer, null, 2);\n",
        "-      const resumeJson = JSON.stringify(userProfile, null, 2);\n",
        "-      \n",
        "-      return await this.runWithJson(jobPostingJson, resumeJson);\n",
        "-    } catch (error) {\n",
        "-      throw new Error(`Failed to generate cover letter: ${error instanceof Error ? error.message : 'Unknown error'}`);\n",
        "-    }\n",
        "+    // Convert to JSON format for consistency\n",
        "+    const jobPostingJson = JSON.stringify(jobOffer, null, 2);\n",
        "+    const resumeJson = JSON.stringify(userProfile, null, 2);\n",
        "+    \n",
        "+    return await this.runWithJson(jobPostingJson, resumeJson);\n",
        "   }\n",
        " \n",
        "   async runWithJson(jobPostingJson: string, resumeJson: string): Promise<string> {\n",
        "-    try {\n",
        "-      console.log('\ud83e\udd16 Generating cover letter with JSON inputs...');\n",
        "-      \n",
        "-      // Execute the JSON prompt\n",
        "-      const result = await this.jsonPrompt.pipe(this.model).invoke({\n",
        "-        jobPostingJson,\n",
        "-        resumeJson\n",
        "-      });\n",
        "-      \n",
        "-      return result.content as string;\n",
        "-    } catch (error) {\n",
        "-      throw new Error(`Failed to generate cover letter with JSON: ${error instanceof Error ? error.message : 'Unknown error'}`);\n",
        "-    }\n",
        "+    return await this.coverLetterRunner.execute({ jobPostingJson, resumeJson });\n",
        "   }\n",
        " \n",
        "   async extractJobInfoFromHtml(html: string): Promise<JobOffer> {\n",
        "-    try {\n",
        "-      console.log('\ud83d\udd0d Extracting job information from HTML using LLM...');\n",
        "-      \n",
        "-      // Truncate HTML if it's too long (to avoid token limits)\n",
        "-      const truncatedHtml = this.truncateHtml(html);\n",
        "-      \n",
        "-      // Execute the job extraction prompt\n",
        "-      const result = await this.jobExtractionPrompt.pipe(this.model).invoke({\n",
        "-        htmlContent: truncatedHtml\n",
        "-      });\n",
        "-      \n",
        "-      // Parse the JSON response\n",
        "-      const extractedData = this.parseJobExtractionResult(result.content as string);\n",
        "-      \n",
        "-      return {\n",
        "-        url: '', // Will be set by the scraper\n",
        "-        title: extractedData.title || 'Unknown Title',\n",
        "-        company: extractedData.company || 'Unknown Company',\n",
        "-        description: extractedData.description || 'No description available',\n",
        "-        requirements: extractedData.requirements || [],\n",
        "-        responsibilities: extractedData.responsibilities || [],\n",
        "-        location: extractedData.location,\n",
        "-        salary: extractedData.salary,\n",
        "-        scrapedAt: new Date()\n",
        "-      };\n",
        "-    } catch (error) {\n",
        "-      throw new Error(`Failed to extract job information from HTML: ${error instanceof Error ? error.message : 'Unknown error'}`);\n",
        "-    }\n",
        "+    const extractedData = await this.jobExtractionRunner.execute(html);\n",
        "+    \n",
        "+    // Ensure required fields and set defaults\n",
        "+    return {\n",
        "+      url: '', // Will be set by the scraper\n",
        "+      title: extractedData.title || 'Unknown Title',\n",
        "+      company: extractedData.company || 'Unknown Company',\n",
        "+      description: extractedData.description || 'No description available',\n",
        "+      requirements: extractedData.requirements || [],\n",
        "+      responsibilities: extractedData.responsibilities || [],\n",
        "+      location: extractedData.location,\n",
        "+      salary: extractedData.salary,\n",
        "+      scrapedAt: new Date()\n",
        "+    };\n",
        "   }\n",
        " \n",
        "   private createJsonPrompt(): PromptTemplate {\n"
      ]
    },
    {
      "path": "packages/core/src/main/resume/infrastructure/langchain/PromptRunner.ts",
      "status": "modified",
      "additions": 18,
      "deletions": 41,
      "patch": "@@ -1,63 +1,40 @@\n-import { ChatOpenAI } from '@langchain/openai';\n-import { ChainFactory, ModelFactory, OpenAICache } from '../../../shared';\n+import { LangchainPromptRunner, ModelFactory, PromptFactory } from '../../../shared';\n import { Resume } from '../../domain';\n \n export class PromptRunner {\n-  private model: ChatOpenAI;\n-  private cache: OpenAICache;\n+  private runner: LangchainPromptRunner<any, Resume>;\n \n   constructor(forceRefresh: boolean = false) {\n-    this.model = ModelFactory.createResumeModel();\n-    this.cache = new OpenAICache({ forceRefresh });\n+    this.runner = new LangchainPromptRunner({\n+      modelFactory: () => ModelFactory.createResumeModel(),\n+      promptFactory: () => PromptFactory.createResumePrompt(),\n+      inputTransformer: (parsedData) => ({\n+        linkedinData: JSON.stringify(parsedData, null, 2)\n+      }),\n+      outputTransformer: (result) => result as Resume,\n+      outputParser: 'json',\n+      cacheConfig: {\n+        ttl: 8 * 60 * 60 * 1000 // 8 hours\n+      },\n+      operationName: 'Generate JSON Resume (LLM)'\n+    });\n   }\n \n   async run(parsedData: any, forceRefresh: boolean = false): Promise<Resume> {\n-    try {\n-      // Create the chain using shared utilities\n-      const chain = await ChainFactory.createResumeChain(this.model);\n-      \n-      // Check cache first\n-      const promptTemplateString = JSON.stringify(parsedData, null, 2);\n-      const cachedResponse = await this.cache.get(parsedData, promptTemplateString, forceRefresh);\n-      if (cachedResponse) {\n-        return cachedResponse as Resume;\n-      }\n-\n-      // Prepare input variables\n-      const inputVariables = {\n-        linkedinData: JSON.stringify(parsedData, null, 2)\n-      };\n-\n-      console.log('\ud83e\udd16 Calling OpenAI API with Langchain...');\n-      \n-      // Execute the chain\n-      const result = await chain.invoke(inputVariables);\n-      \n-      // Cache the response\n-      await this.cache.set(parsedData, promptTemplateString, result);\n-      \n-      // Return as Resume (LLM is configured to output JSON Resume format)\n-      return result as Resume;\n-    } catch (error) {\n-      // Handle JSON parsing errors more gracefully\n-      if (error instanceof Error && error.message.includes('JSON')) {\n-        throw new Error(`Failed to parse LLM response as JSON: ${error.message}`);\n-      }\n-      throw new Error(`Failed to generate resume: ${error instanceof Error ? error.message : 'Unknown error'}`);\n-    }\n+    return await this.runner.execute(parsedData, forceRefresh);\n   }\n \n   /**\n    * Get cache statistics\n    */\n   async getCacheStats(): Promise<{ totalEntries: number; totalSize: number }> {\n-    return await this.cache.getStats();\n+    return await this.runner.getCacheStats();\n   }\n \n   /**\n    * Clear the cache\n    */\n   async clearCache(): Promise<void> {\n-    await this.cache.clear();\n+    await this.runner.clearCache();\n   }\n } \n\\ No newline at end of file",
      "patch_lines": [
        "@@ -1,63 +1,40 @@\n",
        "-import { ChatOpenAI } from '@langchain/openai';\n",
        "-import { ChainFactory, ModelFactory, OpenAICache } from '../../../shared';\n",
        "+import { LangchainPromptRunner, ModelFactory, PromptFactory } from '../../../shared';\n",
        " import { Resume } from '../../domain';\n",
        " \n",
        " export class PromptRunner {\n",
        "-  private model: ChatOpenAI;\n",
        "-  private cache: OpenAICache;\n",
        "+  private runner: LangchainPromptRunner<any, Resume>;\n",
        " \n",
        "   constructor(forceRefresh: boolean = false) {\n",
        "-    this.model = ModelFactory.createResumeModel();\n",
        "-    this.cache = new OpenAICache({ forceRefresh });\n",
        "+    this.runner = new LangchainPromptRunner({\n",
        "+      modelFactory: () => ModelFactory.createResumeModel(),\n",
        "+      promptFactory: () => PromptFactory.createResumePrompt(),\n",
        "+      inputTransformer: (parsedData) => ({\n",
        "+        linkedinData: JSON.stringify(parsedData, null, 2)\n",
        "+      }),\n",
        "+      outputTransformer: (result) => result as Resume,\n",
        "+      outputParser: 'json',\n",
        "+      cacheConfig: {\n",
        "+        ttl: 8 * 60 * 60 * 1000 // 8 hours\n",
        "+      },\n",
        "+      operationName: 'Generate JSON Resume (LLM)'\n",
        "+    });\n",
        "   }\n",
        " \n",
        "   async run(parsedData: any, forceRefresh: boolean = false): Promise<Resume> {\n",
        "-    try {\n",
        "-      // Create the chain using shared utilities\n",
        "-      const chain = await ChainFactory.createResumeChain(this.model);\n",
        "-      \n",
        "-      // Check cache first\n",
        "-      const promptTemplateString = JSON.stringify(parsedData, null, 2);\n",
        "-      const cachedResponse = await this.cache.get(parsedData, promptTemplateString, forceRefresh);\n",
        "-      if (cachedResponse) {\n",
        "-        return cachedResponse as Resume;\n",
        "-      }\n",
        "-\n",
        "-      // Prepare input variables\n",
        "-      const inputVariables = {\n",
        "-        linkedinData: JSON.stringify(parsedData, null, 2)\n",
        "-      };\n",
        "-\n",
        "-      console.log('\ud83e\udd16 Calling OpenAI API with Langchain...');\n",
        "-      \n",
        "-      // Execute the chain\n",
        "-      const result = await chain.invoke(inputVariables);\n",
        "-      \n",
        "-      // Cache the response\n",
        "-      await this.cache.set(parsedData, promptTemplateString, result);\n",
        "-      \n",
        "-      // Return as Resume (LLM is configured to output JSON Resume format)\n",
        "-      return result as Resume;\n",
        "-    } catch (error) {\n",
        "-      // Handle JSON parsing errors more gracefully\n",
        "-      if (error instanceof Error && error.message.includes('JSON')) {\n",
        "-        throw new Error(`Failed to parse LLM response as JSON: ${error.message}`);\n",
        "-      }\n",
        "-      throw new Error(`Failed to generate resume: ${error instanceof Error ? error.message : 'Unknown error'}`);\n",
        "-    }\n",
        "+    return await this.runner.execute(parsedData, forceRefresh);\n",
        "   }\n",
        " \n",
        "   /**\n",
        "    * Get cache statistics\n",
        "    */\n",
        "   async getCacheStats(): Promise<{ totalEntries: number; totalSize: number }> {\n",
        "-    return await this.cache.getStats();\n",
        "+    return await this.runner.getCacheStats();\n",
        "   }\n",
        " \n",
        "   /**\n",
        "    * Clear the cache\n",
        "    */\n",
        "   async clearCache(): Promise<void> {\n",
        "-    await this.cache.clear();\n",
        "+    await this.runner.clearCache();\n",
        "   }\n",
        " } \n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "packages/core/src/main/shared/infrastructure/langchain/LangchainPromptRunner.ts",
      "status": "added",
      "additions": 153,
      "deletions": 0,
      "patch": "@@ -0,0 +1,153 @@\n+import { ChatOpenAI } from '@langchain/openai';\n+import { PromptTemplate } from '@langchain/core/prompts';\n+import { Runnable } from '@langchain/core/runnables';\n+import { OpenAICache } from '../cache';\n+import { performanceMonitor } from '../utils';\n+\n+export interface PromptRunnerConfig<TInput, TOutput> {\n+  /**\n+   * Factory function to create the ChatOpenAI model\n+   */\n+  modelFactory: () => ChatOpenAI;\n+  \n+  /**\n+   * Factory function to create the prompt template\n+   */\n+  promptFactory: () => Promise<PromptTemplate> | PromptTemplate;\n+  \n+  /**\n+   * Transform input data into variables for the prompt template\n+   */\n+  inputTransformer: (input: TInput) => Record<string, any>;\n+  \n+  /**\n+   * Transform the LLM output into the desired output type\n+   */\n+  outputTransformer: (result: any) => TOutput;\n+  \n+  /**\n+   * Output parser type for the chain\n+   */\n+  outputParser?: 'json' | 'string';\n+  \n+  /**\n+   * Cache configuration\n+   */\n+  cacheConfig?: {\n+    ttl?: number; // Time to live in milliseconds\n+    cacheDir?: string;\n+  };\n+  \n+  /**\n+   * Operation name for logging and debugging\n+   */\n+  operationName?: string;\n+}\n+\n+export interface CacheStats {\n+  totalEntries: number;\n+  totalSize: number;\n+}\n+\n+/**\n+ * Generic LLM prompt runner that provides consistent caching, error handling,\n+ * and performance monitoring across all domain contexts.\n+ */\n+export class LangchainPromptRunner<TInput, TOutput> {\n+  private model: ChatOpenAI;\n+  private cache: OpenAICache;\n+  private config: PromptRunnerConfig<TInput, TOutput>;\n+  private promptTemplate: PromptTemplate | null = null;\n+\n+  constructor(config: PromptRunnerConfig<TInput, TOutput>) {\n+    this.config = config;\n+    this.model = config.modelFactory();\n+    this.cache = new OpenAICache({\n+      ttl: config.cacheConfig?.ttl,\n+      cacheDir: config.cacheConfig?.cacheDir\n+    });\n+  }\n+\n+  /**\n+   * Execute the prompt runner with the given input\n+   * @param input The input data for the prompt\n+   * @param forceRefresh Whether to bypass cache for fresh content\n+   * @returns The transformed output\n+   */\n+  async execute(input: TInput, forceRefresh: boolean = false): Promise<TOutput> {\n+    const operationName = this.config.operationName || 'LLM Operation';\n+    \n+    try {\n+      // Lazy load prompt template\n+      if (!this.promptTemplate) {\n+        this.promptTemplate = await this.config.promptFactory();\n+      }\n+\n+      // Transform input to prompt variables\n+      const promptVariables = this.config.inputTransformer(input);\n+      const promptTemplateString = typeof this.promptTemplate.template === 'string' \n+        ? this.promptTemplate.template \n+        : JSON.stringify(this.promptTemplate.template);\n+\n+      // Check cache first\n+      const cachedResponse = await this.cache.get(input, promptTemplateString, forceRefresh);\n+      if (cachedResponse && !forceRefresh) {\n+        return this.config.outputTransformer(cachedResponse);\n+      }\n+\n+      console.log(`\ud83e\udd16 Executing ${operationName}...`);\n+\n+      // Execute the LLM operation with performance monitoring\n+      const result = await performanceMonitor.trackOperation(\n+        operationName,\n+        async () => {\n+          // Create and execute chain\n+          const chain = await this.createChain();\n+          return await chain.invoke(promptVariables);\n+        },\n+        { logToConsole: true }\n+      );\n+\n+      // Cache the response\n+      await this.cache.set(input, promptTemplateString, result);\n+\n+      // Transform and return output\n+      return this.config.outputTransformer(result);\n+\n+    } catch (error) {\n+      // Enhanced error handling with operation context\n+      const errorMessage = error instanceof Error ? error.message : 'Unknown error occurred';\n+      throw new Error(`${operationName} failed: ${errorMessage}`);\n+    }\n+  }\n+\n+  /**\n+   * Get cache statistics\n+   */\n+  async getCacheStats(): Promise<CacheStats> {\n+    return await this.cache.getStats();\n+  }\n+\n+  /**\n+   * Clear the cache\n+   */\n+  async clearCache(): Promise<void> {\n+    await this.cache.clear();\n+  }\n+\n+  /**\n+   * Create a Langchain chain with the configured prompt and model\n+   */\n+  private async createChain(): Promise<Runnable> {\n+    if (!this.promptTemplate) {\n+      this.promptTemplate = await this.config.promptFactory();\n+    }\n+\n+    const { ChainFactory } = await import('./chainFactory');\n+    return ChainFactory.createChain({\n+      model: this.model,\n+      prompt: this.promptTemplate,\n+      outputParser: this.config.outputParser || 'json'\n+    });\n+  }\n+}",
      "patch_lines": [
        "@@ -0,0 +1,153 @@\n",
        "+import { ChatOpenAI } from '@langchain/openai';\n",
        "+import { PromptTemplate } from '@langchain/core/prompts';\n",
        "+import { Runnable } from '@langchain/core/runnables';\n",
        "+import { OpenAICache } from '../cache';\n",
        "+import { performanceMonitor } from '../utils';\n",
        "+\n",
        "+export interface PromptRunnerConfig<TInput, TOutput> {\n",
        "+  /**\n",
        "+   * Factory function to create the ChatOpenAI model\n",
        "+   */\n",
        "+  modelFactory: () => ChatOpenAI;\n",
        "+  \n",
        "+  /**\n",
        "+   * Factory function to create the prompt template\n",
        "+   */\n",
        "+  promptFactory: () => Promise<PromptTemplate> | PromptTemplate;\n",
        "+  \n",
        "+  /**\n",
        "+   * Transform input data into variables for the prompt template\n",
        "+   */\n",
        "+  inputTransformer: (input: TInput) => Record<string, any>;\n",
        "+  \n",
        "+  /**\n",
        "+   * Transform the LLM output into the desired output type\n",
        "+   */\n",
        "+  outputTransformer: (result: any) => TOutput;\n",
        "+  \n",
        "+  /**\n",
        "+   * Output parser type for the chain\n",
        "+   */\n",
        "+  outputParser?: 'json' | 'string';\n",
        "+  \n",
        "+  /**\n",
        "+   * Cache configuration\n",
        "+   */\n",
        "+  cacheConfig?: {\n",
        "+    ttl?: number; // Time to live in milliseconds\n",
        "+    cacheDir?: string;\n",
        "+  };\n",
        "+  \n",
        "+  /**\n",
        "+   * Operation name for logging and debugging\n",
        "+   */\n",
        "+  operationName?: string;\n",
        "+}\n",
        "+\n",
        "+export interface CacheStats {\n",
        "+  totalEntries: number;\n",
        "+  totalSize: number;\n",
        "+}\n",
        "+\n",
        "+/**\n",
        "+ * Generic LLM prompt runner that provides consistent caching, error handling,\n",
        "+ * and performance monitoring across all domain contexts.\n",
        "+ */\n",
        "+export class LangchainPromptRunner<TInput, TOutput> {\n",
        "+  private model: ChatOpenAI;\n",
        "+  private cache: OpenAICache;\n",
        "+  private config: PromptRunnerConfig<TInput, TOutput>;\n",
        "+  private promptTemplate: PromptTemplate | null = null;\n",
        "+\n",
        "+  constructor(config: PromptRunnerConfig<TInput, TOutput>) {\n",
        "+    this.config = config;\n",
        "+    this.model = config.modelFactory();\n",
        "+    this.cache = new OpenAICache({\n",
        "+      ttl: config.cacheConfig?.ttl,\n",
        "+      cacheDir: config.cacheConfig?.cacheDir\n",
        "+    });\n",
        "+  }\n",
        "+\n",
        "+  /**\n",
        "+   * Execute the prompt runner with the given input\n",
        "+   * @param input The input data for the prompt\n",
        "+   * @param forceRefresh Whether to bypass cache for fresh content\n",
        "+   * @returns The transformed output\n",
        "+   */\n",
        "+  async execute(input: TInput, forceRefresh: boolean = false): Promise<TOutput> {\n",
        "+    const operationName = this.config.operationName || 'LLM Operation';\n",
        "+    \n",
        "+    try {\n",
        "+      // Lazy load prompt template\n",
        "+      if (!this.promptTemplate) {\n",
        "+        this.promptTemplate = await this.config.promptFactory();\n",
        "+      }\n",
        "+\n",
        "+      // Transform input to prompt variables\n",
        "+      const promptVariables = this.config.inputTransformer(input);\n",
        "+      const promptTemplateString = typeof this.promptTemplate.template === 'string' \n",
        "+        ? this.promptTemplate.template \n",
        "+        : JSON.stringify(this.promptTemplate.template);\n",
        "+\n",
        "+      // Check cache first\n",
        "+      const cachedResponse = await this.cache.get(input, promptTemplateString, forceRefresh);\n",
        "+      if (cachedResponse && !forceRefresh) {\n",
        "+        return this.config.outputTransformer(cachedResponse);\n",
        "+      }\n",
        "+\n",
        "+      console.log(`\ud83e\udd16 Executing ${operationName}...`);\n",
        "+\n",
        "+      // Execute the LLM operation with performance monitoring\n",
        "+      const result = await performanceMonitor.trackOperation(\n",
        "+        operationName,\n",
        "+        async () => {\n",
        "+          // Create and execute chain\n",
        "+          const chain = await this.createChain();\n",
        "+          return await chain.invoke(promptVariables);\n",
        "+        },\n",
        "+        { logToConsole: true }\n",
        "+      );\n",
        "+\n",
        "+      // Cache the response\n",
        "+      await this.cache.set(input, promptTemplateString, result);\n",
        "+\n",
        "+      // Transform and return output\n",
        "+      return this.config.outputTransformer(result);\n",
        "+\n",
        "+    } catch (error) {\n",
        "+      // Enhanced error handling with operation context\n",
        "+      const errorMessage = error instanceof Error ? error.message : 'Unknown error occurred';\n",
        "+      throw new Error(`${operationName} failed: ${errorMessage}`);\n",
        "+    }\n",
        "+  }\n",
        "+\n",
        "+  /**\n",
        "+   * Get cache statistics\n",
        "+   */\n",
        "+  async getCacheStats(): Promise<CacheStats> {\n",
        "+    return await this.cache.getStats();\n",
        "+  }\n",
        "+\n",
        "+  /**\n",
        "+   * Clear the cache\n",
        "+   */\n",
        "+  async clearCache(): Promise<void> {\n",
        "+    await this.cache.clear();\n",
        "+  }\n",
        "+\n",
        "+  /**\n",
        "+   * Create a Langchain chain with the configured prompt and model\n",
        "+   */\n",
        "+  private async createChain(): Promise<Runnable> {\n",
        "+    if (!this.promptTemplate) {\n",
        "+      this.promptTemplate = await this.config.promptFactory();\n",
        "+    }\n",
        "+\n",
        "+    const { ChainFactory } = await import('./chainFactory');\n",
        "+    return ChainFactory.createChain({\n",
        "+      model: this.model,\n",
        "+      prompt: this.promptTemplate,\n",
        "+      outputParser: this.config.outputParser || 'json'\n",
        "+    });\n",
        "+  }\n",
        "+}\n"
      ]
    },
    {
      "path": "packages/core/src/main/shared/infrastructure/langchain/index.ts",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "patch": "@@ -1,3 +1,4 @@\n export * from './modelFactory';\n export * from './promptFactory';\n-export * from './chainFactory'; \n\\ No newline at end of file\n+export * from './chainFactory';\n+export * from './LangchainPromptRunner'; \n\\ No newline at end of file",
      "patch_lines": [
        "@@ -1,3 +1,4 @@\n",
        " export * from './modelFactory';\n",
        " export * from './promptFactory';\n",
        "-export * from './chainFactory'; \n",
        "\\ No newline at end of file\n",
        "+export * from './chainFactory';\n",
        "+export * from './LangchainPromptRunner'; \n",
        "\\ No newline at end of file\n"
      ]
    }
  ]
}
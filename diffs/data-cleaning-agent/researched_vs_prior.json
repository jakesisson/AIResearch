{
  "project": "Research Data/data-cleaning-agent",
  "repo": "Ziyan0219/data-cleaning-agent",
  "prior_commit": "0796e3380ee3e1cf6866137c80c0d293faedbbe4",
  "researched_commit": "ad59db504daa798862aeafb880b9f357691b1812",
  "compare_url": "https://github.com/Ziyan0219/data-cleaning-agent/compare/0796e3380ee3e1cf6866137c80c0d293faedbbe4...ad59db504daa798862aeafb880b9f357691b1812",
  "ahead_by": 1,
  "behind_by": 0,
  "changed_files": [
    {
      "path": "src/agents/main_controller.py",
      "status": "modified",
      "additions": 167,
      "deletions": 263,
      "patch": "@@ -1,24 +1,21 @@\n \"\"\"\n Main Controller Agent Implementation\n \n-This module implements the main controller agent that orchestrates the entire\n-data cleaning workflow using LangGraph's state graph architecture.\n+This module implements the central orchestrator for the data cleaning system.\n+It coordinates all other agents and manages the overall workflow.\n \"\"\"\n \n-import asyncio\n import uuid\n+import asyncio\n from datetime import datetime\n from typing import Dict, Any, List, Optional\n-from loguru import logger\n+from pathlib import Path\n \n-from langgraph.graph import StateGraph, END\n from langchain_openai import ChatOpenAI\n+from langgraph.graph import StateGraph, END\n+from loguru import logger\n \n-from ..schemas.state import (\n-    DataCleaningState, \n-    create_initial_state, \n-    update_state_phase\n-)\n+from ..schemas.state import DataCleaningState, create_initial_state\n from ..config.settings import get_settings\n from .data_analysis_agent import DataAnalysisAgent\n from .data_cleaning_agent import DataCleaningAgent\n@@ -27,27 +24,26 @@\n \n \n class MainControllerAgent:\n-    \"\"\"Main Controller Agent Class\"\"\"\n+    \"\"\"Main controller for data cleaning workflow\"\"\"\n     \n     def __init__(self):\n+        \"\"\"Initialize the main controller\"\"\"\n         self.settings = get_settings()\n         self.session_id = str(uuid.uuid4())\n-        self.workflow = None\n-        self.llm = self._initialize_llm()\n         \n-        # Initialize specialized agents\n-        self.analysis_agent = DataAnalysisAgent(self.llm)\n-        self.cleaning_agent = DataCleaningAgent(self.llm)\n-        self.validation_agent = QualityValidationAgent(self.llm)\n-        self.aggregation_agent = ResultAggregationAgent(self.llm)\n+        # Initialize sub-agents\n+        self.analysis_agent = DataAnalysisAgent()\n+        self.cleaning_agent = DataCleaningAgent()\n+        self.validation_agent = QualityValidationAgent()\n+        self.aggregation_agent = ResultAggregationAgent()\n         \n         # Build workflow\n-        self._build_workflow()\n+        self.workflow = self._build_workflow()\n         \n         logger.info(\"Main Controller Agent initialized successfully\")\n     \n-    def _initialize_llm(self):\n-        \"\"\"Initialize LLM\"\"\"\n+    def _get_llm(self):\n+        \"\"\"Get configured LLM instance\"\"\"\n         llm_config = self.settings.get_llm_config()\n         \n         return ChatOpenAI(\n@@ -71,7 +67,6 @@ def _build_workflow(self):\n         workflow.add_node(\"execute_cleaning\", self._execute_cleaning)\n         workflow.add_node(\"validate_results\", self._validate_results)\n         workflow.add_node(\"aggregate_results\", self._aggregate_results)\n-        workflow.add_node(\"handle_error\", self._handle_error)\n         \n         # Set entry point\n         workflow.set_entry_point(\"load_data\")\n@@ -84,55 +79,10 @@ def _build_workflow(self):\n         workflow.add_edge(\"validate_results\", \"aggregate_results\")\n         workflow.add_edge(\"aggregate_results\", END)\n         \n-        # Add conditional edges for error handling\n-        workflow.add_conditional_edges(\n-            \"load_data\",\n-            self._should_continue_after_load,\n-            {\n-                \"continue\": \"analyze_data\",\n-                \"error\": \"handle_error\"\n-            }\n-        )\n-        \n-        workflow.add_conditional_edges(\n-            \"analyze_data\",\n-            self._should_continue_after_analysis,\n-            {\n-                \"continue\": \"plan_cleaning\",\n-                \"retry\": \"analyze_data\",\n-                \"error\": \"handle_error\"\n-            }\n-        )\n-        \n-        workflow.add_conditional_edges(\n-            \"execute_cleaning\",\n-            self._should_continue_after_cleaning,\n-            {\n-                \"continue\": \"validate_results\",\n-                \"retry\": \"execute_cleaning\",\n-                \"error\": \"handle_error\"\n-            }\n-        )\n-        \n-        workflow.add_conditional_edges(\n-            \"validate_results\",\n-            self._should_continue_after_validation,\n-            {\n-                \"continue\": \"aggregate_results\",\n-                \"retry_cleaning\": \"execute_cleaning\",\n-                \"error\": \"handle_error\"\n-            }\n-        )\n-        \n-        workflow.add_edge(\"handle_error\", END)\n-        \n-        # Compile workflow\n-        self.workflow = workflow.compile()\n-        \n-        logger.info(\"Workflow built successfully\")\n+        return workflow.compile()\n     \n-    async def process_data(self, user_requirements: str, \n-                          data_source: str) -> Dict[str, Any]:\n+    async def process_cleaning_request(self, user_requirements: str, \n+                                     data_source: str) -> Dict[str, Any]:\n         \"\"\"Process data cleaning request\"\"\"\n         try:\n             logger.info(f\"Starting data cleaning process for session: {self.session_id}\")\n@@ -144,9 +94,11 @@ async def process_data(self, user_requirements: str,\n                 data_source=data_source\n             )\n             \n-            # Execute workflow\n             start_time = datetime.now()\n+            \n+            # Execute workflow\n             final_state = await self.workflow.ainvoke(initial_state)\n+            \n             execution_time = (datetime.now() - start_time).total_seconds()\n             \n             # Prepare result\n@@ -157,6 +109,9 @@ async def process_data(self, user_requirements: str,\n                 \"results\": final_state.get(\"agent_results\", {}),\n                 \"quality_metrics\": final_state.get(\"quality_metrics\", {}),\n                 \"final_data\": final_state.get(\"processed_data\"),\n+                \"final_report\": final_state.get(\"agent_results\", {}).get(\"aggregation\", {}).get(\"final_report\", \"\"),\n+                \"executive_summary\": final_state.get(\"agent_results\", {}).get(\"aggregation\", {}).get(\"executive_summary\", \"\"),\n+                \"detailed_metrics\": final_state.get(\"agent_results\", {}).get(\"aggregation\", {}).get(\"detailed_metrics\", {}),\n                 \"error\": final_state.get(\"error_log\", [])[-1] if final_state.get(\"error_log\") else None\n             }\n             \n@@ -172,10 +127,13 @@ async def process_data(self, user_requirements: str,\n                 \"execution_time\": 0,\n                 \"results\": {},\n                 \"quality_metrics\": {},\n-                \"final_data\": None\n+                \"final_data\": None,\n+                \"final_report\": \"\",\n+                \"executive_summary\": \"\",\n+                \"detailed_metrics\": {}\n             }\n     \n-    def _load_data(self, state: DataCleaningState) -> DataCleaningState:\n+    def _load_data(self, state: DataCleaningState) -> Dict[str, Any]:\n         \"\"\"Load data from source\"\"\"\n         logger.info(\"Loading data from source\")\n         \n@@ -190,293 +148,239 @@ def _load_data(self, state: DataCleaningState) -> DataCleaningState:\n             elif data_source.endswith('.json'):\n                 data_format = \"json\"\n             else:\n-                # Assume it's raw data content\n                 data_format = \"raw\"\n             \n             # Load data based on format\n             if data_format in [\"csv\", \"excel\", \"json\"]:\n-                # Load from file\n                 with open(data_source, 'r', encoding='utf-8') as f:\n                     raw_data = f.read()\n             else:\n-                # Use as raw data\n                 raw_data = data_source\n             \n-            # Update state\n-            state[\"data_format\"] = data_format\n-            state[\"raw_data\"] = raw_data\n-            state = update_state_phase(state, \"data_loaded\", 10.0)\n-            \n             logger.info(f\"Data loaded successfully: {len(raw_data)} characters\")\n             \n+            return {\n+                \"data_format\": data_format,\n+                \"raw_data\": raw_data,\n+                \"current_phase\": \"data_loaded\",\n+                \"progress_percentage\": 10.0\n+            }\n+            \n         except Exception as e:\n             logger.error(f\"Error loading data: {str(e)}\")\n-            state[\"error_log\"].append({\n+            error_entry = {\n                 \"phase\": \"load_data\",\n                 \"error\": str(e),\n                 \"timestamp\": datetime.now()\n-            })\n-            state = update_state_phase(state, \"error\", 0.0)\n-        \n-        return state\n+            }\n+            \n+            return {\n+                \"current_phase\": \"error\",\n+                \"progress_percentage\": 0.0,\n+                \"error_log\": state.get(\"error_log\", []) + [error_entry]\n+            }\n     \n-    def _analyze_data(self, state: DataCleaningState) -> DataCleaningState:\n+    def _analyze_data(self, state: DataCleaningState) -> Dict[str, Any]:\n         \"\"\"Analyze data quality\"\"\"\n         logger.info(\"Analyzing data quality\")\n         \n         try:\n-            # Call data analysis agent\n             analysis_result = self.analysis_agent.analyze_data_quality(\n                 data=state[\"raw_data\"],\n                 user_requirements=state[\"user_requirements\"]\n             )\n             \n-            # Update state with analysis results\n-            state[\"agent_results\"][\"analysis\"] = analysis_result\n-            state[\"quality_issues\"] = analysis_result.get(\"quality_issues\", [])\n-            state[\"quality_metrics\"] = analysis_result.get(\"quality_metrics\", {})\n-            state[\"data_statistics\"] = analysis_result.get(\"basic_statistics\", {})\n+            logger.info(f\"Data analysis completed: {len(analysis_result.get('quality_issues', []))} issues found\")\n             \n-            state = update_state_phase(state, \"analysis_completed\", 30.0)\n+            agent_results = state.get(\"agent_results\", {})\n+            agent_results[\"analysis\"] = analysis_result\n             \n-            logger.info(f\"Data analysis completed: {len(state['quality_issues'])} issues found\")\n+            return {\n+                \"agent_results\": agent_results,\n+                \"quality_issues\": analysis_result.get(\"quality_issues\", []),\n+                \"quality_metrics\": analysis_result.get(\"quality_metrics\", {}),\n+                \"data_statistics\": analysis_result.get(\"basic_statistics\", {}),\n+                \"current_phase\": \"analysis_completed\",\n+                \"progress_percentage\": 30.0\n+            }\n             \n         except Exception as e:\n             logger.error(f\"Error in data analysis: {str(e)}\")\n-            state[\"error_log\"].append({\n+            error_entry = {\n                 \"phase\": \"analyze_data\",\n                 \"error\": str(e),\n                 \"timestamp\": datetime.now()\n-            })\n-            state[\"retry_count\"] += 1\n-        \n-        return state\n+            }\n+            \n+            return {\n+                \"current_phase\": \"error\",\n+                \"progress_percentage\": 0.0,\n+                \"error_log\": state.get(\"error_log\", []) + [error_entry]\n+            }\n     \n-    def _plan_cleaning(self, state: DataCleaningState) -> DataCleaningState:\n+    def _plan_cleaning(self, state: DataCleaningState) -> Dict[str, Any]:\n         \"\"\"Plan cleaning operations\"\"\"\n         logger.info(\"Planning cleaning operations\")\n         \n         try:\n-            # Generate cleaning plan based on analysis results\n-            quality_issues = state[\"quality_issues\"]\n-            user_requirements = state[\"user_requirements\"]\n-            \n-            # Create execution plan\n-            execution_plan = []\n-            \n-            for issue in quality_issues:\n-                operation = {\n-                    \"operation_id\": str(uuid.uuid4()),\n-                    \"issue_type\": issue.get(\"type\"),\n-                    \"description\": issue.get(\"description\"),\n-                    \"affected_columns\": issue.get(\"affected_columns\", []),\n-                    \"severity\": issue.get(\"severity\", \"medium\"),\n-                    \"strategy\": self._determine_cleaning_strategy(issue),\n-                    \"priority\": self._calculate_priority(issue)\n-                }\n-                execution_plan.append(operation)\n-            \n-            # Sort by priority\n-            execution_plan.sort(key=lambda x: x[\"priority\"], reverse=True)\n+            # Simple cleaning plan for cattle data\n+            cleaning_plan = {\n+                \"operations\": [\n+                    {\n+                        \"type\": \"outlier_removal\",\n+                        \"description\": \"Remove extreme weight outliers\",\n+                        \"strategy\": \"remove_outliers\"\n+                    },\n+                    {\n+                        \"type\": \"manual_review\",\n+                        \"description\": \"Flag questionable weights for review\",\n+                        \"strategy\": \"manual_review\"\n+                    },\n+                    {\n+                        \"type\": \"label_correction\",\n+                        \"description\": \"Correct ready_to_load labels\",\n+                        \"strategy\": \"correct_labels\"\n+                    }\n+                ]\n+            }\n             \n-            state[\"execution_plan\"] = execution_plan\n-            state = update_state_phase(state, \"planning_completed\", 40.0)\n+            logger.info(f\"Cleaning plan created with {len(cleaning_plan['operations'])} operations\")\n             \n-            logger.info(f\"Cleaning plan created: {len(execution_plan)} operations\")\n+            return {\n+                \"execution_plan\": cleaning_plan[\"operations\"],\n+                \"current_phase\": \"planning_completed\",\n+                \"progress_percentage\": 50.0\n+            }\n             \n         except Exception as e:\n             logger.error(f\"Error in planning: {str(e)}\")\n-            state[\"error_log\"].append({\n+            error_entry = {\n                 \"phase\": \"plan_cleaning\",\n                 \"error\": str(e),\n                 \"timestamp\": datetime.now()\n-            })\n-        \n-        return state\n+            }\n+            \n+            return {\n+                \"current_phase\": \"error\",\n+                \"progress_percentage\": 0.0,\n+                \"error_log\": state.get(\"error_log\", []) + [error_entry]\n+            }\n     \n-    def _execute_cleaning(self, state: DataCleaningState) -> DataCleaningState:\n+    def _execute_cleaning(self, state: DataCleaningState) -> Dict[str, Any]:\n         \"\"\"Execute cleaning operations\"\"\"\n         logger.info(\"Executing cleaning operations\")\n         \n         try:\n-            # Call data cleaning agent\n-            cleaning_result = self.cleaning_agent.execute_cleaning_plan(\n+            cleaning_result = self.cleaning_agent.clean_data(\n                 data=state[\"raw_data\"],\n-                execution_plan=state[\"execution_plan\"],\n-                config=state.get(\"cleaning_config\", {})\n+                cleaning_plan={\"operations\": state[\"execution_plan\"]},\n+                user_requirements=state[\"user_requirements\"]\n             )\n             \n-            # Update state with cleaning results\n-            state[\"agent_results\"][\"cleaning\"] = cleaning_result\n-            state[\"processed_data\"] = cleaning_result.get(\"cleaned_data\")\n-            state[\"completed_tasks\"] = cleaning_result.get(\"completed_operations\", [])\n+            logger.info(\"Data cleaning completed successfully\")\n             \n-            state = update_state_phase(state, \"cleaning_completed\", 70.0)\n+            agent_results = state.get(\"agent_results\", {})\n+            agent_results[\"cleaning\"] = cleaning_result\n             \n-            logger.info(\"Data cleaning completed successfully\")\n+            return {\n+                \"agent_results\": agent_results,\n+                \"processed_data\": cleaning_result.get(\"cleaned_data\"),\n+                \"current_phase\": \"cleaning_completed\",\n+                \"progress_percentage\": 70.0\n+            }\n             \n         except Exception as e:\n-            logger.error(f\"Error in cleaning execution: {str(e)}\")\n-            state[\"error_log\"].append({\n+            logger.error(f\"Error in cleaning: {str(e)}\")\n+            error_entry = {\n                 \"phase\": \"execute_cleaning\",\n                 \"error\": str(e),\n                 \"timestamp\": datetime.now()\n-            })\n-            state[\"retry_count\"] += 1\n-        \n-        return state\n+            }\n+            \n+            return {\n+                \"current_phase\": \"error\",\n+                \"progress_percentage\": 0.0,\n+                \"error_log\": state.get(\"error_log\", []) + [error_entry]\n+            }\n     \n-    def _validate_results(self, state: DataCleaningState) -> DataCleaningState:\n+    def _validate_results(self, state: DataCleaningState) -> Dict[str, Any]:\n         \"\"\"Validate cleaning results\"\"\"\n         logger.info(\"Validating cleaning results\")\n         \n         try:\n-            # Call quality validation agent\n-            validation_result = self.validation_agent.validate_cleaning_results(\n+            validation_result = self.validation_agent.validate_quality(\n                 original_data=state[\"raw_data\"],\n                 cleaned_data=state[\"processed_data\"],\n-                cleaning_log=state[\"agent_results\"].get(\"cleaning\", {})\n+                cleaning_log=state[\"agent_results\"].get(\"cleaning\", {}),\n+                user_requirements=state[\"user_requirements\"]\n             )\n             \n-            # Update state with validation results\n-            state[\"agent_results\"][\"validation\"] = validation_result\n-            state[\"validation_results\"] = validation_result\n+            logger.info(\"Data validation completed successfully\")\n             \n-            # Update quality metrics\n-            if \"quality_scores\" in validation_result:\n-                state[\"quality_metrics\"].update(validation_result[\"quality_scores\"])\n+            agent_results = state.get(\"agent_results\", {})\n+            agent_results[\"validation\"] = validation_result\n             \n-            state = update_state_phase(state, \"validation_completed\", 85.0)\n-            \n-            logger.info(\"Results validation completed\")\n+            return {\n+                \"agent_results\": agent_results,\n+                \"validation_results\": validation_result,\n+                \"current_phase\": \"validation_completed\",\n+                \"progress_percentage\": 90.0\n+            }\n             \n         except Exception as e:\n             logger.error(f\"Error in validation: {str(e)}\")\n-            state[\"error_log\"].append({\n+            error_entry = {\n                 \"phase\": \"validate_results\",\n                 \"error\": str(e),\n                 \"timestamp\": datetime.now()\n-            })\n-        \n-        return state\n+            }\n+            \n+            return {\n+                \"current_phase\": \"error\",\n+                \"progress_percentage\": 0.0,\n+                \"error_log\": state.get(\"error_log\", []) + [error_entry]\n+            }\n     \n-    def _aggregate_results(self, state: DataCleaningState) -> DataCleaningState:\n-        \"\"\"Aggregate final results\"\"\"\n-        logger.info(\"Aggregating final results\")\n+    def _aggregate_results(self, state: DataCleaningState) -> Dict[str, Any]:\n+        \"\"\"Aggregate all results\"\"\"\n+        logger.info(\"Aggregating results\")\n         \n         try:\n-            # Call result aggregation agent\n             aggregation_result = self.aggregation_agent.aggregate_results(\n                 analysis_results=state[\"agent_results\"].get(\"analysis\", {}),\n                 cleaning_results=state[\"agent_results\"].get(\"cleaning\", {}),\n                 validation_results=state[\"agent_results\"].get(\"validation\", {})\n             )\n             \n-            # Update state with aggregated results\n-            state[\"agent_results\"][\"aggregation\"] = aggregation_result\n+            logger.info(\"Results aggregation completed successfully\")\n             \n-            state = update_state_phase(state, \"completed\", 100.0)\n+            agent_results = state.get(\"agent_results\", {})\n+            agent_results[\"aggregation\"] = aggregation_result\n             \n-            logger.info(\"Results aggregation completed\")\n+            return {\n+                \"agent_results\": agent_results,\n+                \"current_phase\": \"completed\",\n+                \"progress_percentage\": 100.0\n+            }\n             \n         except Exception as e:\n             logger.error(f\"Error in aggregation: {str(e)}\")\n-            state[\"error_log\"].append({\n+            error_entry = {\n                 \"phase\": \"aggregate_results\",\n                 \"error\": str(e),\n                 \"timestamp\": datetime.now()\n-            })\n-        \n-        return state\n-    \n-    def _handle_error(self, state: DataCleaningState) -> DataCleaningState:\n-        \"\"\"Handle errors\"\"\"\n-        logger.error(\"Handling error in workflow\")\n-        \n-        state = update_state_phase(state, \"error\", state[\"progress_percentage\"])\n-        \n-        # Add error summary\n-        if state[\"error_log\"]:\n-            latest_error = state[\"error_log\"][-1]\n-            logger.error(f\"Latest error: {latest_error}\")\n-        \n-        return state\n-    \n-    # Conditional edge functions\n-    def _should_continue_after_load(self, state: DataCleaningState) -> str:\n-        \"\"\"Check if should continue after data loading\"\"\"\n-        if state[\"current_phase\"] == \"error\":\n-            return \"error\"\n-        return \"continue\"\n-    \n-    def _should_continue_after_analysis(self, state: DataCleaningState) -> str:\n-        \"\"\"Check if should continue after analysis\"\"\"\n-        if state[\"current_phase\"] == \"error\":\n-            return \"error\"\n-        \n-        # Check if retry is needed\n-        if state[\"retry_count\"] > 0 and state[\"retry_count\"] < self.settings.agent.retry_attempts:\n-            return \"retry\"\n-        \n-        return \"continue\"\n-    \n-    def _should_continue_after_cleaning(self, state: DataCleaningState) -> str:\n-        \"\"\"Check if should continue after cleaning\"\"\"\n-        if state[\"current_phase\"] == \"error\":\n-            return \"error\"\n-        \n-        # Check if retry is needed\n-        if state[\"retry_count\"] > 0 and state[\"retry_count\"] < self.settings.agent.retry_attempts:\n-            return \"retry\"\n-        \n-        return \"continue\"\n-    \n-    def _should_continue_after_validation(self, state: DataCleaningState) -> str:\n-        \"\"\"Check if should continue after validation\"\"\"\n-        if state[\"current_phase\"] == \"error\":\n-            return \"error\"\n-        \n-        # Check if cleaning needs to be retried based on validation results\n-        validation_results = state.get(\"validation_results\", {})\n-        overall_score = validation_results.get(\"overall_score\", 0)\n-        \n-        if overall_score < 0.7 and state[\"retry_count\"] < self.settings.agent.retry_attempts:\n-            return \"retry_cleaning\"\n-        \n-        return \"continue\"\n-    \n-    # Helper functions\n-    def _determine_cleaning_strategy(self, issue: Dict) -> str:\n-        \"\"\"Determine cleaning strategy for an issue\"\"\"\n-        issue_type = issue.get(\"type\", \"\")\n-        \n-        strategy_mapping = {\n-            \"missing_values\": \"fill_mean\",\n-            \"duplicates\": \"remove_duplicates\",\n-            \"outliers\": \"flag_outliers\",\n-            \"format_inconsistency\": \"standardize_format\",\n-            \"type_inconsistency\": \"convert_type\"\n-        }\n-        \n-        return strategy_mapping.get(issue_type, \"manual_review\")\n-    \n-    def _calculate_priority(self, issue: Dict) -> int:\n-        \"\"\"Calculate priority for an issue\"\"\"\n-        severity = issue.get(\"severity\", \"medium\")\n-        \n-        priority_mapping = {\n-            \"high\": 3,\n-            \"medium\": 2,\n-            \"low\": 1\n-        }\n-        \n-        return priority_mapping.get(severity, 2)\n+            }\n+            \n+            return {\n+                \"current_phase\": \"error\",\n+                \"progress_percentage\": 0.0,\n+                \"error_log\": state.get(\"error_log\", []) + [error_entry]\n+            }\n \n \n-# Convenience function for direct usage\n-async def process_cleaning_request(user_requirements: str, \n-                                 data_source: str) -> Dict[str, Any]:\n-    \"\"\"Process a data cleaning request\"\"\"\n+# Global function for external use\n+async def process_cleaning_request(user_requirements: str, data_source: str) -> Dict[str, Any]:\n+    \"\"\"Process cleaning request using main controller\"\"\"\n     controller = MainControllerAgent()\n-    return await controller.process_data(user_requirements, data_source)\n+    return await controller.process_cleaning_request(user_requirements, data_source)\n ",
      "patch_lines": [
        "@@ -1,24 +1,21 @@\n",
        " \"\"\"\n",
        " Main Controller Agent Implementation\n",
        " \n",
        "-This module implements the main controller agent that orchestrates the entire\n",
        "-data cleaning workflow using LangGraph's state graph architecture.\n",
        "+This module implements the central orchestrator for the data cleaning system.\n",
        "+It coordinates all other agents and manages the overall workflow.\n",
        " \"\"\"\n",
        " \n",
        "-import asyncio\n",
        " import uuid\n",
        "+import asyncio\n",
        " from datetime import datetime\n",
        " from typing import Dict, Any, List, Optional\n",
        "-from loguru import logger\n",
        "+from pathlib import Path\n",
        " \n",
        "-from langgraph.graph import StateGraph, END\n",
        " from langchain_openai import ChatOpenAI\n",
        "+from langgraph.graph import StateGraph, END\n",
        "+from loguru import logger\n",
        " \n",
        "-from ..schemas.state import (\n",
        "-    DataCleaningState, \n",
        "-    create_initial_state, \n",
        "-    update_state_phase\n",
        "-)\n",
        "+from ..schemas.state import DataCleaningState, create_initial_state\n",
        " from ..config.settings import get_settings\n",
        " from .data_analysis_agent import DataAnalysisAgent\n",
        " from .data_cleaning_agent import DataCleaningAgent\n",
        "@@ -27,27 +24,26 @@\n",
        " \n",
        " \n",
        " class MainControllerAgent:\n",
        "-    \"\"\"Main Controller Agent Class\"\"\"\n",
        "+    \"\"\"Main controller for data cleaning workflow\"\"\"\n",
        "     \n",
        "     def __init__(self):\n",
        "+        \"\"\"Initialize the main controller\"\"\"\n",
        "         self.settings = get_settings()\n",
        "         self.session_id = str(uuid.uuid4())\n",
        "-        self.workflow = None\n",
        "-        self.llm = self._initialize_llm()\n",
        "         \n",
        "-        # Initialize specialized agents\n",
        "-        self.analysis_agent = DataAnalysisAgent(self.llm)\n",
        "-        self.cleaning_agent = DataCleaningAgent(self.llm)\n",
        "-        self.validation_agent = QualityValidationAgent(self.llm)\n",
        "-        self.aggregation_agent = ResultAggregationAgent(self.llm)\n",
        "+        # Initialize sub-agents\n",
        "+        self.analysis_agent = DataAnalysisAgent()\n",
        "+        self.cleaning_agent = DataCleaningAgent()\n",
        "+        self.validation_agent = QualityValidationAgent()\n",
        "+        self.aggregation_agent = ResultAggregationAgent()\n",
        "         \n",
        "         # Build workflow\n",
        "-        self._build_workflow()\n",
        "+        self.workflow = self._build_workflow()\n",
        "         \n",
        "         logger.info(\"Main Controller Agent initialized successfully\")\n",
        "     \n",
        "-    def _initialize_llm(self):\n",
        "-        \"\"\"Initialize LLM\"\"\"\n",
        "+    def _get_llm(self):\n",
        "+        \"\"\"Get configured LLM instance\"\"\"\n",
        "         llm_config = self.settings.get_llm_config()\n",
        "         \n",
        "         return ChatOpenAI(\n",
        "@@ -71,7 +67,6 @@ def _build_workflow(self):\n",
        "         workflow.add_node(\"execute_cleaning\", self._execute_cleaning)\n",
        "         workflow.add_node(\"validate_results\", self._validate_results)\n",
        "         workflow.add_node(\"aggregate_results\", self._aggregate_results)\n",
        "-        workflow.add_node(\"handle_error\", self._handle_error)\n",
        "         \n",
        "         # Set entry point\n",
        "         workflow.set_entry_point(\"load_data\")\n",
        "@@ -84,55 +79,10 @@ def _build_workflow(self):\n",
        "         workflow.add_edge(\"validate_results\", \"aggregate_results\")\n",
        "         workflow.add_edge(\"aggregate_results\", END)\n",
        "         \n",
        "-        # Add conditional edges for error handling\n",
        "-        workflow.add_conditional_edges(\n",
        "-            \"load_data\",\n",
        "-            self._should_continue_after_load,\n",
        "-            {\n",
        "-                \"continue\": \"analyze_data\",\n",
        "-                \"error\": \"handle_error\"\n",
        "-            }\n",
        "-        )\n",
        "-        \n",
        "-        workflow.add_conditional_edges(\n",
        "-            \"analyze_data\",\n",
        "-            self._should_continue_after_analysis,\n",
        "-            {\n",
        "-                \"continue\": \"plan_cleaning\",\n",
        "-                \"retry\": \"analyze_data\",\n",
        "-                \"error\": \"handle_error\"\n",
        "-            }\n",
        "-        )\n",
        "-        \n",
        "-        workflow.add_conditional_edges(\n",
        "-            \"execute_cleaning\",\n",
        "-            self._should_continue_after_cleaning,\n",
        "-            {\n",
        "-                \"continue\": \"validate_results\",\n",
        "-                \"retry\": \"execute_cleaning\",\n",
        "-                \"error\": \"handle_error\"\n",
        "-            }\n",
        "-        )\n",
        "-        \n",
        "-        workflow.add_conditional_edges(\n",
        "-            \"validate_results\",\n",
        "-            self._should_continue_after_validation,\n",
        "-            {\n",
        "-                \"continue\": \"aggregate_results\",\n",
        "-                \"retry_cleaning\": \"execute_cleaning\",\n",
        "-                \"error\": \"handle_error\"\n",
        "-            }\n",
        "-        )\n",
        "-        \n",
        "-        workflow.add_edge(\"handle_error\", END)\n",
        "-        \n",
        "-        # Compile workflow\n",
        "-        self.workflow = workflow.compile()\n",
        "-        \n",
        "-        logger.info(\"Workflow built successfully\")\n",
        "+        return workflow.compile()\n",
        "     \n",
        "-    async def process_data(self, user_requirements: str, \n",
        "-                          data_source: str) -> Dict[str, Any]:\n",
        "+    async def process_cleaning_request(self, user_requirements: str, \n",
        "+                                     data_source: str) -> Dict[str, Any]:\n",
        "         \"\"\"Process data cleaning request\"\"\"\n",
        "         try:\n",
        "             logger.info(f\"Starting data cleaning process for session: {self.session_id}\")\n",
        "@@ -144,9 +94,11 @@ async def process_data(self, user_requirements: str,\n",
        "                 data_source=data_source\n",
        "             )\n",
        "             \n",
        "-            # Execute workflow\n",
        "             start_time = datetime.now()\n",
        "+            \n",
        "+            # Execute workflow\n",
        "             final_state = await self.workflow.ainvoke(initial_state)\n",
        "+            \n",
        "             execution_time = (datetime.now() - start_time).total_seconds()\n",
        "             \n",
        "             # Prepare result\n",
        "@@ -157,6 +109,9 @@ async def process_data(self, user_requirements: str,\n",
        "                 \"results\": final_state.get(\"agent_results\", {}),\n",
        "                 \"quality_metrics\": final_state.get(\"quality_metrics\", {}),\n",
        "                 \"final_data\": final_state.get(\"processed_data\"),\n",
        "+                \"final_report\": final_state.get(\"agent_results\", {}).get(\"aggregation\", {}).get(\"final_report\", \"\"),\n",
        "+                \"executive_summary\": final_state.get(\"agent_results\", {}).get(\"aggregation\", {}).get(\"executive_summary\", \"\"),\n",
        "+                \"detailed_metrics\": final_state.get(\"agent_results\", {}).get(\"aggregation\", {}).get(\"detailed_metrics\", {}),\n",
        "                 \"error\": final_state.get(\"error_log\", [])[-1] if final_state.get(\"error_log\") else None\n",
        "             }\n",
        "             \n",
        "@@ -172,10 +127,13 @@ async def process_data(self, user_requirements: str,\n",
        "                 \"execution_time\": 0,\n",
        "                 \"results\": {},\n",
        "                 \"quality_metrics\": {},\n",
        "-                \"final_data\": None\n",
        "+                \"final_data\": None,\n",
        "+                \"final_report\": \"\",\n",
        "+                \"executive_summary\": \"\",\n",
        "+                \"detailed_metrics\": {}\n",
        "             }\n",
        "     \n",
        "-    def _load_data(self, state: DataCleaningState) -> DataCleaningState:\n",
        "+    def _load_data(self, state: DataCleaningState) -> Dict[str, Any]:\n",
        "         \"\"\"Load data from source\"\"\"\n",
        "         logger.info(\"Loading data from source\")\n",
        "         \n",
        "@@ -190,293 +148,239 @@ def _load_data(self, state: DataCleaningState) -> DataCleaningState:\n",
        "             elif data_source.endswith('.json'):\n",
        "                 data_format = \"json\"\n",
        "             else:\n",
        "-                # Assume it's raw data content\n",
        "                 data_format = \"raw\"\n",
        "             \n",
        "             # Load data based on format\n",
        "             if data_format in [\"csv\", \"excel\", \"json\"]:\n",
        "-                # Load from file\n",
        "                 with open(data_source, 'r', encoding='utf-8') as f:\n",
        "                     raw_data = f.read()\n",
        "             else:\n",
        "-                # Use as raw data\n",
        "                 raw_data = data_source\n",
        "             \n",
        "-            # Update state\n",
        "-            state[\"data_format\"] = data_format\n",
        "-            state[\"raw_data\"] = raw_data\n",
        "-            state = update_state_phase(state, \"data_loaded\", 10.0)\n",
        "-            \n",
        "             logger.info(f\"Data loaded successfully: {len(raw_data)} characters\")\n",
        "             \n",
        "+            return {\n",
        "+                \"data_format\": data_format,\n",
        "+                \"raw_data\": raw_data,\n",
        "+                \"current_phase\": \"data_loaded\",\n",
        "+                \"progress_percentage\": 10.0\n",
        "+            }\n",
        "+            \n",
        "         except Exception as e:\n",
        "             logger.error(f\"Error loading data: {str(e)}\")\n",
        "-            state[\"error_log\"].append({\n",
        "+            error_entry = {\n",
        "                 \"phase\": \"load_data\",\n",
        "                 \"error\": str(e),\n",
        "                 \"timestamp\": datetime.now()\n",
        "-            })\n",
        "-            state = update_state_phase(state, \"error\", 0.0)\n",
        "-        \n",
        "-        return state\n",
        "+            }\n",
        "+            \n",
        "+            return {\n",
        "+                \"current_phase\": \"error\",\n",
        "+                \"progress_percentage\": 0.0,\n",
        "+                \"error_log\": state.get(\"error_log\", []) + [error_entry]\n",
        "+            }\n",
        "     \n",
        "-    def _analyze_data(self, state: DataCleaningState) -> DataCleaningState:\n",
        "+    def _analyze_data(self, state: DataCleaningState) -> Dict[str, Any]:\n",
        "         \"\"\"Analyze data quality\"\"\"\n",
        "         logger.info(\"Analyzing data quality\")\n",
        "         \n",
        "         try:\n",
        "-            # Call data analysis agent\n",
        "             analysis_result = self.analysis_agent.analyze_data_quality(\n",
        "                 data=state[\"raw_data\"],\n",
        "                 user_requirements=state[\"user_requirements\"]\n",
        "             )\n",
        "             \n",
        "-            # Update state with analysis results\n",
        "-            state[\"agent_results\"][\"analysis\"] = analysis_result\n",
        "-            state[\"quality_issues\"] = analysis_result.get(\"quality_issues\", [])\n",
        "-            state[\"quality_metrics\"] = analysis_result.get(\"quality_metrics\", {})\n",
        "-            state[\"data_statistics\"] = analysis_result.get(\"basic_statistics\", {})\n",
        "+            logger.info(f\"Data analysis completed: {len(analysis_result.get('quality_issues', []))} issues found\")\n",
        "             \n",
        "-            state = update_state_phase(state, \"analysis_completed\", 30.0)\n",
        "+            agent_results = state.get(\"agent_results\", {})\n",
        "+            agent_results[\"analysis\"] = analysis_result\n",
        "             \n",
        "-            logger.info(f\"Data analysis completed: {len(state['quality_issues'])} issues found\")\n",
        "+            return {\n",
        "+                \"agent_results\": agent_results,\n",
        "+                \"quality_issues\": analysis_result.get(\"quality_issues\", []),\n",
        "+                \"quality_metrics\": analysis_result.get(\"quality_metrics\", {}),\n",
        "+                \"data_statistics\": analysis_result.get(\"basic_statistics\", {}),\n",
        "+                \"current_phase\": \"analysis_completed\",\n",
        "+                \"progress_percentage\": 30.0\n",
        "+            }\n",
        "             \n",
        "         except Exception as e:\n",
        "             logger.error(f\"Error in data analysis: {str(e)}\")\n",
        "-            state[\"error_log\"].append({\n",
        "+            error_entry = {\n",
        "                 \"phase\": \"analyze_data\",\n",
        "                 \"error\": str(e),\n",
        "                 \"timestamp\": datetime.now()\n",
        "-            })\n",
        "-            state[\"retry_count\"] += 1\n",
        "-        \n",
        "-        return state\n",
        "+            }\n",
        "+            \n",
        "+            return {\n",
        "+                \"current_phase\": \"error\",\n",
        "+                \"progress_percentage\": 0.0,\n",
        "+                \"error_log\": state.get(\"error_log\", []) + [error_entry]\n",
        "+            }\n",
        "     \n",
        "-    def _plan_cleaning(self, state: DataCleaningState) -> DataCleaningState:\n",
        "+    def _plan_cleaning(self, state: DataCleaningState) -> Dict[str, Any]:\n",
        "         \"\"\"Plan cleaning operations\"\"\"\n",
        "         logger.info(\"Planning cleaning operations\")\n",
        "         \n",
        "         try:\n",
        "-            # Generate cleaning plan based on analysis results\n",
        "-            quality_issues = state[\"quality_issues\"]\n",
        "-            user_requirements = state[\"user_requirements\"]\n",
        "-            \n",
        "-            # Create execution plan\n",
        "-            execution_plan = []\n",
        "-            \n",
        "-            for issue in quality_issues:\n",
        "-                operation = {\n",
        "-                    \"operation_id\": str(uuid.uuid4()),\n",
        "-                    \"issue_type\": issue.get(\"type\"),\n",
        "-                    \"description\": issue.get(\"description\"),\n",
        "-                    \"affected_columns\": issue.get(\"affected_columns\", []),\n",
        "-                    \"severity\": issue.get(\"severity\", \"medium\"),\n",
        "-                    \"strategy\": self._determine_cleaning_strategy(issue),\n",
        "-                    \"priority\": self._calculate_priority(issue)\n",
        "-                }\n",
        "-                execution_plan.append(operation)\n",
        "-            \n",
        "-            # Sort by priority\n",
        "-            execution_plan.sort(key=lambda x: x[\"priority\"], reverse=True)\n",
        "+            # Simple cleaning plan for cattle data\n",
        "+            cleaning_plan = {\n",
        "+                \"operations\": [\n",
        "+                    {\n",
        "+                        \"type\": \"outlier_removal\",\n",
        "+                        \"description\": \"Remove extreme weight outliers\",\n",
        "+                        \"strategy\": \"remove_outliers\"\n",
        "+                    },\n",
        "+                    {\n",
        "+                        \"type\": \"manual_review\",\n",
        "+                        \"description\": \"Flag questionable weights for review\",\n",
        "+                        \"strategy\": \"manual_review\"\n",
        "+                    },\n",
        "+                    {\n",
        "+                        \"type\": \"label_correction\",\n",
        "+                        \"description\": \"Correct ready_to_load labels\",\n",
        "+                        \"strategy\": \"correct_labels\"\n",
        "+                    }\n",
        "+                ]\n",
        "+            }\n",
        "             \n",
        "-            state[\"execution_plan\"] = execution_plan\n",
        "-            state = update_state_phase(state, \"planning_completed\", 40.0)\n",
        "+            logger.info(f\"Cleaning plan created with {len(cleaning_plan['operations'])} operations\")\n",
        "             \n",
        "-            logger.info(f\"Cleaning plan created: {len(execution_plan)} operations\")\n",
        "+            return {\n",
        "+                \"execution_plan\": cleaning_plan[\"operations\"],\n",
        "+                \"current_phase\": \"planning_completed\",\n",
        "+                \"progress_percentage\": 50.0\n",
        "+            }\n",
        "             \n",
        "         except Exception as e:\n",
        "             logger.error(f\"Error in planning: {str(e)}\")\n",
        "-            state[\"error_log\"].append({\n",
        "+            error_entry = {\n",
        "                 \"phase\": \"plan_cleaning\",\n",
        "                 \"error\": str(e),\n",
        "                 \"timestamp\": datetime.now()\n",
        "-            })\n",
        "-        \n",
        "-        return state\n",
        "+            }\n",
        "+            \n",
        "+            return {\n",
        "+                \"current_phase\": \"error\",\n",
        "+                \"progress_percentage\": 0.0,\n",
        "+                \"error_log\": state.get(\"error_log\", []) + [error_entry]\n",
        "+            }\n",
        "     \n",
        "-    def _execute_cleaning(self, state: DataCleaningState) -> DataCleaningState:\n",
        "+    def _execute_cleaning(self, state: DataCleaningState) -> Dict[str, Any]:\n",
        "         \"\"\"Execute cleaning operations\"\"\"\n",
        "         logger.info(\"Executing cleaning operations\")\n",
        "         \n",
        "         try:\n",
        "-            # Call data cleaning agent\n",
        "-            cleaning_result = self.cleaning_agent.execute_cleaning_plan(\n",
        "+            cleaning_result = self.cleaning_agent.clean_data(\n",
        "                 data=state[\"raw_data\"],\n",
        "-                execution_plan=state[\"execution_plan\"],\n",
        "-                config=state.get(\"cleaning_config\", {})\n",
        "+                cleaning_plan={\"operations\": state[\"execution_plan\"]},\n",
        "+                user_requirements=state[\"user_requirements\"]\n",
        "             )\n",
        "             \n",
        "-            # Update state with cleaning results\n",
        "-            state[\"agent_results\"][\"cleaning\"] = cleaning_result\n",
        "-            state[\"processed_data\"] = cleaning_result.get(\"cleaned_data\")\n",
        "-            state[\"completed_tasks\"] = cleaning_result.get(\"completed_operations\", [])\n",
        "+            logger.info(\"Data cleaning completed successfully\")\n",
        "             \n",
        "-            state = update_state_phase(state, \"cleaning_completed\", 70.0)\n",
        "+            agent_results = state.get(\"agent_results\", {})\n",
        "+            agent_results[\"cleaning\"] = cleaning_result\n",
        "             \n",
        "-            logger.info(\"Data cleaning completed successfully\")\n",
        "+            return {\n",
        "+                \"agent_results\": agent_results,\n",
        "+                \"processed_data\": cleaning_result.get(\"cleaned_data\"),\n",
        "+                \"current_phase\": \"cleaning_completed\",\n",
        "+                \"progress_percentage\": 70.0\n",
        "+            }\n",
        "             \n",
        "         except Exception as e:\n",
        "-            logger.error(f\"Error in cleaning execution: {str(e)}\")\n",
        "-            state[\"error_log\"].append({\n",
        "+            logger.error(f\"Error in cleaning: {str(e)}\")\n",
        "+            error_entry = {\n",
        "                 \"phase\": \"execute_cleaning\",\n",
        "                 \"error\": str(e),\n",
        "                 \"timestamp\": datetime.now()\n",
        "-            })\n",
        "-            state[\"retry_count\"] += 1\n",
        "-        \n",
        "-        return state\n",
        "+            }\n",
        "+            \n",
        "+            return {\n",
        "+                \"current_phase\": \"error\",\n",
        "+                \"progress_percentage\": 0.0,\n",
        "+                \"error_log\": state.get(\"error_log\", []) + [error_entry]\n",
        "+            }\n",
        "     \n",
        "-    def _validate_results(self, state: DataCleaningState) -> DataCleaningState:\n",
        "+    def _validate_results(self, state: DataCleaningState) -> Dict[str, Any]:\n",
        "         \"\"\"Validate cleaning results\"\"\"\n",
        "         logger.info(\"Validating cleaning results\")\n",
        "         \n",
        "         try:\n",
        "-            # Call quality validation agent\n",
        "-            validation_result = self.validation_agent.validate_cleaning_results(\n",
        "+            validation_result = self.validation_agent.validate_quality(\n",
        "                 original_data=state[\"raw_data\"],\n",
        "                 cleaned_data=state[\"processed_data\"],\n",
        "-                cleaning_log=state[\"agent_results\"].get(\"cleaning\", {})\n",
        "+                cleaning_log=state[\"agent_results\"].get(\"cleaning\", {}),\n",
        "+                user_requirements=state[\"user_requirements\"]\n",
        "             )\n",
        "             \n",
        "-            # Update state with validation results\n",
        "-            state[\"agent_results\"][\"validation\"] = validation_result\n",
        "-            state[\"validation_results\"] = validation_result\n",
        "+            logger.info(\"Data validation completed successfully\")\n",
        "             \n",
        "-            # Update quality metrics\n",
        "-            if \"quality_scores\" in validation_result:\n",
        "-                state[\"quality_metrics\"].update(validation_result[\"quality_scores\"])\n",
        "+            agent_results = state.get(\"agent_results\", {})\n",
        "+            agent_results[\"validation\"] = validation_result\n",
        "             \n",
        "-            state = update_state_phase(state, \"validation_completed\", 85.0)\n",
        "-            \n",
        "-            logger.info(\"Results validation completed\")\n",
        "+            return {\n",
        "+                \"agent_results\": agent_results,\n",
        "+                \"validation_results\": validation_result,\n",
        "+                \"current_phase\": \"validation_completed\",\n",
        "+                \"progress_percentage\": 90.0\n",
        "+            }\n",
        "             \n",
        "         except Exception as e:\n",
        "             logger.error(f\"Error in validation: {str(e)}\")\n",
        "-            state[\"error_log\"].append({\n",
        "+            error_entry = {\n",
        "                 \"phase\": \"validate_results\",\n",
        "                 \"error\": str(e),\n",
        "                 \"timestamp\": datetime.now()\n",
        "-            })\n",
        "-        \n",
        "-        return state\n",
        "+            }\n",
        "+            \n",
        "+            return {\n",
        "+                \"current_phase\": \"error\",\n",
        "+                \"progress_percentage\": 0.0,\n",
        "+                \"error_log\": state.get(\"error_log\", []) + [error_entry]\n",
        "+            }\n",
        "     \n",
        "-    def _aggregate_results(self, state: DataCleaningState) -> DataCleaningState:\n",
        "-        \"\"\"Aggregate final results\"\"\"\n",
        "-        logger.info(\"Aggregating final results\")\n",
        "+    def _aggregate_results(self, state: DataCleaningState) -> Dict[str, Any]:\n",
        "+        \"\"\"Aggregate all results\"\"\"\n",
        "+        logger.info(\"Aggregating results\")\n",
        "         \n",
        "         try:\n",
        "-            # Call result aggregation agent\n",
        "             aggregation_result = self.aggregation_agent.aggregate_results(\n",
        "                 analysis_results=state[\"agent_results\"].get(\"analysis\", {}),\n",
        "                 cleaning_results=state[\"agent_results\"].get(\"cleaning\", {}),\n",
        "                 validation_results=state[\"agent_results\"].get(\"validation\", {})\n",
        "             )\n",
        "             \n",
        "-            # Update state with aggregated results\n",
        "-            state[\"agent_results\"][\"aggregation\"] = aggregation_result\n",
        "+            logger.info(\"Results aggregation completed successfully\")\n",
        "             \n",
        "-            state = update_state_phase(state, \"completed\", 100.0)\n",
        "+            agent_results = state.get(\"agent_results\", {})\n",
        "+            agent_results[\"aggregation\"] = aggregation_result\n",
        "             \n",
        "-            logger.info(\"Results aggregation completed\")\n",
        "+            return {\n",
        "+                \"agent_results\": agent_results,\n",
        "+                \"current_phase\": \"completed\",\n",
        "+                \"progress_percentage\": 100.0\n",
        "+            }\n",
        "             \n",
        "         except Exception as e:\n",
        "             logger.error(f\"Error in aggregation: {str(e)}\")\n",
        "-            state[\"error_log\"].append({\n",
        "+            error_entry = {\n",
        "                 \"phase\": \"aggregate_results\",\n",
        "                 \"error\": str(e),\n",
        "                 \"timestamp\": datetime.now()\n",
        "-            })\n",
        "-        \n",
        "-        return state\n",
        "-    \n",
        "-    def _handle_error(self, state: DataCleaningState) -> DataCleaningState:\n",
        "-        \"\"\"Handle errors\"\"\"\n",
        "-        logger.error(\"Handling error in workflow\")\n",
        "-        \n",
        "-        state = update_state_phase(state, \"error\", state[\"progress_percentage\"])\n",
        "-        \n",
        "-        # Add error summary\n",
        "-        if state[\"error_log\"]:\n",
        "-            latest_error = state[\"error_log\"][-1]\n",
        "-            logger.error(f\"Latest error: {latest_error}\")\n",
        "-        \n",
        "-        return state\n",
        "-    \n",
        "-    # Conditional edge functions\n",
        "-    def _should_continue_after_load(self, state: DataCleaningState) -> str:\n",
        "-        \"\"\"Check if should continue after data loading\"\"\"\n",
        "-        if state[\"current_phase\"] == \"error\":\n",
        "-            return \"error\"\n",
        "-        return \"continue\"\n",
        "-    \n",
        "-    def _should_continue_after_analysis(self, state: DataCleaningState) -> str:\n",
        "-        \"\"\"Check if should continue after analysis\"\"\"\n",
        "-        if state[\"current_phase\"] == \"error\":\n",
        "-            return \"error\"\n",
        "-        \n",
        "-        # Check if retry is needed\n",
        "-        if state[\"retry_count\"] > 0 and state[\"retry_count\"] < self.settings.agent.retry_attempts:\n",
        "-            return \"retry\"\n",
        "-        \n",
        "-        return \"continue\"\n",
        "-    \n",
        "-    def _should_continue_after_cleaning(self, state: DataCleaningState) -> str:\n",
        "-        \"\"\"Check if should continue after cleaning\"\"\"\n",
        "-        if state[\"current_phase\"] == \"error\":\n",
        "-            return \"error\"\n",
        "-        \n",
        "-        # Check if retry is needed\n",
        "-        if state[\"retry_count\"] > 0 and state[\"retry_count\"] < self.settings.agent.retry_attempts:\n",
        "-            return \"retry\"\n",
        "-        \n",
        "-        return \"continue\"\n",
        "-    \n",
        "-    def _should_continue_after_validation(self, state: DataCleaningState) -> str:\n",
        "-        \"\"\"Check if should continue after validation\"\"\"\n",
        "-        if state[\"current_phase\"] == \"error\":\n",
        "-            return \"error\"\n",
        "-        \n",
        "-        # Check if cleaning needs to be retried based on validation results\n",
        "-        validation_results = state.get(\"validation_results\", {})\n",
        "-        overall_score = validation_results.get(\"overall_score\", 0)\n",
        "-        \n",
        "-        if overall_score < 0.7 and state[\"retry_count\"] < self.settings.agent.retry_attempts:\n",
        "-            return \"retry_cleaning\"\n",
        "-        \n",
        "-        return \"continue\"\n",
        "-    \n",
        "-    # Helper functions\n",
        "-    def _determine_cleaning_strategy(self, issue: Dict) -> str:\n",
        "-        \"\"\"Determine cleaning strategy for an issue\"\"\"\n",
        "-        issue_type = issue.get(\"type\", \"\")\n",
        "-        \n",
        "-        strategy_mapping = {\n",
        "-            \"missing_values\": \"fill_mean\",\n",
        "-            \"duplicates\": \"remove_duplicates\",\n",
        "-            \"outliers\": \"flag_outliers\",\n",
        "-            \"format_inconsistency\": \"standardize_format\",\n",
        "-            \"type_inconsistency\": \"convert_type\"\n",
        "-        }\n",
        "-        \n",
        "-        return strategy_mapping.get(issue_type, \"manual_review\")\n",
        "-    \n",
        "-    def _calculate_priority(self, issue: Dict) -> int:\n",
        "-        \"\"\"Calculate priority for an issue\"\"\"\n",
        "-        severity = issue.get(\"severity\", \"medium\")\n",
        "-        \n",
        "-        priority_mapping = {\n",
        "-            \"high\": 3,\n",
        "-            \"medium\": 2,\n",
        "-            \"low\": 1\n",
        "-        }\n",
        "-        \n",
        "-        return priority_mapping.get(severity, 2)\n",
        "+            }\n",
        "+            \n",
        "+            return {\n",
        "+                \"current_phase\": \"error\",\n",
        "+                \"progress_percentage\": 0.0,\n",
        "+                \"error_log\": state.get(\"error_log\", []) + [error_entry]\n",
        "+            }\n",
        " \n",
        " \n",
        "-# Convenience function for direct usage\n",
        "-async def process_cleaning_request(user_requirements: str, \n",
        "-                                 data_source: str) -> Dict[str, Any]:\n",
        "-    \"\"\"Process a data cleaning request\"\"\"\n",
        "+# Global function for external use\n",
        "+async def process_cleaning_request(user_requirements: str, data_source: str) -> Dict[str, Any]:\n",
        "+    \"\"\"Process cleaning request using main controller\"\"\"\n",
        "     controller = MainControllerAgent()\n",
        "-    return await controller.process_data(user_requirements, data_source)\n",
        "+    return await controller.process_cleaning_request(user_requirements, data_source)\n",
        " \n"
      ]
    },
    {
      "path": "src/agents/main_controller_backup.py",
      "status": "added",
      "additions": 547,
      "deletions": 0,
      "patch": "@@ -0,0 +1,547 @@\n+\"\"\"\n+Main Controller Agent Implementation\n+\n+This module implements the main controller agent that orchestrates the entire\n+data cleaning workflow using LangGraph's state graph architecture.\n+\"\"\"\n+\n+import asyncio\n+import uuid\n+from datetime import datetime\n+from typing import Dict, Any, List, Optional\n+from loguru import logger\n+\n+from langgraph.graph import StateGraph, END\n+from langchain_openai import ChatOpenAI\n+\n+from ..schemas.state import (\n+    DataCleaningState, \n+    create_initial_state, \n+    update_state_phase\n+)\n+from ..config.settings import get_settings\n+from .data_analysis_agent import DataAnalysisAgent\n+from .data_cleaning_agent import DataCleaningAgent\n+from .quality_validation_agent import QualityValidationAgent\n+from .result_aggregation_agent import ResultAggregationAgent\n+\n+\n+class MainControllerAgent:\n+    \"\"\"Main Controller Agent Class\"\"\"\n+    \n+    def __init__(self):\n+        self.settings = get_settings()\n+        self.session_id = str(uuid.uuid4())\n+        self.workflow = None\n+        self.llm = self._initialize_llm()\n+        \n+        # Initialize specialized agents\n+        self.analysis_agent = DataAnalysisAgent(self.llm)\n+        self.cleaning_agent = DataCleaningAgent(self.llm)\n+        self.validation_agent = QualityValidationAgent(self.llm)\n+        self.aggregation_agent = ResultAggregationAgent(self.llm)\n+        \n+        # Build workflow\n+        self._build_workflow()\n+        \n+        logger.info(\"Main Controller Agent initialized successfully\")\n+    \n+    def _initialize_llm(self):\n+        \"\"\"Initialize LLM\"\"\"\n+        llm_config = self.settings.get_llm_config()\n+        \n+        return ChatOpenAI(\n+            model=llm_config.model,\n+            api_key=llm_config.api_key,\n+            temperature=llm_config.temperature,\n+            max_tokens=llm_config.max_tokens,\n+            timeout=llm_config.timeout,\n+            max_retries=llm_config.max_retries\n+        )\n+    \n+    def _build_workflow(self):\n+        \"\"\"Build LangGraph workflow\"\"\"\n+        # Create state graph\n+        workflow = StateGraph(DataCleaningState)\n+        \n+        # Add nodes\n+        workflow.add_node(\"load_data\", self._load_data)\n+        workflow.add_node(\"analyze_data\", self._analyze_data)\n+        workflow.add_node(\"plan_cleaning\", self._plan_cleaning)\n+        workflow.add_node(\"execute_cleaning\", self._execute_cleaning)\n+        workflow.add_node(\"validate_results\", self._validate_results)\n+        workflow.add_node(\"aggregate_results\", self._aggregate_results)\n+        workflow.add_node(\"handle_error\", self._handle_error)\n+        \n+        # Set entry point\n+        workflow.set_entry_point(\"load_data\")\n+        \n+        # Add edges\n+        workflow.add_edge(\"load_data\", \"analyze_data\")\n+        workflow.add_edge(\"analyze_data\", \"plan_cleaning\")\n+        workflow.add_edge(\"plan_cleaning\", \"execute_cleaning\")\n+        workflow.add_edge(\"execute_cleaning\", \"validate_results\")\n+        workflow.add_edge(\"validate_results\", \"aggregate_results\")\n+        workflow.add_edge(\"aggregate_results\", END)\n+        \n+        # Add conditional edges for error handling\n+        workflow.add_conditional_edges(\n+            \"load_data\",\n+            self._should_continue_after_load,\n+            {\n+                \"continue\": \"analyze_data\",\n+                \"error\": \"handle_error\"\n+            }\n+        )\n+        \n+        workflow.add_conditional_edges(\n+            \"analyze_data\",\n+            self._should_continue_after_analysis,\n+            {\n+                \"continue\": \"plan_cleaning\",\n+                \"retry\": \"analyze_data\",\n+                \"error\": \"handle_error\"\n+            }\n+        )\n+        \n+        workflow.add_conditional_edges(\n+            \"execute_cleaning\",\n+            self._should_continue_after_cleaning,\n+            {\n+                \"continue\": \"validate_results\",\n+                \"retry\": \"execute_cleaning\",\n+                \"error\": \"handle_error\"\n+            }\n+        )\n+        \n+        workflow.add_conditional_edges(\n+            \"validate_results\",\n+            self._should_continue_after_validation,\n+            {\n+                \"continue\": \"aggregate_results\",\n+                \"retry_cleaning\": \"execute_cleaning\",\n+                \"error\": \"handle_error\"\n+            }\n+        )\n+        \n+        workflow.add_edge(\"handle_error\", END)\n+        \n+        # Compile workflow\n+        self.workflow = workflow.compile()\n+        \n+        logger.info(\"Workflow built successfully\")\n+    \n+    async def process_data(self, user_requirements: str, \n+                          data_source: str) -> Dict[str, Any]:\n+        \"\"\"Process data cleaning request\"\"\"\n+        try:\n+            logger.info(f\"Starting data cleaning process for session: {self.session_id}\")\n+            \n+            # Create initial state\n+            initial_state = create_initial_state(\n+                session_id=self.session_id,\n+                user_requirements=user_requirements,\n+                data_source=data_source\n+            )\n+            \n+            # Execute workflow\n+            start_time = datetime.now()\n+            final_state = await self.workflow.ainvoke(initial_state)\n+            execution_time = (datetime.now() - start_time).total_seconds()\n+            \n+            # Prepare result\n+            result = {\n+                \"session_id\": self.session_id,\n+                \"status\": \"completed\" if final_state[\"current_phase\"] != \"error\" else \"failed\",\n+                \"execution_time\": execution_time,\n+                \"results\": final_state.get(\"agent_results\", {}),\n+                \"quality_metrics\": final_state.get(\"quality_metrics\", {}),\n+                \"final_data\": final_state.get(\"processed_data\"),\n+                \"error\": final_state.get(\"error_log\", [])[-1] if final_state.get(\"error_log\") else None\n+            }\n+            \n+            logger.info(f\"Data cleaning process completed: {result['status']}\")\n+            return result\n+            \n+        except Exception as e:\n+            logger.error(f\"Error in data cleaning process: {str(e)}\")\n+            return {\n+                \"session_id\": self.session_id,\n+                \"status\": \"failed\",\n+                \"error\": str(e),\n+                \"execution_time\": 0,\n+                \"results\": {},\n+                \"quality_metrics\": {},\n+                \"final_data\": None\n+            }\n+    \n+    def _load_data(self, state: DataCleaningState) -> Dict[str, Any]:\n+        \"\"\"Load data from source\"\"\"\n+        logger.info(\"Loading data from source\")\n+        \n+        try:\n+            data_source = state[\"data_source\"]\n+            \n+            # Determine data format\n+            if data_source.endswith('.csv'):\n+                data_format = \"csv\"\n+            elif data_source.endswith(('.xlsx', '.xls')):\n+                data_format = \"excel\"\n+            elif data_source.endswith('.json'):\n+                data_format = \"json\"\n+            else:\n+                # Assume it's raw data content\n+                data_format = \"raw\"\n+            \n+            # Load data based on format\n+            if data_format in [\"csv\", \"excel\", \"json\"]:\n+                # Load from file\n+                with open(data_source, 'r', encoding='utf-8') as f:\n+                    raw_data = f.read()\n+            else:\n+                # Use as raw data\n+                raw_data = data_source\n+            \n+            logger.info(f\"Data loaded successfully: {len(raw_data)} characters\")\n+            \n+            # Return only the fields we want to update\n+            return {\n+                \"data_format\": data_format,\n+                \"raw_data\": raw_data,\n+                \"current_phase\": \"data_loaded\",\n+                \"progress_percentage\": 10.0\n+            }\n+            \n+        except Exception as e:\n+            logger.error(f\"Error loading data: {str(e)}\")\n+            error_entry = {\n+                \"phase\": \"load_data\",\n+                \"error\": str(e),\n+                \"timestamp\": datetime.now()\n+            }\n+            \n+            return {\n+                \"current_phase\": \"error\",\n+                \"progress_percentage\": 0.0,\n+                \"error_log\": state.get(\"error_log\", []) + [error_entry]\n+            }\n+    \n+    def _analyze_data(self, state: DataCleaningState) -> Dict[str, Any]:\n+        \"\"\"Analyze data quality\"\"\"\n+        logger.info(\"Analyzing data quality\")\n+        \n+        try:\n+            # Call data analysis agent\n+            analysis_result = self.analysis_agent.analyze_data_quality(\n+                data=state[\"raw_data\"],\n+                user_requirements=state[\"user_requirements\"]\n+            )\n+            \n+            logger.info(f\"Data analysis completed: {len(analysis_result.get('quality_issues', []))} issues found\")\n+            \n+            # Return only the fields we want to update\n+            agent_results = state.get(\"agent_results\", {})\n+            agent_results[\"analysis\"] = analysis_result\n+            \n+            return {\n+                \"agent_results\": agent_results,\n+                \"quality_issues\": analysis_result.get(\"quality_issues\", []),\n+                \"quality_metrics\": analysis_result.get(\"quality_metrics\", {}),\n+                \"data_statistics\": analysis_result.get(\"basic_statistics\", {}),\n+                \"current_phase\": \"analysis_completed\",\n+                \"progress_percentage\": 30.0\n+            }\n+            \n+        except Exception as e:\n+            logger.error(f\"Error in data analysis: {str(e)}\")\n+            error_entry = {\n+                \"phase\": \"analyze_data\",\n+                \"error\": str(e),\n+                \"timestamp\": datetime.now()\n+            }\n+            \n+            return {\n+                \"current_phase\": \"error\",\n+                \"progress_percentage\": 0.0,\n+                \"error_log\": state.get(\"error_log\", []) + [error_entry]\n+            }\n+    \n+    def _plan_cleaning(self, state: DataCleaningState) -> Dict[str, Any]:\n+        \"\"\"Plan cleaning operations\"\"\"\n+        logger.info(\"Planning cleaning operations\")\n+        \n+        try:\n+            # Generate cleaning plan based on analysis\n+            analysis_result = state[\"agent_results\"].get(\"analysis\", {})\n+            \n+            # Simple cleaning plan for cattle data\n+            cleaning_plan = {\n+                \"operations\": [\n+                    {\n+                        \"type\": \"outlier_removal\",\n+                        \"description\": \"Remove extreme weight outliers\",\n+                        \"strategy\": \"remove_outliers\"\n+                    },\n+                    {\n+                        \"type\": \"manual_review\",\n+                        \"description\": \"Flag questionable weights for review\",\n+                        \"strategy\": \"manual_review\"\n+                    },\n+                    {\n+                        \"type\": \"label_correction\",\n+                        \"description\": \"Correct ready_to_load labels\",\n+                        \"strategy\": \"correct_labels\"\n+                    }\n+                ]\n+            }\n+            \n+            logger.info(f\"Cleaning plan created with {len(cleaning_plan['operations'])} operations\")\n+            \n+            return {\n+                \"execution_plan\": cleaning_plan[\"operations\"],\n+                \"current_phase\": \"planning_completed\",\n+                \"progress_percentage\": 50.0\n+            }\n+            \n+        except Exception as e:\n+            logger.error(f\"Error in planning: {str(e)}\")\n+            error_entry = {\n+                \"phase\": \"plan_cleaning\",\n+                \"error\": str(e),\n+                \"timestamp\": datetime.now()\n+            }\n+            \n+            return {\n+                \"current_phase\": \"error\",\n+                \"progress_percentage\": 0.0,\n+                \"error_log\": state.get(\"error_log\", []) + [error_entry]\n+            }\n+    \n+    def _plan_cleaning(self, state: DataCleaningState) -> DataCleaningState:\n+        \"\"\"Plan cleaning operations\"\"\"\n+        logger.info(\"Planning cleaning operations\")\n+        \n+        try:\n+            # Generate cleaning plan based on analysis results\n+            quality_issues = state[\"quality_issues\"]\n+            user_requirements = state[\"user_requirements\"]\n+            \n+            # Create execution plan\n+            execution_plan = []\n+            \n+            for issue in quality_issues:\n+                operation = {\n+                    \"operation_id\": str(uuid.uuid4()),\n+                    \"issue_type\": issue.get(\"type\"),\n+                    \"description\": issue.get(\"description\"),\n+                    \"affected_columns\": issue.get(\"affected_columns\", []),\n+                    \"severity\": issue.get(\"severity\", \"medium\"),\n+                    \"strategy\": self._determine_cleaning_strategy(issue),\n+                    \"priority\": self._calculate_priority(issue)\n+                }\n+                execution_plan.append(operation)\n+            \n+            # Sort by priority\n+            execution_plan.sort(key=lambda x: x[\"priority\"], reverse=True)\n+            \n+            state[\"execution_plan\"] = execution_plan\n+            state = update_state_phase(state, \"planning_completed\", 40.0)\n+            \n+            logger.info(f\"Cleaning plan created: {len(execution_plan)} operations\")\n+            \n+        except Exception as e:\n+            logger.error(f\"Error in planning: {str(e)}\")\n+            state[\"error_log\"].append({\n+                \"phase\": \"plan_cleaning\",\n+                \"error\": str(e),\n+                \"timestamp\": datetime.now()\n+            })\n+        \n+        return state\n+    \n+    def _execute_cleaning(self, state: DataCleaningState) -> DataCleaningState:\n+        \"\"\"Execute cleaning operations\"\"\"\n+        logger.info(\"Executing cleaning operations\")\n+        \n+        try:\n+            # Call data cleaning agent\n+            cleaning_result = self.cleaning_agent.execute_cleaning_plan(\n+                data=state[\"raw_data\"],\n+                execution_plan=state[\"execution_plan\"],\n+                config=state.get(\"cleaning_config\", {})\n+            )\n+            \n+            # Update state with cleaning results\n+            state[\"agent_results\"][\"cleaning\"] = cleaning_result\n+            state[\"processed_data\"] = cleaning_result.get(\"cleaned_data\")\n+            state[\"completed_tasks\"] = cleaning_result.get(\"completed_operations\", [])\n+            \n+            state = update_state_phase(state, \"cleaning_completed\", 70.0)\n+            \n+            logger.info(\"Data cleaning completed successfully\")\n+            \n+        except Exception as e:\n+            logger.error(f\"Error in cleaning execution: {str(e)}\")\n+            state[\"error_log\"].append({\n+                \"phase\": \"execute_cleaning\",\n+                \"error\": str(e),\n+                \"timestamp\": datetime.now()\n+            })\n+            state[\"retry_count\"] += 1\n+        \n+        return state\n+    \n+    def _validate_results(self, state: DataCleaningState) -> DataCleaningState:\n+        \"\"\"Validate cleaning results\"\"\"\n+        logger.info(\"Validating cleaning results\")\n+        \n+        try:\n+            # Call quality validation agent\n+            validation_result = self.validation_agent.validate_cleaning_results(\n+                original_data=state[\"raw_data\"],\n+                cleaned_data=state[\"processed_data\"],\n+                cleaning_log=state[\"agent_results\"].get(\"cleaning\", {})\n+            )\n+            \n+            # Update state with validation results\n+            state[\"agent_results\"][\"validation\"] = validation_result\n+            state[\"validation_results\"] = validation_result\n+            \n+            # Update quality metrics\n+            if \"quality_scores\" in validation_result:\n+                state[\"quality_metrics\"].update(validation_result[\"quality_scores\"])\n+            \n+            state = update_state_phase(state, \"validation_completed\", 85.0)\n+            \n+            logger.info(\"Results validation completed\")\n+            \n+        except Exception as e:\n+            logger.error(f\"Error in validation: {str(e)}\")\n+            state[\"error_log\"].append({\n+                \"phase\": \"validate_results\",\n+                \"error\": str(e),\n+                \"timestamp\": datetime.now()\n+            })\n+        \n+        return state\n+    \n+    def _aggregate_results(self, state: DataCleaningState) -> DataCleaningState:\n+        \"\"\"Aggregate final results\"\"\"\n+        logger.info(\"Aggregating final results\")\n+        \n+        try:\n+            # Call result aggregation agent\n+            aggregation_result = self.aggregation_agent.aggregate_results(\n+                analysis_results=state[\"agent_results\"].get(\"analysis\", {}),\n+                cleaning_results=state[\"agent_results\"].get(\"cleaning\", {}),\n+                validation_results=state[\"agent_results\"].get(\"validation\", {})\n+            )\n+            \n+            # Update state with aggregated results\n+            state[\"agent_results\"][\"aggregation\"] = aggregation_result\n+            \n+            state = update_state_phase(state, \"completed\", 100.0)\n+            \n+            logger.info(\"Results aggregation completed\")\n+            \n+        except Exception as e:\n+            logger.error(f\"Error in aggregation: {str(e)}\")\n+            state[\"error_log\"].append({\n+                \"phase\": \"aggregate_results\",\n+                \"error\": str(e),\n+                \"timestamp\": datetime.now()\n+            })\n+        \n+        return state\n+    \n+    def _handle_error(self, state: DataCleaningState) -> DataCleaningState:\n+        \"\"\"Handle errors\"\"\"\n+        logger.error(\"Handling error in workflow\")\n+        \n+        state = update_state_phase(state, \"error\", state[\"progress_percentage\"])\n+        \n+        # Add error summary\n+        if state[\"error_log\"]:\n+            latest_error = state[\"error_log\"][-1]\n+            logger.error(f\"Latest error: {latest_error}\")\n+        \n+        return state\n+    \n+    # Conditional edge functions\n+    def _should_continue_after_load(self, state: DataCleaningState) -> str:\n+        \"\"\"Check if should continue after data loading\"\"\"\n+        if state[\"current_phase\"] == \"error\":\n+            return \"error\"\n+        return \"continue\"\n+    \n+    def _should_continue_after_analysis(self, state: DataCleaningState) -> str:\n+        \"\"\"Check if should continue after analysis\"\"\"\n+        if state[\"current_phase\"] == \"error\":\n+            return \"error\"\n+        \n+        # Check if retry is needed\n+        if state[\"retry_count\"] > 0 and state[\"retry_count\"] < self.settings.agent.retry_attempts:\n+            return \"retry\"\n+        \n+        return \"continue\"\n+    \n+    def _should_continue_after_cleaning(self, state: DataCleaningState) -> str:\n+        \"\"\"Check if should continue after cleaning\"\"\"\n+        if state[\"current_phase\"] == \"error\":\n+            return \"error\"\n+        \n+        # Check if retry is needed\n+        if state[\"retry_count\"] > 0 and state[\"retry_count\"] < self.settings.agent.retry_attempts:\n+            return \"retry\"\n+        \n+        return \"continue\"\n+    \n+    def _should_continue_after_validation(self, state: DataCleaningState) -> str:\n+        \"\"\"Check if should continue after validation\"\"\"\n+        if state[\"current_phase\"] == \"error\":\n+            return \"error\"\n+        \n+        # Check if cleaning needs to be retried based on validation results\n+        validation_results = state.get(\"validation_results\", {})\n+        overall_score = validation_results.get(\"overall_score\", 0)\n+        \n+        if overall_score < 0.7 and state[\"retry_count\"] < self.settings.agent.retry_attempts:\n+            return \"retry_cleaning\"\n+        \n+        return \"continue\"\n+    \n+    # Helper functions\n+    def _determine_cleaning_strategy(self, issue: Dict) -> str:\n+        \"\"\"Determine cleaning strategy for an issue\"\"\"\n+        issue_type = issue.get(\"type\", \"\")\n+        \n+        strategy_mapping = {\n+            \"missing_values\": \"fill_mean\",\n+            \"duplicates\": \"remove_duplicates\",\n+            \"outliers\": \"flag_outliers\",\n+            \"format_inconsistency\": \"standardize_format\",\n+            \"type_inconsistency\": \"convert_type\"\n+        }\n+        \n+        return strategy_mapping.get(issue_type, \"manual_review\")\n+    \n+    def _calculate_priority(self, issue: Dict) -> int:\n+        \"\"\"Calculate priority for an issue\"\"\"\n+        severity = issue.get(\"severity\", \"medium\")\n+        \n+        priority_mapping = {\n+            \"high\": 3,\n+            \"medium\": 2,\n+            \"low\": 1\n+        }\n+        \n+        return priority_mapping.get(severity, 2)\n+\n+\n+# Convenience function for direct usage\n+async def process_cleaning_request(user_requirements: str, \n+                                 data_source: str) -> Dict[str, Any]:\n+    \"\"\"Process a data cleaning request\"\"\"\n+    controller = MainControllerAgent()\n+    return await controller.process_data(user_requirements, data_source)\n+",
      "patch_lines": [
        "@@ -0,0 +1,547 @@\n",
        "+\"\"\"\n",
        "+Main Controller Agent Implementation\n",
        "+\n",
        "+This module implements the main controller agent that orchestrates the entire\n",
        "+data cleaning workflow using LangGraph's state graph architecture.\n",
        "+\"\"\"\n",
        "+\n",
        "+import asyncio\n",
        "+import uuid\n",
        "+from datetime import datetime\n",
        "+from typing import Dict, Any, List, Optional\n",
        "+from loguru import logger\n",
        "+\n",
        "+from langgraph.graph import StateGraph, END\n",
        "+from langchain_openai import ChatOpenAI\n",
        "+\n",
        "+from ..schemas.state import (\n",
        "+    DataCleaningState, \n",
        "+    create_initial_state, \n",
        "+    update_state_phase\n",
        "+)\n",
        "+from ..config.settings import get_settings\n",
        "+from .data_analysis_agent import DataAnalysisAgent\n",
        "+from .data_cleaning_agent import DataCleaningAgent\n",
        "+from .quality_validation_agent import QualityValidationAgent\n",
        "+from .result_aggregation_agent import ResultAggregationAgent\n",
        "+\n",
        "+\n",
        "+class MainControllerAgent:\n",
        "+    \"\"\"Main Controller Agent Class\"\"\"\n",
        "+    \n",
        "+    def __init__(self):\n",
        "+        self.settings = get_settings()\n",
        "+        self.session_id = str(uuid.uuid4())\n",
        "+        self.workflow = None\n",
        "+        self.llm = self._initialize_llm()\n",
        "+        \n",
        "+        # Initialize specialized agents\n",
        "+        self.analysis_agent = DataAnalysisAgent(self.llm)\n",
        "+        self.cleaning_agent = DataCleaningAgent(self.llm)\n",
        "+        self.validation_agent = QualityValidationAgent(self.llm)\n",
        "+        self.aggregation_agent = ResultAggregationAgent(self.llm)\n",
        "+        \n",
        "+        # Build workflow\n",
        "+        self._build_workflow()\n",
        "+        \n",
        "+        logger.info(\"Main Controller Agent initialized successfully\")\n",
        "+    \n",
        "+    def _initialize_llm(self):\n",
        "+        \"\"\"Initialize LLM\"\"\"\n",
        "+        llm_config = self.settings.get_llm_config()\n",
        "+        \n",
        "+        return ChatOpenAI(\n",
        "+            model=llm_config.model,\n",
        "+            api_key=llm_config.api_key,\n",
        "+            temperature=llm_config.temperature,\n",
        "+            max_tokens=llm_config.max_tokens,\n",
        "+            timeout=llm_config.timeout,\n",
        "+            max_retries=llm_config.max_retries\n",
        "+        )\n",
        "+    \n",
        "+    def _build_workflow(self):\n",
        "+        \"\"\"Build LangGraph workflow\"\"\"\n",
        "+        # Create state graph\n",
        "+        workflow = StateGraph(DataCleaningState)\n",
        "+        \n",
        "+        # Add nodes\n",
        "+        workflow.add_node(\"load_data\", self._load_data)\n",
        "+        workflow.add_node(\"analyze_data\", self._analyze_data)\n",
        "+        workflow.add_node(\"plan_cleaning\", self._plan_cleaning)\n",
        "+        workflow.add_node(\"execute_cleaning\", self._execute_cleaning)\n",
        "+        workflow.add_node(\"validate_results\", self._validate_results)\n",
        "+        workflow.add_node(\"aggregate_results\", self._aggregate_results)\n",
        "+        workflow.add_node(\"handle_error\", self._handle_error)\n",
        "+        \n",
        "+        # Set entry point\n",
        "+        workflow.set_entry_point(\"load_data\")\n",
        "+        \n",
        "+        # Add edges\n",
        "+        workflow.add_edge(\"load_data\", \"analyze_data\")\n",
        "+        workflow.add_edge(\"analyze_data\", \"plan_cleaning\")\n",
        "+        workflow.add_edge(\"plan_cleaning\", \"execute_cleaning\")\n",
        "+        workflow.add_edge(\"execute_cleaning\", \"validate_results\")\n",
        "+        workflow.add_edge(\"validate_results\", \"aggregate_results\")\n",
        "+        workflow.add_edge(\"aggregate_results\", END)\n",
        "+        \n",
        "+        # Add conditional edges for error handling\n",
        "+        workflow.add_conditional_edges(\n",
        "+            \"load_data\",\n",
        "+            self._should_continue_after_load,\n",
        "+            {\n",
        "+                \"continue\": \"analyze_data\",\n",
        "+                \"error\": \"handle_error\"\n",
        "+            }\n",
        "+        )\n",
        "+        \n",
        "+        workflow.add_conditional_edges(\n",
        "+            \"analyze_data\",\n",
        "+            self._should_continue_after_analysis,\n",
        "+            {\n",
        "+                \"continue\": \"plan_cleaning\",\n",
        "+                \"retry\": \"analyze_data\",\n",
        "+                \"error\": \"handle_error\"\n",
        "+            }\n",
        "+        )\n",
        "+        \n",
        "+        workflow.add_conditional_edges(\n",
        "+            \"execute_cleaning\",\n",
        "+            self._should_continue_after_cleaning,\n",
        "+            {\n",
        "+                \"continue\": \"validate_results\",\n",
        "+                \"retry\": \"execute_cleaning\",\n",
        "+                \"error\": \"handle_error\"\n",
        "+            }\n",
        "+        )\n",
        "+        \n",
        "+        workflow.add_conditional_edges(\n",
        "+            \"validate_results\",\n",
        "+            self._should_continue_after_validation,\n",
        "+            {\n",
        "+                \"continue\": \"aggregate_results\",\n",
        "+                \"retry_cleaning\": \"execute_cleaning\",\n",
        "+                \"error\": \"handle_error\"\n",
        "+            }\n",
        "+        )\n",
        "+        \n",
        "+        workflow.add_edge(\"handle_error\", END)\n",
        "+        \n",
        "+        # Compile workflow\n",
        "+        self.workflow = workflow.compile()\n",
        "+        \n",
        "+        logger.info(\"Workflow built successfully\")\n",
        "+    \n",
        "+    async def process_data(self, user_requirements: str, \n",
        "+                          data_source: str) -> Dict[str, Any]:\n",
        "+        \"\"\"Process data cleaning request\"\"\"\n",
        "+        try:\n",
        "+            logger.info(f\"Starting data cleaning process for session: {self.session_id}\")\n",
        "+            \n",
        "+            # Create initial state\n",
        "+            initial_state = create_initial_state(\n",
        "+                session_id=self.session_id,\n",
        "+                user_requirements=user_requirements,\n",
        "+                data_source=data_source\n",
        "+            )\n",
        "+            \n",
        "+            # Execute workflow\n",
        "+            start_time = datetime.now()\n",
        "+            final_state = await self.workflow.ainvoke(initial_state)\n",
        "+            execution_time = (datetime.now() - start_time).total_seconds()\n",
        "+            \n",
        "+            # Prepare result\n",
        "+            result = {\n",
        "+                \"session_id\": self.session_id,\n",
        "+                \"status\": \"completed\" if final_state[\"current_phase\"] != \"error\" else \"failed\",\n",
        "+                \"execution_time\": execution_time,\n",
        "+                \"results\": final_state.get(\"agent_results\", {}),\n",
        "+                \"quality_metrics\": final_state.get(\"quality_metrics\", {}),\n",
        "+                \"final_data\": final_state.get(\"processed_data\"),\n",
        "+                \"error\": final_state.get(\"error_log\", [])[-1] if final_state.get(\"error_log\") else None\n",
        "+            }\n",
        "+            \n",
        "+            logger.info(f\"Data cleaning process completed: {result['status']}\")\n",
        "+            return result\n",
        "+            \n",
        "+        except Exception as e:\n",
        "+            logger.error(f\"Error in data cleaning process: {str(e)}\")\n",
        "+            return {\n",
        "+                \"session_id\": self.session_id,\n",
        "+                \"status\": \"failed\",\n",
        "+                \"error\": str(e),\n",
        "+                \"execution_time\": 0,\n",
        "+                \"results\": {},\n",
        "+                \"quality_metrics\": {},\n",
        "+                \"final_data\": None\n",
        "+            }\n",
        "+    \n",
        "+    def _load_data(self, state: DataCleaningState) -> Dict[str, Any]:\n",
        "+        \"\"\"Load data from source\"\"\"\n",
        "+        logger.info(\"Loading data from source\")\n",
        "+        \n",
        "+        try:\n",
        "+            data_source = state[\"data_source\"]\n",
        "+            \n",
        "+            # Determine data format\n",
        "+            if data_source.endswith('.csv'):\n",
        "+                data_format = \"csv\"\n",
        "+            elif data_source.endswith(('.xlsx', '.xls')):\n",
        "+                data_format = \"excel\"\n",
        "+            elif data_source.endswith('.json'):\n",
        "+                data_format = \"json\"\n",
        "+            else:\n",
        "+                # Assume it's raw data content\n",
        "+                data_format = \"raw\"\n",
        "+            \n",
        "+            # Load data based on format\n",
        "+            if data_format in [\"csv\", \"excel\", \"json\"]:\n",
        "+                # Load from file\n",
        "+                with open(data_source, 'r', encoding='utf-8') as f:\n",
        "+                    raw_data = f.read()\n",
        "+            else:\n",
        "+                # Use as raw data\n",
        "+                raw_data = data_source\n",
        "+            \n",
        "+            logger.info(f\"Data loaded successfully: {len(raw_data)} characters\")\n",
        "+            \n",
        "+            # Return only the fields we want to update\n",
        "+            return {\n",
        "+                \"data_format\": data_format,\n",
        "+                \"raw_data\": raw_data,\n",
        "+                \"current_phase\": \"data_loaded\",\n",
        "+                \"progress_percentage\": 10.0\n",
        "+            }\n",
        "+            \n",
        "+        except Exception as e:\n",
        "+            logger.error(f\"Error loading data: {str(e)}\")\n",
        "+            error_entry = {\n",
        "+                \"phase\": \"load_data\",\n",
        "+                \"error\": str(e),\n",
        "+                \"timestamp\": datetime.now()\n",
        "+            }\n",
        "+            \n",
        "+            return {\n",
        "+                \"current_phase\": \"error\",\n",
        "+                \"progress_percentage\": 0.0,\n",
        "+                \"error_log\": state.get(\"error_log\", []) + [error_entry]\n",
        "+            }\n",
        "+    \n",
        "+    def _analyze_data(self, state: DataCleaningState) -> Dict[str, Any]:\n",
        "+        \"\"\"Analyze data quality\"\"\"\n",
        "+        logger.info(\"Analyzing data quality\")\n",
        "+        \n",
        "+        try:\n",
        "+            # Call data analysis agent\n",
        "+            analysis_result = self.analysis_agent.analyze_data_quality(\n",
        "+                data=state[\"raw_data\"],\n",
        "+                user_requirements=state[\"user_requirements\"]\n",
        "+            )\n",
        "+            \n",
        "+            logger.info(f\"Data analysis completed: {len(analysis_result.get('quality_issues', []))} issues found\")\n",
        "+            \n",
        "+            # Return only the fields we want to update\n",
        "+            agent_results = state.get(\"agent_results\", {})\n",
        "+            agent_results[\"analysis\"] = analysis_result\n",
        "+            \n",
        "+            return {\n",
        "+                \"agent_results\": agent_results,\n",
        "+                \"quality_issues\": analysis_result.get(\"quality_issues\", []),\n",
        "+                \"quality_metrics\": analysis_result.get(\"quality_metrics\", {}),\n",
        "+                \"data_statistics\": analysis_result.get(\"basic_statistics\", {}),\n",
        "+                \"current_phase\": \"analysis_completed\",\n",
        "+                \"progress_percentage\": 30.0\n",
        "+            }\n",
        "+            \n",
        "+        except Exception as e:\n",
        "+            logger.error(f\"Error in data analysis: {str(e)}\")\n",
        "+            error_entry = {\n",
        "+                \"phase\": \"analyze_data\",\n",
        "+                \"error\": str(e),\n",
        "+                \"timestamp\": datetime.now()\n",
        "+            }\n",
        "+            \n",
        "+            return {\n",
        "+                \"current_phase\": \"error\",\n",
        "+                \"progress_percentage\": 0.0,\n",
        "+                \"error_log\": state.get(\"error_log\", []) + [error_entry]\n",
        "+            }\n",
        "+    \n",
        "+    def _plan_cleaning(self, state: DataCleaningState) -> Dict[str, Any]:\n",
        "+        \"\"\"Plan cleaning operations\"\"\"\n",
        "+        logger.info(\"Planning cleaning operations\")\n",
        "+        \n",
        "+        try:\n",
        "+            # Generate cleaning plan based on analysis\n",
        "+            analysis_result = state[\"agent_results\"].get(\"analysis\", {})\n",
        "+            \n",
        "+            # Simple cleaning plan for cattle data\n",
        "+            cleaning_plan = {\n",
        "+                \"operations\": [\n",
        "+                    {\n",
        "+                        \"type\": \"outlier_removal\",\n",
        "+                        \"description\": \"Remove extreme weight outliers\",\n",
        "+                        \"strategy\": \"remove_outliers\"\n",
        "+                    },\n",
        "+                    {\n",
        "+                        \"type\": \"manual_review\",\n",
        "+                        \"description\": \"Flag questionable weights for review\",\n",
        "+                        \"strategy\": \"manual_review\"\n",
        "+                    },\n",
        "+                    {\n",
        "+                        \"type\": \"label_correction\",\n",
        "+                        \"description\": \"Correct ready_to_load labels\",\n",
        "+                        \"strategy\": \"correct_labels\"\n",
        "+                    }\n",
        "+                ]\n",
        "+            }\n",
        "+            \n",
        "+            logger.info(f\"Cleaning plan created with {len(cleaning_plan['operations'])} operations\")\n",
        "+            \n",
        "+            return {\n",
        "+                \"execution_plan\": cleaning_plan[\"operations\"],\n",
        "+                \"current_phase\": \"planning_completed\",\n",
        "+                \"progress_percentage\": 50.0\n",
        "+            }\n",
        "+            \n",
        "+        except Exception as e:\n",
        "+            logger.error(f\"Error in planning: {str(e)}\")\n",
        "+            error_entry = {\n",
        "+                \"phase\": \"plan_cleaning\",\n",
        "+                \"error\": str(e),\n",
        "+                \"timestamp\": datetime.now()\n",
        "+            }\n",
        "+            \n",
        "+            return {\n",
        "+                \"current_phase\": \"error\",\n",
        "+                \"progress_percentage\": 0.0,\n",
        "+                \"error_log\": state.get(\"error_log\", []) + [error_entry]\n",
        "+            }\n",
        "+    \n",
        "+    def _plan_cleaning(self, state: DataCleaningState) -> DataCleaningState:\n",
        "+        \"\"\"Plan cleaning operations\"\"\"\n",
        "+        logger.info(\"Planning cleaning operations\")\n",
        "+        \n",
        "+        try:\n",
        "+            # Generate cleaning plan based on analysis results\n",
        "+            quality_issues = state[\"quality_issues\"]\n",
        "+            user_requirements = state[\"user_requirements\"]\n",
        "+            \n",
        "+            # Create execution plan\n",
        "+            execution_plan = []\n",
        "+            \n",
        "+            for issue in quality_issues:\n",
        "+                operation = {\n",
        "+                    \"operation_id\": str(uuid.uuid4()),\n",
        "+                    \"issue_type\": issue.get(\"type\"),\n",
        "+                    \"description\": issue.get(\"description\"),\n",
        "+                    \"affected_columns\": issue.get(\"affected_columns\", []),\n",
        "+                    \"severity\": issue.get(\"severity\", \"medium\"),\n",
        "+                    \"strategy\": self._determine_cleaning_strategy(issue),\n",
        "+                    \"priority\": self._calculate_priority(issue)\n",
        "+                }\n",
        "+                execution_plan.append(operation)\n",
        "+            \n",
        "+            # Sort by priority\n",
        "+            execution_plan.sort(key=lambda x: x[\"priority\"], reverse=True)\n",
        "+            \n",
        "+            state[\"execution_plan\"] = execution_plan\n",
        "+            state = update_state_phase(state, \"planning_completed\", 40.0)\n",
        "+            \n",
        "+            logger.info(f\"Cleaning plan created: {len(execution_plan)} operations\")\n",
        "+            \n",
        "+        except Exception as e:\n",
        "+            logger.error(f\"Error in planning: {str(e)}\")\n",
        "+            state[\"error_log\"].append({\n",
        "+                \"phase\": \"plan_cleaning\",\n",
        "+                \"error\": str(e),\n",
        "+                \"timestamp\": datetime.now()\n",
        "+            })\n",
        "+        \n",
        "+        return state\n",
        "+    \n",
        "+    def _execute_cleaning(self, state: DataCleaningState) -> DataCleaningState:\n",
        "+        \"\"\"Execute cleaning operations\"\"\"\n",
        "+        logger.info(\"Executing cleaning operations\")\n",
        "+        \n",
        "+        try:\n",
        "+            # Call data cleaning agent\n",
        "+            cleaning_result = self.cleaning_agent.execute_cleaning_plan(\n",
        "+                data=state[\"raw_data\"],\n",
        "+                execution_plan=state[\"execution_plan\"],\n",
        "+                config=state.get(\"cleaning_config\", {})\n",
        "+            )\n",
        "+            \n",
        "+            # Update state with cleaning results\n",
        "+            state[\"agent_results\"][\"cleaning\"] = cleaning_result\n",
        "+            state[\"processed_data\"] = cleaning_result.get(\"cleaned_data\")\n",
        "+            state[\"completed_tasks\"] = cleaning_result.get(\"completed_operations\", [])\n",
        "+            \n",
        "+            state = update_state_phase(state, \"cleaning_completed\", 70.0)\n",
        "+            \n",
        "+            logger.info(\"Data cleaning completed successfully\")\n",
        "+            \n",
        "+        except Exception as e:\n",
        "+            logger.error(f\"Error in cleaning execution: {str(e)}\")\n",
        "+            state[\"error_log\"].append({\n",
        "+                \"phase\": \"execute_cleaning\",\n",
        "+                \"error\": str(e),\n",
        "+                \"timestamp\": datetime.now()\n",
        "+            })\n",
        "+            state[\"retry_count\"] += 1\n",
        "+        \n",
        "+        return state\n",
        "+    \n",
        "+    def _validate_results(self, state: DataCleaningState) -> DataCleaningState:\n",
        "+        \"\"\"Validate cleaning results\"\"\"\n",
        "+        logger.info(\"Validating cleaning results\")\n",
        "+        \n",
        "+        try:\n",
        "+            # Call quality validation agent\n",
        "+            validation_result = self.validation_agent.validate_cleaning_results(\n",
        "+                original_data=state[\"raw_data\"],\n",
        "+                cleaned_data=state[\"processed_data\"],\n",
        "+                cleaning_log=state[\"agent_results\"].get(\"cleaning\", {})\n",
        "+            )\n",
        "+            \n",
        "+            # Update state with validation results\n",
        "+            state[\"agent_results\"][\"validation\"] = validation_result\n",
        "+            state[\"validation_results\"] = validation_result\n",
        "+            \n",
        "+            # Update quality metrics\n",
        "+            if \"quality_scores\" in validation_result:\n",
        "+                state[\"quality_metrics\"].update(validation_result[\"quality_scores\"])\n",
        "+            \n",
        "+            state = update_state_phase(state, \"validation_completed\", 85.0)\n",
        "+            \n",
        "+            logger.info(\"Results validation completed\")\n",
        "+            \n",
        "+        except Exception as e:\n",
        "+            logger.error(f\"Error in validation: {str(e)}\")\n",
        "+            state[\"error_log\"].append({\n",
        "+                \"phase\": \"validate_results\",\n",
        "+                \"error\": str(e),\n",
        "+                \"timestamp\": datetime.now()\n",
        "+            })\n",
        "+        \n",
        "+        return state\n",
        "+    \n",
        "+    def _aggregate_results(self, state: DataCleaningState) -> DataCleaningState:\n",
        "+        \"\"\"Aggregate final results\"\"\"\n",
        "+        logger.info(\"Aggregating final results\")\n",
        "+        \n",
        "+        try:\n",
        "+            # Call result aggregation agent\n",
        "+            aggregation_result = self.aggregation_agent.aggregate_results(\n",
        "+                analysis_results=state[\"agent_results\"].get(\"analysis\", {}),\n",
        "+                cleaning_results=state[\"agent_results\"].get(\"cleaning\", {}),\n",
        "+                validation_results=state[\"agent_results\"].get(\"validation\", {})\n",
        "+            )\n",
        "+            \n",
        "+            # Update state with aggregated results\n",
        "+            state[\"agent_results\"][\"aggregation\"] = aggregation_result\n",
        "+            \n",
        "+            state = update_state_phase(state, \"completed\", 100.0)\n",
        "+            \n",
        "+            logger.info(\"Results aggregation completed\")\n",
        "+            \n",
        "+        except Exception as e:\n",
        "+            logger.error(f\"Error in aggregation: {str(e)}\")\n",
        "+            state[\"error_log\"].append({\n",
        "+                \"phase\": \"aggregate_results\",\n",
        "+                \"error\": str(e),\n",
        "+                \"timestamp\": datetime.now()\n",
        "+            })\n",
        "+        \n",
        "+        return state\n",
        "+    \n",
        "+    def _handle_error(self, state: DataCleaningState) -> DataCleaningState:\n",
        "+        \"\"\"Handle errors\"\"\"\n",
        "+        logger.error(\"Handling error in workflow\")\n",
        "+        \n",
        "+        state = update_state_phase(state, \"error\", state[\"progress_percentage\"])\n",
        "+        \n",
        "+        # Add error summary\n",
        "+        if state[\"error_log\"]:\n",
        "+            latest_error = state[\"error_log\"][-1]\n",
        "+            logger.error(f\"Latest error: {latest_error}\")\n",
        "+        \n",
        "+        return state\n",
        "+    \n",
        "+    # Conditional edge functions\n",
        "+    def _should_continue_after_load(self, state: DataCleaningState) -> str:\n",
        "+        \"\"\"Check if should continue after data loading\"\"\"\n",
        "+        if state[\"current_phase\"] == \"error\":\n",
        "+            return \"error\"\n",
        "+        return \"continue\"\n",
        "+    \n",
        "+    def _should_continue_after_analysis(self, state: DataCleaningState) -> str:\n",
        "+        \"\"\"Check if should continue after analysis\"\"\"\n",
        "+        if state[\"current_phase\"] == \"error\":\n",
        "+            return \"error\"\n",
        "+        \n",
        "+        # Check if retry is needed\n",
        "+        if state[\"retry_count\"] > 0 and state[\"retry_count\"] < self.settings.agent.retry_attempts:\n",
        "+            return \"retry\"\n",
        "+        \n",
        "+        return \"continue\"\n",
        "+    \n",
        "+    def _should_continue_after_cleaning(self, state: DataCleaningState) -> str:\n",
        "+        \"\"\"Check if should continue after cleaning\"\"\"\n",
        "+        if state[\"current_phase\"] == \"error\":\n",
        "+            return \"error\"\n",
        "+        \n",
        "+        # Check if retry is needed\n",
        "+        if state[\"retry_count\"] > 0 and state[\"retry_count\"] < self.settings.agent.retry_attempts:\n",
        "+            return \"retry\"\n",
        "+        \n",
        "+        return \"continue\"\n",
        "+    \n",
        "+    def _should_continue_after_validation(self, state: DataCleaningState) -> str:\n",
        "+        \"\"\"Check if should continue after validation\"\"\"\n",
        "+        if state[\"current_phase\"] == \"error\":\n",
        "+            return \"error\"\n",
        "+        \n",
        "+        # Check if cleaning needs to be retried based on validation results\n",
        "+        validation_results = state.get(\"validation_results\", {})\n",
        "+        overall_score = validation_results.get(\"overall_score\", 0)\n",
        "+        \n",
        "+        if overall_score < 0.7 and state[\"retry_count\"] < self.settings.agent.retry_attempts:\n",
        "+            return \"retry_cleaning\"\n",
        "+        \n",
        "+        return \"continue\"\n",
        "+    \n",
        "+    # Helper functions\n",
        "+    def _determine_cleaning_strategy(self, issue: Dict) -> str:\n",
        "+        \"\"\"Determine cleaning strategy for an issue\"\"\"\n",
        "+        issue_type = issue.get(\"type\", \"\")\n",
        "+        \n",
        "+        strategy_mapping = {\n",
        "+            \"missing_values\": \"fill_mean\",\n",
        "+            \"duplicates\": \"remove_duplicates\",\n",
        "+            \"outliers\": \"flag_outliers\",\n",
        "+            \"format_inconsistency\": \"standardize_format\",\n",
        "+            \"type_inconsistency\": \"convert_type\"\n",
        "+        }\n",
        "+        \n",
        "+        return strategy_mapping.get(issue_type, \"manual_review\")\n",
        "+    \n",
        "+    def _calculate_priority(self, issue: Dict) -> int:\n",
        "+        \"\"\"Calculate priority for an issue\"\"\"\n",
        "+        severity = issue.get(\"severity\", \"medium\")\n",
        "+        \n",
        "+        priority_mapping = {\n",
        "+            \"high\": 3,\n",
        "+            \"medium\": 2,\n",
        "+            \"low\": 1\n",
        "+        }\n",
        "+        \n",
        "+        return priority_mapping.get(severity, 2)\n",
        "+\n",
        "+\n",
        "+# Convenience function for direct usage\n",
        "+async def process_cleaning_request(user_requirements: str, \n",
        "+                                 data_source: str) -> Dict[str, Any]:\n",
        "+    \"\"\"Process a data cleaning request\"\"\"\n",
        "+    controller = MainControllerAgent()\n",
        "+    return await controller.process_data(user_requirements, data_source)\n",
        "+\n"
      ]
    },
    {
      "path": "src/schemas/state.py",
      "status": "modified",
      "additions": 5,
      "deletions": 4,
      "patch": "@@ -5,14 +5,15 @@\n based on TypedDict to ensure type safety and data consistency.\n \"\"\"\n \n-from typing import TypedDict, List, Dict, Optional, Any\n+from typing import TypedDict, List, Dict, Optional, Any, Annotated\n from datetime import datetime\n from langchain_core.messages import BaseMessage\n+from langgraph.graph import add_messages\n \n \n class DataCleaningState(TypedDict):\n     \"\"\"Main data cleaning workflow state\"\"\"\n-    # Session information\n+    # Session information - only set once by main controller\n     session_id: str\n     user_id: Optional[str]\n     created_at: datetime\n@@ -37,8 +38,8 @@ class DataCleaningState(TypedDict):\n     pending_tasks: List[str]\n     progress_percentage: float\n     \n-    # Agent communication\n-    messages: List[BaseMessage]\n+    # Agent communication - use add_messages for concurrent updates\n+    messages: Annotated[List[BaseMessage], add_messages]\n     agent_results: Dict[str, Dict]\n     communication_log: List[Dict]\n     ",
      "patch_lines": [
        "@@ -5,14 +5,15 @@\n",
        " based on TypedDict to ensure type safety and data consistency.\n",
        " \"\"\"\n",
        " \n",
        "-from typing import TypedDict, List, Dict, Optional, Any\n",
        "+from typing import TypedDict, List, Dict, Optional, Any, Annotated\n",
        " from datetime import datetime\n",
        " from langchain_core.messages import BaseMessage\n",
        "+from langgraph.graph import add_messages\n",
        " \n",
        " \n",
        " class DataCleaningState(TypedDict):\n",
        "     \"\"\"Main data cleaning workflow state\"\"\"\n",
        "-    # Session information\n",
        "+    # Session information - only set once by main controller\n",
        "     session_id: str\n",
        "     user_id: Optional[str]\n",
        "     created_at: datetime\n",
        "@@ -37,8 +38,8 @@ class DataCleaningState(TypedDict):\n",
        "     pending_tasks: List[str]\n",
        "     progress_percentage: float\n",
        "     \n",
        "-    # Agent communication\n",
        "-    messages: List[BaseMessage]\n",
        "+    # Agent communication - use add_messages for concurrent updates\n",
        "+    messages: Annotated[List[BaseMessage], add_messages]\n",
        "     agent_results: Dict[str, Dict]\n",
        "     communication_log: List[Dict]\n",
        "     \n"
      ]
    }
  ]
}
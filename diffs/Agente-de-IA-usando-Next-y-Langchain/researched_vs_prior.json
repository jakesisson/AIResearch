{
  "project": "Research Data/Agente-de-IA-usando-Next-y-Langchain",
  "repo": "Lostovayne/Agente-de-IA-usando-Next-y-Langchain",
  "prior_commit": "bd049f6b662373f99f152ccbacebaea3b9936129",
  "researched_commit": "e7cd19cc538f3f5156dd717c457d73df6b6dae67",
  "compare_url": "https://github.com/Lostovayne/Agente-de-IA-usando-Next-y-Langchain/compare/bd049f6b662373f99f152ccbacebaea3b9936129...e7cd19cc538f3f5156dd717c457d73df6b6dae67",
  "ahead_by": 1,
  "behind_by": 0,
  "changed_files": [
    {
      "path": "lib/langgraph.ts",
      "status": "modified",
      "additions": 49,
      "deletions": 14,
      "patch": "@@ -1,12 +1,20 @@\n+import SYSTEM_MESSAGE from \"@/constants/systemMessage\";\n+import { AIMessage, SystemMessage, trimMessages } from \"@langchain/core/messages\";\n+import { ChatPromptTemplate, MessagesPlaceholder } from \"@langchain/core/prompts\";\n import { ChatGroq } from \"@langchain/groq\";\n+import { END, MessagesAnnotation, START, StateGraph } from \"@langchain/langgraph\";\n import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n import wxflows from \"@wxflows/sdk/langchain\";\n-import { END, MessagesAnnotation, START, StateGraph } from \"@langchain/langgraph\";\n-import { ChatPromptTemplate, MessagesPlaceholder } from \"@langchain/core/prompts\";\n-import SYSTEM_MESSAGE from \"@/constants/systemMessage\";\n \n-//Customers at : https://introspection.apis.stepzen.com/customers\n-// Comments at: https://dummyjson.com/comments  a wxflows import curl + url\n+// Create a trimmer to trim the messages\n+const trimmer = trimMessages({\n+  maxTokens: 10,\n+  strategy: \"last\",\n+  tokenCounter: (msgs) => msgs.length,\n+  includeSystem: true,\n+  allowPartial: false,\n+  startOn: \"human\",\n+});\n \n // Connect to WxFlows\n const toolClient = new wxflows({ endpoint: process.env.WXFLOWS_ENDPOINT || \"\", apikey: process.env.WXFLOWS_APIKEY });\n@@ -44,19 +52,46 @@ const initialiserModel = () => {\n   return model;\n };\n \n+// define the function to determine if the conversation should continue\n+const shouldContinue = async (state: typeof MessagesAnnotation.State) => {\n+  const messages = state.messages;\n+  const lastMessage = messages[messages.length - 1] as AIMessage;\n+  // if the LLM makes a tool call, then we route to the tool node\n+  if (lastMessage.tool_calls?.length) {\n+    return \"tools\";\n+  }\n+  if (lastMessage.content && lastMessage._getType() === \"tool\") {\n+    return \"agent\";\n+  }\n+  return END;\n+};\n+\n const createWorkflow = () => {\n   const model = initialiserModel();\n-  const stateGraph = new StateGraph(MessagesAnnotation).addNode(\"agent\", async (state) => {\n-    const systemContent = SYSTEM_MESSAGE;\n \n-    // Create the prompt template with system and message placeholder\n-    const promptTemplate = ChatPromptTemplate.fromMessages([\n-        new SystemMessage(systemContent,{\n-          cache_control: { type: \"ephemeral\"}\n+  const stateGraph = new StateGraph(MessagesAnnotation)\n+    .addNode(\"agent\", async (state) => {\n+      const systemContent = SYSTEM_MESSAGE;\n+\n+      // Create the prompt template with system and message placeholder\n+      const promptTemplate = ChatPromptTemplate.fromMessages([\n+        new SystemMessage(systemContent, {\n+          cache_control: { type: \"ephemeral\" }, // set a cache breakpoint ( max number a breakpoint )\n         }),\n         new MessagesPlaceholder(\"messages\"),\n-      ])\n-    ])\n+      ]);\n+\n+      // Trim the messages to manage conversation history\n+      const trimmedMessages = await trimmer.invoke(state.messages);\n+      // Format the prompt with the current messages\n+      const prompt = await promptTemplate.invoke({ messages: trimmedMessages });\n+      const response = await model.invoke(prompt);\n+      return { messages: [response] };\n+    })\n+    .addEdge(START, \"agent\")\n+    .addNode(\"tool\", toolNode)\n+    .addConditionalEdges(\"agent\", shouldContinue)\n+    .addEdge(\"tool\", \"agent\");\n \n-  });\n+  return stateGraph;\n };",
      "patch_lines": [
        "@@ -1,12 +1,20 @@\n",
        "+import SYSTEM_MESSAGE from \"@/constants/systemMessage\";\n",
        "+import { AIMessage, SystemMessage, trimMessages } from \"@langchain/core/messages\";\n",
        "+import { ChatPromptTemplate, MessagesPlaceholder } from \"@langchain/core/prompts\";\n",
        " import { ChatGroq } from \"@langchain/groq\";\n",
        "+import { END, MessagesAnnotation, START, StateGraph } from \"@langchain/langgraph\";\n",
        " import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
        " import wxflows from \"@wxflows/sdk/langchain\";\n",
        "-import { END, MessagesAnnotation, START, StateGraph } from \"@langchain/langgraph\";\n",
        "-import { ChatPromptTemplate, MessagesPlaceholder } from \"@langchain/core/prompts\";\n",
        "-import SYSTEM_MESSAGE from \"@/constants/systemMessage\";\n",
        " \n",
        "-//Customers at : https://introspection.apis.stepzen.com/customers\n",
        "-// Comments at: https://dummyjson.com/comments  a wxflows import curl + url\n",
        "+// Create a trimmer to trim the messages\n",
        "+const trimmer = trimMessages({\n",
        "+  maxTokens: 10,\n",
        "+  strategy: \"last\",\n",
        "+  tokenCounter: (msgs) => msgs.length,\n",
        "+  includeSystem: true,\n",
        "+  allowPartial: false,\n",
        "+  startOn: \"human\",\n",
        "+});\n",
        " \n",
        " // Connect to WxFlows\n",
        " const toolClient = new wxflows({ endpoint: process.env.WXFLOWS_ENDPOINT || \"\", apikey: process.env.WXFLOWS_APIKEY });\n",
        "@@ -44,19 +52,46 @@ const initialiserModel = () => {\n",
        "   return model;\n",
        " };\n",
        " \n",
        "+// define the function to determine if the conversation should continue\n",
        "+const shouldContinue = async (state: typeof MessagesAnnotation.State) => {\n",
        "+  const messages = state.messages;\n",
        "+  const lastMessage = messages[messages.length - 1] as AIMessage;\n",
        "+  // if the LLM makes a tool call, then we route to the tool node\n",
        "+  if (lastMessage.tool_calls?.length) {\n",
        "+    return \"tools\";\n",
        "+  }\n",
        "+  if (lastMessage.content && lastMessage._getType() === \"tool\") {\n",
        "+    return \"agent\";\n",
        "+  }\n",
        "+  return END;\n",
        "+};\n",
        "+\n",
        " const createWorkflow = () => {\n",
        "   const model = initialiserModel();\n",
        "-  const stateGraph = new StateGraph(MessagesAnnotation).addNode(\"agent\", async (state) => {\n",
        "-    const systemContent = SYSTEM_MESSAGE;\n",
        " \n",
        "-    // Create the prompt template with system and message placeholder\n",
        "-    const promptTemplate = ChatPromptTemplate.fromMessages([\n",
        "-        new SystemMessage(systemContent,{\n",
        "-          cache_control: { type: \"ephemeral\"}\n",
        "+  const stateGraph = new StateGraph(MessagesAnnotation)\n",
        "+    .addNode(\"agent\", async (state) => {\n",
        "+      const systemContent = SYSTEM_MESSAGE;\n",
        "+\n",
        "+      // Create the prompt template with system and message placeholder\n",
        "+      const promptTemplate = ChatPromptTemplate.fromMessages([\n",
        "+        new SystemMessage(systemContent, {\n",
        "+          cache_control: { type: \"ephemeral\" }, // set a cache breakpoint ( max number a breakpoint )\n",
        "         }),\n",
        "         new MessagesPlaceholder(\"messages\"),\n",
        "-      ])\n",
        "-    ])\n",
        "+      ]);\n",
        "+\n",
        "+      // Trim the messages to manage conversation history\n",
        "+      const trimmedMessages = await trimmer.invoke(state.messages);\n",
        "+      // Format the prompt with the current messages\n",
        "+      const prompt = await promptTemplate.invoke({ messages: trimmedMessages });\n",
        "+      const response = await model.invoke(prompt);\n",
        "+      return { messages: [response] };\n",
        "+    })\n",
        "+    .addEdge(START, \"agent\")\n",
        "+    .addNode(\"tool\", toolNode)\n",
        "+    .addConditionalEdges(\"agent\", shouldContinue)\n",
        "+    .addEdge(\"tool\", \"agent\");\n",
        " \n",
        "-  });\n",
        "+  return stateGraph;\n",
        " };\n"
      ]
    }
  ]
}
{
  "project": "Research Data/quark-chat",
  "repo": "anadi45/quark-chat",
  "prior_commit": "c2a6dc9e713d4f553298f88d8e916603af84d29a",
  "researched_commit": "b49fe1ece02ea9d12a3ecb8c1ad251aa0d482501",
  "compare_url": "https://github.com/anadi45/quark-chat/compare/c2a6dc9e713d4f553298f88d8e916603af84d29a...b49fe1ece02ea9d12a3ecb8c1ad251aa0d482501",
  "ahead_by": 1,
  "behind_by": 0,
  "changed_files": [
    {
      "path": "apps/server/src/agents/kronos/builder.ts",
      "status": "modified",
      "additions": 236,
      "deletions": 97,
      "patch": "@@ -20,8 +20,16 @@ import {\n import type { ChatMessage } from '@kronos/core';\n import { MODELS } from '../../constants/models.constants';\n import { formatSystemPrompt } from './prompts';\n-import { getContextValue, extractToolCalls, getCurrentDate, generateConversationId } from './utils';\n-import { createKronosCheckpointerFromEnv, createKronosCheckpointer } from './checkpointer';\n+import {\n+  getContextValue,\n+  extractToolCalls,\n+  getCurrentDate,\n+  generateConversationId,\n+} from './utils';\n+import {\n+  createKronosCheckpointerFromEnv,\n+  createKronosCheckpointer,\n+} from './checkpointer';\n \n /**\n  * Kronos Agent Builder\n@@ -34,10 +42,12 @@ export class KronosAgentBuilder {\n   private tools: any[] = [];\n   private toolProvider: Composio;\n   private checkpointer?: any; // PostgreSQL checkpointer instance\n+  private userId: string;\n \n   AGENT_NAME = 'kronos_agent';\n \n-  constructor() {\n+  constructor(userId: string) {\n+    this.userId = userId;\n     this.initializeProviders();\n   }\n \n@@ -47,7 +57,7 @@ export class KronosAgentBuilder {\n   async build(): Promise<any> {\n     try {\n       console.log('\ud83d\ude80 Starting Kronos agent creation');\n-      await this.loadTools();\n+      await this.loadTools(this.userId);\n       await this.initializeCheckpointer();\n \n       // Build the workflow graph\n@@ -60,14 +70,23 @@ export class KronosAgentBuilder {\n       const compileOptions: any = {\n         name: this.AGENT_NAME,\n       };\n-      \n \n-    compileOptions.checkpointer = this.checkpointer.getPostgresSaver();\n-      \n+      // Only add checkpointer if it's available\n+      if (this.checkpointer) {\n+        compileOptions.checkpointer = this.checkpointer.getPostgresSaver();\n+      }\n \n       const compiledGraph = workflow.compile(compileOptions);\n \n-      console.log('\u2705 Kronos agent created successfully with PostgreSQL checkpointer');\n+      if (this.checkpointer) {\n+        console.log(\n+          '\u2705 Kronos agent created successfully with PostgreSQL checkpointer'\n+        );\n+      } else {\n+        console.log(\n+          '\u2705 Kronos agent created successfully without persistence (checkpointer unavailable)'\n+        );\n+      }\n       return compiledGraph;\n     } catch (error) {\n       console.error('\u274c Failed to create Kronos agent:', error);\n@@ -102,20 +121,23 @@ export class KronosAgentBuilder {\n       this.checkpointer = await createKronosCheckpointer();\n       console.log('\u2705 PostgreSQL checkpointer initialized successfully');\n     } catch (error) {\n-      console.warn('\u26a0\ufe0f Failed to initialize PostgreSQL checkpointer, continuing without persistence:', error);\n+      console.warn(\n+        '\u26a0\ufe0f Failed to initialize PostgreSQL checkpointer, continuing without persistence:',\n+        error\n+      );\n       this.checkpointer = undefined;\n     }\n   }\n \n   /**\n-   * Load all available tools\n+   * Load all available tools for a given user\n    */\n-  private async loadTools(): Promise<void> {\n+  private async loadTools(userId: string): Promise<void> {\n     console.log('\ud83d\udd27 Loading Kronos tools');\n \n     try {\n       // Get tools from Composio (using a default user ID for now)\n-      const composioTools = await this.toolProvider.tools.get('default-user', {\n+      const composioTools = await this.toolProvider.tools.get(userId, {\n         tools: ['GMAIL_FETCH_EMAILS'],\n       });\n \n@@ -159,15 +181,11 @@ export class KronosAgentBuilder {\n     workflow.setEntryPoint('agent');\n \n     // Agent -> tools or final answer node\n-    workflow.addConditionalEdges(\n-      'agent',\n-      this.shouldAct,\n-      {\n-        'continue': 'tool',\n-        'final_answer': 'final_answer',\n-        'complete': 'complete',\n-      }\n-    );\n+    workflow.addConditionalEdges('agent', this.shouldAct, {\n+      continue: 'tool',\n+      final_answer: 'final_answer',\n+      complete: 'complete',\n+    });\n \n     // Tool -> agent (loop back)\n     workflow.addEdge('tool', 'agent');\n@@ -191,11 +209,13 @@ export class KronosAgentBuilder {\n       const toolCalls = aiMessage.tool_calls || [];\n \n       if (toolCalls.length > 0) {\n-        const toolNames = toolCalls.map(tc => tc.name);\n+        const toolNames = toolCalls.map((tc) => tc.name);\n         console.log('Routing: Tool calls requested:', toolNames);\n         return 'continue';\n       } else {\n-        console.log('Routing: LLM provided a direct answer, proceeding to completion.');\n+        console.log(\n+          'Routing: LLM provided a direct answer, proceeding to completion.'\n+        );\n         return 'complete';\n       }\n     }\n@@ -214,7 +234,7 @@ export class KronosAgentBuilder {\n \n       try {\n         const lastMessage = state.messages[state.messages.length - 1];\n-        \n+\n         if (!lastMessage || !(lastMessage instanceof AIMessage)) {\n           console.log('No AI message found, skipping tool execution');\n           return {};\n@@ -224,7 +244,9 @@ export class KronosAgentBuilder {\n         const toolCalls = extractToolCalls(aiMessage);\n \n         if (toolCalls.length === 0) {\n-          console.log('No tool calls found in last message, skipping tool execution');\n+          console.log(\n+            'No tool calls found in last message, skipping tool execution'\n+          );\n           return {};\n         }\n \n@@ -238,28 +260,35 @@ export class KronosAgentBuilder {\n           hasAuthToken: !!authToken,\n           workspaceId,\n           userId,\n-          conversationId\n+          conversationId,\n         });\n \n         // Execute tools with enhanced error handling\n         const toolResults: ToolMessage[] = [];\n-        \n+\n         for (const toolCall of toolCalls) {\n           try {\n             // Find the tool\n-            const tool = this.tools.find(t => t.name === toolCall.name);\n+            const tool = this.tools.find((t) => t.name === toolCall.name);\n             if (!tool) {\n-              console.warn(`Tool ${toolCall.name} not found in available tools`);\n-              toolResults.push(new ToolMessage({\n-                content: `Tool ${toolCall.name} not found`,\n-                tool_call_id: toolCall.id,\n-              }));\n+              console.warn(\n+                `Tool ${toolCall.name} not found in available tools`\n+              );\n+              toolResults.push(\n+                new ToolMessage({\n+                  content: `Tool ${toolCall.name} not found`,\n+                  tool_call_id: toolCall.id,\n+                })\n+              );\n               continue;\n             }\n \n             // Execute the tool with context\n-            console.log(`Executing tool: ${toolCall.name} with args:`, toolCall.args);\n-            \n+            console.log(\n+              `Executing tool: ${toolCall.name} with args:`,\n+              toolCall.args\n+            );\n+\n             // Add context to tool arguments if the tool supports it\n             const toolArgs = {\n               ...toolCall.args,\n@@ -270,26 +299,28 @@ export class KronosAgentBuilder {\n             };\n \n             const result = await tool.invoke(toolArgs);\n-            \n-            console.log(`Tool ${toolCall.name} executed successfully`);\n-            toolResults.push(new ToolMessage({\n-              content: JSON.stringify(result),\n-              tool_call_id: toolCall.id,\n-            }));\n \n+            console.log(`Tool ${toolCall.name} executed successfully`);\n+            toolResults.push(\n+              new ToolMessage({\n+                content: JSON.stringify(result),\n+                tool_call_id: toolCall.id,\n+              })\n+            );\n           } catch (error) {\n             console.error(`Error executing tool ${toolCall.name}:`, error);\n-            toolResults.push(new ToolMessage({\n-              content: `Error executing ${toolCall.name}: ${error.message}`,\n-              tool_call_id: toolCall.id,\n-            }));\n+            toolResults.push(\n+              new ToolMessage({\n+                content: `Error executing ${toolCall.name}: ${error.message}`,\n+                tool_call_id: toolCall.id,\n+              })\n+            );\n           }\n         }\n \n         return {\n           messages: toolResults,\n         };\n-\n       } catch (error) {\n         console.error('\u274c Tool node execution failed:', error);\n         return {\n@@ -310,26 +341,34 @@ export class KronosAgentBuilder {\n         // Get current date for dynamic prompt\n         const todayDate = getCurrentDate();\n         const formattedPrompt = formatSystemPrompt(todayDate);\n-        \n+\n         // Build messages array with proper conversation history\n         const messages = [\n           new SystemMessage(formattedPrompt),\n           ...state.messages, // Use existing messages from state\n         ];\n \n         // Add current message if not already in messages\n-        if (state.currentMessage && !messages.some(msg => \n-          msg instanceof HumanMessage && msg.content === state.currentMessage\n-        )) {\n+        if (\n+          state.currentMessage &&\n+          !messages.some(\n+            (msg) =>\n+              msg instanceof HumanMessage &&\n+              msg.content === state.currentMessage\n+          )\n+        ) {\n           messages.push(new HumanMessage(state.currentMessage));\n         }\n \n-        console.log(`Agent using conversation history: ${messages.length} messages`);\n+        console.log(\n+          `Agent using conversation history: ${messages.length} messages`\n+        );\n \n         // Bind tools to the model with tool choice\n-        const modelWithTools = this.tools.length > 0 \n-          ? this.model.bindTools(this.tools, { tool_choice: 'any' })\n-          : this.model;\n+        const modelWithTools =\n+          this.tools.length > 0\n+            ? this.model.bindTools(this.tools, { tool_choice: 'any' })\n+            : this.model;\n \n         // Generate response\n         const response = await modelWithTools.invoke(messages, config);\n@@ -341,9 +380,11 @@ export class KronosAgentBuilder {\n       } catch (error) {\n         console.error('\u274c Agent node execution failed:', error);\n         return {\n-          messages: [new AIMessage(\n-            'I apologize, but I encountered an error while processing your request.'\n-          )],\n+          messages: [\n+            new AIMessage(\n+              'I apologize, but I encountered an error while processing your request.'\n+            ),\n+          ],\n           error: `Agent execution failed: ${error.message}`,\n         };\n       }\n@@ -361,21 +402,27 @@ export class KronosAgentBuilder {\n         // Generate LLM-based response using conversation history\n         const todayDate = getCurrentDate();\n         const formattedPrompt = formatSystemPrompt(todayDate);\n-        \n+\n         // Build conversation history for final response generation\n         const allMessages = state.messages;\n         const conversationHistory = [\n           new SystemMessage(formattedPrompt),\n-          ...allMessages\n+          ...allMessages,\n         ];\n \n-        console.log(`Final answer using conversation history: ${conversationHistory.length} messages`);\n+        console.log(\n+          `Final answer using conversation history: ${conversationHistory.length} messages`\n+        );\n \n         // Generate comprehensive final response using LLM\n-        const finalResponse = await this.model.invoke(conversationHistory, config);\n+        const finalResponse = await this.model.invoke(\n+          conversationHistory,\n+          config\n+        );\n \n         // Extract content from the response\n-        let responseContent = 'I apologize, but I was unable to generate a response.';\n+        let responseContent =\n+          'I apologize, but I was unable to generate a response.';\n         if (finalResponse && finalResponse.content) {\n           responseContent = finalResponse.content as string;\n         }\n@@ -390,7 +437,8 @@ export class KronosAgentBuilder {\n       } catch (error) {\n         console.error('\u274c Final answer node execution failed:', error);\n         return {\n-          response: 'I apologize, but I encountered an error while finalizing my response.',\n+          response:\n+            'I apologize, but I encountered an error while finalizing my response.',\n           isComplete: true,\n           error: `Final answer generation failed: ${error.message}`,\n         };\n@@ -409,7 +457,8 @@ export class KronosAgentBuilder {\n   }\n \n   /**\n-   * Create a streaming response using the built graph with enhanced context support\n+   * Create a streaming response using LangGraph's proper streaming capabilities\n+   * Supports multiple stream modes: updates, messages, and custom tool updates\n    */\n   async streamResponse(\n     message: string,\n@@ -420,6 +469,7 @@ export class KronosAgentBuilder {\n       workspaceId?: string;\n       conversationId?: string;\n       threadId?: string;\n+      streamModes?: ('updates' | 'messages' | 'custom')[];\n     } = {}\n   ): Promise<ReadableStream> {\n     try {\n@@ -438,69 +488,158 @@ export class KronosAgentBuilder {\n       };\n \n       // Create config with context and thread ID for checkpointer\n-      const threadId = options.threadId || options.conversationId || generateConversationId();\n+      const threadId =\n+        options.threadId || options.conversationId || generateConversationId();\n       const config: RunnableConfig = {\n         configurable: {\n           authToken: options.authToken,\n           workspaceId: options.workspaceId,\n           conversationId: options.conversationId,\n           userId: userId,\n           thread_id: threadId,\n-        }\n+        },\n       };\n \n-      // Create streaming response\n+      // Default stream modes if not specified\n+      const streamModes = options.streamModes || [\n+        'updates',\n+        'messages',\n+        'custom',\n+      ];\n+\n+      // Create streaming response using LangGraph's streaming\n       return new ReadableStream({\n         async start(controller) {\n           try {\n-            console.log('Starting graph execution with context:', {\n+            console.log(\n+              'Starting LangGraph streaming with modes:',\n+              streamModes\n+            );\n+            console.log('Context:', {\n               hasAuthToken: !!options.authToken,\n               workspaceId: options.workspaceId,\n               conversationId: options.conversationId,\n               userId: userId,\n-              threadId: threadId\n+              threadId: threadId,\n             });\n \n-            // Execute the graph with config\n-            const result = await graph.invoke(initialState, config);\n-\n-            // Get the final response from the result\n-            const finalResponse = result.response || 'I apologize, but I was unable to generate a response.';\n+            // Use LangGraph's stream method with multiple modes\n+            const stream = await graph.stream(initialState, {\n+              ...config,\n+              streamMode: streamModes,\n+            });\n \n-            console.log('Graph execution completed, streaming response');\n+            let assistantResponse = '';\n+            let tokenSequence = 0;\n+\n+            // Process the stream based on the modes\n+            for await (const [streamMode, chunk] of stream) {\n+              console.log(`Stream mode: ${streamMode}`, chunk);\n+\n+              switch (streamMode) {\n+                case 'updates':\n+                  // Agent progress updates - emit after each node execution\n+                  if (chunk && typeof chunk === 'object') {\n+                    const updateEvent = {\n+                      type: 'agent_progress',\n+                      data: {\n+                        node: chunk.node || 'unknown',\n+                        status: 'completed',\n+                        timestamp: new Date().toISOString(),\n+                      },\n+                    };\n+                    controller.enqueue(\n+                      new TextEncoder().encode(\n+                        `data: ${JSON.stringify(updateEvent)}\\n\\n`\n+                      )\n+                    );\n+                  }\n+                  break;\n+\n+                case 'messages':\n+                  // LLM tokens - stream tokens as they are generated\n+                  if (chunk && chunk.content) {\n+                    const content =\n+                      typeof chunk.content === 'string'\n+                        ? chunk.content\n+                        : String(chunk.content);\n+\n+                    // Stream content as single token to avoid splitting issues\n+                    if (content.trim()) {\n+                      tokenSequence++;\n+                      const tokenEvent = {\n+                        type: 'token',\n+                        data: content,\n+                        sequence: tokenSequence,\n+                        timestamp: new Date().toISOString(),\n+                      };\n+                      controller.enqueue(\n+                        new TextEncoder().encode(\n+                          `data: ${JSON.stringify(tokenEvent)}\\n\\n`\n+                        )\n+                      );\n+                      assistantResponse += content;\n+                    }\n+                  }\n+                  break;\n+\n+                case 'custom':\n+                  // Custom tool updates - emit custom data from tools\n+                  if (chunk && typeof chunk === 'object') {\n+                    const customEvent = {\n+                      type: 'tool_update',\n+                      data: chunk,\n+                      timestamp: new Date().toISOString(),\n+                    };\n+                    controller.enqueue(\n+                      new TextEncoder().encode(\n+                        `data: ${JSON.stringify(customEvent)}\\n\\n`\n+                      )\n+                    );\n+                  }\n+                  break;\n+              }\n+            }\n \n-            // Stream the response\n-            const contentChunk = `data: ${JSON.stringify({\n-              type: 'content',\n-              data: finalResponse,\n-              timestamp: new Date().toISOString(),\n-            })}\\n\\n`;\n-            controller.enqueue(new TextEncoder().encode(contentChunk));\n+            console.log('LangGraph streaming completed');\n \n-            // Send done signal\n-            const doneChunk = `data: ${JSON.stringify({\n-              type: 'done',\n-              timestamp: new Date().toISOString(),\n-            })}\\n\\n`;\n-            controller.enqueue(new TextEncoder().encode(doneChunk));\n+            // Send completion event\n+            const completionEvent = {\n+              type: 'completion',\n+              data: {\n+                response: assistantResponse,\n+                totalTokens: tokenSequence,\n+                timestamp: new Date().toISOString(),\n+              },\n+            };\n+            controller.enqueue(\n+              new TextEncoder().encode(\n+                `data: ${JSON.stringify(completionEvent)}\\n\\n`\n+              )\n+            );\n \n-            // Send final [DONE] marker\n-            controller.enqueue(new TextEncoder().encode('data: [DONE]\\n\\n'));\n+            // Close the stream without duplicate [DONE] markers\n             controller.close();\n           } catch (error) {\n-            console.error('Streaming error:', error);\n-            const errorChunk = `data: ${JSON.stringify({\n+            console.error('LangGraph streaming error:', error);\n+            const errorEvent = {\n               type: 'error',\n-              error: 'Failed to generate response',\n-              timestamp: new Date().toISOString(),\n-            })}\\n\\n`;\n-            controller.enqueue(new TextEncoder().encode(errorChunk));\n+              data: {\n+                error: error.message,\n+                timestamp: new Date().toISOString(),\n+              },\n+            };\n+            controller.enqueue(\n+              new TextEncoder().encode(\n+                `data: ${JSON.stringify(errorEvent)}\\n\\n`\n+              )\n+            );\n             controller.close();\n           }\n         },\n       });\n     } catch (error) {\n-      console.error('Failed to create streaming response:', error);\n+      console.error('Failed to create LangGraph streaming response:', error);\n       throw error;\n     }\n   }",
      "patch_lines": [
        "@@ -20,8 +20,16 @@ import {\n",
        " import type { ChatMessage } from '@kronos/core';\n",
        " import { MODELS } from '../../constants/models.constants';\n",
        " import { formatSystemPrompt } from './prompts';\n",
        "-import { getContextValue, extractToolCalls, getCurrentDate, generateConversationId } from './utils';\n",
        "-import { createKronosCheckpointerFromEnv, createKronosCheckpointer } from './checkpointer';\n",
        "+import {\n",
        "+  getContextValue,\n",
        "+  extractToolCalls,\n",
        "+  getCurrentDate,\n",
        "+  generateConversationId,\n",
        "+} from './utils';\n",
        "+import {\n",
        "+  createKronosCheckpointerFromEnv,\n",
        "+  createKronosCheckpointer,\n",
        "+} from './checkpointer';\n",
        " \n",
        " /**\n",
        "  * Kronos Agent Builder\n",
        "@@ -34,10 +42,12 @@ export class KronosAgentBuilder {\n",
        "   private tools: any[] = [];\n",
        "   private toolProvider: Composio;\n",
        "   private checkpointer?: any; // PostgreSQL checkpointer instance\n",
        "+  private userId: string;\n",
        " \n",
        "   AGENT_NAME = 'kronos_agent';\n",
        " \n",
        "-  constructor() {\n",
        "+  constructor(userId: string) {\n",
        "+    this.userId = userId;\n",
        "     this.initializeProviders();\n",
        "   }\n",
        " \n",
        "@@ -47,7 +57,7 @@ export class KronosAgentBuilder {\n",
        "   async build(): Promise<any> {\n",
        "     try {\n",
        "       console.log('\ud83d\ude80 Starting Kronos agent creation');\n",
        "-      await this.loadTools();\n",
        "+      await this.loadTools(this.userId);\n",
        "       await this.initializeCheckpointer();\n",
        " \n",
        "       // Build the workflow graph\n",
        "@@ -60,14 +70,23 @@ export class KronosAgentBuilder {\n",
        "       const compileOptions: any = {\n",
        "         name: this.AGENT_NAME,\n",
        "       };\n",
        "-      \n",
        " \n",
        "-    compileOptions.checkpointer = this.checkpointer.getPostgresSaver();\n",
        "-      \n",
        "+      // Only add checkpointer if it's available\n",
        "+      if (this.checkpointer) {\n",
        "+        compileOptions.checkpointer = this.checkpointer.getPostgresSaver();\n",
        "+      }\n",
        " \n",
        "       const compiledGraph = workflow.compile(compileOptions);\n",
        " \n",
        "-      console.log('\u2705 Kronos agent created successfully with PostgreSQL checkpointer');\n",
        "+      if (this.checkpointer) {\n",
        "+        console.log(\n",
        "+          '\u2705 Kronos agent created successfully with PostgreSQL checkpointer'\n",
        "+        );\n",
        "+      } else {\n",
        "+        console.log(\n",
        "+          '\u2705 Kronos agent created successfully without persistence (checkpointer unavailable)'\n",
        "+        );\n",
        "+      }\n",
        "       return compiledGraph;\n",
        "     } catch (error) {\n",
        "       console.error('\u274c Failed to create Kronos agent:', error);\n",
        "@@ -102,20 +121,23 @@ export class KronosAgentBuilder {\n",
        "       this.checkpointer = await createKronosCheckpointer();\n",
        "       console.log('\u2705 PostgreSQL checkpointer initialized successfully');\n",
        "     } catch (error) {\n",
        "-      console.warn('\u26a0\ufe0f Failed to initialize PostgreSQL checkpointer, continuing without persistence:', error);\n",
        "+      console.warn(\n",
        "+        '\u26a0\ufe0f Failed to initialize PostgreSQL checkpointer, continuing without persistence:',\n",
        "+        error\n",
        "+      );\n",
        "       this.checkpointer = undefined;\n",
        "     }\n",
        "   }\n",
        " \n",
        "   /**\n",
        "-   * Load all available tools\n",
        "+   * Load all available tools for a given user\n",
        "    */\n",
        "-  private async loadTools(): Promise<void> {\n",
        "+  private async loadTools(userId: string): Promise<void> {\n",
        "     console.log('\ud83d\udd27 Loading Kronos tools');\n",
        " \n",
        "     try {\n",
        "       // Get tools from Composio (using a default user ID for now)\n",
        "-      const composioTools = await this.toolProvider.tools.get('default-user', {\n",
        "+      const composioTools = await this.toolProvider.tools.get(userId, {\n",
        "         tools: ['GMAIL_FETCH_EMAILS'],\n",
        "       });\n",
        " \n",
        "@@ -159,15 +181,11 @@ export class KronosAgentBuilder {\n",
        "     workflow.setEntryPoint('agent');\n",
        " \n",
        "     // Agent -> tools or final answer node\n",
        "-    workflow.addConditionalEdges(\n",
        "-      'agent',\n",
        "-      this.shouldAct,\n",
        "-      {\n",
        "-        'continue': 'tool',\n",
        "-        'final_answer': 'final_answer',\n",
        "-        'complete': 'complete',\n",
        "-      }\n",
        "-    );\n",
        "+    workflow.addConditionalEdges('agent', this.shouldAct, {\n",
        "+      continue: 'tool',\n",
        "+      final_answer: 'final_answer',\n",
        "+      complete: 'complete',\n",
        "+    });\n",
        " \n",
        "     // Tool -> agent (loop back)\n",
        "     workflow.addEdge('tool', 'agent');\n",
        "@@ -191,11 +209,13 @@ export class KronosAgentBuilder {\n",
        "       const toolCalls = aiMessage.tool_calls || [];\n",
        " \n",
        "       if (toolCalls.length > 0) {\n",
        "-        const toolNames = toolCalls.map(tc => tc.name);\n",
        "+        const toolNames = toolCalls.map((tc) => tc.name);\n",
        "         console.log('Routing: Tool calls requested:', toolNames);\n",
        "         return 'continue';\n",
        "       } else {\n",
        "-        console.log('Routing: LLM provided a direct answer, proceeding to completion.');\n",
        "+        console.log(\n",
        "+          'Routing: LLM provided a direct answer, proceeding to completion.'\n",
        "+        );\n",
        "         return 'complete';\n",
        "       }\n",
        "     }\n",
        "@@ -214,7 +234,7 @@ export class KronosAgentBuilder {\n",
        " \n",
        "       try {\n",
        "         const lastMessage = state.messages[state.messages.length - 1];\n",
        "-        \n",
        "+\n",
        "         if (!lastMessage || !(lastMessage instanceof AIMessage)) {\n",
        "           console.log('No AI message found, skipping tool execution');\n",
        "           return {};\n",
        "@@ -224,7 +244,9 @@ export class KronosAgentBuilder {\n",
        "         const toolCalls = extractToolCalls(aiMessage);\n",
        " \n",
        "         if (toolCalls.length === 0) {\n",
        "-          console.log('No tool calls found in last message, skipping tool execution');\n",
        "+          console.log(\n",
        "+            'No tool calls found in last message, skipping tool execution'\n",
        "+          );\n",
        "           return {};\n",
        "         }\n",
        " \n",
        "@@ -238,28 +260,35 @@ export class KronosAgentBuilder {\n",
        "           hasAuthToken: !!authToken,\n",
        "           workspaceId,\n",
        "           userId,\n",
        "-          conversationId\n",
        "+          conversationId,\n",
        "         });\n",
        " \n",
        "         // Execute tools with enhanced error handling\n",
        "         const toolResults: ToolMessage[] = [];\n",
        "-        \n",
        "+\n",
        "         for (const toolCall of toolCalls) {\n",
        "           try {\n",
        "             // Find the tool\n",
        "-            const tool = this.tools.find(t => t.name === toolCall.name);\n",
        "+            const tool = this.tools.find((t) => t.name === toolCall.name);\n",
        "             if (!tool) {\n",
        "-              console.warn(`Tool ${toolCall.name} not found in available tools`);\n",
        "-              toolResults.push(new ToolMessage({\n",
        "-                content: `Tool ${toolCall.name} not found`,\n",
        "-                tool_call_id: toolCall.id,\n",
        "-              }));\n",
        "+              console.warn(\n",
        "+                `Tool ${toolCall.name} not found in available tools`\n",
        "+              );\n",
        "+              toolResults.push(\n",
        "+                new ToolMessage({\n",
        "+                  content: `Tool ${toolCall.name} not found`,\n",
        "+                  tool_call_id: toolCall.id,\n",
        "+                })\n",
        "+              );\n",
        "               continue;\n",
        "             }\n",
        " \n",
        "             // Execute the tool with context\n",
        "-            console.log(`Executing tool: ${toolCall.name} with args:`, toolCall.args);\n",
        "-            \n",
        "+            console.log(\n",
        "+              `Executing tool: ${toolCall.name} with args:`,\n",
        "+              toolCall.args\n",
        "+            );\n",
        "+\n",
        "             // Add context to tool arguments if the tool supports it\n",
        "             const toolArgs = {\n",
        "               ...toolCall.args,\n",
        "@@ -270,26 +299,28 @@ export class KronosAgentBuilder {\n",
        "             };\n",
        " \n",
        "             const result = await tool.invoke(toolArgs);\n",
        "-            \n",
        "-            console.log(`Tool ${toolCall.name} executed successfully`);\n",
        "-            toolResults.push(new ToolMessage({\n",
        "-              content: JSON.stringify(result),\n",
        "-              tool_call_id: toolCall.id,\n",
        "-            }));\n",
        " \n",
        "+            console.log(`Tool ${toolCall.name} executed successfully`);\n",
        "+            toolResults.push(\n",
        "+              new ToolMessage({\n",
        "+                content: JSON.stringify(result),\n",
        "+                tool_call_id: toolCall.id,\n",
        "+              })\n",
        "+            );\n",
        "           } catch (error) {\n",
        "             console.error(`Error executing tool ${toolCall.name}:`, error);\n",
        "-            toolResults.push(new ToolMessage({\n",
        "-              content: `Error executing ${toolCall.name}: ${error.message}`,\n",
        "-              tool_call_id: toolCall.id,\n",
        "-            }));\n",
        "+            toolResults.push(\n",
        "+              new ToolMessage({\n",
        "+                content: `Error executing ${toolCall.name}: ${error.message}`,\n",
        "+                tool_call_id: toolCall.id,\n",
        "+              })\n",
        "+            );\n",
        "           }\n",
        "         }\n",
        " \n",
        "         return {\n",
        "           messages: toolResults,\n",
        "         };\n",
        "-\n",
        "       } catch (error) {\n",
        "         console.error('\u274c Tool node execution failed:', error);\n",
        "         return {\n",
        "@@ -310,26 +341,34 @@ export class KronosAgentBuilder {\n",
        "         // Get current date for dynamic prompt\n",
        "         const todayDate = getCurrentDate();\n",
        "         const formattedPrompt = formatSystemPrompt(todayDate);\n",
        "-        \n",
        "+\n",
        "         // Build messages array with proper conversation history\n",
        "         const messages = [\n",
        "           new SystemMessage(formattedPrompt),\n",
        "           ...state.messages, // Use existing messages from state\n",
        "         ];\n",
        " \n",
        "         // Add current message if not already in messages\n",
        "-        if (state.currentMessage && !messages.some(msg => \n",
        "-          msg instanceof HumanMessage && msg.content === state.currentMessage\n",
        "-        )) {\n",
        "+        if (\n",
        "+          state.currentMessage &&\n",
        "+          !messages.some(\n",
        "+            (msg) =>\n",
        "+              msg instanceof HumanMessage &&\n",
        "+              msg.content === state.currentMessage\n",
        "+          )\n",
        "+        ) {\n",
        "           messages.push(new HumanMessage(state.currentMessage));\n",
        "         }\n",
        " \n",
        "-        console.log(`Agent using conversation history: ${messages.length} messages`);\n",
        "+        console.log(\n",
        "+          `Agent using conversation history: ${messages.length} messages`\n",
        "+        );\n",
        " \n",
        "         // Bind tools to the model with tool choice\n",
        "-        const modelWithTools = this.tools.length > 0 \n",
        "-          ? this.model.bindTools(this.tools, { tool_choice: 'any' })\n",
        "-          : this.model;\n",
        "+        const modelWithTools =\n",
        "+          this.tools.length > 0\n",
        "+            ? this.model.bindTools(this.tools, { tool_choice: 'any' })\n",
        "+            : this.model;\n",
        " \n",
        "         // Generate response\n",
        "         const response = await modelWithTools.invoke(messages, config);\n",
        "@@ -341,9 +380,11 @@ export class KronosAgentBuilder {\n",
        "       } catch (error) {\n",
        "         console.error('\u274c Agent node execution failed:', error);\n",
        "         return {\n",
        "-          messages: [new AIMessage(\n",
        "-            'I apologize, but I encountered an error while processing your request.'\n",
        "-          )],\n",
        "+          messages: [\n",
        "+            new AIMessage(\n",
        "+              'I apologize, but I encountered an error while processing your request.'\n",
        "+            ),\n",
        "+          ],\n",
        "           error: `Agent execution failed: ${error.message}`,\n",
        "         };\n",
        "       }\n",
        "@@ -361,21 +402,27 @@ export class KronosAgentBuilder {\n",
        "         // Generate LLM-based response using conversation history\n",
        "         const todayDate = getCurrentDate();\n",
        "         const formattedPrompt = formatSystemPrompt(todayDate);\n",
        "-        \n",
        "+\n",
        "         // Build conversation history for final response generation\n",
        "         const allMessages = state.messages;\n",
        "         const conversationHistory = [\n",
        "           new SystemMessage(formattedPrompt),\n",
        "-          ...allMessages\n",
        "+          ...allMessages,\n",
        "         ];\n",
        " \n",
        "-        console.log(`Final answer using conversation history: ${conversationHistory.length} messages`);\n",
        "+        console.log(\n",
        "+          `Final answer using conversation history: ${conversationHistory.length} messages`\n",
        "+        );\n",
        " \n",
        "         // Generate comprehensive final response using LLM\n",
        "-        const finalResponse = await this.model.invoke(conversationHistory, config);\n",
        "+        const finalResponse = await this.model.invoke(\n",
        "+          conversationHistory,\n",
        "+          config\n",
        "+        );\n",
        " \n",
        "         // Extract content from the response\n",
        "-        let responseContent = 'I apologize, but I was unable to generate a response.';\n",
        "+        let responseContent =\n",
        "+          'I apologize, but I was unable to generate a response.';\n",
        "         if (finalResponse && finalResponse.content) {\n",
        "           responseContent = finalResponse.content as string;\n",
        "         }\n",
        "@@ -390,7 +437,8 @@ export class KronosAgentBuilder {\n",
        "       } catch (error) {\n",
        "         console.error('\u274c Final answer node execution failed:', error);\n",
        "         return {\n",
        "-          response: 'I apologize, but I encountered an error while finalizing my response.',\n",
        "+          response:\n",
        "+            'I apologize, but I encountered an error while finalizing my response.',\n",
        "           isComplete: true,\n",
        "           error: `Final answer generation failed: ${error.message}`,\n",
        "         };\n",
        "@@ -409,7 +457,8 @@ export class KronosAgentBuilder {\n",
        "   }\n",
        " \n",
        "   /**\n",
        "-   * Create a streaming response using the built graph with enhanced context support\n",
        "+   * Create a streaming response using LangGraph's proper streaming capabilities\n",
        "+   * Supports multiple stream modes: updates, messages, and custom tool updates\n",
        "    */\n",
        "   async streamResponse(\n",
        "     message: string,\n",
        "@@ -420,6 +469,7 @@ export class KronosAgentBuilder {\n",
        "       workspaceId?: string;\n",
        "       conversationId?: string;\n",
        "       threadId?: string;\n",
        "+      streamModes?: ('updates' | 'messages' | 'custom')[];\n",
        "     } = {}\n",
        "   ): Promise<ReadableStream> {\n",
        "     try {\n",
        "@@ -438,69 +488,158 @@ export class KronosAgentBuilder {\n",
        "       };\n",
        " \n",
        "       // Create config with context and thread ID for checkpointer\n",
        "-      const threadId = options.threadId || options.conversationId || generateConversationId();\n",
        "+      const threadId =\n",
        "+        options.threadId || options.conversationId || generateConversationId();\n",
        "       const config: RunnableConfig = {\n",
        "         configurable: {\n",
        "           authToken: options.authToken,\n",
        "           workspaceId: options.workspaceId,\n",
        "           conversationId: options.conversationId,\n",
        "           userId: userId,\n",
        "           thread_id: threadId,\n",
        "-        }\n",
        "+        },\n",
        "       };\n",
        " \n",
        "-      // Create streaming response\n",
        "+      // Default stream modes if not specified\n",
        "+      const streamModes = options.streamModes || [\n",
        "+        'updates',\n",
        "+        'messages',\n",
        "+        'custom',\n",
        "+      ];\n",
        "+\n",
        "+      // Create streaming response using LangGraph's streaming\n",
        "       return new ReadableStream({\n",
        "         async start(controller) {\n",
        "           try {\n",
        "-            console.log('Starting graph execution with context:', {\n",
        "+            console.log(\n",
        "+              'Starting LangGraph streaming with modes:',\n",
        "+              streamModes\n",
        "+            );\n",
        "+            console.log('Context:', {\n",
        "               hasAuthToken: !!options.authToken,\n",
        "               workspaceId: options.workspaceId,\n",
        "               conversationId: options.conversationId,\n",
        "               userId: userId,\n",
        "-              threadId: threadId\n",
        "+              threadId: threadId,\n",
        "             });\n",
        " \n",
        "-            // Execute the graph with config\n",
        "-            const result = await graph.invoke(initialState, config);\n",
        "-\n",
        "-            // Get the final response from the result\n",
        "-            const finalResponse = result.response || 'I apologize, but I was unable to generate a response.';\n",
        "+            // Use LangGraph's stream method with multiple modes\n",
        "+            const stream = await graph.stream(initialState, {\n",
        "+              ...config,\n",
        "+              streamMode: streamModes,\n",
        "+            });\n",
        " \n",
        "-            console.log('Graph execution completed, streaming response');\n",
        "+            let assistantResponse = '';\n",
        "+            let tokenSequence = 0;\n",
        "+\n",
        "+            // Process the stream based on the modes\n",
        "+            for await (const [streamMode, chunk] of stream) {\n",
        "+              console.log(`Stream mode: ${streamMode}`, chunk);\n",
        "+\n",
        "+              switch (streamMode) {\n",
        "+                case 'updates':\n",
        "+                  // Agent progress updates - emit after each node execution\n",
        "+                  if (chunk && typeof chunk === 'object') {\n",
        "+                    const updateEvent = {\n",
        "+                      type: 'agent_progress',\n",
        "+                      data: {\n",
        "+                        node: chunk.node || 'unknown',\n",
        "+                        status: 'completed',\n",
        "+                        timestamp: new Date().toISOString(),\n",
        "+                      },\n",
        "+                    };\n",
        "+                    controller.enqueue(\n",
        "+                      new TextEncoder().encode(\n",
        "+                        `data: ${JSON.stringify(updateEvent)}\\n\\n`\n",
        "+                      )\n",
        "+                    );\n",
        "+                  }\n",
        "+                  break;\n",
        "+\n",
        "+                case 'messages':\n",
        "+                  // LLM tokens - stream tokens as they are generated\n",
        "+                  if (chunk && chunk.content) {\n",
        "+                    const content =\n",
        "+                      typeof chunk.content === 'string'\n",
        "+                        ? chunk.content\n",
        "+                        : String(chunk.content);\n",
        "+\n",
        "+                    // Stream content as single token to avoid splitting issues\n",
        "+                    if (content.trim()) {\n",
        "+                      tokenSequence++;\n",
        "+                      const tokenEvent = {\n",
        "+                        type: 'token',\n",
        "+                        data: content,\n",
        "+                        sequence: tokenSequence,\n",
        "+                        timestamp: new Date().toISOString(),\n",
        "+                      };\n",
        "+                      controller.enqueue(\n",
        "+                        new TextEncoder().encode(\n",
        "+                          `data: ${JSON.stringify(tokenEvent)}\\n\\n`\n",
        "+                        )\n",
        "+                      );\n",
        "+                      assistantResponse += content;\n",
        "+                    }\n",
        "+                  }\n",
        "+                  break;\n",
        "+\n",
        "+                case 'custom':\n",
        "+                  // Custom tool updates - emit custom data from tools\n",
        "+                  if (chunk && typeof chunk === 'object') {\n",
        "+                    const customEvent = {\n",
        "+                      type: 'tool_update',\n",
        "+                      data: chunk,\n",
        "+                      timestamp: new Date().toISOString(),\n",
        "+                    };\n",
        "+                    controller.enqueue(\n",
        "+                      new TextEncoder().encode(\n",
        "+                        `data: ${JSON.stringify(customEvent)}\\n\\n`\n",
        "+                      )\n",
        "+                    );\n",
        "+                  }\n",
        "+                  break;\n",
        "+              }\n",
        "+            }\n",
        " \n",
        "-            // Stream the response\n",
        "-            const contentChunk = `data: ${JSON.stringify({\n",
        "-              type: 'content',\n",
        "-              data: finalResponse,\n",
        "-              timestamp: new Date().toISOString(),\n",
        "-            })}\\n\\n`;\n",
        "-            controller.enqueue(new TextEncoder().encode(contentChunk));\n",
        "+            console.log('LangGraph streaming completed');\n",
        " \n",
        "-            // Send done signal\n",
        "-            const doneChunk = `data: ${JSON.stringify({\n",
        "-              type: 'done',\n",
        "-              timestamp: new Date().toISOString(),\n",
        "-            })}\\n\\n`;\n",
        "-            controller.enqueue(new TextEncoder().encode(doneChunk));\n",
        "+            // Send completion event\n",
        "+            const completionEvent = {\n",
        "+              type: 'completion',\n",
        "+              data: {\n",
        "+                response: assistantResponse,\n",
        "+                totalTokens: tokenSequence,\n",
        "+                timestamp: new Date().toISOString(),\n",
        "+              },\n",
        "+            };\n",
        "+            controller.enqueue(\n",
        "+              new TextEncoder().encode(\n",
        "+                `data: ${JSON.stringify(completionEvent)}\\n\\n`\n",
        "+              )\n",
        "+            );\n",
        " \n",
        "-            // Send final [DONE] marker\n",
        "-            controller.enqueue(new TextEncoder().encode('data: [DONE]\\n\\n'));\n",
        "+            // Close the stream without duplicate [DONE] markers\n",
        "             controller.close();\n",
        "           } catch (error) {\n",
        "-            console.error('Streaming error:', error);\n",
        "-            const errorChunk = `data: ${JSON.stringify({\n",
        "+            console.error('LangGraph streaming error:', error);\n",
        "+            const errorEvent = {\n",
        "               type: 'error',\n",
        "-              error: 'Failed to generate response',\n",
        "-              timestamp: new Date().toISOString(),\n",
        "-            })}\\n\\n`;\n",
        "-            controller.enqueue(new TextEncoder().encode(errorChunk));\n",
        "+              data: {\n",
        "+                error: error.message,\n",
        "+                timestamp: new Date().toISOString(),\n",
        "+              },\n",
        "+            };\n",
        "+            controller.enqueue(\n",
        "+              new TextEncoder().encode(\n",
        "+                `data: ${JSON.stringify(errorEvent)}\\n\\n`\n",
        "+              )\n",
        "+            );\n",
        "             controller.close();\n",
        "           }\n",
        "         },\n",
        "       });\n",
        "     } catch (error) {\n",
        "-      console.error('Failed to create streaming response:', error);\n",
        "+      console.error('Failed to create LangGraph streaming response:', error);\n",
        "       throw error;\n",
        "     }\n",
        "   }\n"
      ]
    },
    {
      "path": "apps/server/src/chat/chat.service.ts",
      "status": "modified",
      "additions": 116,
      "deletions": 63,
      "patch": "@@ -3,39 +3,29 @@ import { InjectRepository } from '@nestjs/typeorm';\n import { Repository } from 'typeorm';\n import type { ChatRequest, PaginatedResponse } from '@kronos/core';\n import { StreamEventFactory, StreamEventSerializer } from '@kronos/core';\n-import { KronosAgent } from '../agents/kronos/agent';\n+import { KronosAgentBuilder } from '../agents/kronos/builder';\n import { Conversation, ChatMessage } from '../entities/conversation.entity';\n import { ChatMessageRole } from '../enum/roles.enum';\n \n @Injectable()\n export class ChatService {\n-  private kronosAgent: KronosAgent;\n-\n   constructor(\n     @InjectRepository(Conversation)\n     private conversationRepository: Repository<Conversation>\n-  ) {\n-    try {\n-      this.kronosAgent = new KronosAgent();\n-    } catch (error) {\n-      console.error('Failed to initialize KronosAgent:', error);\n-      throw new Error(\n-        'Failed to initialize AI service. Please check your GEMINI_API_KEY environment variable.'\n-      );\n-    }\n-  }\n+  ) {}\n \n   /**\n-   * Send a chat message with streaming response using LangChain and Gemini\n+   * Send a chat message with streaming response using LangGraph streaming\n    * @param request The chat request\n    * @param userId The user ID\n    * @returns A ReadableStream that sends the response\n    */\n-  async sendMessage(request: ChatRequest, userId: string): Promise<ReadableStream> {\n-    const kronosAgent = this.kronosAgent; // Capture reference to avoid 'this' context issues\n-    const conversationRepository = this.conversationRepository; // Capture repository reference\n-    const sessionId = `session_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n-    const correlationId = `chat_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n+  async sendMessage(\n+    request: ChatRequest,\n+    userId: string\n+  ): Promise<ReadableStream> {\n+    const kronosAgentBuilder = new KronosAgentBuilder(userId);\n+    const conversationRepository = this.conversationRepository;\n \n     return new ReadableStream({\n       async start(controller) {\n@@ -44,18 +34,15 @@ export class ChatService {\n         let isNewConversation = false;\n \n         try {\n-          // Get or create conversation\n           if (request.conversationId) {\n-            // Load existing conversation\n             conversation = await conversationRepository.findOne({\n-              where: { id: request.conversationId, createdBy: userId }\n+              where: { id: request.conversationId, createdBy: userId },\n             });\n-            \n+\n             if (!conversation) {\n               throw new Error('Conversation not found');\n             }\n           } else {\n-            // Create new conversation\n             isNewConversation = true;\n             conversation = conversationRepository.create({\n               title: null,\n@@ -67,14 +54,16 @@ export class ChatService {\n             await conversationRepository.save(conversation);\n           }\n \n-          // Send START event\n           const startEvent = StreamEventFactory.createStartEvent(\n             conversation.id,\n-            isNewConversation,\n+            isNewConversation\n+          );\n+          controller.enqueue(\n+            new TextEncoder().encode(\n+              StreamEventSerializer.serialize(startEvent)\n+            )\n           );\n-          controller.enqueue(new TextEncoder().encode(StreamEventSerializer.serialize(startEvent)));\n \n-          // Add user message to conversation\n           const userMessage: ChatMessage = {\n             role: ChatMessageRole.USER,\n             content: request.message,\n@@ -83,12 +72,14 @@ export class ChatService {\n \n           conversation.messages.push(userMessage);\n \n-          // Get streaming response from Kronos agent\n-          const stream = await kronosAgent.streamResponse(\n+          const stream = await kronosAgentBuilder.streamResponse(\n             request.message,\n             request.conversationHistory || [],\n             userId,\n-            { correlationId }\n+            {\n+              conversationId: conversation.id,\n+              streamModes: ['updates', 'messages', 'custom'],\n+            }\n           );\n \n           const reader = stream.getReader();\n@@ -102,7 +93,6 @@ export class ChatService {\n               break;\n             }\n \n-            // Decode and process assistant response\n             const chunk = new TextDecoder().decode(value);\n             if (chunk.includes('data: ')) {\n               const lines = chunk.split('\\n');\n@@ -112,29 +102,78 @@ export class ChatService {\n                   if (data !== '[DONE]' && data !== '') {\n                     try {\n                       const parsed = JSON.parse(data);\n-                      if (parsed.type === 'content' && parsed.data) {\n-                        // Stream tokens for better UX\n-                        const content = parsed.data;\n-                        const tokens = content.split(/(\\s+)/);\n-                        \n-                        for (const token of tokens) {\n-                          if (token.trim()) {\n+\n+                      // Handle different stream event types from LangGraph\n+                      switch (parsed.type) {\n+                        case 'agent_progress':\n+                          // Agent progress updates - forward to client\n+                          const progressEvent =\n+                            StreamEventFactory.createTokenEvent(\n+                              `\ud83e\udd16 ${parsed.data.node} completed`\n+                            );\n+                          controller.enqueue(\n+                            new TextEncoder().encode(\n+                              StreamEventSerializer.serialize(progressEvent)\n+                            )\n+                          );\n+                          break;\n+\n+                        case 'token':\n+                          // LLM tokens - stream to client (avoid duplicates)\n+                          if (parsed.data && parsed.data.trim()) {\n                             tokenSequence++;\n-                            const tokenEvent = StreamEventFactory.createTokenEvent(token);\n-                            controller.enqueue(new TextEncoder().encode(StreamEventSerializer.serialize(tokenEvent)));\n-                            assistantResponse += token;\n+                            const tokenEvent =\n+                              StreamEventFactory.createTokenEvent(parsed.data);\n+                            controller.enqueue(\n+                              new TextEncoder().encode(\n+                                StreamEventSerializer.serialize(tokenEvent)\n+                              )\n+                            );\n+                            assistantResponse += parsed.data;\n                           }\n-                        }\n+                          break;\n+\n+                        case 'tool_update':\n+                          // Tool execution updates - forward to client\n+                          const toolEvent = StreamEventFactory.createTokenEvent(\n+                            `\ud83d\udd27 ${parsed.data}`\n+                          );\n+                          controller.enqueue(\n+                            new TextEncoder().encode(\n+                              StreamEventSerializer.serialize(toolEvent)\n+                            )\n+                          );\n+                          break;\n+\n+                        case 'completion':\n+                          // Final completion event\n+                          console.log('LangGraph streaming completed');\n+                          break;\n+\n+                        case 'error':\n+                          // Error handling\n+                          const errorEvent =\n+                            StreamEventFactory.createTokenEvent(\n+                              `\u274c ${parsed.data.error}`\n+                            );\n+                          controller.enqueue(\n+                            new TextEncoder().encode(\n+                              StreamEventSerializer.serialize(errorEvent)\n+                            )\n+                          );\n+                          break;\n                       }\n                     } catch (e) {\n                       // Skip invalid JSON\n+                      console.warn('Failed to parse streaming data:', e);\n                     }\n                   }\n                 }\n               }\n+            } else {\n+              // Forward non-data chunks directly to client\n+              controller.enqueue(value);\n             }\n-\n-            controller.enqueue(value);\n           }\n \n           // Add assistant message to conversation\n@@ -148,7 +187,9 @@ export class ChatService {\n \n             // Update conversation title if it's new and this is the first exchange\n             if (isNewConversation && conversation.messages.length === 2) {\n-              conversation.title = request.message.slice(0, 50) + (request.message.length > 50 ? '...' : '');\n+              conversation.title =\n+                request.message.slice(0, 50) +\n+                (request.message.length > 50 ? '...' : '');\n             }\n \n             // Save updated conversation\n@@ -159,19 +200,24 @@ export class ChatService {\n           // Send END event\n           const processingTime = Date.now() - startTime;\n           const endEvent = StreamEventFactory.createEndEvent(conversation.id);\n-          controller.enqueue(new TextEncoder().encode(StreamEventSerializer.serialize(endEvent)));\n+          controller.enqueue(\n+            new TextEncoder().encode(StreamEventSerializer.serialize(endEvent))\n+          );\n \n           controller.close();\n         } catch (error) {\n           console.error('Chat service error:', error);\n \n           // Send error event\n-          // For now, we'll create a simple token event with error message\n-          const errorEvent = StreamEventFactory.createTokenEvent(`Error: ${error.message}`);\n-          controller.enqueue(new TextEncoder().encode(StreamEventSerializer.serialize(errorEvent)));\n+          const errorEvent = StreamEventFactory.createTokenEvent(\n+            `Error: ${error.message}`\n+          );\n+          controller.enqueue(\n+            new TextEncoder().encode(\n+              StreamEventSerializer.serialize(errorEvent)\n+            )\n+          );\n \n-          // Send final [DONE] marker\n-          controller.enqueue(new TextEncoder().encode('data: [DONE]\\n\\n'));\n           controller.close();\n         }\n       },\n@@ -198,18 +244,19 @@ export class ChatService {\n    * @returns Paginated conversations\n    */\n   async getConversationsPaginated(\n-    userId: string, \n-    page: number = 1, \n+    userId: string,\n+    page: number = 1,\n     limit: number = 10\n   ): Promise<PaginatedResponse<Conversation>> {\n     const skip = (page - 1) * limit;\n-    \n-    const [conversations, total] = await this.conversationRepository.findAndCount({\n-      where: { createdBy: userId },\n-      order: { updatedAt: 'DESC' },\n-      skip,\n-      take: limit,\n-    });\n+\n+    const [conversations, total] =\n+      await this.conversationRepository.findAndCount({\n+        where: { createdBy: userId },\n+        order: { updatedAt: 'DESC' },\n+        skip,\n+        take: limit,\n+      });\n \n     const totalPages = Math.ceil(total / limit);\n \n@@ -228,7 +275,10 @@ export class ChatService {\n    * @param userId The user ID\n    * @returns Conversation with messages\n    */\n-  async getConversationMessages(conversationId: string, userId: string): Promise<Conversation | null> {\n+  async getConversationMessages(\n+    conversationId: string,\n+    userId: string\n+  ): Promise<Conversation | null> {\n     return this.conversationRepository.findOne({\n       where: { id: conversationId, createdBy: userId },\n     });\n@@ -240,7 +290,10 @@ export class ChatService {\n    * @param userId The user ID\n    * @returns Success status\n    */\n-  async deleteConversation(conversationId: string, userId: string): Promise<{ success: boolean; message: string }> {\n+  async deleteConversation(\n+    conversationId: string,\n+    userId: string\n+  ): Promise<{ success: boolean; message: string }> {\n     try {\n       const conversation = await this.conversationRepository.findOne({\n         where: { id: conversationId, createdBy: userId },",
      "patch_lines": [
        "@@ -3,39 +3,29 @@ import { InjectRepository } from '@nestjs/typeorm';\n",
        " import { Repository } from 'typeorm';\n",
        " import type { ChatRequest, PaginatedResponse } from '@kronos/core';\n",
        " import { StreamEventFactory, StreamEventSerializer } from '@kronos/core';\n",
        "-import { KronosAgent } from '../agents/kronos/agent';\n",
        "+import { KronosAgentBuilder } from '../agents/kronos/builder';\n",
        " import { Conversation, ChatMessage } from '../entities/conversation.entity';\n",
        " import { ChatMessageRole } from '../enum/roles.enum';\n",
        " \n",
        " @Injectable()\n",
        " export class ChatService {\n",
        "-  private kronosAgent: KronosAgent;\n",
        "-\n",
        "   constructor(\n",
        "     @InjectRepository(Conversation)\n",
        "     private conversationRepository: Repository<Conversation>\n",
        "-  ) {\n",
        "-    try {\n",
        "-      this.kronosAgent = new KronosAgent();\n",
        "-    } catch (error) {\n",
        "-      console.error('Failed to initialize KronosAgent:', error);\n",
        "-      throw new Error(\n",
        "-        'Failed to initialize AI service. Please check your GEMINI_API_KEY environment variable.'\n",
        "-      );\n",
        "-    }\n",
        "-  }\n",
        "+  ) {}\n",
        " \n",
        "   /**\n",
        "-   * Send a chat message with streaming response using LangChain and Gemini\n",
        "+   * Send a chat message with streaming response using LangGraph streaming\n",
        "    * @param request The chat request\n",
        "    * @param userId The user ID\n",
        "    * @returns A ReadableStream that sends the response\n",
        "    */\n",
        "-  async sendMessage(request: ChatRequest, userId: string): Promise<ReadableStream> {\n",
        "-    const kronosAgent = this.kronosAgent; // Capture reference to avoid 'this' context issues\n",
        "-    const conversationRepository = this.conversationRepository; // Capture repository reference\n",
        "-    const sessionId = `session_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n",
        "-    const correlationId = `chat_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n",
        "+  async sendMessage(\n",
        "+    request: ChatRequest,\n",
        "+    userId: string\n",
        "+  ): Promise<ReadableStream> {\n",
        "+    const kronosAgentBuilder = new KronosAgentBuilder(userId);\n",
        "+    const conversationRepository = this.conversationRepository;\n",
        " \n",
        "     return new ReadableStream({\n",
        "       async start(controller) {\n",
        "@@ -44,18 +34,15 @@ export class ChatService {\n",
        "         let isNewConversation = false;\n",
        " \n",
        "         try {\n",
        "-          // Get or create conversation\n",
        "           if (request.conversationId) {\n",
        "-            // Load existing conversation\n",
        "             conversation = await conversationRepository.findOne({\n",
        "-              where: { id: request.conversationId, createdBy: userId }\n",
        "+              where: { id: request.conversationId, createdBy: userId },\n",
        "             });\n",
        "-            \n",
        "+\n",
        "             if (!conversation) {\n",
        "               throw new Error('Conversation not found');\n",
        "             }\n",
        "           } else {\n",
        "-            // Create new conversation\n",
        "             isNewConversation = true;\n",
        "             conversation = conversationRepository.create({\n",
        "               title: null,\n",
        "@@ -67,14 +54,16 @@ export class ChatService {\n",
        "             await conversationRepository.save(conversation);\n",
        "           }\n",
        " \n",
        "-          // Send START event\n",
        "           const startEvent = StreamEventFactory.createStartEvent(\n",
        "             conversation.id,\n",
        "-            isNewConversation,\n",
        "+            isNewConversation\n",
        "+          );\n",
        "+          controller.enqueue(\n",
        "+            new TextEncoder().encode(\n",
        "+              StreamEventSerializer.serialize(startEvent)\n",
        "+            )\n",
        "           );\n",
        "-          controller.enqueue(new TextEncoder().encode(StreamEventSerializer.serialize(startEvent)));\n",
        " \n",
        "-          // Add user message to conversation\n",
        "           const userMessage: ChatMessage = {\n",
        "             role: ChatMessageRole.USER,\n",
        "             content: request.message,\n",
        "@@ -83,12 +72,14 @@ export class ChatService {\n",
        " \n",
        "           conversation.messages.push(userMessage);\n",
        " \n",
        "-          // Get streaming response from Kronos agent\n",
        "-          const stream = await kronosAgent.streamResponse(\n",
        "+          const stream = await kronosAgentBuilder.streamResponse(\n",
        "             request.message,\n",
        "             request.conversationHistory || [],\n",
        "             userId,\n",
        "-            { correlationId }\n",
        "+            {\n",
        "+              conversationId: conversation.id,\n",
        "+              streamModes: ['updates', 'messages', 'custom'],\n",
        "+            }\n",
        "           );\n",
        " \n",
        "           const reader = stream.getReader();\n",
        "@@ -102,7 +93,6 @@ export class ChatService {\n",
        "               break;\n",
        "             }\n",
        " \n",
        "-            // Decode and process assistant response\n",
        "             const chunk = new TextDecoder().decode(value);\n",
        "             if (chunk.includes('data: ')) {\n",
        "               const lines = chunk.split('\\n');\n",
        "@@ -112,29 +102,78 @@ export class ChatService {\n",
        "                   if (data !== '[DONE]' && data !== '') {\n",
        "                     try {\n",
        "                       const parsed = JSON.parse(data);\n",
        "-                      if (parsed.type === 'content' && parsed.data) {\n",
        "-                        // Stream tokens for better UX\n",
        "-                        const content = parsed.data;\n",
        "-                        const tokens = content.split(/(\\s+)/);\n",
        "-                        \n",
        "-                        for (const token of tokens) {\n",
        "-                          if (token.trim()) {\n",
        "+\n",
        "+                      // Handle different stream event types from LangGraph\n",
        "+                      switch (parsed.type) {\n",
        "+                        case 'agent_progress':\n",
        "+                          // Agent progress updates - forward to client\n",
        "+                          const progressEvent =\n",
        "+                            StreamEventFactory.createTokenEvent(\n",
        "+                              `\ud83e\udd16 ${parsed.data.node} completed`\n",
        "+                            );\n",
        "+                          controller.enqueue(\n",
        "+                            new TextEncoder().encode(\n",
        "+                              StreamEventSerializer.serialize(progressEvent)\n",
        "+                            )\n",
        "+                          );\n",
        "+                          break;\n",
        "+\n",
        "+                        case 'token':\n",
        "+                          // LLM tokens - stream to client (avoid duplicates)\n",
        "+                          if (parsed.data && parsed.data.trim()) {\n",
        "                             tokenSequence++;\n",
        "-                            const tokenEvent = StreamEventFactory.createTokenEvent(token);\n",
        "-                            controller.enqueue(new TextEncoder().encode(StreamEventSerializer.serialize(tokenEvent)));\n",
        "-                            assistantResponse += token;\n",
        "+                            const tokenEvent =\n",
        "+                              StreamEventFactory.createTokenEvent(parsed.data);\n",
        "+                            controller.enqueue(\n",
        "+                              new TextEncoder().encode(\n",
        "+                                StreamEventSerializer.serialize(tokenEvent)\n",
        "+                              )\n",
        "+                            );\n",
        "+                            assistantResponse += parsed.data;\n",
        "                           }\n",
        "-                        }\n",
        "+                          break;\n",
        "+\n",
        "+                        case 'tool_update':\n",
        "+                          // Tool execution updates - forward to client\n",
        "+                          const toolEvent = StreamEventFactory.createTokenEvent(\n",
        "+                            `\ud83d\udd27 ${parsed.data}`\n",
        "+                          );\n",
        "+                          controller.enqueue(\n",
        "+                            new TextEncoder().encode(\n",
        "+                              StreamEventSerializer.serialize(toolEvent)\n",
        "+                            )\n",
        "+                          );\n",
        "+                          break;\n",
        "+\n",
        "+                        case 'completion':\n",
        "+                          // Final completion event\n",
        "+                          console.log('LangGraph streaming completed');\n",
        "+                          break;\n",
        "+\n",
        "+                        case 'error':\n",
        "+                          // Error handling\n",
        "+                          const errorEvent =\n",
        "+                            StreamEventFactory.createTokenEvent(\n",
        "+                              `\u274c ${parsed.data.error}`\n",
        "+                            );\n",
        "+                          controller.enqueue(\n",
        "+                            new TextEncoder().encode(\n",
        "+                              StreamEventSerializer.serialize(errorEvent)\n",
        "+                            )\n",
        "+                          );\n",
        "+                          break;\n",
        "                       }\n",
        "                     } catch (e) {\n",
        "                       // Skip invalid JSON\n",
        "+                      console.warn('Failed to parse streaming data:', e);\n",
        "                     }\n",
        "                   }\n",
        "                 }\n",
        "               }\n",
        "+            } else {\n",
        "+              // Forward non-data chunks directly to client\n",
        "+              controller.enqueue(value);\n",
        "             }\n",
        "-\n",
        "-            controller.enqueue(value);\n",
        "           }\n",
        " \n",
        "           // Add assistant message to conversation\n",
        "@@ -148,7 +187,9 @@ export class ChatService {\n",
        " \n",
        "             // Update conversation title if it's new and this is the first exchange\n",
        "             if (isNewConversation && conversation.messages.length === 2) {\n",
        "-              conversation.title = request.message.slice(0, 50) + (request.message.length > 50 ? '...' : '');\n",
        "+              conversation.title =\n",
        "+                request.message.slice(0, 50) +\n",
        "+                (request.message.length > 50 ? '...' : '');\n",
        "             }\n",
        " \n",
        "             // Save updated conversation\n",
        "@@ -159,19 +200,24 @@ export class ChatService {\n",
        "           // Send END event\n",
        "           const processingTime = Date.now() - startTime;\n",
        "           const endEvent = StreamEventFactory.createEndEvent(conversation.id);\n",
        "-          controller.enqueue(new TextEncoder().encode(StreamEventSerializer.serialize(endEvent)));\n",
        "+          controller.enqueue(\n",
        "+            new TextEncoder().encode(StreamEventSerializer.serialize(endEvent))\n",
        "+          );\n",
        " \n",
        "           controller.close();\n",
        "         } catch (error) {\n",
        "           console.error('Chat service error:', error);\n",
        " \n",
        "           // Send error event\n",
        "-          // For now, we'll create a simple token event with error message\n",
        "-          const errorEvent = StreamEventFactory.createTokenEvent(`Error: ${error.message}`);\n",
        "-          controller.enqueue(new TextEncoder().encode(StreamEventSerializer.serialize(errorEvent)));\n",
        "+          const errorEvent = StreamEventFactory.createTokenEvent(\n",
        "+            `Error: ${error.message}`\n",
        "+          );\n",
        "+          controller.enqueue(\n",
        "+            new TextEncoder().encode(\n",
        "+              StreamEventSerializer.serialize(errorEvent)\n",
        "+            )\n",
        "+          );\n",
        " \n",
        "-          // Send final [DONE] marker\n",
        "-          controller.enqueue(new TextEncoder().encode('data: [DONE]\\n\\n'));\n",
        "           controller.close();\n",
        "         }\n",
        "       },\n",
        "@@ -198,18 +244,19 @@ export class ChatService {\n",
        "    * @returns Paginated conversations\n",
        "    */\n",
        "   async getConversationsPaginated(\n",
        "-    userId: string, \n",
        "-    page: number = 1, \n",
        "+    userId: string,\n",
        "+    page: number = 1,\n",
        "     limit: number = 10\n",
        "   ): Promise<PaginatedResponse<Conversation>> {\n",
        "     const skip = (page - 1) * limit;\n",
        "-    \n",
        "-    const [conversations, total] = await this.conversationRepository.findAndCount({\n",
        "-      where: { createdBy: userId },\n",
        "-      order: { updatedAt: 'DESC' },\n",
        "-      skip,\n",
        "-      take: limit,\n",
        "-    });\n",
        "+\n",
        "+    const [conversations, total] =\n",
        "+      await this.conversationRepository.findAndCount({\n",
        "+        where: { createdBy: userId },\n",
        "+        order: { updatedAt: 'DESC' },\n",
        "+        skip,\n",
        "+        take: limit,\n",
        "+      });\n",
        " \n",
        "     const totalPages = Math.ceil(total / limit);\n",
        " \n",
        "@@ -228,7 +275,10 @@ export class ChatService {\n",
        "    * @param userId The user ID\n",
        "    * @returns Conversation with messages\n",
        "    */\n",
        "-  async getConversationMessages(conversationId: string, userId: string): Promise<Conversation | null> {\n",
        "+  async getConversationMessages(\n",
        "+    conversationId: string,\n",
        "+    userId: string\n",
        "+  ): Promise<Conversation | null> {\n",
        "     return this.conversationRepository.findOne({\n",
        "       where: { id: conversationId, createdBy: userId },\n",
        "     });\n",
        "@@ -240,7 +290,10 @@ export class ChatService {\n",
        "    * @param userId The user ID\n",
        "    * @returns Success status\n",
        "    */\n",
        "-  async deleteConversation(conversationId: string, userId: string): Promise<{ success: boolean; message: string }> {\n",
        "+  async deleteConversation(\n",
        "+    conversationId: string,\n",
        "+    userId: string\n",
        "+  ): Promise<{ success: boolean; message: string }> {\n",
        "     try {\n",
        "       const conversation = await this.conversationRepository.findOne({\n",
        "         where: { id: conversationId, createdBy: userId },\n"
      ]
    }
  ]
}
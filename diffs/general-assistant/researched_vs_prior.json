{
  "project": "Research Data/general-assistant",
  "repo": "kafkaGen/general-assistant",
  "prior_commit": "fe11f9b7b610817d5e1f08d417d1f4009f39fa7d",
  "researched_commit": "c83e2588e088adf5f4803dff9688ef07f5e925f3",
  "compare_url": "https://github.com/kafkaGen/general-assistant/compare/fe11f9b7b610817d5e1f08d417d1f4009f39fa7d...c83e2588e088adf5f4803dff9688ef07f5e925f3",
  "ahead_by": 1,
  "behind_by": 0,
  "changed_files": [
    {
      "path": ".env.example",
      "status": "modified",
      "additions": 11,
      "deletions": 1,
      "patch": "@@ -1,4 +1,14 @@\n # API keys\n HUGGING_FACE_TOKEN=<token>\n OPENAI_API_KEY=<token>\n-ANTHROPIC_API_KEY=<token>\n\\ No newline at end of file\n+ANTHROPIC_API_KEY=<token>\n+TAVILY_API_KEY=<token>\n+LANGSMITH_API_KEY=<token>\n+LANGSMITH_TRACING=true\n+LANGSMITH_PROJECT=general-assistant\n+\n+# WEBUI\n+WEBUI_PORT=8080\n+\n+# ASSISTANT\n+ASSISTANT_PORT=8000\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -1,4 +1,14 @@\n",
        " # API keys\n",
        " HUGGING_FACE_TOKEN=<token>\n",
        " OPENAI_API_KEY=<token>\n",
        "-ANTHROPIC_API_KEY=<token>\n",
        "\\ No newline at end of file\n",
        "+ANTHROPIC_API_KEY=<token>\n",
        "+TAVILY_API_KEY=<token>\n",
        "+LANGSMITH_API_KEY=<token>\n",
        "+LANGSMITH_TRACING=true\n",
        "+LANGSMITH_PROJECT=general-assistant\n",
        "+\n",
        "+# WEBUI\n",
        "+WEBUI_PORT=8080\n",
        "+\n",
        "+# ASSISTANT\n",
        "+ASSISTANT_PORT=8000\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": ".pre-commit-config.yaml",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "patch": "@@ -17,12 +17,12 @@ repos:\n       args: [--py311-plus]\n \n   - repo: https://github.com/astral-sh/uv-pre-commit\n-    rev: 0.7.19\n+    rev: 0.7.22\n     hooks:\n       - id: uv-lock\n \n   - repo: https://github.com/astral-sh/ruff-pre-commit\n-    rev: v0.12.1\n+    rev: v0.12.4\n     hooks:\n       - id: ruff\n       - id: ruff-format",
      "patch_lines": [
        "@@ -17,12 +17,12 @@ repos:\n",
        "       args: [--py311-plus]\n",
        " \n",
        "   - repo: https://github.com/astral-sh/uv-pre-commit\n",
        "-    rev: 0.7.19\n",
        "+    rev: 0.7.22\n",
        "     hooks:\n",
        "       - id: uv-lock\n",
        " \n",
        "   - repo: https://github.com/astral-sh/ruff-pre-commit\n",
        "-    rev: v0.12.1\n",
        "+    rev: v0.12.4\n",
        "     hooks:\n",
        "       - id: ruff\n",
        "       - id: ruff-format\n"
      ]
    },
    {
      "path": "Makefile",
      "status": "modified",
      "additions": 13,
      "deletions": 3,
      "patch": "@@ -1,6 +1,6 @@\n # Makefile for the General Assistant project\n \n-.PHONY: help setup-venv sync-venv update-venv run-assistant run-webui download-dataset\n+.PHONY: help setup-venv sync-venv update-venv run-assistant run-webui download-dataset run-docker-compose stop-docker-compose\n \n # Set default goal to 'help'\n .DEFAULT_GOAL := help\n@@ -14,6 +14,8 @@ help:\n \t@echo \"  make run-assistant         - Run the assistant API server\"\n \t@echo \"  make run-webui             - Run the assistant Web UI server\"\n \t@echo \"  make run-langgraph-studio  - Run the LangGraph Studio server\"\n+\t@echo \"  make run-docker-compose    - Run the Docker Compose services\"\n+\t@echo \"  make stop-docker-compose   - Stop the Docker Compose services\"\n \n setup-venv:\n \t@echo \"Setting up development environment...\"\n@@ -43,8 +45,16 @@ run-assistant:\n \n run-webui:\n \t@echo \"Starting the assistant Web UI server...\"\n-\tuv run chainlit run src/webui/chainlit_main.py -h --no-cache --port 8080\n+\tuv run chainlit run src/webui/chainlit_main.py -h --no-cache --port 8080 -w\n \n run-langgraph-studio:\n \t@echo \"Starting the LangGraph Studio server...\"\n-\tuv run langgraph dev --config config/langgraph_studio.json --port 5677 --debug-port 5678\n\\ No newline at end of file\n+\tuv run langgraph dev --config scripts/langgraph_studio.json --port 5677 --debug-port 5678\n+\n+run-docker-compose:\n+\t@echo \"Starting the Docker Compose services...\"\n+\tdocker compose --env-file .env -f deployment/docker/docker-compose.yaml -p general-assistant up --build -d\n+\n+stop-docker-compose:\n+\t@echo \"Stopping the Docker Compose services...\"\n+\tdocker compose -p general-assistant down\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -1,6 +1,6 @@\n",
        " # Makefile for the General Assistant project\n",
        " \n",
        "-.PHONY: help setup-venv sync-venv update-venv run-assistant run-webui download-dataset\n",
        "+.PHONY: help setup-venv sync-venv update-venv run-assistant run-webui download-dataset run-docker-compose stop-docker-compose\n",
        " \n",
        " # Set default goal to 'help'\n",
        " .DEFAULT_GOAL := help\n",
        "@@ -14,6 +14,8 @@ help:\n",
        " \t@echo \"  make run-assistant         - Run the assistant API server\"\n",
        " \t@echo \"  make run-webui             - Run the assistant Web UI server\"\n",
        " \t@echo \"  make run-langgraph-studio  - Run the LangGraph Studio server\"\n",
        "+\t@echo \"  make run-docker-compose    - Run the Docker Compose services\"\n",
        "+\t@echo \"  make stop-docker-compose   - Stop the Docker Compose services\"\n",
        " \n",
        " setup-venv:\n",
        " \t@echo \"Setting up development environment...\"\n",
        "@@ -43,8 +45,16 @@ run-assistant:\n",
        " \n",
        " run-webui:\n",
        " \t@echo \"Starting the assistant Web UI server...\"\n",
        "-\tuv run chainlit run src/webui/chainlit_main.py -h --no-cache --port 8080\n",
        "+\tuv run chainlit run src/webui/chainlit_main.py -h --no-cache --port 8080 -w\n",
        " \n",
        " run-langgraph-studio:\n",
        " \t@echo \"Starting the LangGraph Studio server...\"\n",
        "-\tuv run langgraph dev --config config/langgraph_studio.json --port 5677 --debug-port 5678\n",
        "\\ No newline at end of file\n",
        "+\tuv run langgraph dev --config scripts/langgraph_studio.json --port 5677 --debug-port 5678\n",
        "+\n",
        "+run-docker-compose:\n",
        "+\t@echo \"Starting the Docker Compose services...\"\n",
        "+\tdocker compose --env-file .env -f deployment/docker/docker-compose.yaml -p general-assistant up --build -d\n",
        "+\n",
        "+stop-docker-compose:\n",
        "+\t@echo \"Stopping the Docker Compose services...\"\n",
        "+\tdocker compose -p general-assistant down\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "config/langgraph_studio.json",
      "status": "removed",
      "additions": 0,
      "deletions": 7,
      "patch": "@@ -1,7 +0,0 @@\n-{\n-  \"dependencies\": [\"./src/general_assistant\"],\n-  \"graphs\": {\n-    \"agent\": \"./src/general_assistant/utils/langgraph_studio.py:graph\"\n-  },\n-  \"env\": \".env\"\n-}\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -1,7 +0,0 @@\n",
        "-{\n",
        "-  \"dependencies\": [\"./src/general_assistant\"],\n",
        "-  \"graphs\": {\n",
        "-    \"agent\": \"./src/general_assistant/utils/langgraph_studio.py:graph\"\n",
        "-  },\n",
        "-  \"env\": \".env\"\n",
        "-}\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "deployment/docker/docker-compose.yaml",
      "status": "added",
      "additions": 14,
      "deletions": 0,
      "patch": "@@ -0,0 +1,14 @@\n+services:\n+  webui:\n+    build:\n+      context: ../../\n+      dockerfile: ./deployment/docker/images/webui.Dockerfile\n+    image: webui\n+    container_name: webui_container\n+    restart: unless-stopped\n+    env_file:\n+      - ../../.env\n+    ports:\n+      - ${WEBUI_PORT}:8080\n+    environment:\n+      - ASSISTANT_BASE_URL=http://host.docker.internal:${ASSISTANT_PORT}\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -0,0 +1,14 @@\n",
        "+services:\n",
        "+  webui:\n",
        "+    build:\n",
        "+      context: ../../\n",
        "+      dockerfile: ./deployment/docker/images/webui.Dockerfile\n",
        "+    image: webui\n",
        "+    container_name: webui_container\n",
        "+    restart: unless-stopped\n",
        "+    env_file:\n",
        "+      - ../../.env\n",
        "+    ports:\n",
        "+      - ${WEBUI_PORT}:8080\n",
        "+    environment:\n",
        "+      - ASSISTANT_BASE_URL=http://host.docker.internal:${ASSISTANT_PORT}\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "deployment/docker/images/webui.Dockerfile",
      "status": "added",
      "additions": 29,
      "deletions": 0,
      "patch": "@@ -0,0 +1,29 @@\n+FROM python:3.11-slim-buster AS builder\n+\n+ENV PYTHONDONTWRITEBYTECODE=1 \\\n+    PYTHONUNBUFFERED=1 \\\n+    DEBIAN_FRONTEND=noninteractive\n+\n+RUN pip install uv\n+\n+COPY pyproject.toml uv.lock ./\n+\n+RUN uv sync --frozen\n+\n+####################\n+\n+FROM python:3.11-slim-buster AS runtime\n+\n+WORKDIR /app\n+\n+COPY --from=builder /.venv /.venv\n+\n+ENV PATH=\"/.venv/bin:$PATH\" \\\n+    PYTHONDONTWRITEBYTECODE=1 \\\n+    PYTHONUNBUFFERED=1 \\\n+    PYTHONPATH=/app\n+\n+COPY src /app/src\n+COPY chainlit.md /app/chainlit.md\n+\n+CMD [\"chainlit\", \"run\", \"src/webui/chainlit_main.py\", \"-h\", \"--no-cache\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -0,0 +1,29 @@\n",
        "+FROM python:3.11-slim-buster AS builder\n",
        "+\n",
        "+ENV PYTHONDONTWRITEBYTECODE=1 \\\n",
        "+    PYTHONUNBUFFERED=1 \\\n",
        "+    DEBIAN_FRONTEND=noninteractive\n",
        "+\n",
        "+RUN pip install uv\n",
        "+\n",
        "+COPY pyproject.toml uv.lock ./\n",
        "+\n",
        "+RUN uv sync --frozen\n",
        "+\n",
        "+####################\n",
        "+\n",
        "+FROM python:3.11-slim-buster AS runtime\n",
        "+\n",
        "+WORKDIR /app\n",
        "+\n",
        "+COPY --from=builder /.venv /.venv\n",
        "+\n",
        "+ENV PATH=\"/.venv/bin:$PATH\" \\\n",
        "+    PYTHONDONTWRITEBYTECODE=1 \\\n",
        "+    PYTHONUNBUFFERED=1 \\\n",
        "+    PYTHONPATH=/app\n",
        "+\n",
        "+COPY src /app/src\n",
        "+COPY chainlit.md /app/chainlit.md\n",
        "+\n",
        "+CMD [\"chainlit\", \"run\", \"src/webui/chainlit_main.py\", \"-h\", \"--no-cache\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "doc/ProjectStatus.md",
      "status": "renamed",
      "additions": 13,
      "deletions": 4,
      "patch": "@@ -17,16 +17,25 @@\n - [x] develop Agent Workflow with web search tool\n - [x] add for agent two tools - search the web and Python interprenter (Calculator)\n \n-- [ ] In WebUI i want to see chat as user input, agent final output and hidden (or not) intermediate results with expand tools output\n-- [ ] I want my endpoints output the structure with list of new messages\n-- [ ] I want to stream messages to the WebUI (separate endpoints for stream and not stream)\n-- [ ] refactor endpoint inputs and outputs models\n+- [x] In WebUI i want to see chat as user input, agent final output and hidden (or not) intermediate results with expand tools output\n+    - [x] update chainlit to hide intermediat state, pretty print of jsons (showing intermediat step during streaming)\n+    - [x] issue with double response\n+    - [x] error on continued dialog\n+    - [x] deploy with docker and left\n+- [x] I want my endpoints output the structure with list of new messages\n+- [x] I want to stream messages to the WebUI (separate endpoints for stream and not stream)\n+- [x] refactor endpoint inputs and outputs models\n - [x] I want to observe graph with langgraph studio\n - [x] I want to manage all prompts with langsmith\n - [x] I want to configure GeneralWorkflow, Tools and AgentFactory with global Settings\n - [x] I want to redo my Workflow to async and tools also\n \n+- [ ] create the evaluation pipeline with metrics and DeepEval (any better alternatives?) on task level 1\n+- [ ] finish course on the langsmith to unialize it during evaluaiton\n+- [ ] potentialy create onw ReAct like agent, do not use prebuild\n - [ ] fetch page tool can fetch to long page that blow the context window\n+- [ ] create tool to get current date or paste it with prompt\n+- [ ] use githup assistant for pr code review\n \n - [ ] get back to ShortTerm memory implementation issue\n - [ ] after come back to chat endpoint to improve the error handling from workflow and its output",
      "patch_lines": [
        "@@ -17,16 +17,25 @@\n",
        " - [x] develop Agent Workflow with web search tool\n",
        " - [x] add for agent two tools - search the web and Python interprenter (Calculator)\n",
        " \n",
        "-- [ ] In WebUI i want to see chat as user input, agent final output and hidden (or not) intermediate results with expand tools output\n",
        "-- [ ] I want my endpoints output the structure with list of new messages\n",
        "-- [ ] I want to stream messages to the WebUI (separate endpoints for stream and not stream)\n",
        "-- [ ] refactor endpoint inputs and outputs models\n",
        "+- [x] In WebUI i want to see chat as user input, agent final output and hidden (or not) intermediate results with expand tools output\n",
        "+    - [x] update chainlit to hide intermediat state, pretty print of jsons (showing intermediat step during streaming)\n",
        "+    - [x] issue with double response\n",
        "+    - [x] error on continued dialog\n",
        "+    - [x] deploy with docker and left\n",
        "+- [x] I want my endpoints output the structure with list of new messages\n",
        "+- [x] I want to stream messages to the WebUI (separate endpoints for stream and not stream)\n",
        "+- [x] refactor endpoint inputs and outputs models\n",
        " - [x] I want to observe graph with langgraph studio\n",
        " - [x] I want to manage all prompts with langsmith\n",
        " - [x] I want to configure GeneralWorkflow, Tools and AgentFactory with global Settings\n",
        " - [x] I want to redo my Workflow to async and tools also\n",
        " \n",
        "+- [ ] create the evaluation pipeline with metrics and DeepEval (any better alternatives?) on task level 1\n",
        "+- [ ] finish course on the langsmith to unialize it during evaluaiton\n",
        "+- [ ] potentialy create onw ReAct like agent, do not use prebuild\n",
        " - [ ] fetch page tool can fetch to long page that blow the context window\n",
        "+- [ ] create tool to get current date or paste it with prompt\n",
        "+- [ ] use githup assistant for pr code review\n",
        " \n",
        " - [ ] get back to ShortTerm memory implementation issue\n",
        " - [ ] after come back to chat endpoint to improve the error handling from workflow and its output\n"
      ],
      "previous_filename": ".doc/ProjectStatus.md"
    },
    {
      "path": "pyproject.toml",
      "status": "modified",
      "additions": 1,
      "deletions": 2,
      "patch": "@@ -1,7 +1,7 @@\n [project]\n name = \"general-assistant\"\n version = \"0.1.0\"\n-description = \"A general-purpose AI assistant built with FastAPI and LlamaIndex.\"\n+description = \"A general-purpose AI assistant.\"\n authors = [\n     {name = \"Oleh Borysevych\", email = \"borysevych.oleh87@gmail.com\"},\n ]\n@@ -22,7 +22,6 @@ dependencies = [\n     \"langsmith>=0.4.4\",\n     \"loguru>=0.7.3\",\n     \"numpy>=2.3.0\",\n-    \"openinference-instrumentation-llama-index>=4.3.1\",\n     \"pandas>=2.3.0\",\n     \"tavily-python>=0.7.8\",\n ]",
      "patch_lines": [
        "@@ -1,7 +1,7 @@\n",
        " [project]\n",
        " name = \"general-assistant\"\n",
        " version = \"0.1.0\"\n",
        "-description = \"A general-purpose AI assistant built with FastAPI and LlamaIndex.\"\n",
        "+description = \"A general-purpose AI assistant.\"\n",
        " authors = [\n",
        "     {name = \"Oleh Borysevych\", email = \"borysevych.oleh87@gmail.com\"},\n",
        " ]\n",
        "@@ -22,7 +22,6 @@ dependencies = [\n",
        "     \"langsmith>=0.4.4\",\n",
        "     \"loguru>=0.7.3\",\n",
        "     \"numpy>=2.3.0\",\n",
        "-    \"openinference-instrumentation-llama-index>=4.3.1\",\n",
        "     \"pandas>=2.3.0\",\n",
        "     \"tavily-python>=0.7.8\",\n",
        " ]\n"
      ]
    },
    {
      "path": "scripts/langgraph_studio.json",
      "status": "added",
      "additions": 7,
      "deletions": 0,
      "patch": "@@ -0,0 +1,7 @@\n+{\n+  \"dependencies\": [\"./\"],\n+  \"graphs\": {\n+    \"agent\": \"./scripts/langgraph_studio.py:graph\"\n+  },\n+  \"env\": \".env\"\n+}\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -0,0 +1,7 @@\n",
        "+{\n",
        "+  \"dependencies\": [\"./\"],\n",
        "+  \"graphs\": {\n",
        "+    \"agent\": \"./scripts/langgraph_studio.py:graph\"\n",
        "+  },\n",
        "+  \"env\": \".env\"\n",
        "+}\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "scripts/langgraph_studio.py",
      "status": "added",
      "additions": 14,
      "deletions": 0,
      "patch": "@@ -0,0 +1,14 @@\n+from langgraph.graph.state import CompiledStateGraph\n+\n+from src.general_assistant.config.settings import settings\n+from src.general_assistant.core.workflows import GeneralAssistantWorkflow\n+\n+\n+def get_general_assistant_workflow_graph() -> CompiledStateGraph:\n+    workflow = GeneralAssistantWorkflow(\n+        settings=settings.workflows,\n+    )\n+    return workflow.graph\n+\n+\n+graph = get_general_assistant_workflow_graph()",
      "patch_lines": [
        "@@ -0,0 +1,14 @@\n",
        "+from langgraph.graph.state import CompiledStateGraph\n",
        "+\n",
        "+from src.general_assistant.config.settings import settings\n",
        "+from src.general_assistant.core.workflows import GeneralAssistantWorkflow\n",
        "+\n",
        "+\n",
        "+def get_general_assistant_workflow_graph() -> CompiledStateGraph:\n",
        "+    workflow = GeneralAssistantWorkflow(\n",
        "+        settings=settings.workflows,\n",
        "+    )\n",
        "+    return workflow.graph\n",
        "+\n",
        "+\n",
        "+graph = get_general_assistant_workflow_graph()\n"
      ]
    },
    {
      "path": "src/general_assistant/api/routes/chat.py",
      "status": "modified",
      "additions": 8,
      "deletions": 8,
      "patch": "@@ -64,14 +64,14 @@ async def chat_invoke(\n         logger.error(\n             f\"Unexpected error processing chat request: {str(e)}\",\n             error_type=error_type,\n-            messages=chat_input.model_dump()[\"messages\"],\n             exc_info=True,\n         )\n         raise HTTPException(\n             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n             detail=(\n                 \"An unexpected internal error occurred while processing your request. \"\n-                f\"Type: {error_type}. Please try again later or contact support.\"\n+                f\"Type: {error_type}, details: {str(e)}. Please try again later or \"\n+                \"contact support.\"\n             ),\n         ) from e\n \n@@ -104,7 +104,8 @@ async def jsonl_workflow_stream(\n                     error_msg = ChatMessage(\n                         role=\"assistant\",\n                         content=(\n-                            f\"[Error processing message: {type(msg_error).__name__}]\"\n+                            f\"[Error processing message: {type(msg_error).__name__}, \"\n+                            f\"details: {str(msg_error)}]\"\n                         ),\n                     )\n                     yield f\"{error_msg.model_dump()}\\n\"\n@@ -121,8 +122,8 @@ async def jsonl_workflow_stream(\n                 role=\"assistant\",\n                 content=(\n                     f\"An error occurred while processing your request. \"\n-                    f\"Type: {error_type}. Please try again later or contact support \"\n-                    \"if the issue persists.\"\n+                    f\"Type: {error_type}, details: {str(workflow_error)}. Please try \"\n+                    \"again later or contact support if the issue persists.\"\n                 ),\n             )\n             yield f\"{error_msg.model_dump()}\\n\"\n@@ -147,14 +148,13 @@ async def jsonl_workflow_stream(\n         logger.error(\n             f\"Unexpected error in streaming chat request: {str(e)}\",\n             error_type=error_type,\n-            messages=chat_input.model_dump()[\"messages\"],\n             exc_info=True,\n         )\n         raise HTTPException(\n             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n             detail=(\n                 \"An unexpected internal error occurred while setting up your streaming \"\n-                f\"request. Type: {error_type}. Please try again later or contact \"\n-                \"support.\"\n+                f\"request. Type: {error_type}, details: {str(e)}. Please try again \"\n+                \"later or contact support.\"\n             ),\n         ) from e",
      "patch_lines": [
        "@@ -64,14 +64,14 @@ async def chat_invoke(\n",
        "         logger.error(\n",
        "             f\"Unexpected error processing chat request: {str(e)}\",\n",
        "             error_type=error_type,\n",
        "-            messages=chat_input.model_dump()[\"messages\"],\n",
        "             exc_info=True,\n",
        "         )\n",
        "         raise HTTPException(\n",
        "             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
        "             detail=(\n",
        "                 \"An unexpected internal error occurred while processing your request. \"\n",
        "-                f\"Type: {error_type}. Please try again later or contact support.\"\n",
        "+                f\"Type: {error_type}, details: {str(e)}. Please try again later or \"\n",
        "+                \"contact support.\"\n",
        "             ),\n",
        "         ) from e\n",
        " \n",
        "@@ -104,7 +104,8 @@ async def jsonl_workflow_stream(\n",
        "                     error_msg = ChatMessage(\n",
        "                         role=\"assistant\",\n",
        "                         content=(\n",
        "-                            f\"[Error processing message: {type(msg_error).__name__}]\"\n",
        "+                            f\"[Error processing message: {type(msg_error).__name__}, \"\n",
        "+                            f\"details: {str(msg_error)}]\"\n",
        "                         ),\n",
        "                     )\n",
        "                     yield f\"{error_msg.model_dump()}\\n\"\n",
        "@@ -121,8 +122,8 @@ async def jsonl_workflow_stream(\n",
        "                 role=\"assistant\",\n",
        "                 content=(\n",
        "                     f\"An error occurred while processing your request. \"\n",
        "-                    f\"Type: {error_type}. Please try again later or contact support \"\n",
        "-                    \"if the issue persists.\"\n",
        "+                    f\"Type: {error_type}, details: {str(workflow_error)}. Please try \"\n",
        "+                    \"again later or contact support if the issue persists.\"\n",
        "                 ),\n",
        "             )\n",
        "             yield f\"{error_msg.model_dump()}\\n\"\n",
        "@@ -147,14 +148,13 @@ async def jsonl_workflow_stream(\n",
        "         logger.error(\n",
        "             f\"Unexpected error in streaming chat request: {str(e)}\",\n",
        "             error_type=error_type,\n",
        "-            messages=chat_input.model_dump()[\"messages\"],\n",
        "             exc_info=True,\n",
        "         )\n",
        "         raise HTTPException(\n",
        "             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
        "             detail=(\n",
        "                 \"An unexpected internal error occurred while setting up your streaming \"\n",
        "-                f\"request. Type: {error_type}. Please try again later or contact \"\n",
        "-                \"support.\"\n",
        "+                f\"request. Type: {error_type}, details: {str(e)}. Please try again \"\n",
        "+                \"later or contact support.\"\n",
        "             ),\n",
        "         ) from e\n"
      ]
    },
    {
      "path": "src/general_assistant/api/schemas/chat_schemas.py",
      "status": "modified",
      "additions": 30,
      "deletions": 5,
      "patch": "@@ -1,5 +1,5 @@\n import json\n-from typing import Literal\n+from typing import Any, Literal\n \n from langchain_core.messages import (\n     AIMessage,\n@@ -20,6 +20,10 @@ class ChatMessage(BaseModel):\n         min_length=1,\n         description=\"The content of the message\",\n     )\n+    additional_kwargs: dict[str, Any] = Field(\n+        default_factory=dict,\n+        description=\"Additional kwargs of the message\",\n+    )\n \n     model_config = ConfigDict(\n         json_schema_extra={\n@@ -41,9 +45,15 @@ def to_langchain_message(self) -> AnyMessage:\n         if self.role == \"user\":\n             return HumanMessage(content=self.content)\n         elif self.role == \"assistant\":\n-            return AIMessage(content=self.content)\n+            return AIMessage(\n+                content=self.content,\n+                **self.additional_kwargs,\n+            )\n         elif self.role == \"tool\":\n-            return ToolMessage(content=self.content)\n+            return ToolMessage(\n+                content=self.content,\n+                **self.additional_kwargs,\n+            )\n         else:\n             raise ValueError(f\"Invalid role: {self.role}\")\n \n@@ -53,21 +63,36 @@ def from_langchain_message(cls, message: AnyMessage) -> \"ChatMessage\":\n             return cls(role=\"user\", content=message.content)\n         elif isinstance(message, AIMessage):\n             content = (message.content + \"\\n\\n\").strip()\n+            additional_kwargs = {}\n             if message.tool_calls:\n                 tool_calls = [\n                     f\"{tool_call['name']}\\n\\n{json.dumps(tool_call['args'], indent=4)}\"\n                     for tool_call in message.tool_calls\n                 ]\n                 content += \"\\n\\n\".join(tool_calls)\n-            return cls(role=\"assistant\", content=content)\n+                additional_kwargs = {\n+                    \"tool_calls\": message.tool_calls,\n+                }\n+            return cls(\n+                role=\"assistant\",\n+                content=content,\n+                additional_kwargs=additional_kwargs,\n+            )\n         elif isinstance(message, ToolMessage):\n             content = message.content\n             try:\n                 if content[0] == \"{\" and content[-1] == \"}\":\n                     content = json.dumps(json.loads(content), indent=4)\n             except json.JSONDecodeError:\n                 pass\n-            return cls(role=\"tool\", content=content)\n+            return cls(\n+                role=\"tool\",\n+                content=content,\n+                additional_kwargs={\n+                    \"tool_call_id\": message.tool_call_id,\n+                    \"name\": message.name,\n+                },\n+            )\n         else:\n             raise ValueError(f\"Invalid message type: {type(message)}\")\n ",
      "patch_lines": [
        "@@ -1,5 +1,5 @@\n",
        " import json\n",
        "-from typing import Literal\n",
        "+from typing import Any, Literal\n",
        " \n",
        " from langchain_core.messages import (\n",
        "     AIMessage,\n",
        "@@ -20,6 +20,10 @@ class ChatMessage(BaseModel):\n",
        "         min_length=1,\n",
        "         description=\"The content of the message\",\n",
        "     )\n",
        "+    additional_kwargs: dict[str, Any] = Field(\n",
        "+        default_factory=dict,\n",
        "+        description=\"Additional kwargs of the message\",\n",
        "+    )\n",
        " \n",
        "     model_config = ConfigDict(\n",
        "         json_schema_extra={\n",
        "@@ -41,9 +45,15 @@ def to_langchain_message(self) -> AnyMessage:\n",
        "         if self.role == \"user\":\n",
        "             return HumanMessage(content=self.content)\n",
        "         elif self.role == \"assistant\":\n",
        "-            return AIMessage(content=self.content)\n",
        "+            return AIMessage(\n",
        "+                content=self.content,\n",
        "+                **self.additional_kwargs,\n",
        "+            )\n",
        "         elif self.role == \"tool\":\n",
        "-            return ToolMessage(content=self.content)\n",
        "+            return ToolMessage(\n",
        "+                content=self.content,\n",
        "+                **self.additional_kwargs,\n",
        "+            )\n",
        "         else:\n",
        "             raise ValueError(f\"Invalid role: {self.role}\")\n",
        " \n",
        "@@ -53,21 +63,36 @@ def from_langchain_message(cls, message: AnyMessage) -> \"ChatMessage\":\n",
        "             return cls(role=\"user\", content=message.content)\n",
        "         elif isinstance(message, AIMessage):\n",
        "             content = (message.content + \"\\n\\n\").strip()\n",
        "+            additional_kwargs = {}\n",
        "             if message.tool_calls:\n",
        "                 tool_calls = [\n",
        "                     f\"{tool_call['name']}\\n\\n{json.dumps(tool_call['args'], indent=4)}\"\n",
        "                     for tool_call in message.tool_calls\n",
        "                 ]\n",
        "                 content += \"\\n\\n\".join(tool_calls)\n",
        "-            return cls(role=\"assistant\", content=content)\n",
        "+                additional_kwargs = {\n",
        "+                    \"tool_calls\": message.tool_calls,\n",
        "+                }\n",
        "+            return cls(\n",
        "+                role=\"assistant\",\n",
        "+                content=content,\n",
        "+                additional_kwargs=additional_kwargs,\n",
        "+            )\n",
        "         elif isinstance(message, ToolMessage):\n",
        "             content = message.content\n",
        "             try:\n",
        "                 if content[0] == \"{\" and content[-1] == \"}\":\n",
        "                     content = json.dumps(json.loads(content), indent=4)\n",
        "             except json.JSONDecodeError:\n",
        "                 pass\n",
        "-            return cls(role=\"tool\", content=content)\n",
        "+            return cls(\n",
        "+                role=\"tool\",\n",
        "+                content=content,\n",
        "+                additional_kwargs={\n",
        "+                    \"tool_call_id\": message.tool_call_id,\n",
        "+                    \"name\": message.name,\n",
        "+                },\n",
        "+            )\n",
        "         else:\n",
        "             raise ValueError(f\"Invalid message type: {type(message)}\")\n",
        " \n"
      ]
    },
    {
      "path": "src/general_assistant/config/settings.py",
      "status": "modified",
      "additions": 7,
      "deletions": 3,
      "patch": "@@ -39,9 +39,13 @@ class WebUISettings(BaseSettings):\n         \"http://127.0.0.1:8000\",\n         description=\"The base URL of the assistant service.\",\n     )\n-    assistant_chat_endpoint: str = Field(\n-        \"/v0.1.0/chat/chat_completions\",\n-        description=\"The chat endpoint path.\",\n+    assistant_chat_invoke_endpoint: str = Field(\n+        \"/v0.1.0/chat/chat_invoke\",\n+        description=\"The chat invoke endpoint path.\",\n+    )\n+    assistant_chat_stream_endpoint: str = Field(\n+        \"/v0.1.0/chat/chat_stream\",\n+        description=\"The chat stream endpoint path.\",\n     )\n \n     model_config = SettingsConfigDict(",
      "patch_lines": [
        "@@ -39,9 +39,13 @@ class WebUISettings(BaseSettings):\n",
        "         \"http://127.0.0.1:8000\",\n",
        "         description=\"The base URL of the assistant service.\",\n",
        "     )\n",
        "-    assistant_chat_endpoint: str = Field(\n",
        "-        \"/v0.1.0/chat/chat_completions\",\n",
        "-        description=\"The chat endpoint path.\",\n",
        "+    assistant_chat_invoke_endpoint: str = Field(\n",
        "+        \"/v0.1.0/chat/chat_invoke\",\n",
        "+        description=\"The chat invoke endpoint path.\",\n",
        "+    )\n",
        "+    assistant_chat_stream_endpoint: str = Field(\n",
        "+        \"/v0.1.0/chat/chat_stream\",\n",
        "+        description=\"The chat stream endpoint path.\",\n",
        "     )\n",
        " \n",
        "     model_config = SettingsConfigDict(\n"
      ]
    },
    {
      "path": "src/general_assistant/core/workflows/general_assistant_workflow.py",
      "status": "modified",
      "additions": 4,
      "deletions": 2,
      "patch": "@@ -55,6 +55,8 @@ async def stream(\n         )\n \n         async for _, node_event in stream_response:\n-            for node_event_value in node_event.values():\n-                for message in node_event_value.get(\"messages\", []):\n+            for node_name, node_output in node_event.items():\n+                if node_name == \"general_agent\":\n+                    continue\n+                for message in node_output.get(\"messages\", []):\n                     yield message",
      "patch_lines": [
        "@@ -55,6 +55,8 @@ async def stream(\n",
        "         )\n",
        " \n",
        "         async for _, node_event in stream_response:\n",
        "-            for node_event_value in node_event.values():\n",
        "-                for message in node_event_value.get(\"messages\", []):\n",
        "+            for node_name, node_output in node_event.items():\n",
        "+                if node_name == \"general_agent\":\n",
        "+                    continue\n",
        "+                for message in node_output.get(\"messages\", []):\n",
        "                     yield message\n"
      ]
    },
    {
      "path": "src/general_assistant/utils/assistant_client.py",
      "status": "modified",
      "additions": 312,
      "deletions": 197,
      "patch": "@@ -1,4 +1,7 @@\n+import ast\n import asyncio\n+import json\n+from collections.abc import AsyncGenerator, Generator\n from typing import Any\n \n import httpx\n@@ -10,40 +13,77 @@\n )\n \n from src.general_assistant.api.schemas.chat_schemas import (\n-    AssistantChatInput,\n-    AssistantChatOutput,\n-    ChatMessageSchema,\n+    ChatInput,\n+    ChatInvokeOutput,\n+    ChatMessage,\n )\n from src.general_assistant.config.logger import create_logger\n from src.general_assistant.config.settings import settings\n \n \n class AssistantClient:\n     \"\"\"\n-    Assistant client that works as both sync and async context manager.\n-    Handles HTTP client lifecycle, resource management, and robust error handling.\n+    HTTP client for the General Assistant API with sync and async support.\n+\n+    This client handles HTTP communication with the General Assistant API endpoints,\n+    providing both invoke (request-response) and streaming modes. Conversation\n+    management is handled client-side - you must provide complete conversation\n+    history as ChatMessage objects.\n+\n+    Architecture:\n+    - Pure HTTP client - no conversation state management\n+    - Client-side conversation building with ChatMessage objects\n+    - Comprehensive error handling and retry logic\n+    - Support for both sync and async operations\n+    - Streaming and non-streaming modes\n+\n+    Features:\n+    - Sync and async context management\n+    - Invoke mode: Traditional request-response chat\n+    - Streaming mode: Real-time message streaming (sync and async)\n+    - Automatic retry logic with exponential backoff\n+    - Comprehensive error handling and logging\n+    - Batch request processing (sync and async)\n+    - Input validation and sanitization\n+\n+    Usage:\n+        # Sync invoke\n+        with AssistantClient(base_url, invoke_endpoint, stream_endpoint) as client:\n+            messages = [ChatMessage(role=\"user\", content=\"Hello\")]\n+            response = client.invoke_chat(messages)\n+\n+        # Async streaming\n+        async with AssistantClient(\n+            base_url,\n+            invoke_endpoint,\n+            stream_endpoint,\n+        ) as client:\n+            messages = [ChatMessage(role=\"user\", content=\"Hello\")]\n+            async for message in client.async_stream_chat(messages):\n+                print(message.content)\n     \"\"\"\n \n     logger = create_logger(\"assistant_client\", console_level=\"INFO\")\n \n     def __init__(\n         self,\n         base_url: str,\n-        chat_endpoint: str,\n-        timeout: float | None = None,\n+        invoke_endpoint: str,\n+        stream_endpoint: str,\n+        timeout: float = settings.assistant_client.timeout,\n     ) -> None:\n         \"\"\"\n         Initialize AssistantClient.\n \n         Args:\n             base_url: Base URL of the assistant service\n-            chat_endpoint: Chat endpoint path\n+            invoke_endpoint: Chat invoke endpoint path\n+            stream_endpoint: Chat stream endpoint path\n             timeout: Request timeout in seconds\n         \"\"\"\n-        self.base_url = base_url.rstrip(\"/\")\n-        self.chat_endpoint = chat_endpoint.strip(\"/\")\n-        self.chat_url = f\"{self.base_url}/{self.chat_endpoint}\"\n-        self.timeout = timeout or settings.assistant_client.timeout\n+        self.invoke_url = f\"{base_url.rstrip('/')}/{invoke_endpoint.strip('/')}\"\n+        self.stream_url = f\"{base_url.rstrip('/')}/{stream_endpoint.strip('/')}\"\n+        self.timeout = timeout\n \n         # HTTP clients (will be initialized in context managers)\n         self._sync_client: httpx.Client | None = None\n@@ -79,87 +119,122 @@ async def __aexit__(\n             await self._async_client.aclose()\n             self._async_client = None\n \n-    def _prepare_chat_input(\n-        self, message: str, conversation_history: list[ChatMessageSchema]\n-    ) -> tuple[AssistantChatInput, None] | tuple[None, AssistantChatOutput]:\n+    def _handle_chat_request_error(self, err: Exception) -> ChatMessage:\n         \"\"\"\n-        Validates the user message and prepares the chat input object.\n+        Handle errors from chat requests and return appropriate error response.\n \n-        This method does NOT mutate the original conversation_history.\n+        Args:\n+            err: The exception that occurred during the request\n \n         Returns:\n-            A tuple of (AssistantChatInput, None) on success,\n-            or (None, AssistantChatOutput) on validation failure.\n+            ChatMessage containing an error message\n         \"\"\"\n-        error_response = None\n-        chat_input = None\n-\n-        try:\n-            # Create a new history list to avoid mutating the original\n-            new_history = conversation_history + [\n-                ChatMessageSchema(role=\"user\", content=message)\n-            ]\n-            chat_input = AssistantChatInput(\n-                conversation_history=new_history,\n-            )\n-        except ValidationError as err:\n-            self.logger.error(f\"Validation error preparing chat input: {err}\")\n-            error_response = AssistantChatOutput(\n-                assistant_message=(\n-                    \"ERROR. There was an issue with your message \"\n-                    f\"format: {err.errors()[0]['msg']}\"\n-                )\n-            )\n-        except Exception as err:\n-            self.logger.error(f\"Unexpected error preparing chat input: {err}\")\n-            error_response = AssistantChatOutput(\n-                assistant_message=(\n-                    \"An unexpected error occurred while processing your message. \"\n-                    \"Please try again. If the issue continues, please report it.\"\n-                )\n-            )\n-\n-        return chat_input, error_response\n-\n-    def _handle_chat_request_error(self, err: Exception) -> AssistantChatOutput:\n         if isinstance(err, httpx.TimeoutException):\n             self.logger.error(f\"TimeoutException from assistant API: {err}\")\n-            error_msg = (\n-                \"I'm sorry, but there was an issue communicating with the assistant \"\n-                f\"(Timeout after {self.timeout}s). Please try again later. If the \"\n-                \"problem persists, please contact support.\"\n+            error_msg = ChatMessage(\n+                role=\"assistant\",\n+                content=(\n+                    \"I'm sorry, but there was an issue communicating with the \"\n+                    f\"assistant (Timeout after {self.timeout}s). Please try again \"\n+                    \"later. If the problem persists, please contact support.\"\n+                ),\n             )\n-            response = AssistantChatOutput(assistant_message=error_msg)\n         elif isinstance(err, httpx.HTTPStatusError):\n+            # Handle streaming responses that can't be read\n+            try:\n+                response_text = err.response.text\n+            except Exception:\n+                response_text = \"<response body not available>\"\n+\n             self.logger.error(\n                 f\"HTTPStatusError from assistant API: {err.response.status_code} - \"\n-                f\"Response: {err.response.text}\"\n+                f\"Response: {response_text}\"\n             )\n-            error_msg = (\n-                \"I'm sorry, but there was an issue communicating with the assistant \"\n-                f\"(HTTP {err.response.status_code}). Please try again later. If the \"\n-                \"problem persists, please contact support.\"\n+            error_msg = ChatMessage(\n+                role=\"assistant\",\n+                content=(\n+                    \"I'm sorry, but there was an issue communicating with the \"\n+                    f\"assistant (HTTP {err.response.status_code}). Please try \"\n+                    \"again later. If the problem persists, please contact \"\n+                    \"support.\"\n+                ),\n             )\n-            response = AssistantChatOutput(assistant_message=error_msg)\n         elif isinstance(err, httpx.RequestError):\n             self.logger.error(\n                 f\"RequestError connecting to assistant API: {err}\",\n             )\n-            error_msg = (\n-                \"I'm sorry, but I couldn't connect to the assistant service. \"\n-                \"Please check your network connection and try again. If the \"\n-                \"problem persists, please contact support.\"\n+            error_msg = ChatMessage(\n+                role=\"assistant\",\n+                content=(\n+                    \"I'm sorry, but I couldn't connect to the assistant service. \"\n+                    \"Please check your network connection and try again. If the \"\n+                    \"problem persists, please contact support.\"\n+                ),\n+            )\n+        else:\n+            self.logger.exception(\n+                f\"Unexpected error in chat request. {err.__class__.__name__}: {err}\"\n             )\n-            response = AssistantChatOutput(assistant_message=error_msg)\n-        elif isinstance(err, Exception):\n-            self.logger.exception(\"Unexpected error in get_assistant_response.\")\n-            error_msg = (\n-                \"An unexpected error occurred while trying to get a response. \"\n-                \"Please try again. If the issue continues, please report it.\"\n+            error_msg = ChatMessage(\n+                role=\"assistant\",\n+                content=(\n+                    \"An unexpected error occurred while trying to get a response. \"\n+                    \"Please try again. If the issue continues, please report it.\"\n+                ),\n             )\n-            response = AssistantChatOutput(assistant_message=error_msg)\n \n-        return response\n+        return error_msg\n+\n+    def _validate_conversation_history(\n+        self, conversation_history: list[ChatMessage]\n+    ) -> None:\n+        \"\"\"\n+        Validate conversation history before processing.\n+\n+        Args:\n+            conversation_history: List of ChatMessage objects to validate\n+\n+        Raises:\n+            ValueError: If conversation history is invalid\n+        \"\"\"\n+        if not conversation_history:\n+            raise ValueError(\n+                \"Conversation history cannot be empty. Please provide at least \"\n+                \"one message.\"\n+            )\n+\n+        # Additional validation could be added here if needed\n+        # e.g., checking for alternating roles, valid message types, etc.\n+\n+    def _parse_stream_chunk(self, chunk: str) -> ChatMessage:\n+        \"\"\"\n+        Parse a streaming chunk into a ChatMessage with robust error handling.\n+\n+        Args:\n+            chunk: Raw chunk data from the stream\n+\n+        Returns:\n+            ChatMessage with parsed content or error message if parsing failed\n+        \"\"\"\n+        try:\n+            chunk = chunk.strip()\n+            if not chunk:\n+                # Return empty content message for empty chunks\n+                return ChatMessage(role=\"assistant\", content=\"\")\n+\n+            try:\n+                data = json.loads(chunk)\n+            except json.JSONDecodeError:\n+                data = ast.literal_eval(chunk)\n+\n+            return ChatMessage.model_validate(data)\n+\n+        except (ValueError, SyntaxError, ValidationError) as e:\n+            self.logger.warning(f\"Failed to parse stream chunk '{chunk[:100]}...': {e}\")\n+            return ChatMessage(\n+                role=\"assistant\",\n+                content=\"[ERROR: Message parsing failed]\",\n+            )\n \n     @retry(\n         stop=stop_after_attempt(settings.assistant_client.retry_attempts),\n@@ -170,65 +245,59 @@ def _handle_chat_request_error(self, err: Exception) -> AssistantChatOutput:\n         ),\n         reraise=True,\n     )\n-    def _core_sync_chat_request(\n+    def _core_sync_chat_invoke(\n         self,\n-        chat_input: AssistantChatInput,\n-    ) -> AssistantChatOutput:\n+        chat_input: ChatInput,\n+    ) -> ChatInvokeOutput:\n+        \"\"\"\n+        Core synchronous chat request with retry logic.\n+\n+        Args:\n+            chat_input: Validated chat input object\n+\n+        Returns:\n+            ChatInvokeOutput from the API response\n+        \"\"\"\n         response = self._sync_client.post(\n-            self.chat_url,\n+            self.invoke_url,\n             json=chat_input.model_dump(),\n         )\n         response.raise_for_status()\n-        response = AssistantChatOutput.model_validate(response.json())\n+        response = ChatInvokeOutput.model_validate(response.json())\n \n         return response\n \n-    def request_chat(\n+    def invoke_chat(\n         self,\n-        user_message: str,\n-        conversation_history: list[ChatMessageSchema],\n-    ) -> tuple[AssistantChatOutput, list[ChatMessageSchema]]:\n+        conversation_history: list[ChatMessage],\n+    ) -> ChatInvokeOutput:\n         \"\"\"\n         Synchronous chat request method with retry logic.\n         Must be used within sync context manager.\n \n         Args:\n-            user_message: User message\n-            conversation_history: Conversation history\n+            conversation_history: List of ChatMessage objects representing the\n+                complete conversation history including the user's latest message\n \n         Returns:\n-            Tuple of AssistantChatOutput with assistant response text or error\n-            message and updated conversation history.\n+            ChatInvokeOutput with assistant response messages\n         \"\"\"\n \n         if not self._sync_client:\n             raise RuntimeError(\n                 \"Must use AssistantClient within 'with' statement for sync requests.\"\n             )\n \n-        chat_input, error_response = self._prepare_chat_input(\n-            user_message,\n-            conversation_history,\n-        )\n-        if error_response:\n-            # Return original history, as it was not modified\n-            return error_response, conversation_history\n+        self._validate_conversation_history(conversation_history)\n \n         try:\n-            response = self._core_sync_chat_request(chat_input)\n+            chat_input = ChatInput(messages=conversation_history)\n+            response = self._core_sync_chat_invoke(chat_input)\n         except Exception as err:\n-            response = self._handle_chat_request_error(err)\n-        else:\n-            conversation_history.extend(\n-                [\n-                    ChatMessageSchema(role=\"user\", content=user_message),\n-                    ChatMessageSchema(\n-                        role=\"assistant\", content=response.assistant_message\n-                    ),\n-                ]\n-            )\n+            err_msg = self._handle_chat_request_error(err)\n+            return ChatInvokeOutput(messages=[err_msg])\n \n-        return response, conversation_history\n+        return response\n \n     @retry(\n         stop=stop_after_attempt(settings.assistant_client.retry_attempts),\n@@ -239,35 +308,42 @@ def request_chat(\n         ),\n         reraise=True,\n     )\n-    async def _core_async_chat_request(\n+    async def _core_async_chat_invoke(\n         self,\n-        chat_input: AssistantChatInput,\n-    ) -> AssistantChatOutput:\n+        chat_input: ChatInput,\n+    ) -> ChatInvokeOutput:\n+        \"\"\"\n+        Core asynchronous chat request with retry logic.\n+\n+        Args:\n+            chat_input: Validated chat input object\n+\n+        Returns:\n+            ChatInvokeOutput from the API response\n+        \"\"\"\n         response = await self._async_client.post(\n-            self.chat_url,\n+            self.invoke_url,\n             json=chat_input.model_dump(),\n         )\n         response.raise_for_status()\n-        response = AssistantChatOutput.model_validate(response.json())\n+        response = ChatInvokeOutput.model_validate(response.json())\n \n         return response\n \n-    async def async_request_chat(\n+    async def async_invoke_chat(\n         self,\n-        user_message: str,\n-        conversation_history: list[ChatMessageSchema],\n-    ) -> tuple[AssistantChatOutput, list[ChatMessageSchema]]:\n+        conversation_history: list[ChatMessage],\n+    ) -> ChatInvokeOutput:\n         \"\"\"\n         Asynchronous chat request method with retry logic.\n         Must be used within async context manager.\n \n         Args:\n-            user_message: Message to send to the assistant\n-            conversation_history: Conversation history\n+            conversation_history: List of ChatMessage objects representing the\n+                complete conversation history including the user's latest message\n \n         Returns:\n-            Tuple of AssistantChatOutput with assistant response text or error\n-            message and updated conversation history.\n+            ChatInvokeOutput with assistant response messages\n         \"\"\"\n \n         if not self._async_client:\n@@ -276,54 +352,120 @@ async def async_request_chat(\n                 \"for async requests.\"\n             )\n \n-        chat_input, error_response = self._prepare_chat_input(\n-            user_message,\n-            conversation_history,\n-        )\n-        if error_response:\n-            # Return original history, as it was not modified\n-            return error_response, conversation_history\n+        self._validate_conversation_history(conversation_history)\n \n         try:\n-            response = await self._core_async_chat_request(chat_input)\n+            chat_input = ChatInput(messages=conversation_history)\n+            response = await self._core_async_chat_invoke(chat_input)\n         except Exception as err:\n-            response = self._handle_chat_request_error(err)\n-        else:\n-            conversation_history.extend(\n-                [\n-                    ChatMessageSchema(role=\"user\", content=user_message),\n-                    ChatMessageSchema(\n-                        role=\"assistant\", content=response.assistant_message\n-                    ),\n-                ]\n+            err_msg = self._handle_chat_request_error(err)\n+            return ChatInvokeOutput(messages=[err_msg])\n+\n+        return response\n+\n+    def stream_chat(\n+        self,\n+        conversation_history: list[ChatMessage],\n+    ) -> Generator[ChatMessage, None]:\n+        \"\"\"\n+        Synchronous chat streaming method.\n+        Must be used within sync context manager.\n+\n+        Args:\n+            conversation_history: List of ChatMessage objects representing the\n+                complete conversation history including the user's latest message\n+\n+        Yields:\n+            ChatMessage objects as they are received from the stream\n+        \"\"\"\n+\n+        if not self._sync_client:\n+            raise RuntimeError(\n+                \"Must use AssistantClient within 'with' statement for sync streaming.\"\n+            )\n+\n+        self._validate_conversation_history(conversation_history)\n+\n+        try:\n+            chat_input = ChatInput(messages=conversation_history)\n+\n+            with self._sync_client.stream(\n+                \"POST\",\n+                self.stream_url,\n+                json=chat_input.model_dump(),\n+            ) as response:\n+                response.raise_for_status()\n+\n+                for chunk in response.iter_lines():\n+                    message = self._parse_stream_chunk(chunk)\n+                    yield message\n+\n+        except Exception as err:\n+            err_msg = self._handle_chat_request_error(err)\n+            yield err_msg\n+\n+    async def async_stream_chat(\n+        self,\n+        conversation_history: list[ChatMessage],\n+    ) -> AsyncGenerator[ChatMessage, None]:\n+        \"\"\"\n+        Asynchronous chat streaming method.\n+        Must be used within async context manager.\n+\n+        Args:\n+            conversation_history: List of ChatMessage objects representing the\n+                complete conversation history including the user's latest message\n+\n+        Yields:\n+            ChatMessage objects as they are received from the stream\n+        \"\"\"\n+        if not self._async_client:\n+            raise RuntimeError(\n+                \"Must use AssistantClient within 'async with' statement \"\n+                \"for async streaming requests.\"\n             )\n \n-        return response, conversation_history\n+        self._validate_conversation_history(conversation_history)\n \n-    def batch_request_chat(\n+        try:\n+            chat_input = ChatInput(messages=conversation_history)\n+\n+            async with self._async_client.stream(\n+                \"POST\",\n+                self.stream_url,\n+                json=chat_input.model_dump(),\n+            ) as response:\n+                response.raise_for_status()\n+\n+                async for chunk in response.aiter_lines():\n+                    message = self._parse_stream_chunk(chunk)\n+                    yield message\n+\n+        except Exception as err:\n+            err_msg = self._handle_chat_request_error(err)\n+            yield err_msg\n+\n+    def batch_invoke_chat(\n         self,\n-        messages: list[str],\n-        conversation_histories: list[list[ChatMessageSchema]],\n-    ) -> list[tuple[AssistantChatOutput, list[ChatMessageSchema]] | None]:\n+        conversation_histories: list[list[ChatMessage]],\n+    ) -> list[ChatInvokeOutput | None]:\n         \"\"\"\n         Synchronously processes a batch of chat requests.\n \n-        Each message is processed sequentially. If an error occurs for a specific\n-        message, its corresponding entry in the returned list will be None, and\n-        the error will be logged.\n+        Each conversation history is processed sequentially. If an error occurs for\n+        a specific conversation history, its corresponding entry in the returned list\n+        will be None, and the error will be logged.\n \n         Args:\n-            messages: A list of user message strings.\n             conversation_histories: A list of conversation histories. Each history\n-                is a list of ChatMessageSchema objects. The length of this list\n-                must match the length of the `messages` list.\n+                is a list of ChatMessage objects representing a complete conversation.\n \n         Returns:\n             A list where each element is either:\n-            - A tuple containing (AssistantChatOutput, updated_conversation_history)\n-              if the request was successful.\n-            - None if an error occurred for that specific request.\n-            The order of results corresponds to the order of input messages.\n+            - ChatInvokeOutput if the request was successful\n+            - None if an error occurred for that specific request\n+            The order of results corresponds to the order of input conversation\n+            histories.\n         \"\"\"\n \n         if not self._sync_client:\n@@ -332,54 +474,38 @@ def batch_request_chat(\n                 \"sync batch requests.\"\n             )\n \n-        # Validate conversation_histories length if provided\n-        if len(conversation_histories) != len(messages):\n-            raise ValueError(\n-                f\"Mismatch in lengths: 'messages' has length {len(messages)}, \"\n-                \"but 'conversation_histories' has length \"\n-                f\"{len(conversation_histories)}. Both lists must have the same \"\n-                \"number of elements.\"\n-            )\n-\n         responses = []\n-        for message, conversation_history in zip(\n-            messages, conversation_histories, strict=True\n-        ):\n+        for conversation_history in conversation_histories:\n             try:\n-                response, conversation_history = self.request_chat(\n-                    message, conversation_history\n-                )\n-                responses.append((response, conversation_history))\n+                response = self.invoke_chat(conversation_history)\n+                responses.append(response)\n             except Exception as e:\n-                self.logger.error(f\"Error processing batch message {message}: {str(e)}\")\n+                self.logger.exception(f\"Error processing batch message: {str(e)}\")\n                 responses.append(None)\n \n         return responses\n \n-    async def async_batch_request_chat(\n+    async def async_batch_invoke_chat(\n         self,\n-        messages: list[str],\n-        conversation_histories: list[list[ChatMessageSchema]],\n-    ) -> list[tuple[AssistantChatOutput, list[ChatMessageSchema]] | None]:\n+        conversation_histories: list[list[ChatMessage]],\n+    ) -> list[ChatInvokeOutput | None]:\n         \"\"\"\n         Asynchronously processes a batch of chat requests.\n \n-        All messages are processed concurrently. If an error occurs for a specific\n-        message, its corresponding entry in the returned list will be None, and\n-        the error will be logged.\n+        All conversation histories are processed concurrently. If an error occurs\n+        for a specific conversation history, its corresponding entry in the returned\n+        list will be None, and the error will be logged.\n \n         Args:\n-            messages: A list of user message strings.\n             conversation_histories: A list of conversation histories. Each history\n-                is a list of ChatMessageSchema objects. The length of this list\n-                must match the length of the `messages` list.\n+                is a list of ChatMessage objects representing a complete conversation.\n \n         Returns:\n             A list where each element is either:\n-            - A tuple containing (AssistantChatOutput, updated_conversation_history)\n-              if the request was successful.\n-            - None if an error occurred for that specific request.\n-            The order of results corresponds to the order of input messages.\n+            - ChatInvokeOutput if the request was successful\n+            - None if an error occurred for that specific request\n+            The order of results corresponds to the order of input conversation\n+            histories.\n         \"\"\"\n \n         if not self._async_client:\n@@ -388,32 +514,21 @@ async def async_batch_request_chat(\n                 \"for async batch requests.\"\n             )\n \n-        # Validate conversation_histories length if provided\n-        if len(conversation_histories) != len(messages):\n-            raise ValueError(\n-                f\"Mismatch in lengths: 'messages' has length {len(messages)}, \"\n-                \"but 'conversation_histories' has length \"\n-                f\"{len(conversation_histories)}. Both lists must have the same \"\n-                \"number of elements.\"\n-            )\n-\n         # Create concurrent tasks for all requests\n         tasks = []\n-        for message, conversation_history in zip(\n-            messages, conversation_histories, strict=True\n-        ):\n-            task = self.async_request_chat(message, conversation_history)\n+        for conversation_history in conversation_histories:\n+            task = self.async_invoke_chat(conversation_history)\n             tasks.append(task)\n \n         # Execute all tasks concurrently and handle exceptions\n         responses = await asyncio.gather(*tasks, return_exceptions=True)\n \n         # Convert exceptions to None and log errors\n         processed_responses = []\n-        for i, resp in enumerate(responses):\n+        for resp in responses:\n             if isinstance(resp, Exception):\n-                self.logger.error(\n-                    f\"Error processing async batch message {messages[i]}: {str(resp)}\"\n+                self.logger.exception(\n+                    f\"Error processing async batch message: {str(resp)}\"\n                 )\n                 processed_responses.append(None)\n             else:",
      "patch_lines": [
        "@@ -1,4 +1,7 @@\n",
        "+import ast\n",
        " import asyncio\n",
        "+import json\n",
        "+from collections.abc import AsyncGenerator, Generator\n",
        " from typing import Any\n",
        " \n",
        " import httpx\n",
        "@@ -10,40 +13,77 @@\n",
        " )\n",
        " \n",
        " from src.general_assistant.api.schemas.chat_schemas import (\n",
        "-    AssistantChatInput,\n",
        "-    AssistantChatOutput,\n",
        "-    ChatMessageSchema,\n",
        "+    ChatInput,\n",
        "+    ChatInvokeOutput,\n",
        "+    ChatMessage,\n",
        " )\n",
        " from src.general_assistant.config.logger import create_logger\n",
        " from src.general_assistant.config.settings import settings\n",
        " \n",
        " \n",
        " class AssistantClient:\n",
        "     \"\"\"\n",
        "-    Assistant client that works as both sync and async context manager.\n",
        "-    Handles HTTP client lifecycle, resource management, and robust error handling.\n",
        "+    HTTP client for the General Assistant API with sync and async support.\n",
        "+\n",
        "+    This client handles HTTP communication with the General Assistant API endpoints,\n",
        "+    providing both invoke (request-response) and streaming modes. Conversation\n",
        "+    management is handled client-side - you must provide complete conversation\n",
        "+    history as ChatMessage objects.\n",
        "+\n",
        "+    Architecture:\n",
        "+    - Pure HTTP client - no conversation state management\n",
        "+    - Client-side conversation building with ChatMessage objects\n",
        "+    - Comprehensive error handling and retry logic\n",
        "+    - Support for both sync and async operations\n",
        "+    - Streaming and non-streaming modes\n",
        "+\n",
        "+    Features:\n",
        "+    - Sync and async context management\n",
        "+    - Invoke mode: Traditional request-response chat\n",
        "+    - Streaming mode: Real-time message streaming (sync and async)\n",
        "+    - Automatic retry logic with exponential backoff\n",
        "+    - Comprehensive error handling and logging\n",
        "+    - Batch request processing (sync and async)\n",
        "+    - Input validation and sanitization\n",
        "+\n",
        "+    Usage:\n",
        "+        # Sync invoke\n",
        "+        with AssistantClient(base_url, invoke_endpoint, stream_endpoint) as client:\n",
        "+            messages = [ChatMessage(role=\"user\", content=\"Hello\")]\n",
        "+            response = client.invoke_chat(messages)\n",
        "+\n",
        "+        # Async streaming\n",
        "+        async with AssistantClient(\n",
        "+            base_url,\n",
        "+            invoke_endpoint,\n",
        "+            stream_endpoint,\n",
        "+        ) as client:\n",
        "+            messages = [ChatMessage(role=\"user\", content=\"Hello\")]\n",
        "+            async for message in client.async_stream_chat(messages):\n",
        "+                print(message.content)\n",
        "     \"\"\"\n",
        " \n",
        "     logger = create_logger(\"assistant_client\", console_level=\"INFO\")\n",
        " \n",
        "     def __init__(\n",
        "         self,\n",
        "         base_url: str,\n",
        "-        chat_endpoint: str,\n",
        "-        timeout: float | None = None,\n",
        "+        invoke_endpoint: str,\n",
        "+        stream_endpoint: str,\n",
        "+        timeout: float = settings.assistant_client.timeout,\n",
        "     ) -> None:\n",
        "         \"\"\"\n",
        "         Initialize AssistantClient.\n",
        " \n",
        "         Args:\n",
        "             base_url: Base URL of the assistant service\n",
        "-            chat_endpoint: Chat endpoint path\n",
        "+            invoke_endpoint: Chat invoke endpoint path\n",
        "+            stream_endpoint: Chat stream endpoint path\n",
        "             timeout: Request timeout in seconds\n",
        "         \"\"\"\n",
        "-        self.base_url = base_url.rstrip(\"/\")\n",
        "-        self.chat_endpoint = chat_endpoint.strip(\"/\")\n",
        "-        self.chat_url = f\"{self.base_url}/{self.chat_endpoint}\"\n",
        "-        self.timeout = timeout or settings.assistant_client.timeout\n",
        "+        self.invoke_url = f\"{base_url.rstrip('/')}/{invoke_endpoint.strip('/')}\"\n",
        "+        self.stream_url = f\"{base_url.rstrip('/')}/{stream_endpoint.strip('/')}\"\n",
        "+        self.timeout = timeout\n",
        " \n",
        "         # HTTP clients (will be initialized in context managers)\n",
        "         self._sync_client: httpx.Client | None = None\n",
        "@@ -79,87 +119,122 @@ async def __aexit__(\n",
        "             await self._async_client.aclose()\n",
        "             self._async_client = None\n",
        " \n",
        "-    def _prepare_chat_input(\n",
        "-        self, message: str, conversation_history: list[ChatMessageSchema]\n",
        "-    ) -> tuple[AssistantChatInput, None] | tuple[None, AssistantChatOutput]:\n",
        "+    def _handle_chat_request_error(self, err: Exception) -> ChatMessage:\n",
        "         \"\"\"\n",
        "-        Validates the user message and prepares the chat input object.\n",
        "+        Handle errors from chat requests and return appropriate error response.\n",
        " \n",
        "-        This method does NOT mutate the original conversation_history.\n",
        "+        Args:\n",
        "+            err: The exception that occurred during the request\n",
        " \n",
        "         Returns:\n",
        "-            A tuple of (AssistantChatInput, None) on success,\n",
        "-            or (None, AssistantChatOutput) on validation failure.\n",
        "+            ChatMessage containing an error message\n",
        "         \"\"\"\n",
        "-        error_response = None\n",
        "-        chat_input = None\n",
        "-\n",
        "-        try:\n",
        "-            # Create a new history list to avoid mutating the original\n",
        "-            new_history = conversation_history + [\n",
        "-                ChatMessageSchema(role=\"user\", content=message)\n",
        "-            ]\n",
        "-            chat_input = AssistantChatInput(\n",
        "-                conversation_history=new_history,\n",
        "-            )\n",
        "-        except ValidationError as err:\n",
        "-            self.logger.error(f\"Validation error preparing chat input: {err}\")\n",
        "-            error_response = AssistantChatOutput(\n",
        "-                assistant_message=(\n",
        "-                    \"ERROR. There was an issue with your message \"\n",
        "-                    f\"format: {err.errors()[0]['msg']}\"\n",
        "-                )\n",
        "-            )\n",
        "-        except Exception as err:\n",
        "-            self.logger.error(f\"Unexpected error preparing chat input: {err}\")\n",
        "-            error_response = AssistantChatOutput(\n",
        "-                assistant_message=(\n",
        "-                    \"An unexpected error occurred while processing your message. \"\n",
        "-                    \"Please try again. If the issue continues, please report it.\"\n",
        "-                )\n",
        "-            )\n",
        "-\n",
        "-        return chat_input, error_response\n",
        "-\n",
        "-    def _handle_chat_request_error(self, err: Exception) -> AssistantChatOutput:\n",
        "         if isinstance(err, httpx.TimeoutException):\n",
        "             self.logger.error(f\"TimeoutException from assistant API: {err}\")\n",
        "-            error_msg = (\n",
        "-                \"I'm sorry, but there was an issue communicating with the assistant \"\n",
        "-                f\"(Timeout after {self.timeout}s). Please try again later. If the \"\n",
        "-                \"problem persists, please contact support.\"\n",
        "+            error_msg = ChatMessage(\n",
        "+                role=\"assistant\",\n",
        "+                content=(\n",
        "+                    \"I'm sorry, but there was an issue communicating with the \"\n",
        "+                    f\"assistant (Timeout after {self.timeout}s). Please try again \"\n",
        "+                    \"later. If the problem persists, please contact support.\"\n",
        "+                ),\n",
        "             )\n",
        "-            response = AssistantChatOutput(assistant_message=error_msg)\n",
        "         elif isinstance(err, httpx.HTTPStatusError):\n",
        "+            # Handle streaming responses that can't be read\n",
        "+            try:\n",
        "+                response_text = err.response.text\n",
        "+            except Exception:\n",
        "+                response_text = \"<response body not available>\"\n",
        "+\n",
        "             self.logger.error(\n",
        "                 f\"HTTPStatusError from assistant API: {err.response.status_code} - \"\n",
        "-                f\"Response: {err.response.text}\"\n",
        "+                f\"Response: {response_text}\"\n",
        "             )\n",
        "-            error_msg = (\n",
        "-                \"I'm sorry, but there was an issue communicating with the assistant \"\n",
        "-                f\"(HTTP {err.response.status_code}). Please try again later. If the \"\n",
        "-                \"problem persists, please contact support.\"\n",
        "+            error_msg = ChatMessage(\n",
        "+                role=\"assistant\",\n",
        "+                content=(\n",
        "+                    \"I'm sorry, but there was an issue communicating with the \"\n",
        "+                    f\"assistant (HTTP {err.response.status_code}). Please try \"\n",
        "+                    \"again later. If the problem persists, please contact \"\n",
        "+                    \"support.\"\n",
        "+                ),\n",
        "             )\n",
        "-            response = AssistantChatOutput(assistant_message=error_msg)\n",
        "         elif isinstance(err, httpx.RequestError):\n",
        "             self.logger.error(\n",
        "                 f\"RequestError connecting to assistant API: {err}\",\n",
        "             )\n",
        "-            error_msg = (\n",
        "-                \"I'm sorry, but I couldn't connect to the assistant service. \"\n",
        "-                \"Please check your network connection and try again. If the \"\n",
        "-                \"problem persists, please contact support.\"\n",
        "+            error_msg = ChatMessage(\n",
        "+                role=\"assistant\",\n",
        "+                content=(\n",
        "+                    \"I'm sorry, but I couldn't connect to the assistant service. \"\n",
        "+                    \"Please check your network connection and try again. If the \"\n",
        "+                    \"problem persists, please contact support.\"\n",
        "+                ),\n",
        "+            )\n",
        "+        else:\n",
        "+            self.logger.exception(\n",
        "+                f\"Unexpected error in chat request. {err.__class__.__name__}: {err}\"\n",
        "             )\n",
        "-            response = AssistantChatOutput(assistant_message=error_msg)\n",
        "-        elif isinstance(err, Exception):\n",
        "-            self.logger.exception(\"Unexpected error in get_assistant_response.\")\n",
        "-            error_msg = (\n",
        "-                \"An unexpected error occurred while trying to get a response. \"\n",
        "-                \"Please try again. If the issue continues, please report it.\"\n",
        "+            error_msg = ChatMessage(\n",
        "+                role=\"assistant\",\n",
        "+                content=(\n",
        "+                    \"An unexpected error occurred while trying to get a response. \"\n",
        "+                    \"Please try again. If the issue continues, please report it.\"\n",
        "+                ),\n",
        "             )\n",
        "-            response = AssistantChatOutput(assistant_message=error_msg)\n",
        " \n",
        "-        return response\n",
        "+        return error_msg\n",
        "+\n",
        "+    def _validate_conversation_history(\n",
        "+        self, conversation_history: list[ChatMessage]\n",
        "+    ) -> None:\n",
        "+        \"\"\"\n",
        "+        Validate conversation history before processing.\n",
        "+\n",
        "+        Args:\n",
        "+            conversation_history: List of ChatMessage objects to validate\n",
        "+\n",
        "+        Raises:\n",
        "+            ValueError: If conversation history is invalid\n",
        "+        \"\"\"\n",
        "+        if not conversation_history:\n",
        "+            raise ValueError(\n",
        "+                \"Conversation history cannot be empty. Please provide at least \"\n",
        "+                \"one message.\"\n",
        "+            )\n",
        "+\n",
        "+        # Additional validation could be added here if needed\n",
        "+        # e.g., checking for alternating roles, valid message types, etc.\n",
        "+\n",
        "+    def _parse_stream_chunk(self, chunk: str) -> ChatMessage:\n",
        "+        \"\"\"\n",
        "+        Parse a streaming chunk into a ChatMessage with robust error handling.\n",
        "+\n",
        "+        Args:\n",
        "+            chunk: Raw chunk data from the stream\n",
        "+\n",
        "+        Returns:\n",
        "+            ChatMessage with parsed content or error message if parsing failed\n",
        "+        \"\"\"\n",
        "+        try:\n",
        "+            chunk = chunk.strip()\n",
        "+            if not chunk:\n",
        "+                # Return empty content message for empty chunks\n",
        "+                return ChatMessage(role=\"assistant\", content=\"\")\n",
        "+\n",
        "+            try:\n",
        "+                data = json.loads(chunk)\n",
        "+            except json.JSONDecodeError:\n",
        "+                data = ast.literal_eval(chunk)\n",
        "+\n",
        "+            return ChatMessage.model_validate(data)\n",
        "+\n",
        "+        except (ValueError, SyntaxError, ValidationError) as e:\n",
        "+            self.logger.warning(f\"Failed to parse stream chunk '{chunk[:100]}...': {e}\")\n",
        "+            return ChatMessage(\n",
        "+                role=\"assistant\",\n",
        "+                content=\"[ERROR: Message parsing failed]\",\n",
        "+            )\n",
        " \n",
        "     @retry(\n",
        "         stop=stop_after_attempt(settings.assistant_client.retry_attempts),\n",
        "@@ -170,65 +245,59 @@ def _handle_chat_request_error(self, err: Exception) -> AssistantChatOutput:\n",
        "         ),\n",
        "         reraise=True,\n",
        "     )\n",
        "-    def _core_sync_chat_request(\n",
        "+    def _core_sync_chat_invoke(\n",
        "         self,\n",
        "-        chat_input: AssistantChatInput,\n",
        "-    ) -> AssistantChatOutput:\n",
        "+        chat_input: ChatInput,\n",
        "+    ) -> ChatInvokeOutput:\n",
        "+        \"\"\"\n",
        "+        Core synchronous chat request with retry logic.\n",
        "+\n",
        "+        Args:\n",
        "+            chat_input: Validated chat input object\n",
        "+\n",
        "+        Returns:\n",
        "+            ChatInvokeOutput from the API response\n",
        "+        \"\"\"\n",
        "         response = self._sync_client.post(\n",
        "-            self.chat_url,\n",
        "+            self.invoke_url,\n",
        "             json=chat_input.model_dump(),\n",
        "         )\n",
        "         response.raise_for_status()\n",
        "-        response = AssistantChatOutput.model_validate(response.json())\n",
        "+        response = ChatInvokeOutput.model_validate(response.json())\n",
        " \n",
        "         return response\n",
        " \n",
        "-    def request_chat(\n",
        "+    def invoke_chat(\n",
        "         self,\n",
        "-        user_message: str,\n",
        "-        conversation_history: list[ChatMessageSchema],\n",
        "-    ) -> tuple[AssistantChatOutput, list[ChatMessageSchema]]:\n",
        "+        conversation_history: list[ChatMessage],\n",
        "+    ) -> ChatInvokeOutput:\n",
        "         \"\"\"\n",
        "         Synchronous chat request method with retry logic.\n",
        "         Must be used within sync context manager.\n",
        " \n",
        "         Args:\n",
        "-            user_message: User message\n",
        "-            conversation_history: Conversation history\n",
        "+            conversation_history: List of ChatMessage objects representing the\n",
        "+                complete conversation history including the user's latest message\n",
        " \n",
        "         Returns:\n",
        "-            Tuple of AssistantChatOutput with assistant response text or error\n",
        "-            message and updated conversation history.\n",
        "+            ChatInvokeOutput with assistant response messages\n",
        "         \"\"\"\n",
        " \n",
        "         if not self._sync_client:\n",
        "             raise RuntimeError(\n",
        "                 \"Must use AssistantClient within 'with' statement for sync requests.\"\n",
        "             )\n",
        " \n",
        "-        chat_input, error_response = self._prepare_chat_input(\n",
        "-            user_message,\n",
        "-            conversation_history,\n",
        "-        )\n",
        "-        if error_response:\n",
        "-            # Return original history, as it was not modified\n",
        "-            return error_response, conversation_history\n",
        "+        self._validate_conversation_history(conversation_history)\n",
        " \n",
        "         try:\n",
        "-            response = self._core_sync_chat_request(chat_input)\n",
        "+            chat_input = ChatInput(messages=conversation_history)\n",
        "+            response = self._core_sync_chat_invoke(chat_input)\n",
        "         except Exception as err:\n",
        "-            response = self._handle_chat_request_error(err)\n",
        "-        else:\n",
        "-            conversation_history.extend(\n",
        "-                [\n",
        "-                    ChatMessageSchema(role=\"user\", content=user_message),\n",
        "-                    ChatMessageSchema(\n",
        "-                        role=\"assistant\", content=response.assistant_message\n",
        "-                    ),\n",
        "-                ]\n",
        "-            )\n",
        "+            err_msg = self._handle_chat_request_error(err)\n",
        "+            return ChatInvokeOutput(messages=[err_msg])\n",
        " \n",
        "-        return response, conversation_history\n",
        "+        return response\n",
        " \n",
        "     @retry(\n",
        "         stop=stop_after_attempt(settings.assistant_client.retry_attempts),\n",
        "@@ -239,35 +308,42 @@ def request_chat(\n",
        "         ),\n",
        "         reraise=True,\n",
        "     )\n",
        "-    async def _core_async_chat_request(\n",
        "+    async def _core_async_chat_invoke(\n",
        "         self,\n",
        "-        chat_input: AssistantChatInput,\n",
        "-    ) -> AssistantChatOutput:\n",
        "+        chat_input: ChatInput,\n",
        "+    ) -> ChatInvokeOutput:\n",
        "+        \"\"\"\n",
        "+        Core asynchronous chat request with retry logic.\n",
        "+\n",
        "+        Args:\n",
        "+            chat_input: Validated chat input object\n",
        "+\n",
        "+        Returns:\n",
        "+            ChatInvokeOutput from the API response\n",
        "+        \"\"\"\n",
        "         response = await self._async_client.post(\n",
        "-            self.chat_url,\n",
        "+            self.invoke_url,\n",
        "             json=chat_input.model_dump(),\n",
        "         )\n",
        "         response.raise_for_status()\n",
        "-        response = AssistantChatOutput.model_validate(response.json())\n",
        "+        response = ChatInvokeOutput.model_validate(response.json())\n",
        " \n",
        "         return response\n",
        " \n",
        "-    async def async_request_chat(\n",
        "+    async def async_invoke_chat(\n",
        "         self,\n",
        "-        user_message: str,\n",
        "-        conversation_history: list[ChatMessageSchema],\n",
        "-    ) -> tuple[AssistantChatOutput, list[ChatMessageSchema]]:\n",
        "+        conversation_history: list[ChatMessage],\n",
        "+    ) -> ChatInvokeOutput:\n",
        "         \"\"\"\n",
        "         Asynchronous chat request method with retry logic.\n",
        "         Must be used within async context manager.\n",
        " \n",
        "         Args:\n",
        "-            user_message: Message to send to the assistant\n",
        "-            conversation_history: Conversation history\n",
        "+            conversation_history: List of ChatMessage objects representing the\n",
        "+                complete conversation history including the user's latest message\n",
        " \n",
        "         Returns:\n",
        "-            Tuple of AssistantChatOutput with assistant response text or error\n",
        "-            message and updated conversation history.\n",
        "+            ChatInvokeOutput with assistant response messages\n",
        "         \"\"\"\n",
        " \n",
        "         if not self._async_client:\n",
        "@@ -276,54 +352,120 @@ async def async_request_chat(\n",
        "                 \"for async requests.\"\n",
        "             )\n",
        " \n",
        "-        chat_input, error_response = self._prepare_chat_input(\n",
        "-            user_message,\n",
        "-            conversation_history,\n",
        "-        )\n",
        "-        if error_response:\n",
        "-            # Return original history, as it was not modified\n",
        "-            return error_response, conversation_history\n",
        "+        self._validate_conversation_history(conversation_history)\n",
        " \n",
        "         try:\n",
        "-            response = await self._core_async_chat_request(chat_input)\n",
        "+            chat_input = ChatInput(messages=conversation_history)\n",
        "+            response = await self._core_async_chat_invoke(chat_input)\n",
        "         except Exception as err:\n",
        "-            response = self._handle_chat_request_error(err)\n",
        "-        else:\n",
        "-            conversation_history.extend(\n",
        "-                [\n",
        "-                    ChatMessageSchema(role=\"user\", content=user_message),\n",
        "-                    ChatMessageSchema(\n",
        "-                        role=\"assistant\", content=response.assistant_message\n",
        "-                    ),\n",
        "-                ]\n",
        "+            err_msg = self._handle_chat_request_error(err)\n",
        "+            return ChatInvokeOutput(messages=[err_msg])\n",
        "+\n",
        "+        return response\n",
        "+\n",
        "+    def stream_chat(\n",
        "+        self,\n",
        "+        conversation_history: list[ChatMessage],\n",
        "+    ) -> Generator[ChatMessage, None]:\n",
        "+        \"\"\"\n",
        "+        Synchronous chat streaming method.\n",
        "+        Must be used within sync context manager.\n",
        "+\n",
        "+        Args:\n",
        "+            conversation_history: List of ChatMessage objects representing the\n",
        "+                complete conversation history including the user's latest message\n",
        "+\n",
        "+        Yields:\n",
        "+            ChatMessage objects as they are received from the stream\n",
        "+        \"\"\"\n",
        "+\n",
        "+        if not self._sync_client:\n",
        "+            raise RuntimeError(\n",
        "+                \"Must use AssistantClient within 'with' statement for sync streaming.\"\n",
        "+            )\n",
        "+\n",
        "+        self._validate_conversation_history(conversation_history)\n",
        "+\n",
        "+        try:\n",
        "+            chat_input = ChatInput(messages=conversation_history)\n",
        "+\n",
        "+            with self._sync_client.stream(\n",
        "+                \"POST\",\n",
        "+                self.stream_url,\n",
        "+                json=chat_input.model_dump(),\n",
        "+            ) as response:\n",
        "+                response.raise_for_status()\n",
        "+\n",
        "+                for chunk in response.iter_lines():\n",
        "+                    message = self._parse_stream_chunk(chunk)\n",
        "+                    yield message\n",
        "+\n",
        "+        except Exception as err:\n",
        "+            err_msg = self._handle_chat_request_error(err)\n",
        "+            yield err_msg\n",
        "+\n",
        "+    async def async_stream_chat(\n",
        "+        self,\n",
        "+        conversation_history: list[ChatMessage],\n",
        "+    ) -> AsyncGenerator[ChatMessage, None]:\n",
        "+        \"\"\"\n",
        "+        Asynchronous chat streaming method.\n",
        "+        Must be used within async context manager.\n",
        "+\n",
        "+        Args:\n",
        "+            conversation_history: List of ChatMessage objects representing the\n",
        "+                complete conversation history including the user's latest message\n",
        "+\n",
        "+        Yields:\n",
        "+            ChatMessage objects as they are received from the stream\n",
        "+        \"\"\"\n",
        "+        if not self._async_client:\n",
        "+            raise RuntimeError(\n",
        "+                \"Must use AssistantClient within 'async with' statement \"\n",
        "+                \"for async streaming requests.\"\n",
        "             )\n",
        " \n",
        "-        return response, conversation_history\n",
        "+        self._validate_conversation_history(conversation_history)\n",
        " \n",
        "-    def batch_request_chat(\n",
        "+        try:\n",
        "+            chat_input = ChatInput(messages=conversation_history)\n",
        "+\n",
        "+            async with self._async_client.stream(\n",
        "+                \"POST\",\n",
        "+                self.stream_url,\n",
        "+                json=chat_input.model_dump(),\n",
        "+            ) as response:\n",
        "+                response.raise_for_status()\n",
        "+\n",
        "+                async for chunk in response.aiter_lines():\n",
        "+                    message = self._parse_stream_chunk(chunk)\n",
        "+                    yield message\n",
        "+\n",
        "+        except Exception as err:\n",
        "+            err_msg = self._handle_chat_request_error(err)\n",
        "+            yield err_msg\n",
        "+\n",
        "+    def batch_invoke_chat(\n",
        "         self,\n",
        "-        messages: list[str],\n",
        "-        conversation_histories: list[list[ChatMessageSchema]],\n",
        "-    ) -> list[tuple[AssistantChatOutput, list[ChatMessageSchema]] | None]:\n",
        "+        conversation_histories: list[list[ChatMessage]],\n",
        "+    ) -> list[ChatInvokeOutput | None]:\n",
        "         \"\"\"\n",
        "         Synchronously processes a batch of chat requests.\n",
        " \n",
        "-        Each message is processed sequentially. If an error occurs for a specific\n",
        "-        message, its corresponding entry in the returned list will be None, and\n",
        "-        the error will be logged.\n",
        "+        Each conversation history is processed sequentially. If an error occurs for\n",
        "+        a specific conversation history, its corresponding entry in the returned list\n",
        "+        will be None, and the error will be logged.\n",
        " \n",
        "         Args:\n",
        "-            messages: A list of user message strings.\n",
        "             conversation_histories: A list of conversation histories. Each history\n",
        "-                is a list of ChatMessageSchema objects. The length of this list\n",
        "-                must match the length of the `messages` list.\n",
        "+                is a list of ChatMessage objects representing a complete conversation.\n",
        " \n",
        "         Returns:\n",
        "             A list where each element is either:\n",
        "-            - A tuple containing (AssistantChatOutput, updated_conversation_history)\n",
        "-              if the request was successful.\n",
        "-            - None if an error occurred for that specific request.\n",
        "-            The order of results corresponds to the order of input messages.\n",
        "+            - ChatInvokeOutput if the request was successful\n",
        "+            - None if an error occurred for that specific request\n",
        "+            The order of results corresponds to the order of input conversation\n",
        "+            histories.\n",
        "         \"\"\"\n",
        " \n",
        "         if not self._sync_client:\n",
        "@@ -332,54 +474,38 @@ def batch_request_chat(\n",
        "                 \"sync batch requests.\"\n",
        "             )\n",
        " \n",
        "-        # Validate conversation_histories length if provided\n",
        "-        if len(conversation_histories) != len(messages):\n",
        "-            raise ValueError(\n",
        "-                f\"Mismatch in lengths: 'messages' has length {len(messages)}, \"\n",
        "-                \"but 'conversation_histories' has length \"\n",
        "-                f\"{len(conversation_histories)}. Both lists must have the same \"\n",
        "-                \"number of elements.\"\n",
        "-            )\n",
        "-\n",
        "         responses = []\n",
        "-        for message, conversation_history in zip(\n",
        "-            messages, conversation_histories, strict=True\n",
        "-        ):\n",
        "+        for conversation_history in conversation_histories:\n",
        "             try:\n",
        "-                response, conversation_history = self.request_chat(\n",
        "-                    message, conversation_history\n",
        "-                )\n",
        "-                responses.append((response, conversation_history))\n",
        "+                response = self.invoke_chat(conversation_history)\n",
        "+                responses.append(response)\n",
        "             except Exception as e:\n",
        "-                self.logger.error(f\"Error processing batch message {message}: {str(e)}\")\n",
        "+                self.logger.exception(f\"Error processing batch message: {str(e)}\")\n",
        "                 responses.append(None)\n",
        " \n",
        "         return responses\n",
        " \n",
        "-    async def async_batch_request_chat(\n",
        "+    async def async_batch_invoke_chat(\n",
        "         self,\n",
        "-        messages: list[str],\n",
        "-        conversation_histories: list[list[ChatMessageSchema]],\n",
        "-    ) -> list[tuple[AssistantChatOutput, list[ChatMessageSchema]] | None]:\n",
        "+        conversation_histories: list[list[ChatMessage]],\n",
        "+    ) -> list[ChatInvokeOutput | None]:\n",
        "         \"\"\"\n",
        "         Asynchronously processes a batch of chat requests.\n",
        " \n",
        "-        All messages are processed concurrently. If an error occurs for a specific\n",
        "-        message, its corresponding entry in the returned list will be None, and\n",
        "-        the error will be logged.\n",
        "+        All conversation histories are processed concurrently. If an error occurs\n",
        "+        for a specific conversation history, its corresponding entry in the returned\n",
        "+        list will be None, and the error will be logged.\n",
        " \n",
        "         Args:\n",
        "-            messages: A list of user message strings.\n",
        "             conversation_histories: A list of conversation histories. Each history\n",
        "-                is a list of ChatMessageSchema objects. The length of this list\n",
        "-                must match the length of the `messages` list.\n",
        "+                is a list of ChatMessage objects representing a complete conversation.\n",
        " \n",
        "         Returns:\n",
        "             A list where each element is either:\n",
        "-            - A tuple containing (AssistantChatOutput, updated_conversation_history)\n",
        "-              if the request was successful.\n",
        "-            - None if an error occurred for that specific request.\n",
        "-            The order of results corresponds to the order of input messages.\n",
        "+            - ChatInvokeOutput if the request was successful\n",
        "+            - None if an error occurred for that specific request\n",
        "+            The order of results corresponds to the order of input conversation\n",
        "+            histories.\n",
        "         \"\"\"\n",
        " \n",
        "         if not self._async_client:\n",
        "@@ -388,32 +514,21 @@ async def async_batch_request_chat(\n",
        "                 \"for async batch requests.\"\n",
        "             )\n",
        " \n",
        "-        # Validate conversation_histories length if provided\n",
        "-        if len(conversation_histories) != len(messages):\n",
        "-            raise ValueError(\n",
        "-                f\"Mismatch in lengths: 'messages' has length {len(messages)}, \"\n",
        "-                \"but 'conversation_histories' has length \"\n",
        "-                f\"{len(conversation_histories)}. Both lists must have the same \"\n",
        "-                \"number of elements.\"\n",
        "-            )\n",
        "-\n",
        "         # Create concurrent tasks for all requests\n",
        "         tasks = []\n",
        "-        for message, conversation_history in zip(\n",
        "-            messages, conversation_histories, strict=True\n",
        "-        ):\n",
        "-            task = self.async_request_chat(message, conversation_history)\n",
        "+        for conversation_history in conversation_histories:\n",
        "+            task = self.async_invoke_chat(conversation_history)\n",
        "             tasks.append(task)\n",
        " \n",
        "         # Execute all tasks concurrently and handle exceptions\n",
        "         responses = await asyncio.gather(*tasks, return_exceptions=True)\n",
        " \n",
        "         # Convert exceptions to None and log errors\n",
        "         processed_responses = []\n",
        "-        for i, resp in enumerate(responses):\n",
        "+        for resp in responses:\n",
        "             if isinstance(resp, Exception):\n",
        "-                self.logger.error(\n",
        "-                    f\"Error processing async batch message {messages[i]}: {str(resp)}\"\n",
        "+                self.logger.exception(\n",
        "+                    f\"Error processing async batch message: {str(resp)}\"\n",
        "                 )\n",
        "                 processed_responses.append(None)\n",
        "             else:\n"
      ]
    },
    {
      "path": "src/general_assistant/utils/langgraph_studio.py",
      "status": "removed",
      "additions": 0,
      "deletions": 11,
      "patch": "@@ -1,11 +0,0 @@\n-from langgraph.graph.state import CompiledStateGraph\n-\n-from src.general_assistant.core.workflow import GeneralAssistantWorkflow\n-\n-\n-def get_general_assistant_workflow_graph() -> CompiledStateGraph:\n-    workflow = GeneralAssistantWorkflow()\n-    return workflow.graph\n-\n-\n-graph = get_general_assistant_workflow_graph()",
      "patch_lines": [
        "@@ -1,11 +0,0 @@\n",
        "-from langgraph.graph.state import CompiledStateGraph\n",
        "-\n",
        "-from src.general_assistant.core.workflow import GeneralAssistantWorkflow\n",
        "-\n",
        "-\n",
        "-def get_general_assistant_workflow_graph() -> CompiledStateGraph:\n",
        "-    workflow = GeneralAssistantWorkflow()\n",
        "-    return workflow.graph\n",
        "-\n",
        "-\n",
        "-graph = get_general_assistant_workflow_graph()\n"
      ]
    },
    {
      "path": "src/webui/chainlit_main.py",
      "status": "modified",
      "additions": 46,
      "deletions": 12,
      "patch": "@@ -1,37 +1,71 @@\n import chainlit as cl\n+from pydantic import ValidationError\n \n+from src.general_assistant.api.schemas.chat_schemas import ChatMessage\n from src.general_assistant.config.settings import settings\n from src.general_assistant.utils.assistant_client import AssistantClient\n \n \n @cl.on_chat_start\n async def on_chat_start():\n-    \"\"\"Initialize the assistant client and conversation history for the user session.\"\"\"\n-\n     client = AssistantClient(\n         base_url=settings.webui.assistant_base_url,\n-        chat_endpoint=settings.webui.assistant_chat_endpoint,\n+        invoke_endpoint=settings.webui.assistant_chat_invoke_endpoint,\n+        stream_endpoint=settings.webui.assistant_chat_stream_endpoint,\n     )\n     cl.user_session.set(\"client\", client)\n     cl.user_session.set(\"conversation_history\", [])\n \n \n+async def create_user_message(message: cl.Message) -> tuple[ChatMessage | None, bool]:\n+    user_message, is_error = None, False\n+\n+    try:\n+        user_message = ChatMessage(\n+            role=\"user\",\n+            content=message.content,\n+        )\n+    except ValidationError as err:\n+        await cl.Message(\n+            content=(\n+                \"\u274c Error: There was an issue with your message \"\n+                f\"format - {err.errors()[0]['msg']}\"\n+            ),\n+            author=\"System\",\n+        ).send()\n+        is_error = True\n+\n+    return user_message, is_error\n+\n+\n+async def plot_step(message: ChatMessage):\n+    step_name = message.role\n+    if message.role == \"tool\":\n+        step_name += \" \" + message.additional_kwargs.get(\"name\", \"\")\n+    async with cl.Step(name=step_name) as step:\n+        step.input = message.content\n+\n+\n @cl.on_message\n async def main(message: cl.Message):\n     conversation_history = cl.user_session.get(\"conversation_history\")\n     client = cl.user_session.get(\"client\")\n \n-    # Show a \"thinking\" indicator to the user\n-    assistant_msg = cl.Message(content=\"Thinking...\")\n-    await assistant_msg.send()\n+    user_message, is_error = await create_user_message(message)\n+    if is_error:\n+        return\n+\n+    conversation_history.append(user_message)\n \n+    final_message = None\n     async with client:\n-        response, conversation_history = await client.async_request_chat(\n-            user_message=message.content,\n+        async for response_message in client.async_stream_chat(\n             conversation_history=conversation_history,\n-        )\n+        ):\n+            await plot_step(response_message)\n+            final_message = response_message\n+            conversation_history.append(response_message)\n \n+    if final_message:\n+        await cl.Message(content=final_message.content).send()\n     cl.user_session.set(\"conversation_history\", conversation_history)\n-\n-    assistant_msg.content = response.assistant_message\n-    await assistant_msg.update()",
      "patch_lines": [
        "@@ -1,37 +1,71 @@\n",
        " import chainlit as cl\n",
        "+from pydantic import ValidationError\n",
        " \n",
        "+from src.general_assistant.api.schemas.chat_schemas import ChatMessage\n",
        " from src.general_assistant.config.settings import settings\n",
        " from src.general_assistant.utils.assistant_client import AssistantClient\n",
        " \n",
        " \n",
        " @cl.on_chat_start\n",
        " async def on_chat_start():\n",
        "-    \"\"\"Initialize the assistant client and conversation history for the user session.\"\"\"\n",
        "-\n",
        "     client = AssistantClient(\n",
        "         base_url=settings.webui.assistant_base_url,\n",
        "-        chat_endpoint=settings.webui.assistant_chat_endpoint,\n",
        "+        invoke_endpoint=settings.webui.assistant_chat_invoke_endpoint,\n",
        "+        stream_endpoint=settings.webui.assistant_chat_stream_endpoint,\n",
        "     )\n",
        "     cl.user_session.set(\"client\", client)\n",
        "     cl.user_session.set(\"conversation_history\", [])\n",
        " \n",
        " \n",
        "+async def create_user_message(message: cl.Message) -> tuple[ChatMessage | None, bool]:\n",
        "+    user_message, is_error = None, False\n",
        "+\n",
        "+    try:\n",
        "+        user_message = ChatMessage(\n",
        "+            role=\"user\",\n",
        "+            content=message.content,\n",
        "+        )\n",
        "+    except ValidationError as err:\n",
        "+        await cl.Message(\n",
        "+            content=(\n",
        "+                \"\u274c Error: There was an issue with your message \"\n",
        "+                f\"format - {err.errors()[0]['msg']}\"\n",
        "+            ),\n",
        "+            author=\"System\",\n",
        "+        ).send()\n",
        "+        is_error = True\n",
        "+\n",
        "+    return user_message, is_error\n",
        "+\n",
        "+\n",
        "+async def plot_step(message: ChatMessage):\n",
        "+    step_name = message.role\n",
        "+    if message.role == \"tool\":\n",
        "+        step_name += \" \" + message.additional_kwargs.get(\"name\", \"\")\n",
        "+    async with cl.Step(name=step_name) as step:\n",
        "+        step.input = message.content\n",
        "+\n",
        "+\n",
        " @cl.on_message\n",
        " async def main(message: cl.Message):\n",
        "     conversation_history = cl.user_session.get(\"conversation_history\")\n",
        "     client = cl.user_session.get(\"client\")\n",
        " \n",
        "-    # Show a \"thinking\" indicator to the user\n",
        "-    assistant_msg = cl.Message(content=\"Thinking...\")\n",
        "-    await assistant_msg.send()\n",
        "+    user_message, is_error = await create_user_message(message)\n",
        "+    if is_error:\n",
        "+        return\n",
        "+\n",
        "+    conversation_history.append(user_message)\n",
        " \n",
        "+    final_message = None\n",
        "     async with client:\n",
        "-        response, conversation_history = await client.async_request_chat(\n",
        "-            user_message=message.content,\n",
        "+        async for response_message in client.async_stream_chat(\n",
        "             conversation_history=conversation_history,\n",
        "-        )\n",
        "+        ):\n",
        "+            await plot_step(response_message)\n",
        "+            final_message = response_message\n",
        "+            conversation_history.append(response_message)\n",
        " \n",
        "+    if final_message:\n",
        "+        await cl.Message(content=final_message.content).send()\n",
        "     cl.user_session.set(\"conversation_history\", conversation_history)\n",
        "-\n",
        "-    assistant_msg.content = response.assistant_message\n",
        "-    await assistant_msg.update()\n"
      ]
    },
    {
      "path": "uv.lock",
      "status": "modified",
      "additions": 0,
      "deletions": 43,
      "patch": "@@ -572,7 +572,6 @@ dependencies = [\n     { name = \"langsmith\" },\n     { name = \"loguru\" },\n     { name = \"numpy\" },\n-    { name = \"openinference-instrumentation-llama-index\" },\n     { name = \"pandas\" },\n     { name = \"tavily-python\" },\n ]\n@@ -601,7 +600,6 @@ requires-dist = [\n     { name = \"langsmith\", specifier = \">=0.4.4\" },\n     { name = \"loguru\", specifier = \">=0.7.3\" },\n     { name = \"numpy\", specifier = \">=2.3.0\" },\n-    { name = \"openinference-instrumentation-llama-index\", specifier = \">=4.3.1\" },\n     { name = \"pandas\", specifier = \">=2.3.0\" },\n     { name = \"pre-commit\", marker = \"extra == 'dev'\", specifier = \">=4.2.0\" },\n     { name = \"pytest\", marker = \"extra == 'dev'\", specifier = \">=8.3.2\" },\n@@ -1313,47 +1311,6 @@ wheels = [\n     { url = \"https://files.pythonhosted.org/packages/64/46/a10d9df4673df56f71201d129ba1cb19eaff3366d08c8664d61a7df52e65/openai-1.93.0-py3-none-any.whl\", hash = \"sha256:3d746fe5498f0dd72e0d9ab706f26c91c0f646bf7459e5629af8ba7c9dbdf090\", size = 755038, upload-time = \"2025-06-27T21:21:37.532Z\" },\n ]\n \n-[[package]]\n-name = \"openinference-instrumentation\"\n-version = \"0.1.34\"\n-source = { registry = \"https://pypi.org/simple\" }\n-dependencies = [\n-    { name = \"openinference-semantic-conventions\" },\n-    { name = \"opentelemetry-api\" },\n-    { name = \"opentelemetry-sdk\" },\n-]\n-sdist = { url = \"https://files.pythonhosted.org/packages/2e/18/d074b45b04ba69bd03260d2dc0a034e5d586d8854e957695f40569278136/openinference_instrumentation-0.1.34.tar.gz\", hash = \"sha256:fa0328e8b92fc3e22e150c46f108794946ce39fe13670aed15f23ba0105f72ab\", size = 22373, upload-time = \"2025-06-17T16:47:22.641Z\" }\n-wheels = [\n-    { url = \"https://files.pythonhosted.org/packages/c1/ad/1a0a5c0a755918269f71fbca225fd70759dd79dd5bffc4723e44f0d87240/openinference_instrumentation-0.1.34-py3-none-any.whl\", hash = \"sha256:0fff1cc6d9b86f3450fc1c88347c51c5467855992b75e7addb85bf09fd048d2d\", size = 28137, upload-time = \"2025-06-17T16:47:21.658Z\" },\n-]\n-\n-[[package]]\n-name = \"openinference-instrumentation-llama-index\"\n-version = \"4.3.1\"\n-source = { registry = \"https://pypi.org/simple\" }\n-dependencies = [\n-    { name = \"openinference-instrumentation\" },\n-    { name = \"openinference-semantic-conventions\" },\n-    { name = \"opentelemetry-api\" },\n-    { name = \"opentelemetry-instrumentation\" },\n-    { name = \"opentelemetry-semantic-conventions\" },\n-    { name = \"typing-extensions\" },\n-    { name = \"wrapt\" },\n-]\n-sdist = { url = \"https://files.pythonhosted.org/packages/15/8d/dd691556848761022de808adb376195e3b6c53c50e971b27ea56e828d9ba/openinference_instrumentation_llama_index-4.3.1.tar.gz\", hash = \"sha256:d3a2502be7ffce6547eaf5aaf1b6973e7f700954538a202a95fae6b87fc645c1\", size = 59327, upload-time = \"2025-06-27T16:35:11.434Z\" }\n-wheels = [\n-    { url = \"https://files.pythonhosted.org/packages/0a/ad/f9d0e5d41d9ad8a71daecd77d1702d1f4f328768aab9a84de588bf883426/openinference_instrumentation_llama_index-4.3.1-py3-none-any.whl\", hash = \"sha256:f1fbf8d4177f3a2eb039ede5d86968332c1066de32343c3017ee5272bc9e92e0\", size = 28381, upload-time = \"2025-06-27T16:35:10.279Z\" },\n-]\n-\n-[[package]]\n-name = \"openinference-semantic-conventions\"\n-version = \"0.1.21\"\n-source = { registry = \"https://pypi.org/simple\" }\n-sdist = { url = \"https://files.pythonhosted.org/packages/75/0f/b794eb009846d4b10af50e205a323ca359f284563ef4d1778f35a80522ac/openinference_semantic_conventions-0.1.21.tar.gz\", hash = \"sha256:328405b9f79ff72a659c7712b8429c0d7ea68c6a4a1679e3eb44372aa228119b\", size = 12534, upload-time = \"2025-06-13T05:22:18.982Z\" }\n-wheels = [\n-    { url = \"https://files.pythonhosted.org/packages/6e/4d/092766f8e610f2c513e483c4adc892eea1634945022a73371fe01f621165/openinference_semantic_conventions-0.1.21-py3-none-any.whl\", hash = \"sha256:acde8282c20da1de900cdc0d6258a793ec3eb8031bfc496bd823dae17d32e326\", size = 10167, upload-time = \"2025-06-13T05:22:18.118Z\" },\n-]\n-\n [[package]]\n name = \"opentelemetry-api\"\n version = \"1.34.1\"",
      "patch_lines": [
        "@@ -572,7 +572,6 @@ dependencies = [\n",
        "     { name = \"langsmith\" },\n",
        "     { name = \"loguru\" },\n",
        "     { name = \"numpy\" },\n",
        "-    { name = \"openinference-instrumentation-llama-index\" },\n",
        "     { name = \"pandas\" },\n",
        "     { name = \"tavily-python\" },\n",
        " ]\n",
        "@@ -601,7 +600,6 @@ requires-dist = [\n",
        "     { name = \"langsmith\", specifier = \">=0.4.4\" },\n",
        "     { name = \"loguru\", specifier = \">=0.7.3\" },\n",
        "     { name = \"numpy\", specifier = \">=2.3.0\" },\n",
        "-    { name = \"openinference-instrumentation-llama-index\", specifier = \">=4.3.1\" },\n",
        "     { name = \"pandas\", specifier = \">=2.3.0\" },\n",
        "     { name = \"pre-commit\", marker = \"extra == 'dev'\", specifier = \">=4.2.0\" },\n",
        "     { name = \"pytest\", marker = \"extra == 'dev'\", specifier = \">=8.3.2\" },\n",
        "@@ -1313,47 +1311,6 @@ wheels = [\n",
        "     { url = \"https://files.pythonhosted.org/packages/64/46/a10d9df4673df56f71201d129ba1cb19eaff3366d08c8664d61a7df52e65/openai-1.93.0-py3-none-any.whl\", hash = \"sha256:3d746fe5498f0dd72e0d9ab706f26c91c0f646bf7459e5629af8ba7c9dbdf090\", size = 755038, upload-time = \"2025-06-27T21:21:37.532Z\" },\n",
        " ]\n",
        " \n",
        "-[[package]]\n",
        "-name = \"openinference-instrumentation\"\n",
        "-version = \"0.1.34\"\n",
        "-source = { registry = \"https://pypi.org/simple\" }\n",
        "-dependencies = [\n",
        "-    { name = \"openinference-semantic-conventions\" },\n",
        "-    { name = \"opentelemetry-api\" },\n",
        "-    { name = \"opentelemetry-sdk\" },\n",
        "-]\n",
        "-sdist = { url = \"https://files.pythonhosted.org/packages/2e/18/d074b45b04ba69bd03260d2dc0a034e5d586d8854e957695f40569278136/openinference_instrumentation-0.1.34.tar.gz\", hash = \"sha256:fa0328e8b92fc3e22e150c46f108794946ce39fe13670aed15f23ba0105f72ab\", size = 22373, upload-time = \"2025-06-17T16:47:22.641Z\" }\n",
        "-wheels = [\n",
        "-    { url = \"https://files.pythonhosted.org/packages/c1/ad/1a0a5c0a755918269f71fbca225fd70759dd79dd5bffc4723e44f0d87240/openinference_instrumentation-0.1.34-py3-none-any.whl\", hash = \"sha256:0fff1cc6d9b86f3450fc1c88347c51c5467855992b75e7addb85bf09fd048d2d\", size = 28137, upload-time = \"2025-06-17T16:47:21.658Z\" },\n",
        "-]\n",
        "-\n",
        "-[[package]]\n",
        "-name = \"openinference-instrumentation-llama-index\"\n",
        "-version = \"4.3.1\"\n",
        "-source = { registry = \"https://pypi.org/simple\" }\n",
        "-dependencies = [\n",
        "-    { name = \"openinference-instrumentation\" },\n",
        "-    { name = \"openinference-semantic-conventions\" },\n",
        "-    { name = \"opentelemetry-api\" },\n",
        "-    { name = \"opentelemetry-instrumentation\" },\n",
        "-    { name = \"opentelemetry-semantic-conventions\" },\n",
        "-    { name = \"typing-extensions\" },\n",
        "-    { name = \"wrapt\" },\n",
        "-]\n",
        "-sdist = { url = \"https://files.pythonhosted.org/packages/15/8d/dd691556848761022de808adb376195e3b6c53c50e971b27ea56e828d9ba/openinference_instrumentation_llama_index-4.3.1.tar.gz\", hash = \"sha256:d3a2502be7ffce6547eaf5aaf1b6973e7f700954538a202a95fae6b87fc645c1\", size = 59327, upload-time = \"2025-06-27T16:35:11.434Z\" }\n",
        "-wheels = [\n",
        "-    { url = \"https://files.pythonhosted.org/packages/0a/ad/f9d0e5d41d9ad8a71daecd77d1702d1f4f328768aab9a84de588bf883426/openinference_instrumentation_llama_index-4.3.1-py3-none-any.whl\", hash = \"sha256:f1fbf8d4177f3a2eb039ede5d86968332c1066de32343c3017ee5272bc9e92e0\", size = 28381, upload-time = \"2025-06-27T16:35:10.279Z\" },\n",
        "-]\n",
        "-\n",
        "-[[package]]\n",
        "-name = \"openinference-semantic-conventions\"\n",
        "-version = \"0.1.21\"\n",
        "-source = { registry = \"https://pypi.org/simple\" }\n",
        "-sdist = { url = \"https://files.pythonhosted.org/packages/75/0f/b794eb009846d4b10af50e205a323ca359f284563ef4d1778f35a80522ac/openinference_semantic_conventions-0.1.21.tar.gz\", hash = \"sha256:328405b9f79ff72a659c7712b8429c0d7ea68c6a4a1679e3eb44372aa228119b\", size = 12534, upload-time = \"2025-06-13T05:22:18.982Z\" }\n",
        "-wheels = [\n",
        "-    { url = \"https://files.pythonhosted.org/packages/6e/4d/092766f8e610f2c513e483c4adc892eea1634945022a73371fe01f621165/openinference_semantic_conventions-0.1.21-py3-none-any.whl\", hash = \"sha256:acde8282c20da1de900cdc0d6258a793ec3eb8031bfc496bd823dae17d32e326\", size = 10167, upload-time = \"2025-06-13T05:22:18.118Z\" },\n",
        "-]\n",
        "-\n",
        " [[package]]\n",
        " name = \"opentelemetry-api\"\n",
        " version = \"1.34.1\"\n"
      ]
    }
  ]
}
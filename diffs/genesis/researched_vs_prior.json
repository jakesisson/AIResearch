{
  "project": "Research Data/genesis",
  "repo": "pablo-lozano-martin/genesis",
  "prior_commit": "38f2556b0f7f6ebc079b9e4e774c71402a002665",
  "researched_commit": "25d916b0f6920e360f0b7bbcc4c03a13d767596f",
  "compare_url": "https://github.com/pablo-lozano-martin/genesis/compare/38f2556b0f7f6ebc079b9e4e774c71402a002665...25d916b0f6920e360f0b7bbcc4c03a13d767596f",
  "ahead_by": 1,
  "behind_by": 0,
  "changed_files": [
    {
      "path": "backend/app/adapters/inbound/websocket_handler.py",
      "status": "modified",
      "additions": 39,
      "deletions": 38,
      "patch": "@@ -1,9 +1,10 @@\n-# ABOUTME: WebSocket handler for real-time chat streaming\n-# ABOUTME: Manages WebSocket connections and streams LLM responses token-by-token\n+# ABOUTME: WebSocket handler for real-time chat streaming using LangGraph\n+# ABOUTME: Uses graph.astream_events() for token-by-token streaming with automatic checkpointing\n \n import json\n from typing import Dict\n from fastapi import WebSocket, WebSocketDisconnect\n+from langgraph.types import RunnableConfig\n from app.adapters.inbound.websocket_schemas import (\n     ClientMessage,\n     ServerTokenMessage,\n@@ -14,9 +15,7 @@\n     MessageType\n )\n from app.core.domain.user import User\n-from app.core.domain.message import Message, MessageRole\n from app.core.ports.llm_provider import ILLMProvider\n-from app.core.ports.message_repository import IMessageRepository\n from app.core.ports.conversation_repository import IConversationRepository\n from app.infrastructure.config.logging_config import get_logger\n \n@@ -56,26 +55,26 @@ async def send_message(self, websocket: WebSocket, message: dict):\n async def handle_websocket_chat(\n     websocket: WebSocket,\n     user: User,\n+    graph,\n     llm_provider: ILLMProvider,\n-    message_repository: IMessageRepository,\n     conversation_repository: IConversationRepository\n ):\n     \"\"\"\n-    Handle WebSocket chat session for a user.\n+    Handle WebSocket chat session for a user using LangGraph.\n \n     This function:\n     - Establishes the WebSocket connection\n     - Receives user messages\n-    - Streams LLM responses token-by-token\n-    - Persists messages to the database\n+    - Streams LLM responses token-by-token via graph.astream_events()\n+    - Messages are automatically persisted via LangGraph checkpointer\n     - Handles errors and disconnections\n \n     Args:\n         websocket: WebSocket connection instance\n         user: Authenticated user\n+        graph: Compiled LangGraph instance with checkpointing\n         llm_provider: LLM provider for generating responses\n-        message_repository: Repository for persisting messages\n-        conversation_repository: Repository for conversation metadata\n+        conversation_repository: Repository for conversation ownership verification\n     \"\"\"\n     await manager.connect(websocket, user.id)\n \n@@ -107,6 +106,7 @@ async def handle_websocket_chat(\n                 conversation_id = client_message.conversation_id\n                 conversation = await conversation_repository.get_by_id(conversation_id)\n \n+                # Verify conversation ownership (authorization at App DB layer)\n                 if not conversation or conversation.user_id != user.id:\n                     logger.warning(f\"User {user.id} attempted to access conversation {conversation_id}\")\n                     error_msg = ServerErrorMessage(\n@@ -116,45 +116,46 @@ async def handle_websocket_chat(\n                     await manager.send_message(websocket, error_msg.model_dump())\n                     continue\n \n-                user_message = Message(\n-                    conversation_id=conversation_id,\n-                    role=MessageRole.USER,\n-                    content=client_message.content\n+                # Create RunnableConfig with thread_id (conversation.id) and llm_provider\n+                config = RunnableConfig(\n+                    configurable={\n+                        \"thread_id\": conversation.id,\n+                        \"llm_provider\": llm_provider,\n+                        \"user_id\": user.id\n+                    }\n                 )\n \n-                saved_user_message = await message_repository.create(user_message)\n-                logger.info(f\"Saved user message {saved_user_message.id} to conversation {conversation_id}\")\n+                # Prepare input for graph\n+                input_data = {\n+                    \"user_input\": client_message.content,\n+                    \"conversation_id\": conversation.id,\n+                    \"user_id\": user.id\n+                }\n \n-                messages = await message_repository.get_by_conversation_id(conversation_id)\n+                logger.info(f\"Starting LangGraph streaming for conversation {conversation_id}\")\n \n-                full_response = []\n                 try:\n-                    async for token in llm_provider.stream(messages):\n-                        full_response.append(token)\n-                        token_msg = ServerTokenMessage(content=token)\n-                        await manager.send_message(websocket, token_msg.model_dump())\n-\n-                    response_content = \"\".join(full_response)\n-\n-                    assistant_message = Message(\n-                        conversation_id=conversation_id,\n-                        role=MessageRole.ASSISTANT,\n-                        content=response_content\n-                    )\n-\n-                    saved_assistant_message = await message_repository.create(assistant_message)\n-                    logger.info(f\"Saved assistant message {saved_assistant_message.id} to conversation {conversation_id}\")\n-\n-                    await conversation_repository.increment_message_count(conversation_id, 2)\n-\n+                    # Stream LLM tokens using graph.astream_events()\n+                    # Checkpointing happens automatically\n+                    async for event in graph.astream_events(input_data, config, version=\"v2\"):\n+                        # Stream tokens from LLM to client\n+                        if event[\"event\"] == \"on_chat_model_stream\":\n+                            chunk = event[\"data\"][\"chunk\"]\n+                            if hasattr(chunk, 'content') and chunk.content:\n+                                token_msg = ServerTokenMessage(content=chunk.content)\n+                                await manager.send_message(websocket, token_msg.model_dump())\n+\n+                    logger.info(f\"LangGraph streaming completed for conversation {conversation_id}\")\n+\n+                    # Send completion message (checkpointing already done)\n                     complete_msg = ServerCompleteMessage(\n-                        message_id=saved_assistant_message.id,\n+                        message_id=None,  # No longer tracking individual message IDs\n                         conversation_id=conversation_id\n                     )\n                     await manager.send_message(websocket, complete_msg.model_dump())\n \n                 except Exception as e:\n-                    logger.error(f\"LLM streaming failed for user {user.id}: {e}\")\n+                    logger.error(f\"LangGraph streaming failed for user {user.id}: {e}\")\n                     error_msg = ServerErrorMessage(\n                         message=f\"Failed to generate response: {str(e)}\",\n                         code=\"LLM_ERROR\"",
      "patch_lines": [
        "@@ -1,9 +1,10 @@\n",
        "-# ABOUTME: WebSocket handler for real-time chat streaming\n",
        "-# ABOUTME: Manages WebSocket connections and streams LLM responses token-by-token\n",
        "+# ABOUTME: WebSocket handler for real-time chat streaming using LangGraph\n",
        "+# ABOUTME: Uses graph.astream_events() for token-by-token streaming with automatic checkpointing\n",
        " \n",
        " import json\n",
        " from typing import Dict\n",
        " from fastapi import WebSocket, WebSocketDisconnect\n",
        "+from langgraph.types import RunnableConfig\n",
        " from app.adapters.inbound.websocket_schemas import (\n",
        "     ClientMessage,\n",
        "     ServerTokenMessage,\n",
        "@@ -14,9 +15,7 @@\n",
        "     MessageType\n",
        " )\n",
        " from app.core.domain.user import User\n",
        "-from app.core.domain.message import Message, MessageRole\n",
        " from app.core.ports.llm_provider import ILLMProvider\n",
        "-from app.core.ports.message_repository import IMessageRepository\n",
        " from app.core.ports.conversation_repository import IConversationRepository\n",
        " from app.infrastructure.config.logging_config import get_logger\n",
        " \n",
        "@@ -56,26 +55,26 @@ async def send_message(self, websocket: WebSocket, message: dict):\n",
        " async def handle_websocket_chat(\n",
        "     websocket: WebSocket,\n",
        "     user: User,\n",
        "+    graph,\n",
        "     llm_provider: ILLMProvider,\n",
        "-    message_repository: IMessageRepository,\n",
        "     conversation_repository: IConversationRepository\n",
        " ):\n",
        "     \"\"\"\n",
        "-    Handle WebSocket chat session for a user.\n",
        "+    Handle WebSocket chat session for a user using LangGraph.\n",
        " \n",
        "     This function:\n",
        "     - Establishes the WebSocket connection\n",
        "     - Receives user messages\n",
        "-    - Streams LLM responses token-by-token\n",
        "-    - Persists messages to the database\n",
        "+    - Streams LLM responses token-by-token via graph.astream_events()\n",
        "+    - Messages are automatically persisted via LangGraph checkpointer\n",
        "     - Handles errors and disconnections\n",
        " \n",
        "     Args:\n",
        "         websocket: WebSocket connection instance\n",
        "         user: Authenticated user\n",
        "+        graph: Compiled LangGraph instance with checkpointing\n",
        "         llm_provider: LLM provider for generating responses\n",
        "-        message_repository: Repository for persisting messages\n",
        "-        conversation_repository: Repository for conversation metadata\n",
        "+        conversation_repository: Repository for conversation ownership verification\n",
        "     \"\"\"\n",
        "     await manager.connect(websocket, user.id)\n",
        " \n",
        "@@ -107,6 +106,7 @@ async def handle_websocket_chat(\n",
        "                 conversation_id = client_message.conversation_id\n",
        "                 conversation = await conversation_repository.get_by_id(conversation_id)\n",
        " \n",
        "+                # Verify conversation ownership (authorization at App DB layer)\n",
        "                 if not conversation or conversation.user_id != user.id:\n",
        "                     logger.warning(f\"User {user.id} attempted to access conversation {conversation_id}\")\n",
        "                     error_msg = ServerErrorMessage(\n",
        "@@ -116,45 +116,46 @@ async def handle_websocket_chat(\n",
        "                     await manager.send_message(websocket, error_msg.model_dump())\n",
        "                     continue\n",
        " \n",
        "-                user_message = Message(\n",
        "-                    conversation_id=conversation_id,\n",
        "-                    role=MessageRole.USER,\n",
        "-                    content=client_message.content\n",
        "+                # Create RunnableConfig with thread_id (conversation.id) and llm_provider\n",
        "+                config = RunnableConfig(\n",
        "+                    configurable={\n",
        "+                        \"thread_id\": conversation.id,\n",
        "+                        \"llm_provider\": llm_provider,\n",
        "+                        \"user_id\": user.id\n",
        "+                    }\n",
        "                 )\n",
        " \n",
        "-                saved_user_message = await message_repository.create(user_message)\n",
        "-                logger.info(f\"Saved user message {saved_user_message.id} to conversation {conversation_id}\")\n",
        "+                # Prepare input for graph\n",
        "+                input_data = {\n",
        "+                    \"user_input\": client_message.content,\n",
        "+                    \"conversation_id\": conversation.id,\n",
        "+                    \"user_id\": user.id\n",
        "+                }\n",
        " \n",
        "-                messages = await message_repository.get_by_conversation_id(conversation_id)\n",
        "+                logger.info(f\"Starting LangGraph streaming for conversation {conversation_id}\")\n",
        " \n",
        "-                full_response = []\n",
        "                 try:\n",
        "-                    async for token in llm_provider.stream(messages):\n",
        "-                        full_response.append(token)\n",
        "-                        token_msg = ServerTokenMessage(content=token)\n",
        "-                        await manager.send_message(websocket, token_msg.model_dump())\n",
        "-\n",
        "-                    response_content = \"\".join(full_response)\n",
        "-\n",
        "-                    assistant_message = Message(\n",
        "-                        conversation_id=conversation_id,\n",
        "-                        role=MessageRole.ASSISTANT,\n",
        "-                        content=response_content\n",
        "-                    )\n",
        "-\n",
        "-                    saved_assistant_message = await message_repository.create(assistant_message)\n",
        "-                    logger.info(f\"Saved assistant message {saved_assistant_message.id} to conversation {conversation_id}\")\n",
        "-\n",
        "-                    await conversation_repository.increment_message_count(conversation_id, 2)\n",
        "-\n",
        "+                    # Stream LLM tokens using graph.astream_events()\n",
        "+                    # Checkpointing happens automatically\n",
        "+                    async for event in graph.astream_events(input_data, config, version=\"v2\"):\n",
        "+                        # Stream tokens from LLM to client\n",
        "+                        if event[\"event\"] == \"on_chat_model_stream\":\n",
        "+                            chunk = event[\"data\"][\"chunk\"]\n",
        "+                            if hasattr(chunk, 'content') and chunk.content:\n",
        "+                                token_msg = ServerTokenMessage(content=chunk.content)\n",
        "+                                await manager.send_message(websocket, token_msg.model_dump())\n",
        "+\n",
        "+                    logger.info(f\"LangGraph streaming completed for conversation {conversation_id}\")\n",
        "+\n",
        "+                    # Send completion message (checkpointing already done)\n",
        "                     complete_msg = ServerCompleteMessage(\n",
        "-                        message_id=saved_assistant_message.id,\n",
        "+                        message_id=None,  # No longer tracking individual message IDs\n",
        "                         conversation_id=conversation_id\n",
        "                     )\n",
        "                     await manager.send_message(websocket, complete_msg.model_dump())\n",
        " \n",
        "                 except Exception as e:\n",
        "-                    logger.error(f\"LLM streaming failed for user {user.id}: {e}\")\n",
        "+                    logger.error(f\"LangGraph streaming failed for user {user.id}: {e}\")\n",
        "                     error_msg = ServerErrorMessage(\n",
        "                         message=f\"Failed to generate response: {str(e)}\",\n",
        "                         code=\"LLM_ERROR\"\n"
      ]
    },
    {
      "path": "backend/app/adapters/inbound/websocket_router.py",
      "status": "modified",
      "additions": 10,
      "deletions": 9,
      "patch": "@@ -1,11 +1,10 @@\n-# ABOUTME: WebSocket router for chat endpoints\n-# ABOUTME: Defines WebSocket routes with authentication and dependency injection\n+# ABOUTME: WebSocket router for chat endpoints with LangGraph integration\n+# ABOUTME: Uses LangGraph streaming with automatic checkpointing\n \n from fastapi import APIRouter, WebSocket, WebSocketException\n from app.adapters.inbound.websocket_handler import handle_websocket_chat\n from app.infrastructure.security.websocket_auth import get_user_from_websocket\n from app.adapters.outbound.llm_providers.provider_factory import get_llm_provider\n-from app.adapters.outbound.repositories.mongo_message_repository import MongoMessageRepository\n from app.adapters.outbound.repositories.mongo_conversation_repository import MongoConversationRepository\n from app.infrastructure.config.logging_config import get_logger\n \n@@ -17,14 +16,14 @@\n @router.websocket(\"/ws/chat\")\n async def websocket_chat_endpoint(websocket: WebSocket):\n     \"\"\"\n-    WebSocket endpoint for real-time chat streaming.\n+    WebSocket endpoint for real-time chat streaming using LangGraph.\n \n     This endpoint:\n     - Authenticates the user via token (query param or header)\n     - Establishes a persistent WebSocket connection\n     - Receives user messages\n-    - Streams LLM responses token-by-token\n-    - Persists all messages to the database\n+    - Streams LLM responses token-by-token via graph.astream_events()\n+    - Messages are automatically persisted via LangGraph checkpointer\n \n     Authentication:\n     - Send token as query parameter: /ws/chat?token=<jwt_token>\n@@ -33,7 +32,7 @@ async def websocket_chat_endpoint(websocket: WebSocket):\n     Protocol:\n     - Client sends: {\"type\": \"message\", \"conversation_id\": \"uuid\", \"content\": \"...\"}\n     - Server streams: {\"type\": \"token\", \"content\": \"...\"}\n-    - Server completes: {\"type\": \"complete\", \"message_id\": \"uuid\", \"conversation_id\": \"uuid\"}\n+    - Server completes: {\"type\": \"complete\", \"conversation_id\": \"uuid\"}\n     - Server errors: {\"type\": \"error\", \"message\": \"...\", \"code\": \"...\"}\n \n     Raises:\n@@ -42,15 +41,17 @@ async def websocket_chat_endpoint(websocket: WebSocket):\n     try:\n         user = await get_user_from_websocket(websocket)\n \n+        # Get compiled graph from app state\n+        graph = websocket.app.state.streaming_chat_graph\n+\n         llm_provider = get_llm_provider()\n-        message_repository = MongoMessageRepository()\n         conversation_repository = MongoConversationRepository()\n \n         await handle_websocket_chat(\n             websocket=websocket,\n             user=user,\n+            graph=graph,\n             llm_provider=llm_provider,\n-            message_repository=message_repository,\n             conversation_repository=conversation_repository\n         )\n ",
      "patch_lines": [
        "@@ -1,11 +1,10 @@\n",
        "-# ABOUTME: WebSocket router for chat endpoints\n",
        "-# ABOUTME: Defines WebSocket routes with authentication and dependency injection\n",
        "+# ABOUTME: WebSocket router for chat endpoints with LangGraph integration\n",
        "+# ABOUTME: Uses LangGraph streaming with automatic checkpointing\n",
        " \n",
        " from fastapi import APIRouter, WebSocket, WebSocketException\n",
        " from app.adapters.inbound.websocket_handler import handle_websocket_chat\n",
        " from app.infrastructure.security.websocket_auth import get_user_from_websocket\n",
        " from app.adapters.outbound.llm_providers.provider_factory import get_llm_provider\n",
        "-from app.adapters.outbound.repositories.mongo_message_repository import MongoMessageRepository\n",
        " from app.adapters.outbound.repositories.mongo_conversation_repository import MongoConversationRepository\n",
        " from app.infrastructure.config.logging_config import get_logger\n",
        " \n",
        "@@ -17,14 +16,14 @@\n",
        " @router.websocket(\"/ws/chat\")\n",
        " async def websocket_chat_endpoint(websocket: WebSocket):\n",
        "     \"\"\"\n",
        "-    WebSocket endpoint for real-time chat streaming.\n",
        "+    WebSocket endpoint for real-time chat streaming using LangGraph.\n",
        " \n",
        "     This endpoint:\n",
        "     - Authenticates the user via token (query param or header)\n",
        "     - Establishes a persistent WebSocket connection\n",
        "     - Receives user messages\n",
        "-    - Streams LLM responses token-by-token\n",
        "-    - Persists all messages to the database\n",
        "+    - Streams LLM responses token-by-token via graph.astream_events()\n",
        "+    - Messages are automatically persisted via LangGraph checkpointer\n",
        " \n",
        "     Authentication:\n",
        "     - Send token as query parameter: /ws/chat?token=<jwt_token>\n",
        "@@ -33,7 +32,7 @@ async def websocket_chat_endpoint(websocket: WebSocket):\n",
        "     Protocol:\n",
        "     - Client sends: {\"type\": \"message\", \"conversation_id\": \"uuid\", \"content\": \"...\"}\n",
        "     - Server streams: {\"type\": \"token\", \"content\": \"...\"}\n",
        "-    - Server completes: {\"type\": \"complete\", \"message_id\": \"uuid\", \"conversation_id\": \"uuid\"}\n",
        "+    - Server completes: {\"type\": \"complete\", \"conversation_id\": \"uuid\"}\n",
        "     - Server errors: {\"type\": \"error\", \"message\": \"...\", \"code\": \"...\"}\n",
        " \n",
        "     Raises:\n",
        "@@ -42,15 +41,17 @@ async def websocket_chat_endpoint(websocket: WebSocket):\n",
        "     try:\n",
        "         user = await get_user_from_websocket(websocket)\n",
        " \n",
        "+        # Get compiled graph from app state\n",
        "+        graph = websocket.app.state.streaming_chat_graph\n",
        "+\n",
        "         llm_provider = get_llm_provider()\n",
        "-        message_repository = MongoMessageRepository()\n",
        "         conversation_repository = MongoConversationRepository()\n",
        " \n",
        "         await handle_websocket_chat(\n",
        "             websocket=websocket,\n",
        "             user=user,\n",
        "+            graph=graph,\n",
        "             llm_provider=llm_provider,\n",
        "-            message_repository=message_repository,\n",
        "             conversation_repository=conversation_repository\n",
        "         )\n",
        " \n"
      ]
    }
  ]
}
{
  "project": "Research Data/JAA",
  "repo": "JaymieX/JAA",
  "prior_commit": "bcd9c1c5414efe14115e5f05a62d2e4b5b7d9b5e",
  "researched_commit": "c373419d444a224aaf561b032ba5446e00138286",
  "compare_url": "https://github.com/JaymieX/JAA/compare/bcd9c1c5414efe14115e5f05a62d2e4b5b7d9b5e...c373419d444a224aaf561b032ba5446e00138286",
  "ahead_by": 1,
  "behind_by": 0,
  "changed_files": [
    {
      "path": "llm/llm.py",
      "status": "modified",
      "additions": 116,
      "deletions": 78,
      "patch": "@@ -1,13 +1,11 @@\n-import json\n import platform\n-from pathlib import Path\n from pydantic import BaseModel, TypeAdapter\n-import torch\n-from transformers import BitsAndBytesConfig, pipeline, GenerationConfig, AutoModelForCausalLM, AutoTokenizer\n-from peft import PeftModel\n+from transformers import GenerationConfig\n from enum import Enum\n-from typing import Optional, Type, TypedDict, Literal, Union\n+from typing import Optional, Type, TypedDict, Literal, Union, Annotated\n from langgraph.graph import StateGraph, END\n+from langgraph.graph.message import add_messages\n+from langgraph.checkpoint.memory import MemorySaver\n from json_repair import repair_json\n \n \n@@ -33,17 +31,17 @@ class RouterFunction(str, Enum):\n # LangGraph State Definition\n class AgentState(TypedDict):\n     user_input:           str\n-    router_decision:      RouterFunction\n-    router_query:         str\n-    query_keywords:       set[str]\n-    vuln_response_dict:   dict  # Store vulnerability check response\n-    final_response:       str\n-    conversation_history: list\n+    router_decision:      Optional[RouterFunction]\n+    router_query:         Optional[str]\n+    query_keywords:       Optional[set[str]]\n+    vuln_response_dict:   Optional[dict]  # Store vulnerability check response\n+    final_response:       Optional[str]\n+    conversation_history: Annotated[list, add_messages]  # Use LangGraph's message reducer\n \n \n class LLM:\n     def __init__(self, profile : llm_loader.LLMProFile, notion_token, notion_page_id):\n-        self.conversation_history = []\n+        # Remove self.conversation_history - now managed by LangGraph state\n \n         self.llm = llm_loader.load_llm(profile)\n         if (self.llm is not None):    \n@@ -115,7 +113,9 @@ def _setup_langgraph_workflow(self):\n             }\n         )\n \n-        self.workflow = workflow.compile()\n+        # Compile with MemorySaver for conversation persistence\n+        self.checkpointer = MemorySaver()\n+        self.workflow = workflow.compile(checkpointer=self.checkpointer)\n         \n     \n     def _check_rag_keyword(self, state: AgentState) -> Literal[\"vuln_relevance\", \"END\"]:\n@@ -132,6 +132,25 @@ def _extract_keywords(self, text: str, keywords: set[str] = {'rag'}) -> set[str]\n             if keyword.lower() in text_lower:\n                 found.add(keyword)\n         return found\n+\n+    def _messages_to_dicts(self, messages: list) -> list:\n+        \"\"\"Convert LangChain message objects to plain dict format\"\"\"\n+        result = []\n+        for msg in messages:\n+            # Check if it's a LangChain message object\n+            if hasattr(msg, 'type') and hasattr(msg, 'content'):\n+                # Convert LangChain message to dict\n+                role_map = {\n+                    'human': 'user',\n+                    'ai': 'assistant',\n+                    'system': 'system'\n+                }\n+                role = role_map.get(msg.type, 'user')\n+                result.append({\"role\": role, \"content\": msg.content})\n+            elif isinstance(msg, dict):\n+                # Already a dict, keep as is\n+                result.append(msg)\n+        return result\n         \n         \n     def _gen(self, messages, gen_cfg : GenerationConfig, structured_output_schema: Optional[Union[Type[BaseModel], type]] = None):\n@@ -169,15 +188,15 @@ def _gen(self, messages, gen_cfg : GenerationConfig, structured_output_schema: O\n         return True, generated_text\n \n \n-    def _router_node(self, state: AgentState) -> AgentState:\n+    def _router_node(self, state: AgentState) -> dict:\n         \"\"\"Router node that decides which agent to use\"\"\"\n         gen_cfg = GenerationConfig(\n             max_new_tokens=50,\n             do_sample=False,\n             repetition_penalty=1.05,\n             eos_token_id=self.eos_ids\n         )\n-        \n+\n         keywords = self._extract_keywords(state[\"user_input\"])\n \n         succeed, gen_obj = self._gen([llm_prompts.ROUTER_SYSTEM_PROMPT, {\"role\":\"user\",\"content\":state[\"user_input\"]}], gen_cfg, llm_prompts.ROUTER_RESPONSE_JSON_ENFORCE)\n@@ -195,62 +214,60 @@ def _router_node(self, state: AgentState) -> AgentState:\n \n         else:\n             func_name = RouterFunction.HUMAN_TEXT\n+            query     = \"\"\n \n-        state[\"router_decision\"] = func_name\n-        state[\"router_query\"]    = query\n-        state[\"query_keywords\"]  = keywords\n-        \n-        return state\n+        # Return only updates - LangGraph will merge into state\n+        return {\n+            \"router_decision\": func_name,\n+            \"router_query\":    query,\n+            \"query_keywords\":  keywords\n+        }\n \n \n     def _route_decision(self, state: AgentState) -> str:\n         \"\"\"Routing function for conditional edges\"\"\"\n         return state[\"router_decision\"]\n \n \n-    def _search_arxiv_node(self, state: AgentState) -> AgentState:\n+    def _search_arxiv_node(self, state: AgentState) -> dict:\n         \"\"\"Search ArXiv agent node\"\"\"\n-        query = state[\"router_query\"]\n+        query = state.get(\"router_query\", \"\")\n         print(f\"FUNCTION CALL: search_arxiv({query})\")\n-        \n+\n         if query == \"\":\n-            state[\"final_response\"] = \"I am sorry I could not search the paper you need\"\n-            return state\n-        \n+            return {\"final_response\": \"I am sorry I could not search the paper you need\"}\n+\n         result = \"\"\n         rag_results = self.rag_search.hybrid_search(query, 3)\n-        \n+\n         if len(rag_results) <= 0:\n-            state[\"final_response\"] = \"I am sorry I could not search the paper you need\"\n-            return state\n-        \n+            return {\"final_response\": \"I am sorry I could not search the paper you need\"}\n+\n         # only sub summerize if we have more than 1 results\n         if len(rag_results) > 1:\n             for rag_result in rag_results:\n                 print(f\"RAG result: {str(rag_result)} \\n\\n\")\n                 result += self.summarizer.summarize(rag_result['text'], 180, 80)[0][\"summary_text\"] + \"\\n\"\n-            \n+\n         result = self.summarizer.summarize(result, 200, 60)[0][\"summary_text\"]\n-        \n-        state[\"final_response\"] = result\n-        return state\n \n+        return {\"final_response\": result}\n \n-    def _notion_node(self, state: AgentState) -> AgentState:\n+\n+    def _notion_node(self, state: AgentState) -> dict:\n         \"\"\"Notion agent node\"\"\"\n         print(f\"FUNCTION CALL: notion()\")\n \n         if not self.is_notion_connected:\n             result = \"You are not connected to notion. I cannot write to it.\"\n         else:\n-            self.notion.write_blocks(self.notion.conversation_to_notion_blocks(state[\"conversation_history\"][-10:]))\n+            self.notion.write_blocks(self.notion.conversation_to_notion_blocks(state.get(\"conversation_history\", [])[-10:]))\n             result = \"I have written the conversation to notion.\"\n \n-        state[\"final_response\"] = result\n-        return state\n+        return {\"final_response\": result}\n \n \n-    def _human_text_node(self, state: AgentState) -> AgentState:\n+    def _human_text_node(self, state: AgentState) -> dict:\n         \"\"\"Human text conversation agent node\"\"\"\n         print(f\"FUNCTION CALL: human_text()\")\n \n@@ -263,10 +280,16 @@ def _human_text_node(self, state: AgentState) -> AgentState:\n             eos_token_id=self.eos_ids\n         )\n \n-        succeed, response = self._gen([llm_prompts.CHAT_SYSTEM_PROMPT] + state[\"conversation_history\"][-10:], gen_cfg)\n+        # Convert LangChain messages back to dict format for LLM\n+        conversation = state.get(\"conversation_history\", [])[-10:]\n+        conversation_dicts = self._messages_to_dicts(conversation)\n+\n+        prompt = [llm_prompts.CHAT_SYSTEM_PROMPT] + conversation_dicts\n+        print(str(prompt))\n \n-        state[\"final_response\"] = response\n-        return state\n+        succeed, response = self._gen(prompt, gen_cfg)\n+\n+        return {\"final_response\": response}\n \n \n     def _print_vlun_code_human(self, response):\n@@ -313,7 +336,7 @@ def _print_vlun_code_human(self, response):\n         return '\\n'.join(sections)\n \n \n-    def _check_vuln_node(self, state: AgentState) -> AgentState:\n+    def _check_vuln_node(self, state: AgentState) -> dict:\n         \"\"\"Vulnerability check agent node\"\"\"\n         print(f\"FUNCTION CALL: vulnerability_check()\")\n \n@@ -329,14 +352,14 @@ def _check_vuln_node(self, state: AgentState) -> AgentState:\n         succeed, response = self._gen([llm_prompts.SECURITY_SYSTEM_PROMPT, {\"role\":\"user\",\"content\":state[\"user_input\"]}], gen_cfg, llm_prompts.VlunCheckResponse)\n \n         if succeed:\n-            state[\"vuln_response_dict\"] = response\n-            state[\"final_response\"] = self._print_vlun_code_human(response)\n+            return {\n+                \"vuln_response_dict\": response,\n+                \"final_response\": self._print_vlun_code_human(response)\n+            }\n         else:\n-            state[\"final_response\"] = \"I am sorry I could not help you with this, lets try something else.\"\n+            return {\"final_response\": \"I am sorry I could not help you with this, lets try something else.\"}\n \n-        return state\n-\n-    def _vlun_relevence_node(self, state: AgentState) -> AgentState:\n+    def _vlun_relevence_node(self, state: AgentState) -> dict:\n         \"\"\"Add RAG-based relevance to vulnerability check response\"\"\"\n         print(f\"FUNCTION CALL: vuln_relevance()\")\n \n@@ -382,40 +405,55 @@ def _vlun_relevence_node(self, state: AgentState) -> AgentState:\n         if succeed:\n             # Add relevance to vuln dict and regenerate response\n             vuln_dict[\"relevance\"] = relevance_text\n-            state[\"final_response\"] = self._print_vlun_code_human(vuln_dict)\n+            return {\"final_response\": self._print_vlun_code_human(vuln_dict)}\n \n-        return state\n+        # If failed, return empty dict (no state updates)\n+        return {}\n \n \n-    def generate_response(self, user_text):\n-        \"\"\"Main response generation using LangGraph workflow\"\"\"\n-        # Update conversation history\n-        self.conversation_history.append({\"role\":\"user\",\"content\":user_text})\n+    def generate_response(self, user_text, thread_id: str = \"default\", stream: bool = False):\n+        \"\"\"\n+        Main response generation using LangGraph workflow\n \n-        # Create initial state\n-        initial_state = AgentState(\n-            user_input=user_text,\n-            router_decision=RouterFunction.HUMAN_TEXT,\n-            router_query=\"\",\n-            query_keywords={},\n-            final_response=\"\",\n-            json_response=\"\",\n-            conversation_history=self.conversation_history.copy()\n-        )\n+        Args:\n+            user_text: User input message\n+            thread_id: Thread ID for conversation persistence (default: \"default\")\n+            stream: Whether to use streaming mode (default: False)\n \n-        # Run LangGraph workflow\n-        try:\n-            result = self.workflow.invoke(initial_state)\n-            output = result[\"final_response\"]\n+        Returns:\n+            Response text (or generator if stream=True)\n+        \"\"\"\n+        # Create initial state - only pass required inputs\n+        initial_state = {\n+            \"user_input\": user_text,\n+            \"conversation_history\": [{\"role\": \"user\", \"content\": user_text}]\n+        }\n \n-            # Update conversation history with response\n-            self.conversation_history.append({\"role\":\"assistant\",\"content\":output})\n+        # Config for thread-based persistence\n+        config = {\"configurable\": {\"thread_id\": thread_id}}\n+\n+        # Run LangGraph workflow\n+        if stream:\n+            # Streaming mode - return generator\n+            return self._stream_response(initial_state, config)\n+        else:\n+            # Standard invoke mode\n+            result = self.workflow.invoke(initial_state, config)\n+            output = result.get(\"final_response\", \"I'm sorry, I couldn't generate a response.\")\n+\n+            # Add assistant response to conversation history automatically\n+            # This ensures it's recorded regardless of which node was executed\n+            self.workflow.update_state(\n+                config,\n+                {\"conversation_history\": [{\"role\": \"assistant\", \"content\": output}]}\n+            )\n \n             return output\n \n-        except Exception as e:\n-            print(f\"LangGraph workflow error: {e}\")\n-            # Fallback to simple response\n-            fallback_response = \"I'm sorry, I encountered an error processing your request.\"\n-            \n-            return fallback_response\n\\ No newline at end of file\n+    def _stream_response(self, initial_state, config):\n+        \"\"\"Generator for streaming responses\"\"\"\n+        for event in self.workflow.stream(initial_state, config):\n+            # Yield intermediate results as they become available\n+            for node_name, node_output in event.items():\n+                if \"final_response\" in node_output:\n+                    yield node_output[\"final_response\"]\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -1,13 +1,11 @@\n",
        "-import json\n",
        " import platform\n",
        "-from pathlib import Path\n",
        " from pydantic import BaseModel, TypeAdapter\n",
        "-import torch\n",
        "-from transformers import BitsAndBytesConfig, pipeline, GenerationConfig, AutoModelForCausalLM, AutoTokenizer\n",
        "-from peft import PeftModel\n",
        "+from transformers import GenerationConfig\n",
        " from enum import Enum\n",
        "-from typing import Optional, Type, TypedDict, Literal, Union\n",
        "+from typing import Optional, Type, TypedDict, Literal, Union, Annotated\n",
        " from langgraph.graph import StateGraph, END\n",
        "+from langgraph.graph.message import add_messages\n",
        "+from langgraph.checkpoint.memory import MemorySaver\n",
        " from json_repair import repair_json\n",
        " \n",
        " \n",
        "@@ -33,17 +31,17 @@ class RouterFunction(str, Enum):\n",
        " # LangGraph State Definition\n",
        " class AgentState(TypedDict):\n",
        "     user_input:           str\n",
        "-    router_decision:      RouterFunction\n",
        "-    router_query:         str\n",
        "-    query_keywords:       set[str]\n",
        "-    vuln_response_dict:   dict  # Store vulnerability check response\n",
        "-    final_response:       str\n",
        "-    conversation_history: list\n",
        "+    router_decision:      Optional[RouterFunction]\n",
        "+    router_query:         Optional[str]\n",
        "+    query_keywords:       Optional[set[str]]\n",
        "+    vuln_response_dict:   Optional[dict]  # Store vulnerability check response\n",
        "+    final_response:       Optional[str]\n",
        "+    conversation_history: Annotated[list, add_messages]  # Use LangGraph's message reducer\n",
        " \n",
        " \n",
        " class LLM:\n",
        "     def __init__(self, profile : llm_loader.LLMProFile, notion_token, notion_page_id):\n",
        "-        self.conversation_history = []\n",
        "+        # Remove self.conversation_history - now managed by LangGraph state\n",
        " \n",
        "         self.llm = llm_loader.load_llm(profile)\n",
        "         if (self.llm is not None):    \n",
        "@@ -115,7 +113,9 @@ def _setup_langgraph_workflow(self):\n",
        "             }\n",
        "         )\n",
        " \n",
        "-        self.workflow = workflow.compile()\n",
        "+        # Compile with MemorySaver for conversation persistence\n",
        "+        self.checkpointer = MemorySaver()\n",
        "+        self.workflow = workflow.compile(checkpointer=self.checkpointer)\n",
        "         \n",
        "     \n",
        "     def _check_rag_keyword(self, state: AgentState) -> Literal[\"vuln_relevance\", \"END\"]:\n",
        "@@ -132,6 +132,25 @@ def _extract_keywords(self, text: str, keywords: set[str] = {'rag'}) -> set[str]\n",
        "             if keyword.lower() in text_lower:\n",
        "                 found.add(keyword)\n",
        "         return found\n",
        "+\n",
        "+    def _messages_to_dicts(self, messages: list) -> list:\n",
        "+        \"\"\"Convert LangChain message objects to plain dict format\"\"\"\n",
        "+        result = []\n",
        "+        for msg in messages:\n",
        "+            # Check if it's a LangChain message object\n",
        "+            if hasattr(msg, 'type') and hasattr(msg, 'content'):\n",
        "+                # Convert LangChain message to dict\n",
        "+                role_map = {\n",
        "+                    'human': 'user',\n",
        "+                    'ai': 'assistant',\n",
        "+                    'system': 'system'\n",
        "+                }\n",
        "+                role = role_map.get(msg.type, 'user')\n",
        "+                result.append({\"role\": role, \"content\": msg.content})\n",
        "+            elif isinstance(msg, dict):\n",
        "+                # Already a dict, keep as is\n",
        "+                result.append(msg)\n",
        "+        return result\n",
        "         \n",
        "         \n",
        "     def _gen(self, messages, gen_cfg : GenerationConfig, structured_output_schema: Optional[Union[Type[BaseModel], type]] = None):\n",
        "@@ -169,15 +188,15 @@ def _gen(self, messages, gen_cfg : GenerationConfig, structured_output_schema: O\n",
        "         return True, generated_text\n",
        " \n",
        " \n",
        "-    def _router_node(self, state: AgentState) -> AgentState:\n",
        "+    def _router_node(self, state: AgentState) -> dict:\n",
        "         \"\"\"Router node that decides which agent to use\"\"\"\n",
        "         gen_cfg = GenerationConfig(\n",
        "             max_new_tokens=50,\n",
        "             do_sample=False,\n",
        "             repetition_penalty=1.05,\n",
        "             eos_token_id=self.eos_ids\n",
        "         )\n",
        "-        \n",
        "+\n",
        "         keywords = self._extract_keywords(state[\"user_input\"])\n",
        " \n",
        "         succeed, gen_obj = self._gen([llm_prompts.ROUTER_SYSTEM_PROMPT, {\"role\":\"user\",\"content\":state[\"user_input\"]}], gen_cfg, llm_prompts.ROUTER_RESPONSE_JSON_ENFORCE)\n",
        "@@ -195,62 +214,60 @@ def _router_node(self, state: AgentState) -> AgentState:\n",
        " \n",
        "         else:\n",
        "             func_name = RouterFunction.HUMAN_TEXT\n",
        "+            query     = \"\"\n",
        " \n",
        "-        state[\"router_decision\"] = func_name\n",
        "-        state[\"router_query\"]    = query\n",
        "-        state[\"query_keywords\"]  = keywords\n",
        "-        \n",
        "-        return state\n",
        "+        # Return only updates - LangGraph will merge into state\n",
        "+        return {\n",
        "+            \"router_decision\": func_name,\n",
        "+            \"router_query\":    query,\n",
        "+            \"query_keywords\":  keywords\n",
        "+        }\n",
        " \n",
        " \n",
        "     def _route_decision(self, state: AgentState) -> str:\n",
        "         \"\"\"Routing function for conditional edges\"\"\"\n",
        "         return state[\"router_decision\"]\n",
        " \n",
        " \n",
        "-    def _search_arxiv_node(self, state: AgentState) -> AgentState:\n",
        "+    def _search_arxiv_node(self, state: AgentState) -> dict:\n",
        "         \"\"\"Search ArXiv agent node\"\"\"\n",
        "-        query = state[\"router_query\"]\n",
        "+        query = state.get(\"router_query\", \"\")\n",
        "         print(f\"FUNCTION CALL: search_arxiv({query})\")\n",
        "-        \n",
        "+\n",
        "         if query == \"\":\n",
        "-            state[\"final_response\"] = \"I am sorry I could not search the paper you need\"\n",
        "-            return state\n",
        "-        \n",
        "+            return {\"final_response\": \"I am sorry I could not search the paper you need\"}\n",
        "+\n",
        "         result = \"\"\n",
        "         rag_results = self.rag_search.hybrid_search(query, 3)\n",
        "-        \n",
        "+\n",
        "         if len(rag_results) <= 0:\n",
        "-            state[\"final_response\"] = \"I am sorry I could not search the paper you need\"\n",
        "-            return state\n",
        "-        \n",
        "+            return {\"final_response\": \"I am sorry I could not search the paper you need\"}\n",
        "+\n",
        "         # only sub summerize if we have more than 1 results\n",
        "         if len(rag_results) > 1:\n",
        "             for rag_result in rag_results:\n",
        "                 print(f\"RAG result: {str(rag_result)} \\n\\n\")\n",
        "                 result += self.summarizer.summarize(rag_result['text'], 180, 80)[0][\"summary_text\"] + \"\\n\"\n",
        "-            \n",
        "+\n",
        "         result = self.summarizer.summarize(result, 200, 60)[0][\"summary_text\"]\n",
        "-        \n",
        "-        state[\"final_response\"] = result\n",
        "-        return state\n",
        " \n",
        "+        return {\"final_response\": result}\n",
        " \n",
        "-    def _notion_node(self, state: AgentState) -> AgentState:\n",
        "+\n",
        "+    def _notion_node(self, state: AgentState) -> dict:\n",
        "         \"\"\"Notion agent node\"\"\"\n",
        "         print(f\"FUNCTION CALL: notion()\")\n",
        " \n",
        "         if not self.is_notion_connected:\n",
        "             result = \"You are not connected to notion. I cannot write to it.\"\n",
        "         else:\n",
        "-            self.notion.write_blocks(self.notion.conversation_to_notion_blocks(state[\"conversation_history\"][-10:]))\n",
        "+            self.notion.write_blocks(self.notion.conversation_to_notion_blocks(state.get(\"conversation_history\", [])[-10:]))\n",
        "             result = \"I have written the conversation to notion.\"\n",
        " \n",
        "-        state[\"final_response\"] = result\n",
        "-        return state\n",
        "+        return {\"final_response\": result}\n",
        " \n",
        " \n",
        "-    def _human_text_node(self, state: AgentState) -> AgentState:\n",
        "+    def _human_text_node(self, state: AgentState) -> dict:\n",
        "         \"\"\"Human text conversation agent node\"\"\"\n",
        "         print(f\"FUNCTION CALL: human_text()\")\n",
        " \n",
        "@@ -263,10 +280,16 @@ def _human_text_node(self, state: AgentState) -> AgentState:\n",
        "             eos_token_id=self.eos_ids\n",
        "         )\n",
        " \n",
        "-        succeed, response = self._gen([llm_prompts.CHAT_SYSTEM_PROMPT] + state[\"conversation_history\"][-10:], gen_cfg)\n",
        "+        # Convert LangChain messages back to dict format for LLM\n",
        "+        conversation = state.get(\"conversation_history\", [])[-10:]\n",
        "+        conversation_dicts = self._messages_to_dicts(conversation)\n",
        "+\n",
        "+        prompt = [llm_prompts.CHAT_SYSTEM_PROMPT] + conversation_dicts\n",
        "+        print(str(prompt))\n",
        " \n",
        "-        state[\"final_response\"] = response\n",
        "-        return state\n",
        "+        succeed, response = self._gen(prompt, gen_cfg)\n",
        "+\n",
        "+        return {\"final_response\": response}\n",
        " \n",
        " \n",
        "     def _print_vlun_code_human(self, response):\n",
        "@@ -313,7 +336,7 @@ def _print_vlun_code_human(self, response):\n",
        "         return '\\n'.join(sections)\n",
        " \n",
        " \n",
        "-    def _check_vuln_node(self, state: AgentState) -> AgentState:\n",
        "+    def _check_vuln_node(self, state: AgentState) -> dict:\n",
        "         \"\"\"Vulnerability check agent node\"\"\"\n",
        "         print(f\"FUNCTION CALL: vulnerability_check()\")\n",
        " \n",
        "@@ -329,14 +352,14 @@ def _check_vuln_node(self, state: AgentState) -> AgentState:\n",
        "         succeed, response = self._gen([llm_prompts.SECURITY_SYSTEM_PROMPT, {\"role\":\"user\",\"content\":state[\"user_input\"]}], gen_cfg, llm_prompts.VlunCheckResponse)\n",
        " \n",
        "         if succeed:\n",
        "-            state[\"vuln_response_dict\"] = response\n",
        "-            state[\"final_response\"] = self._print_vlun_code_human(response)\n",
        "+            return {\n",
        "+                \"vuln_response_dict\": response,\n",
        "+                \"final_response\": self._print_vlun_code_human(response)\n",
        "+            }\n",
        "         else:\n",
        "-            state[\"final_response\"] = \"I am sorry I could not help you with this, lets try something else.\"\n",
        "+            return {\"final_response\": \"I am sorry I could not help you with this, lets try something else.\"}\n",
        " \n",
        "-        return state\n",
        "-\n",
        "-    def _vlun_relevence_node(self, state: AgentState) -> AgentState:\n",
        "+    def _vlun_relevence_node(self, state: AgentState) -> dict:\n",
        "         \"\"\"Add RAG-based relevance to vulnerability check response\"\"\"\n",
        "         print(f\"FUNCTION CALL: vuln_relevance()\")\n",
        " \n",
        "@@ -382,40 +405,55 @@ def _vlun_relevence_node(self, state: AgentState) -> AgentState:\n",
        "         if succeed:\n",
        "             # Add relevance to vuln dict and regenerate response\n",
        "             vuln_dict[\"relevance\"] = relevance_text\n",
        "-            state[\"final_response\"] = self._print_vlun_code_human(vuln_dict)\n",
        "+            return {\"final_response\": self._print_vlun_code_human(vuln_dict)}\n",
        " \n",
        "-        return state\n",
        "+        # If failed, return empty dict (no state updates)\n",
        "+        return {}\n",
        " \n",
        " \n",
        "-    def generate_response(self, user_text):\n",
        "-        \"\"\"Main response generation using LangGraph workflow\"\"\"\n",
        "-        # Update conversation history\n",
        "-        self.conversation_history.append({\"role\":\"user\",\"content\":user_text})\n",
        "+    def generate_response(self, user_text, thread_id: str = \"default\", stream: bool = False):\n",
        "+        \"\"\"\n",
        "+        Main response generation using LangGraph workflow\n",
        " \n",
        "-        # Create initial state\n",
        "-        initial_state = AgentState(\n",
        "-            user_input=user_text,\n",
        "-            router_decision=RouterFunction.HUMAN_TEXT,\n",
        "-            router_query=\"\",\n",
        "-            query_keywords={},\n",
        "-            final_response=\"\",\n",
        "-            json_response=\"\",\n",
        "-            conversation_history=self.conversation_history.copy()\n",
        "-        )\n",
        "+        Args:\n",
        "+            user_text: User input message\n",
        "+            thread_id: Thread ID for conversation persistence (default: \"default\")\n",
        "+            stream: Whether to use streaming mode (default: False)\n",
        " \n",
        "-        # Run LangGraph workflow\n",
        "-        try:\n",
        "-            result = self.workflow.invoke(initial_state)\n",
        "-            output = result[\"final_response\"]\n",
        "+        Returns:\n",
        "+            Response text (or generator if stream=True)\n",
        "+        \"\"\"\n",
        "+        # Create initial state - only pass required inputs\n",
        "+        initial_state = {\n",
        "+            \"user_input\": user_text,\n",
        "+            \"conversation_history\": [{\"role\": \"user\", \"content\": user_text}]\n",
        "+        }\n",
        " \n",
        "-            # Update conversation history with response\n",
        "-            self.conversation_history.append({\"role\":\"assistant\",\"content\":output})\n",
        "+        # Config for thread-based persistence\n",
        "+        config = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "+\n",
        "+        # Run LangGraph workflow\n",
        "+        if stream:\n",
        "+            # Streaming mode - return generator\n",
        "+            return self._stream_response(initial_state, config)\n",
        "+        else:\n",
        "+            # Standard invoke mode\n",
        "+            result = self.workflow.invoke(initial_state, config)\n",
        "+            output = result.get(\"final_response\", \"I'm sorry, I couldn't generate a response.\")\n",
        "+\n",
        "+            # Add assistant response to conversation history automatically\n",
        "+            # This ensures it's recorded regardless of which node was executed\n",
        "+            self.workflow.update_state(\n",
        "+                config,\n",
        "+                {\"conversation_history\": [{\"role\": \"assistant\", \"content\": output}]}\n",
        "+            )\n",
        " \n",
        "             return output\n",
        " \n",
        "-        except Exception as e:\n",
        "-            print(f\"LangGraph workflow error: {e}\")\n",
        "-            # Fallback to simple response\n",
        "-            fallback_response = \"I'm sorry, I encountered an error processing your request.\"\n",
        "-            \n",
        "-            return fallback_response\n",
        "\\ No newline at end of file\n",
        "+    def _stream_response(self, initial_state, config):\n",
        "+        \"\"\"Generator for streaming responses\"\"\"\n",
        "+        for event in self.workflow.stream(initial_state, config):\n",
        "+            # Yield intermediate results as they become available\n",
        "+            for node_name, node_output in event.items():\n",
        "+                if \"final_response\" in node_output:\n",
        "+                    yield node_output[\"final_response\"]\n",
        "\\ No newline at end of file\n"
      ]
    }
  ]
}
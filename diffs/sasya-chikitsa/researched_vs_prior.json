{
  "project": "Research Data/sasya-chikitsa",
  "repo": "cds-9-group-6/sasya-chikitsa",
  "prior_commit": "c4dae086a9b7d91697e5350ec97a6df299a2b380",
  "researched_commit": "7600564b2048f95d4e1e7e22ad67a0417cade07d",
  "compare_url": "https://github.com/cds-9-group-6/sasya-chikitsa/compare/c4dae086a9b7d91697e5350ec97a6df299a2b380...7600564b2048f95d4e1e7e22ad67a0417cade07d",
  "ahead_by": 1,
  "behind_by": 0,
  "changed_files": [
    {
      "path": "engine/fsm_agent/core/langgraph_workflow.py",
      "status": "modified",
      "additions": 27,
      "deletions": 13,
      "patch": "@@ -477,22 +477,36 @@ async def stream_process_message(self, session_id: str, user_message: str, user_\n                             \"data\": filtered_delta\n                         }\n                 \n-                # SPECIAL CASE: Stream attention overlay if classification completed\n-                current_node = actual_state_data.get(\"current_node\", \"\")\n-                if current_node == \"classifying\" and \"attention_overlay\" in actual_state_data and actual_state_data.get(\"attention_overlay\"):\n+                # GENERIC: Stream attention overlay if any node produces one\n+                if \"attention_overlay\" in actual_state_data and actual_state_data.get(\"attention_overlay\"):\n                     attention_overlay = actual_state_data[\"attention_overlay\"]\n+                    current_node = actual_state_data.get(\"current_node\", \"\")\n                     \n-                    # Stream attention overlay as separate event (not saved in state persistence)\n-                    yield {\n-                        \"type\": \"attention_overlay\",\n-                        \"session_id\": session_id,\n-                        \"data\": {\n-                            \"attention_overlay\": attention_overlay,\n-                            \"disease_name\": actual_state_data.get(\"disease_name\"),\n-                            \"confidence\": actual_state_data.get(\"confidence\")\n+                    # Prevent duplicate streaming using overlay content hash\n+                    overlay_hash = hash(attention_overlay[:100] + current_node)  # Hash first 100 chars + node\n+                    overlay_hash_key = f\"_overlay_hashes_{session_id}\"\n+                    streamed_overlays = getattr(self, overlay_hash_key, set())\n+                    \n+                    if overlay_hash not in streamed_overlays:\n+                        # Stream attention overlay as separate event (not saved in state persistence)\n+                        yield {\n+                            \"type\": \"attention_overlay\",\n+                            \"session_id\": session_id,\n+                            \"data\": {\n+                                \"attention_overlay\": attention_overlay,\n+                                \"disease_name\": actual_state_data.get(\"disease_name\"),\n+                                \"confidence\": actual_state_data.get(\"confidence\"),\n+                                \"source_node\": current_node\n+                            }\n                         }\n-                    }\n-                    logger.info(f\"\ud83c\udfaf Streamed attention overlay for session {session_id}\")\n+                        \n+                        # Track streamed overlay to prevent duplicates\n+                        streamed_overlays.add(overlay_hash)\n+                        setattr(self, overlay_hash_key, streamed_overlays)\n+                        \n+                        logger.info(f\"\ud83c\udfaf Streamed attention overlay from node '{current_node}' for session {session_id}\")\n+                    else:\n+                        logger.debug(f\"\ud83d\udd04 Skipped duplicate attention overlay from node '{current_node}' for session {session_id}\")\n                 \n                 # CRITICAL FIX: Only stream NEW assistant_response, not old accumulated data\n                 previous_node = actual_state_data.get(\"previous_node\", \"\")",
      "patch_lines": [
        "@@ -477,22 +477,36 @@ async def stream_process_message(self, session_id: str, user_message: str, user_\n",
        "                             \"data\": filtered_delta\n",
        "                         }\n",
        "                 \n",
        "-                # SPECIAL CASE: Stream attention overlay if classification completed\n",
        "-                current_node = actual_state_data.get(\"current_node\", \"\")\n",
        "-                if current_node == \"classifying\" and \"attention_overlay\" in actual_state_data and actual_state_data.get(\"attention_overlay\"):\n",
        "+                # GENERIC: Stream attention overlay if any node produces one\n",
        "+                if \"attention_overlay\" in actual_state_data and actual_state_data.get(\"attention_overlay\"):\n",
        "                     attention_overlay = actual_state_data[\"attention_overlay\"]\n",
        "+                    current_node = actual_state_data.get(\"current_node\", \"\")\n",
        "                     \n",
        "-                    # Stream attention overlay as separate event (not saved in state persistence)\n",
        "-                    yield {\n",
        "-                        \"type\": \"attention_overlay\",\n",
        "-                        \"session_id\": session_id,\n",
        "-                        \"data\": {\n",
        "-                            \"attention_overlay\": attention_overlay,\n",
        "-                            \"disease_name\": actual_state_data.get(\"disease_name\"),\n",
        "-                            \"confidence\": actual_state_data.get(\"confidence\")\n",
        "+                    # Prevent duplicate streaming using overlay content hash\n",
        "+                    overlay_hash = hash(attention_overlay[:100] + current_node)  # Hash first 100 chars + node\n",
        "+                    overlay_hash_key = f\"_overlay_hashes_{session_id}\"\n",
        "+                    streamed_overlays = getattr(self, overlay_hash_key, set())\n",
        "+                    \n",
        "+                    if overlay_hash not in streamed_overlays:\n",
        "+                        # Stream attention overlay as separate event (not saved in state persistence)\n",
        "+                        yield {\n",
        "+                            \"type\": \"attention_overlay\",\n",
        "+                            \"session_id\": session_id,\n",
        "+                            \"data\": {\n",
        "+                                \"attention_overlay\": attention_overlay,\n",
        "+                                \"disease_name\": actual_state_data.get(\"disease_name\"),\n",
        "+                                \"confidence\": actual_state_data.get(\"confidence\"),\n",
        "+                                \"source_node\": current_node\n",
        "+                            }\n",
        "                         }\n",
        "-                    }\n",
        "-                    logger.info(f\"\ud83c\udfaf Streamed attention overlay for session {session_id}\")\n",
        "+                        \n",
        "+                        # Track streamed overlay to prevent duplicates\n",
        "+                        streamed_overlays.add(overlay_hash)\n",
        "+                        setattr(self, overlay_hash_key, streamed_overlays)\n",
        "+                        \n",
        "+                        logger.info(f\"\ud83c\udfaf Streamed attention overlay from node '{current_node}' for session {session_id}\")\n",
        "+                    else:\n",
        "+                        logger.debug(f\"\ud83d\udd04 Skipped duplicate attention overlay from node '{current_node}' for session {session_id}\")\n",
        "                 \n",
        "                 # CRITICAL FIX: Only stream NEW assistant_response, not old accumulated data\n",
        "                 previous_node = actual_state_data.get(\"previous_node\", \"\")\n"
      ]
    },
    {
      "path": "engine/fsm_agent/core/langgraph_workflow_refactored.py",
      "status": "modified",
      "additions": 27,
      "deletions": 13,
      "patch": "@@ -442,22 +442,36 @@ async def stream_process_message(self, session_id: str, user_message: str, user_\n                     if isinstance(state_data, dict):\n                         actual_state_data.update(state_data)\n                 \n-                # SPECIAL CASE: Stream attention overlay if classification completed\n-                current_node = actual_state_data.get(\"current_node\", \"\")\n-                if current_node == \"classifying\" and \"attention_overlay\" in actual_state_data and actual_state_data.get(\"attention_overlay\"):\n+                # GENERIC: Stream attention overlay if any node produces one\n+                if \"attention_overlay\" in actual_state_data and actual_state_data.get(\"attention_overlay\"):\n                     attention_overlay = actual_state_data[\"attention_overlay\"]\n+                    current_node = actual_state_data.get(\"current_node\", \"\")\n                     \n-                    # Stream attention overlay as separate event (not saved in state persistence)\n-                    yield {\n-                        \"type\": \"attention_overlay\",\n-                        \"session_id\": session_id,\n-                        \"data\": {\n-                            \"attention_overlay\": attention_overlay,\n-                            \"disease_name\": actual_state_data.get(\"disease_name\"),\n-                            \"confidence\": actual_state_data.get(\"confidence\")\n+                    # Prevent duplicate streaming using overlay content hash\n+                    overlay_hash = hash(attention_overlay[:100] + current_node)  # Hash first 100 chars + node\n+                    overlay_hash_key = f\"_overlay_hashes_{session_id}\"\n+                    streamed_overlays = getattr(self, overlay_hash_key, set())\n+                    \n+                    if overlay_hash not in streamed_overlays:\n+                        # Stream attention overlay as separate event (not saved in state persistence)\n+                        yield {\n+                            \"type\": \"attention_overlay\",\n+                            \"session_id\": session_id,\n+                            \"data\": {\n+                                \"attention_overlay\": attention_overlay,\n+                                \"disease_name\": actual_state_data.get(\"disease_name\"),\n+                                \"confidence\": actual_state_data.get(\"confidence\"),\n+                                \"source_node\": current_node\n+                            }\n                         }\n-                    }\n-                    logger.info(f\"\ud83c\udfaf Streamed attention overlay for session {session_id}\")\n+                        \n+                        # Track streamed overlay to prevent duplicates\n+                        streamed_overlays.add(overlay_hash)\n+                        setattr(self, overlay_hash_key, streamed_overlays)\n+                        \n+                        logger.info(f\"\ud83c\udfaf Streamed attention overlay from node '{current_node}' for session {session_id}\")\n+                    else:\n+                        logger.debug(f\"\ud83d\udd04 Skipped duplicate attention overlay from node '{current_node}' for session {session_id}\")\n                 \n                 # Only track state transitions for logging purposes\n                 if current_node and current_node != last_node:",
      "patch_lines": [
        "@@ -442,22 +442,36 @@ async def stream_process_message(self, session_id: str, user_message: str, user_\n",
        "                     if isinstance(state_data, dict):\n",
        "                         actual_state_data.update(state_data)\n",
        "                 \n",
        "-                # SPECIAL CASE: Stream attention overlay if classification completed\n",
        "-                current_node = actual_state_data.get(\"current_node\", \"\")\n",
        "-                if current_node == \"classifying\" and \"attention_overlay\" in actual_state_data and actual_state_data.get(\"attention_overlay\"):\n",
        "+                # GENERIC: Stream attention overlay if any node produces one\n",
        "+                if \"attention_overlay\" in actual_state_data and actual_state_data.get(\"attention_overlay\"):\n",
        "                     attention_overlay = actual_state_data[\"attention_overlay\"]\n",
        "+                    current_node = actual_state_data.get(\"current_node\", \"\")\n",
        "                     \n",
        "-                    # Stream attention overlay as separate event (not saved in state persistence)\n",
        "-                    yield {\n",
        "-                        \"type\": \"attention_overlay\",\n",
        "-                        \"session_id\": session_id,\n",
        "-                        \"data\": {\n",
        "-                            \"attention_overlay\": attention_overlay,\n",
        "-                            \"disease_name\": actual_state_data.get(\"disease_name\"),\n",
        "-                            \"confidence\": actual_state_data.get(\"confidence\")\n",
        "+                    # Prevent duplicate streaming using overlay content hash\n",
        "+                    overlay_hash = hash(attention_overlay[:100] + current_node)  # Hash first 100 chars + node\n",
        "+                    overlay_hash_key = f\"_overlay_hashes_{session_id}\"\n",
        "+                    streamed_overlays = getattr(self, overlay_hash_key, set())\n",
        "+                    \n",
        "+                    if overlay_hash not in streamed_overlays:\n",
        "+                        # Stream attention overlay as separate event (not saved in state persistence)\n",
        "+                        yield {\n",
        "+                            \"type\": \"attention_overlay\",\n",
        "+                            \"session_id\": session_id,\n",
        "+                            \"data\": {\n",
        "+                                \"attention_overlay\": attention_overlay,\n",
        "+                                \"disease_name\": actual_state_data.get(\"disease_name\"),\n",
        "+                                \"confidence\": actual_state_data.get(\"confidence\"),\n",
        "+                                \"source_node\": current_node\n",
        "+                            }\n",
        "                         }\n",
        "-                    }\n",
        "-                    logger.info(f\"\ud83c\udfaf Streamed attention overlay for session {session_id}\")\n",
        "+                        \n",
        "+                        # Track streamed overlay to prevent duplicates\n",
        "+                        streamed_overlays.add(overlay_hash)\n",
        "+                        setattr(self, overlay_hash_key, streamed_overlays)\n",
        "+                        \n",
        "+                        logger.info(f\"\ud83c\udfaf Streamed attention overlay from node '{current_node}' for session {session_id}\")\n",
        "+                    else:\n",
        "+                        logger.debug(f\"\ud83d\udd04 Skipped duplicate attention overlay from node '{current_node}' for session {session_id}\")\n",
        "                 \n",
        "                 # Only track state transitions for logging purposes\n",
        "                 if current_node and current_node != last_node:\n"
      ]
    },
    {
      "path": "engine/fsm_agent/tools/classification_tool.py",
      "status": "modified",
      "additions": 28,
      "deletions": 44,
      "patch": "@@ -75,57 +75,41 @@ def _run(self, **kwargs) -> Dict[str, Any]:\n             if not self.classifier:\n                 return {\"error\": \"CNN classifier not available\"}\n             \n-            # Run classification (it's a generator that yields status messages)\n+            # Run classification using the new complete method\n             try:\n-                classification_generator = self.classifier.predict_leaf_classification(\n+                input_context = f\"Plant: {kwargs.get('plant_type', 'unknown')}, Location: {kwargs.get('location', 'unknown')}, Season: {kwargs.get('season', 'unknown')}\"\n+                \n+                # Use the new complete method that returns all results at once\n+                result = self.classifier.predict_leaf_classification_complete(\n                     image_bytes=kwargs[\"image_b64\"],\n-                    input_text=f\"Plant: {kwargs.get('plant_type', 'unknown')}, Location: {kwargs.get('location', 'unknown')}, Season: {kwargs.get('season', 'unknown')}\"\n+                    input_text=input_context\n                 )\n                 \n-                # Consume all yielded messages to get final result and extract attention overlay\n-                messages = []\n-                attention_overlay_b64 = None\n-                \n-                for message in classification_generator:\n-                    messages.append(message)\n-                    \n-                    # Check for attention overlay data\n-                    if \"ATTENTION_OVERLAY_BASE64:\" in message:\n-                        attention_overlay_b64 = message.split(\"ATTENTION_OVERLAY_BASE64:\")[1].strip()\n-                        logger.info(\"Attention overlay captured successfully\")\n+                # Check for errors from the classifier\n+                if result.get(\"error\"):\n+                    return {\"error\": result[\"error\"]}\n                 \n-                # Parse the final diagnosis message\n-                if messages:\n-                    final_message = messages[-1]  # Last message should contain diagnosis\n+                # Format the results for the workflow\n+                if result.get(\"success\"):\n+                    formatted_result = {\n+                        \"disease_name\": result.get(\"disease_name\"),\n+                        \"confidence\": result.get(\"confidence\"),\n+                        \"severity\": \"Unknown\",  # Not provided by current model\n+                        \"description\": f\"Detected {result.get('disease_name')} with {result.get('confidence', 0):.2%} confidence\",\n+                        \"attention_overlay\": result.get(\"attention_overlay\"),  # Direct from classifier\n+                        \"raw_class_label\": result.get(\"raw_class_label\"),\n+                        \"plant_context\": {\n+                            \"plant_type\": kwargs.get(\"plant_type\"),\n+                            \"location\": kwargs.get(\"location\"),\n+                            \"season\": kwargs.get(\"season\"),\n+                            \"growth_stage\": kwargs.get(\"growth_stage\")\n+                        }\n+                    }\n                     \n-                    # Extract disease name and confidence from final message\n-                    # Format: \"Diagnosis Complete! Health Status: {disease} with confidence {confidence}\"\n-                    if \"Health Status:\" in final_message and \"confidence\" in final_message:\n-                        import re\n-                        match = re.search(r'Health Status: (.+?) with confidence ([0-9.]+)', final_message)\n-                        if match:\n-                            disease_name = match.group(1).strip()\n-                            confidence = float(match.group(2))\n-                            \n-                            formatted_result = {\n-                                \"disease_name\": disease_name,\n-                                \"confidence\": confidence,\n-                                \"severity\": \"Unknown\",  # Not provided by current model\n-                                \"description\": f\"Detected {disease_name} with {confidence:.2%} confidence\",\n-                                \"attention_overlay\": attention_overlay_b64,  # Include captured attention overlay\n-                                \"raw_predictions\": messages,  # Include all status messages\n-                                \"plant_context\": {\n-                                    \"plant_type\": kwargs.get(\"plant_type\"),\n-                                    \"location\": kwargs.get(\"location\"),\n-                                    \"season\": kwargs.get(\"season\"),\n-                                    \"growth_stage\": kwargs.get(\"growth_stage\")\n-                                }\n-                            }\n-                            \n-                            logger.info(f\"Classification successful: {disease_name} ({confidence:.2f}) with attention overlay\")\n-                            return formatted_result\n+                    logger.info(f\"Classification successful: {result.get('disease_name')} ({result.get('confidence', 0):.2f}) with attention overlay\")\n+                    return formatted_result\n                 \n-                return {\"error\": \"Classification failed - could not parse diagnosis result\"}\n+                return {\"error\": \"Classification failed - unexpected result format\"}\n                 \n             except Exception as e:\n                 return {\"error\": f\"Classification failed: {str(e)}\"}",
      "patch_lines": [
        "@@ -75,57 +75,41 @@ def _run(self, **kwargs) -> Dict[str, Any]:\n",
        "             if not self.classifier:\n",
        "                 return {\"error\": \"CNN classifier not available\"}\n",
        "             \n",
        "-            # Run classification (it's a generator that yields status messages)\n",
        "+            # Run classification using the new complete method\n",
        "             try:\n",
        "-                classification_generator = self.classifier.predict_leaf_classification(\n",
        "+                input_context = f\"Plant: {kwargs.get('plant_type', 'unknown')}, Location: {kwargs.get('location', 'unknown')}, Season: {kwargs.get('season', 'unknown')}\"\n",
        "+                \n",
        "+                # Use the new complete method that returns all results at once\n",
        "+                result = self.classifier.predict_leaf_classification_complete(\n",
        "                     image_bytes=kwargs[\"image_b64\"],\n",
        "-                    input_text=f\"Plant: {kwargs.get('plant_type', 'unknown')}, Location: {kwargs.get('location', 'unknown')}, Season: {kwargs.get('season', 'unknown')}\"\n",
        "+                    input_text=input_context\n",
        "                 )\n",
        "                 \n",
        "-                # Consume all yielded messages to get final result and extract attention overlay\n",
        "-                messages = []\n",
        "-                attention_overlay_b64 = None\n",
        "-                \n",
        "-                for message in classification_generator:\n",
        "-                    messages.append(message)\n",
        "-                    \n",
        "-                    # Check for attention overlay data\n",
        "-                    if \"ATTENTION_OVERLAY_BASE64:\" in message:\n",
        "-                        attention_overlay_b64 = message.split(\"ATTENTION_OVERLAY_BASE64:\")[1].strip()\n",
        "-                        logger.info(\"Attention overlay captured successfully\")\n",
        "+                # Check for errors from the classifier\n",
        "+                if result.get(\"error\"):\n",
        "+                    return {\"error\": result[\"error\"]}\n",
        "                 \n",
        "-                # Parse the final diagnosis message\n",
        "-                if messages:\n",
        "-                    final_message = messages[-1]  # Last message should contain diagnosis\n",
        "+                # Format the results for the workflow\n",
        "+                if result.get(\"success\"):\n",
        "+                    formatted_result = {\n",
        "+                        \"disease_name\": result.get(\"disease_name\"),\n",
        "+                        \"confidence\": result.get(\"confidence\"),\n",
        "+                        \"severity\": \"Unknown\",  # Not provided by current model\n",
        "+                        \"description\": f\"Detected {result.get('disease_name')} with {result.get('confidence', 0):.2%} confidence\",\n",
        "+                        \"attention_overlay\": result.get(\"attention_overlay\"),  # Direct from classifier\n",
        "+                        \"raw_class_label\": result.get(\"raw_class_label\"),\n",
        "+                        \"plant_context\": {\n",
        "+                            \"plant_type\": kwargs.get(\"plant_type\"),\n",
        "+                            \"location\": kwargs.get(\"location\"),\n",
        "+                            \"season\": kwargs.get(\"season\"),\n",
        "+                            \"growth_stage\": kwargs.get(\"growth_stage\")\n",
        "+                        }\n",
        "+                    }\n",
        "                     \n",
        "-                    # Extract disease name and confidence from final message\n",
        "-                    # Format: \"Diagnosis Complete! Health Status: {disease} with confidence {confidence}\"\n",
        "-                    if \"Health Status:\" in final_message and \"confidence\" in final_message:\n",
        "-                        import re\n",
        "-                        match = re.search(r'Health Status: (.+?) with confidence ([0-9.]+)', final_message)\n",
        "-                        if match:\n",
        "-                            disease_name = match.group(1).strip()\n",
        "-                            confidence = float(match.group(2))\n",
        "-                            \n",
        "-                            formatted_result = {\n",
        "-                                \"disease_name\": disease_name,\n",
        "-                                \"confidence\": confidence,\n",
        "-                                \"severity\": \"Unknown\",  # Not provided by current model\n",
        "-                                \"description\": f\"Detected {disease_name} with {confidence:.2%} confidence\",\n",
        "-                                \"attention_overlay\": attention_overlay_b64,  # Include captured attention overlay\n",
        "-                                \"raw_predictions\": messages,  # Include all status messages\n",
        "-                                \"plant_context\": {\n",
        "-                                    \"plant_type\": kwargs.get(\"plant_type\"),\n",
        "-                                    \"location\": kwargs.get(\"location\"),\n",
        "-                                    \"season\": kwargs.get(\"season\"),\n",
        "-                                    \"growth_stage\": kwargs.get(\"growth_stage\")\n",
        "-                                }\n",
        "-                            }\n",
        "-                            \n",
        "-                            logger.info(f\"Classification successful: {disease_name} ({confidence:.2f}) with attention overlay\")\n",
        "-                            return formatted_result\n",
        "+                    logger.info(f\"Classification successful: {result.get('disease_name')} ({result.get('confidence', 0):.2f}) with attention overlay\")\n",
        "+                    return formatted_result\n",
        "                 \n",
        "-                return {\"error\": \"Classification failed - could not parse diagnosis result\"}\n",
        "+                return {\"error\": \"Classification failed - unexpected result format\"}\n",
        "                 \n",
        "             except Exception as e:\n",
        "                 return {\"error\": f\"Classification failed: {str(e)}\"}\n"
      ]
    },
    {
      "path": "engine/ml/cnn_attn_classifier_improved.py",
      "status": "modified",
      "additions": 172,
      "deletions": 39,
      "patch": "@@ -209,23 +209,22 @@ def visualize_self_attention_overlay(self, image, target_size=(64, 64)):\n         except Exception as e:\n             yield f\"Error generating attention visualization: {str(e)}\\n\"\n \n-    def predict_leaf_classification(self, image_bytes, input_text=\"\"):\n+    def predict_leaf_classification_complete(self, image_bytes, input_text=\"\"):\n         \"\"\"\n-        Predicts plant disease with attention visualization.\n+        Predicts plant disease with attention visualization - returns complete results.\n \n         Args:\n             image_bytes (str): Base64-encoded image bytes.\n             input_text (str): Optional additional text.\n-        Yields:\n-            str: Intermediate output strings including attention visualization.\n+        \n+        Returns:\n+            dict: Complete classification results including attention overlay\n         \"\"\"\n         if self.loaded_model is None:\n-            yield \"Error: Model is not loaded.\\n\"\n-            return\n+            return {\"error\": \"Model is not loaded\"}\n \n         if image_bytes is None:\n-            yield \"Error: Mandatory argument 'image_bytes' is missing.\\n\"\n-            return\n+            return {\"error\": \"Mandatory argument 'image_bytes' is missing\"}\n \n         try:\n             # Clean the base64 string - remove whitespace and potential prefixes\n@@ -244,46 +243,180 @@ def predict_leaf_classification(self, image_bytes, input_text=\"\"):\n             image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n \n         except Exception as e:\n-            yield f\"Error: Could not decode base64 image ({str(e)})\\n\"\n-            yield f\"Debug: Base64 string length: {len(image_bytes) if image_bytes else 0}\\n\"\n-            yield f\"Debug: First 100 chars: {image_bytes[:100] if image_bytes else 'None'}\\n\"\n-            return\n+            return {\n+                \"error\": f\"Could not decode base64 image ({str(e)})\",\n+                \"debug_info\": {\n+                    \"base64_length\": len(image_bytes) if image_bytes else 0,\n+                    \"first_100_chars\": image_bytes[:100] if image_bytes else 'None'\n+                }\n+            }\n         \n         if image is None:\n-            yield \"Error: Could not load image from bytes.\\n\"\n-            return\n-\n-        # Image preprocessing\n-        image_resized = cv2.resize(image, TARGET_IMG_SIZE)\n-        yield f\"Resized image, normalizing and preprocessing...\\n\"\n-        time.sleep(1.0)\n+            return {\"error\": \"Could not load image from bytes\"}\n \n-        yield f\"Preparing image for neural network analysis...\\n\"\n-        time.sleep(0.8)\n-\n-        image_preprocessed = image_resized.astype(np.float32) / 255.0\n-        image_for_prediction = np.expand_dims(image_preprocessed, axis=0)\n+        try:\n+            # Image preprocessing\n+            image_resized = cv2.resize(image, TARGET_IMG_SIZE)\n+            image_preprocessed = image_resized.astype(np.float32) / 255.0\n+            image_for_prediction = np.expand_dims(image_preprocessed, axis=0)\n \n-        yield f\"Running CNN model inference...\\n\"\n-        time.sleep(0.8)\n+            # Run CNN model inference\n+            prediction = self.loaded_model.predict(image_for_prediction)\n \n-        prediction = self.loaded_model.predict(image_for_prediction)\n+            # Get prediction results\n+            predicted_class_index = np.argmax(prediction)\n+            predicted_class_label = MODEL_LABEL_CLASSES[predicted_class_index]\n+            prediction_probability = prediction[0][predicted_class_index]\n \n-        yield f\"Analyzing prediction results...\\n\"\n-        time.sleep(0.8)\n+            # Transform to Kisan CC label\n+            kissan_cc_class_label = LABEL_MAPPINGS.get(predicted_class_label, predicted_class_label)\n+            \n+            # Generate attention visualization (synchronously)\n+            attention_overlay_b64 = self._generate_attention_overlay_sync(image, TARGET_IMG_SIZE)\n+            \n+            # Return complete results\n+            return {\n+                \"success\": True,\n+                \"disease_name\": kissan_cc_class_label,\n+                \"confidence\": float(prediction_probability),\n+                \"raw_class_label\": predicted_class_label,\n+                \"attention_overlay\": attention_overlay_b64,\n+                \"input_context\": input_text,\n+                \"processing_status\": \"complete\"\n+            }\n+            \n+        except Exception as e:\n+            return {\"error\": f\"Classification failed: {str(e)}\"}\n \n-        predicted_class_index = np.argmax(prediction)\n-        predicted_class_label = MODEL_LABEL_CLASSES[predicted_class_index]\n-        prediction_probability = prediction[0][predicted_class_index]\n+    def _generate_attention_overlay_sync(self, image, target_size=(64, 64)):\n+        \"\"\"\n+        Generate attention overlay synchronously and return as base64 string.\n+        \n+        Args:\n+            image (np.ndarray): Input image array\n+            target_size (tuple): Target size for processing\n+            \n+        Returns:\n+            str: Base64 encoded attention overlay image or None if failed\n+        \"\"\"\n+        try:\n+            # Prepare image for attention model\n+            processed_image = cv2.resize(image, target_size)\n+            processed_image = processed_image.astype(np.float32) / 255.0\n+            processed_image = np.expand_dims(processed_image, axis=0)\n+            \n+            # Get predictions and attention weights\n+            if self.attention_model:\n+                predictions = self.attention_model.predict(processed_image)\n+                \n+                if isinstance(predictions, list) and len(predictions) > 1:\n+                    attention_weights = predictions[1]\n+                    \n+                    # Process attention weights to create heatmap\n+                    sequence_length = attention_weights.shape[1]\n+                    original_spatial_size = int(np.sqrt(sequence_length))\n+                    \n+                    if original_spatial_size * original_spatial_size == sequence_length:\n+                        # Reshape attention weights to spatial dimensions\n+                        batch_size = attention_weights.shape[0]\n+                        features = attention_weights.shape[2]\n+                        spatial_attention = np.reshape(\n+                            attention_weights, \n+                            (batch_size, original_spatial_size, original_spatial_size, features)\n+                        )\n+                        \n+                        # Aggregate attention across features\n+                        attention_map = np.mean(spatial_attention, axis=-1)\n+                        attention_map = np.squeeze(attention_map, axis=0)\n+                        \n+                        # Resize to original image dimensions\n+                        original_image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n+                        resized_attention = cv2.resize(\n+                            attention_map, \n+                            (original_image_rgb.shape[1], original_image_rgb.shape[0]),\n+                            interpolation=cv2.INTER_CUBIC\n+                        )\n+                        \n+                        # Normalize attention map\n+                        min_att = np.min(resized_attention)\n+                        max_att = np.max(resized_attention)\n+                        if max_att - min_att > 1e-6:\n+                            normalized_attention = (resized_attention - min_att) / (max_att - min_att)\n+                        else:\n+                            normalized_attention = np.zeros_like(resized_attention)\n+                        \n+                        # Create heatmap overlay\n+                        heatmap = cv2.applyColorMap(\n+                            np.uint8(255 * normalized_attention), \n+                            cv2.COLORMAP_VIRIDIS\n+                        )\n+                        heatmap_rgb = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n+                        \n+                        # Blend with original image\n+                        original_float = original_image_rgb.astype(np.float32) / 255.0\n+                        heatmap_float = heatmap_rgb.astype(np.float32) / 255.0\n+                        alpha = np.expand_dims(normalized_attention, axis=-1)\n+                        alpha = np.repeat(alpha, 3, axis=-1)\n+                        \n+                        overlay = (1 - alpha) * original_float + alpha * heatmap_float\n+                        overlay = np.clip(overlay, 0, 1)\n+                        \n+                        # Convert to image and encode as base64\n+                        plt.figure(figsize=(10, 5))\n+                        \n+                        plt.subplot(1, 2, 1)\n+                        plt.imshow(original_image_rgb)\n+                        plt.title(\"Original Image\")\n+                        plt.axis('off')\n+                        \n+                        plt.subplot(1, 2, 2)\n+                        plt.imshow(overlay)\n+                        plt.title(\"Attention Overlay\")\n+                        plt.axis('off')\n+                        \n+                        plt.tight_layout()\n+                        \n+                        # Save to bytes buffer\n+                        buffer = io.BytesIO()\n+                        plt.savefig(buffer, format='png', bbox_inches='tight', dpi=100)\n+                        plt.close()\n+                        \n+                        buffer.seek(0)\n+                        image_base64 = base64.b64encode(buffer.read()).decode('utf-8')\n+                        \n+                        return image_base64\n+                        \n+            return None\n+                \n+        except Exception as e:\n+            print(f\"Error generating attention overlay: {str(e)}\")\n+            return None\n \n-        # Transform to Kisan CC label\n-        kissan_cc_class_label = LABEL_MAPPINGS.get(predicted_class_label, predicted_class_label)\n+    def predict_leaf_classification(self, image_bytes, input_text=\"\"):\n+        \"\"\"\n+        Legacy method - kept for backward compatibility.\n+        Now uses the new complete method and yields the results.\n+        \"\"\"\n+        result = self.predict_leaf_classification_complete(image_bytes, input_text)\n         \n-        yield f\"Finalizing diagnosis...\\n\"\n-        time.sleep(1.0)\n+        if result.get(\"error\"):\n+            yield f\"Error: {result['error']}\\n\"\n+            return\n+            \n+        # Yield status messages for streaming compatibility\n+        yield \"Resized image, normalizing and preprocessing...\\n\"\n+        yield \"Preparing image for neural network analysis...\\n\"\n+        yield \"Running CNN model inference...\\n\"\n+        yield \"Analyzing prediction results...\\n\"\n+        yield \"Finalizing diagnosis...\\n\"\n         \n-        # Generate attention visualization\n-        yield from self.visualize_self_attention_overlay(image, TARGET_IMG_SIZE)\n+        # Generate attention visualization messages\n+        if result.get(\"attention_overlay\"):\n+            yield \"Generating attention visualization...\\n\"\n+            yield \"Processing attention weights...\\n\"\n+            yield \"Creating attention heatmap overlay...\\n\"\n+            yield \"Attention visualization completed! Generated overlay showing model focus areas.\\n\"\n+            yield f\"ATTENTION_OVERLAY_BASE64:{result['attention_overlay']}\\n\"\n         \n-        yield f\"Diagnosis Complete! Health Status: {kissan_cc_class_label} with confidence {prediction_probability:.2f}\\n\"\n+        yield f\"Diagnosis Complete! Health Status: {result['disease_name']} with confidence {result['confidence']:.2f}\\n\"\n         return\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -209,23 +209,22 @@ def visualize_self_attention_overlay(self, image, target_size=(64, 64)):\n",
        "         except Exception as e:\n",
        "             yield f\"Error generating attention visualization: {str(e)}\\n\"\n",
        " \n",
        "-    def predict_leaf_classification(self, image_bytes, input_text=\"\"):\n",
        "+    def predict_leaf_classification_complete(self, image_bytes, input_text=\"\"):\n",
        "         \"\"\"\n",
        "-        Predicts plant disease with attention visualization.\n",
        "+        Predicts plant disease with attention visualization - returns complete results.\n",
        " \n",
        "         Args:\n",
        "             image_bytes (str): Base64-encoded image bytes.\n",
        "             input_text (str): Optional additional text.\n",
        "-        Yields:\n",
        "-            str: Intermediate output strings including attention visualization.\n",
        "+        \n",
        "+        Returns:\n",
        "+            dict: Complete classification results including attention overlay\n",
        "         \"\"\"\n",
        "         if self.loaded_model is None:\n",
        "-            yield \"Error: Model is not loaded.\\n\"\n",
        "-            return\n",
        "+            return {\"error\": \"Model is not loaded\"}\n",
        " \n",
        "         if image_bytes is None:\n",
        "-            yield \"Error: Mandatory argument 'image_bytes' is missing.\\n\"\n",
        "-            return\n",
        "+            return {\"error\": \"Mandatory argument 'image_bytes' is missing\"}\n",
        " \n",
        "         try:\n",
        "             # Clean the base64 string - remove whitespace and potential prefixes\n",
        "@@ -244,46 +243,180 @@ def predict_leaf_classification(self, image_bytes, input_text=\"\"):\n",
        "             image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        " \n",
        "         except Exception as e:\n",
        "-            yield f\"Error: Could not decode base64 image ({str(e)})\\n\"\n",
        "-            yield f\"Debug: Base64 string length: {len(image_bytes) if image_bytes else 0}\\n\"\n",
        "-            yield f\"Debug: First 100 chars: {image_bytes[:100] if image_bytes else 'None'}\\n\"\n",
        "-            return\n",
        "+            return {\n",
        "+                \"error\": f\"Could not decode base64 image ({str(e)})\",\n",
        "+                \"debug_info\": {\n",
        "+                    \"base64_length\": len(image_bytes) if image_bytes else 0,\n",
        "+                    \"first_100_chars\": image_bytes[:100] if image_bytes else 'None'\n",
        "+                }\n",
        "+            }\n",
        "         \n",
        "         if image is None:\n",
        "-            yield \"Error: Could not load image from bytes.\\n\"\n",
        "-            return\n",
        "-\n",
        "-        # Image preprocessing\n",
        "-        image_resized = cv2.resize(image, TARGET_IMG_SIZE)\n",
        "-        yield f\"Resized image, normalizing and preprocessing...\\n\"\n",
        "-        time.sleep(1.0)\n",
        "+            return {\"error\": \"Could not load image from bytes\"}\n",
        " \n",
        "-        yield f\"Preparing image for neural network analysis...\\n\"\n",
        "-        time.sleep(0.8)\n",
        "-\n",
        "-        image_preprocessed = image_resized.astype(np.float32) / 255.0\n",
        "-        image_for_prediction = np.expand_dims(image_preprocessed, axis=0)\n",
        "+        try:\n",
        "+            # Image preprocessing\n",
        "+            image_resized = cv2.resize(image, TARGET_IMG_SIZE)\n",
        "+            image_preprocessed = image_resized.astype(np.float32) / 255.0\n",
        "+            image_for_prediction = np.expand_dims(image_preprocessed, axis=0)\n",
        " \n",
        "-        yield f\"Running CNN model inference...\\n\"\n",
        "-        time.sleep(0.8)\n",
        "+            # Run CNN model inference\n",
        "+            prediction = self.loaded_model.predict(image_for_prediction)\n",
        " \n",
        "-        prediction = self.loaded_model.predict(image_for_prediction)\n",
        "+            # Get prediction results\n",
        "+            predicted_class_index = np.argmax(prediction)\n",
        "+            predicted_class_label = MODEL_LABEL_CLASSES[predicted_class_index]\n",
        "+            prediction_probability = prediction[0][predicted_class_index]\n",
        " \n",
        "-        yield f\"Analyzing prediction results...\\n\"\n",
        "-        time.sleep(0.8)\n",
        "+            # Transform to Kisan CC label\n",
        "+            kissan_cc_class_label = LABEL_MAPPINGS.get(predicted_class_label, predicted_class_label)\n",
        "+            \n",
        "+            # Generate attention visualization (synchronously)\n",
        "+            attention_overlay_b64 = self._generate_attention_overlay_sync(image, TARGET_IMG_SIZE)\n",
        "+            \n",
        "+            # Return complete results\n",
        "+            return {\n",
        "+                \"success\": True,\n",
        "+                \"disease_name\": kissan_cc_class_label,\n",
        "+                \"confidence\": float(prediction_probability),\n",
        "+                \"raw_class_label\": predicted_class_label,\n",
        "+                \"attention_overlay\": attention_overlay_b64,\n",
        "+                \"input_context\": input_text,\n",
        "+                \"processing_status\": \"complete\"\n",
        "+            }\n",
        "+            \n",
        "+        except Exception as e:\n",
        "+            return {\"error\": f\"Classification failed: {str(e)}\"}\n",
        " \n",
        "-        predicted_class_index = np.argmax(prediction)\n",
        "-        predicted_class_label = MODEL_LABEL_CLASSES[predicted_class_index]\n",
        "-        prediction_probability = prediction[0][predicted_class_index]\n",
        "+    def _generate_attention_overlay_sync(self, image, target_size=(64, 64)):\n",
        "+        \"\"\"\n",
        "+        Generate attention overlay synchronously and return as base64 string.\n",
        "+        \n",
        "+        Args:\n",
        "+            image (np.ndarray): Input image array\n",
        "+            target_size (tuple): Target size for processing\n",
        "+            \n",
        "+        Returns:\n",
        "+            str: Base64 encoded attention overlay image or None if failed\n",
        "+        \"\"\"\n",
        "+        try:\n",
        "+            # Prepare image for attention model\n",
        "+            processed_image = cv2.resize(image, target_size)\n",
        "+            processed_image = processed_image.astype(np.float32) / 255.0\n",
        "+            processed_image = np.expand_dims(processed_image, axis=0)\n",
        "+            \n",
        "+            # Get predictions and attention weights\n",
        "+            if self.attention_model:\n",
        "+                predictions = self.attention_model.predict(processed_image)\n",
        "+                \n",
        "+                if isinstance(predictions, list) and len(predictions) > 1:\n",
        "+                    attention_weights = predictions[1]\n",
        "+                    \n",
        "+                    # Process attention weights to create heatmap\n",
        "+                    sequence_length = attention_weights.shape[1]\n",
        "+                    original_spatial_size = int(np.sqrt(sequence_length))\n",
        "+                    \n",
        "+                    if original_spatial_size * original_spatial_size == sequence_length:\n",
        "+                        # Reshape attention weights to spatial dimensions\n",
        "+                        batch_size = attention_weights.shape[0]\n",
        "+                        features = attention_weights.shape[2]\n",
        "+                        spatial_attention = np.reshape(\n",
        "+                            attention_weights, \n",
        "+                            (batch_size, original_spatial_size, original_spatial_size, features)\n",
        "+                        )\n",
        "+                        \n",
        "+                        # Aggregate attention across features\n",
        "+                        attention_map = np.mean(spatial_attention, axis=-1)\n",
        "+                        attention_map = np.squeeze(attention_map, axis=0)\n",
        "+                        \n",
        "+                        # Resize to original image dimensions\n",
        "+                        original_image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "+                        resized_attention = cv2.resize(\n",
        "+                            attention_map, \n",
        "+                            (original_image_rgb.shape[1], original_image_rgb.shape[0]),\n",
        "+                            interpolation=cv2.INTER_CUBIC\n",
        "+                        )\n",
        "+                        \n",
        "+                        # Normalize attention map\n",
        "+                        min_att = np.min(resized_attention)\n",
        "+                        max_att = np.max(resized_attention)\n",
        "+                        if max_att - min_att > 1e-6:\n",
        "+                            normalized_attention = (resized_attention - min_att) / (max_att - min_att)\n",
        "+                        else:\n",
        "+                            normalized_attention = np.zeros_like(resized_attention)\n",
        "+                        \n",
        "+                        # Create heatmap overlay\n",
        "+                        heatmap = cv2.applyColorMap(\n",
        "+                            np.uint8(255 * normalized_attention), \n",
        "+                            cv2.COLORMAP_VIRIDIS\n",
        "+                        )\n",
        "+                        heatmap_rgb = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "+                        \n",
        "+                        # Blend with original image\n",
        "+                        original_float = original_image_rgb.astype(np.float32) / 255.0\n",
        "+                        heatmap_float = heatmap_rgb.astype(np.float32) / 255.0\n",
        "+                        alpha = np.expand_dims(normalized_attention, axis=-1)\n",
        "+                        alpha = np.repeat(alpha, 3, axis=-1)\n",
        "+                        \n",
        "+                        overlay = (1 - alpha) * original_float + alpha * heatmap_float\n",
        "+                        overlay = np.clip(overlay, 0, 1)\n",
        "+                        \n",
        "+                        # Convert to image and encode as base64\n",
        "+                        plt.figure(figsize=(10, 5))\n",
        "+                        \n",
        "+                        plt.subplot(1, 2, 1)\n",
        "+                        plt.imshow(original_image_rgb)\n",
        "+                        plt.title(\"Original Image\")\n",
        "+                        plt.axis('off')\n",
        "+                        \n",
        "+                        plt.subplot(1, 2, 2)\n",
        "+                        plt.imshow(overlay)\n",
        "+                        plt.title(\"Attention Overlay\")\n",
        "+                        plt.axis('off')\n",
        "+                        \n",
        "+                        plt.tight_layout()\n",
        "+                        \n",
        "+                        # Save to bytes buffer\n",
        "+                        buffer = io.BytesIO()\n",
        "+                        plt.savefig(buffer, format='png', bbox_inches='tight', dpi=100)\n",
        "+                        plt.close()\n",
        "+                        \n",
        "+                        buffer.seek(0)\n",
        "+                        image_base64 = base64.b64encode(buffer.read()).decode('utf-8')\n",
        "+                        \n",
        "+                        return image_base64\n",
        "+                        \n",
        "+            return None\n",
        "+                \n",
        "+        except Exception as e:\n",
        "+            print(f\"Error generating attention overlay: {str(e)}\")\n",
        "+            return None\n",
        " \n",
        "-        # Transform to Kisan CC label\n",
        "-        kissan_cc_class_label = LABEL_MAPPINGS.get(predicted_class_label, predicted_class_label)\n",
        "+    def predict_leaf_classification(self, image_bytes, input_text=\"\"):\n",
        "+        \"\"\"\n",
        "+        Legacy method - kept for backward compatibility.\n",
        "+        Now uses the new complete method and yields the results.\n",
        "+        \"\"\"\n",
        "+        result = self.predict_leaf_classification_complete(image_bytes, input_text)\n",
        "         \n",
        "-        yield f\"Finalizing diagnosis...\\n\"\n",
        "-        time.sleep(1.0)\n",
        "+        if result.get(\"error\"):\n",
        "+            yield f\"Error: {result['error']}\\n\"\n",
        "+            return\n",
        "+            \n",
        "+        # Yield status messages for streaming compatibility\n",
        "+        yield \"Resized image, normalizing and preprocessing...\\n\"\n",
        "+        yield \"Preparing image for neural network analysis...\\n\"\n",
        "+        yield \"Running CNN model inference...\\n\"\n",
        "+        yield \"Analyzing prediction results...\\n\"\n",
        "+        yield \"Finalizing diagnosis...\\n\"\n",
        "         \n",
        "-        # Generate attention visualization\n",
        "-        yield from self.visualize_self_attention_overlay(image, TARGET_IMG_SIZE)\n",
        "+        # Generate attention visualization messages\n",
        "+        if result.get(\"attention_overlay\"):\n",
        "+            yield \"Generating attention visualization...\\n\"\n",
        "+            yield \"Processing attention weights...\\n\"\n",
        "+            yield \"Creating attention heatmap overlay...\\n\"\n",
        "+            yield \"Attention visualization completed! Generated overlay showing model focus areas.\\n\"\n",
        "+            yield f\"ATTENTION_OVERLAY_BASE64:{result['attention_overlay']}\\n\"\n",
        "         \n",
        "-        yield f\"Diagnosis Complete! Health Status: {kissan_cc_class_label} with confidence {prediction_probability:.2f}\\n\"\n",
        "+        yield f\"Diagnosis Complete! Health Status: {result['disease_name']} with confidence {result['confidence']:.2f}\\n\"\n",
        "         return\n",
        "\\ No newline at end of file\n"
      ]
    }
  ]
}
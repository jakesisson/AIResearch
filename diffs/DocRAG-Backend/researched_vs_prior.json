{
  "project": "Research Data/DocRAG-Backend",
  "repo": "Rajathbharadwaj/DocRAG-Backend",
  "prior_commit": "8c0d68755d31ad633fc8c026bc8e866c836149ad",
  "researched_commit": "cbe1f370adbd46b191523631613b6afd9ff213bb",
  "compare_url": "https://github.com/Rajathbharadwaj/DocRAG-Backend/compare/8c0d68755d31ad633fc8c026bc8e866c836149ad...cbe1f370adbd46b191523631613b6afd9ff213bb",
  "ahead_by": 1,
  "behind_by": 0,
  "changed_files": [
    {
      "path": "langgraph.json",
      "status": "modified",
      "additions": 6,
      "deletions": 3,
      "patch": "@@ -1,8 +1,8 @@\n {\n   \"dockerfile_lines\": [],\n   \"graphs\": {\n-    \"chat\": \"./src/rag/retrieval_agent/graph.py:graph\",\n-    \"research\": \"./src/rag/research_agent/graph.py:graph\"\n+    \"chat\": \"./src/rag/chat_agent/backend/retrieval_graph/graph.py:graph\",\n+    \"research\": \"./src/rag/chat_agent/backend/retrieval_graph/researcher_graph/graph.py:graph\"\n   },\n   \"env\": \".env\",\n   \"python_version\": \"3.12\",\n@@ -18,6 +18,9 @@\n     \"openai\",\n     \"langchain\",\n     \"langchain-community\",\n-    \"langchain-openai\"\n+    \"langchain-openai\",\n+    \"langchain-pinecone\",\n+    \"langchain-experimental\"\n+\n   ]\n }\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -1,8 +1,8 @@\n",
        " {\n",
        "   \"dockerfile_lines\": [],\n",
        "   \"graphs\": {\n",
        "-    \"chat\": \"./src/rag/retrieval_agent/graph.py:graph\",\n",
        "-    \"research\": \"./src/rag/research_agent/graph.py:graph\"\n",
        "+    \"chat\": \"./src/rag/chat_agent/backend/retrieval_graph/graph.py:graph\",\n",
        "+    \"research\": \"./src/rag/chat_agent/backend/retrieval_graph/researcher_graph/graph.py:graph\"\n",
        "   },\n",
        "   \"env\": \".env\",\n",
        "   \"python_version\": \"3.12\",\n",
        "@@ -18,6 +18,9 @@\n",
        "     \"openai\",\n",
        "     \"langchain\",\n",
        "     \"langchain-community\",\n",
        "-    \"langchain-openai\"\n",
        "+    \"langchain-openai\",\n",
        "+    \"langchain-pinecone\",\n",
        "+    \"langchain-experimental\"\n",
        "+\n",
        "   ]\n",
        " }\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "requirements.txt",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "patch": "@@ -6,4 +6,6 @@ openai\n langchain\n langchain-community\n langchain-openai\n-protoc-gen-openapiv2\n\\ No newline at end of file\n+protoc-gen-openapiv2\n+langchain-experimental\n+langchain-pinecone\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -6,4 +6,6 @@ openai\n",
        " langchain\n",
        " langchain-community\n",
        " langchain-openai\n",
        "-protoc-gen-openapiv2\n",
        "\\ No newline at end of file\n",
        "+protoc-gen-openapiv2\n",
        "+langchain-experimental\n",
        "+langchain-pinecone\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "src/rag/__init__.py",
      "status": "modified",
      "additions": 6,
      "deletions": 4,
      "patch": "@@ -1,5 +1,7 @@\n-from .retriever import retrieve_documents\n+\"\"\"RAG package initialization.\"\"\"\n+from . import utils\n+from . import chat_agent\n+from . import vectorstore_engine\n+from . import retriever\n \n-__all__ = [\n-    \"retrieve_documents\"\n-]\n\\ No newline at end of file\n+__all__ = [\"utils\", \"chat_agent\", \"vectorstore_engine\", \"retriever\"]\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -1,5 +1,7 @@\n",
        "-from .retriever import retrieve_documents\n",
        "+\"\"\"RAG package initialization.\"\"\"\n",
        "+from . import utils\n",
        "+from . import chat_agent\n",
        "+from . import vectorstore_engine\n",
        "+from . import retriever\n",
        " \n",
        "-__all__ = [\n",
        "-    \"retrieve_documents\"\n",
        "-]\n",
        "\\ No newline at end of file\n",
        "+__all__ = [\"utils\", \"chat_agent\", \"vectorstore_engine\", \"retriever\"]\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "src/rag/chat_agent",
      "status": "added",
      "additions": 1,
      "deletions": 0,
      "patch": "@@ -0,0 +1 @@\n+Subproject commit c02935fec860d38939b699b0e461863ec2a68c4b",
      "patch_lines": [
        "@@ -0,0 +1 @@\n",
        "+Subproject commit c02935fec860d38939b699b0e461863ec2a68c4b\n"
      ]
    },
    {
      "path": "src/rag/configuration.py",
      "status": "modified",
      "additions": 9,
      "deletions": 3,
      "patch": "@@ -13,7 +13,7 @@\n }\n \n @dataclass(kw_only=True)\n-class AgentConfiguration:\n+class BaseConfiguration:\n     \"\"\"Configuration for the research agent.\n     \n     This class defines parameters for query generation, document retrieval,\n@@ -81,7 +81,13 @@ class AgentConfiguration:\n             \"description\": \"Name of the Pinecone index to use for retrieval\"\n         },\n     )\n-    \n+\n+    region: str = field(\n+        default=\"us-east-1\",\n+        metadata={\n+            \"description\": \"AWS region for Pinecone\"\n+        },\n+    )\n \n     # Prompts\n     generate_queries_system_prompt: str = field(\n@@ -114,4 +120,4 @@ def from_runnable_config(\n         return cls(**{k: v for k, v in configurable.items() if k in _fields})\n \n \n-T = TypeVar(\"T\", bound=AgentConfiguration)\n\\ No newline at end of file\n+T = TypeVar(\"T\", bound=BaseConfiguration)",
      "patch_lines": [
        "@@ -13,7 +13,7 @@\n",
        " }\n",
        " \n",
        " @dataclass(kw_only=True)\n",
        "-class AgentConfiguration:\n",
        "+class BaseConfiguration:\n",
        "     \"\"\"Configuration for the research agent.\n",
        "     \n",
        "     This class defines parameters for query generation, document retrieval,\n",
        "@@ -81,7 +81,13 @@ class AgentConfiguration:\n",
        "             \"description\": \"Name of the Pinecone index to use for retrieval\"\n",
        "         },\n",
        "     )\n",
        "-    \n",
        "+\n",
        "+    region: str = field(\n",
        "+        default=\"us-east-1\",\n",
        "+        metadata={\n",
        "+            \"description\": \"AWS region for Pinecone\"\n",
        "+        },\n",
        "+    )\n",
        " \n",
        "     # Prompts\n",
        "     generate_queries_system_prompt: str = field(\n",
        "@@ -114,4 +120,4 @@ def from_runnable_config(\n",
        "         return cls(**{k: v for k, v in configurable.items() if k in _fields})\n",
        " \n",
        " \n",
        "-T = TypeVar(\"T\", bound=AgentConfiguration)\n",
        "\\ No newline at end of file\n",
        "+T = TypeVar(\"T\", bound=BaseConfiguration)\n"
      ]
    },
    {
      "path": "src/rag/research_agent/__init__.py",
      "status": "removed",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "src/rag/research_agent/graph.py",
      "status": "removed",
      "additions": 0,
      "deletions": 97,
      "patch": "@@ -1,97 +0,0 @@\n-\"\"\"Researcher graph using Pinecone retriever for document retrieval.\"\"\"\n-\n-from typing import cast, List\n-from langchain_core.documents import Document\n-from langchain_core.runnables import RunnableConfig\n-from langgraph.constants import Send\n-from langgraph.graph import END, START, StateGraph\n-from typing_extensions import TypedDict\n-\n-from rag.retriever import retrieve_documents as pinecone_retrieve\n-from rag.research_agent.state import QueryState, ResearcherState\n-from rag.configuration import AgentConfiguration\n-from rag.utils import load_chat_model\n-\n-import logging\n-logger = logging.getLogger(__name__)\n-\n-async def generate_queries(\n-    state: ResearcherState, \n-    *, \n-    config: RunnableConfig\n-) -> dict[str, list[str]]:\n-    \"\"\"Generate search queries based on the research question.\"\"\"\n-    \n-    class Response(TypedDict):\n-        queries: list[str]\n-\n-    configuration = AgentConfiguration.from_runnable_config(config)\n-    model = load_chat_model(configuration.query_model).with_structured_output(Response)\n-    \n-    messages = [\n-        {\n-            \"role\": \"system\", \n-            \"content\": configuration.generate_queries_system_prompt\n-        },\n-        {\n-            \"role\": \"human\", \n-            \"content\": state.question\n-        },\n-    ]\n-    \n-    response = cast(Response, await model.ainvoke(messages))\n-    return {\"queries\": response[\"queries\"]}\n-\n-async def retrieve_documents(\n-    state: QueryState, \n-    *, \n-    config: RunnableConfig\n-) -> dict[str, List[Document]]:\n-    \"\"\"Retrieve documents using Pinecone.\"\"\"\n-    try:\n-        # Get configuration\n-        configuration = AgentConfiguration.from_runnable_config(config)\n-        \n-        # Use our Pinecone retriever\n-        docs = await pinecone_retrieve(\n-            query=state.query,\n-            index_name=configuration.index_name,  # Add this to AgentConfiguration\n-            top_k=configuration.top_k  # Add this to AgentConfiguration\n-        )\n-        \n-        # Add query metadata\n-        for doc in docs:\n-            doc.metadata[\"query\"] = state.query\n-            \n-        return {\"documents\": docs}\n-        \n-    except Exception as e:\n-        logger.error(f\"Error retrieving documents: {str(e)}\")\n-        return {\"documents\": []}\n-\n-def retrieve_in_parallel(state: ResearcherState) -> list[Send]:\n-    \"\"\"Create parallel retrieval tasks for each query.\"\"\"\n-    return [\n-        Send(\"retrieve_documents\", QueryState(query=query)) \n-        for query in state.queries\n-    ]\n-\n-# Define the graph\n-builder = StateGraph(ResearcherState)\n-\n-# Add nodes\n-builder.add_node(\"generate_queries\", generate_queries)\n-builder.add_node(\"retrieve_documents\", retrieve_documents)\n-\n-# Add edges\n-builder.add_edge(START, \"generate_queries\")\n-builder.add_conditional_edges(\n-    \"generate_queries\",\n-    retrieve_in_parallel,\n-    path_map=[\"retrieve_documents\"],\n-)\n-builder.add_edge(\"retrieve_documents\", END)\n-\n-# Compile graph\n-graph = builder.compile()\n-graph.name = \"ResearcherGraph\"\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -1,97 +0,0 @@\n",
        "-\"\"\"Researcher graph using Pinecone retriever for document retrieval.\"\"\"\n",
        "-\n",
        "-from typing import cast, List\n",
        "-from langchain_core.documents import Document\n",
        "-from langchain_core.runnables import RunnableConfig\n",
        "-from langgraph.constants import Send\n",
        "-from langgraph.graph import END, START, StateGraph\n",
        "-from typing_extensions import TypedDict\n",
        "-\n",
        "-from rag.retriever import retrieve_documents as pinecone_retrieve\n",
        "-from rag.research_agent.state import QueryState, ResearcherState\n",
        "-from rag.configuration import AgentConfiguration\n",
        "-from rag.utils import load_chat_model\n",
        "-\n",
        "-import logging\n",
        "-logger = logging.getLogger(__name__)\n",
        "-\n",
        "-async def generate_queries(\n",
        "-    state: ResearcherState, \n",
        "-    *, \n",
        "-    config: RunnableConfig\n",
        "-) -> dict[str, list[str]]:\n",
        "-    \"\"\"Generate search queries based on the research question.\"\"\"\n",
        "-    \n",
        "-    class Response(TypedDict):\n",
        "-        queries: list[str]\n",
        "-\n",
        "-    configuration = AgentConfiguration.from_runnable_config(config)\n",
        "-    model = load_chat_model(configuration.query_model).with_structured_output(Response)\n",
        "-    \n",
        "-    messages = [\n",
        "-        {\n",
        "-            \"role\": \"system\", \n",
        "-            \"content\": configuration.generate_queries_system_prompt\n",
        "-        },\n",
        "-        {\n",
        "-            \"role\": \"human\", \n",
        "-            \"content\": state.question\n",
        "-        },\n",
        "-    ]\n",
        "-    \n",
        "-    response = cast(Response, await model.ainvoke(messages))\n",
        "-    return {\"queries\": response[\"queries\"]}\n",
        "-\n",
        "-async def retrieve_documents(\n",
        "-    state: QueryState, \n",
        "-    *, \n",
        "-    config: RunnableConfig\n",
        "-) -> dict[str, List[Document]]:\n",
        "-    \"\"\"Retrieve documents using Pinecone.\"\"\"\n",
        "-    try:\n",
        "-        # Get configuration\n",
        "-        configuration = AgentConfiguration.from_runnable_config(config)\n",
        "-        \n",
        "-        # Use our Pinecone retriever\n",
        "-        docs = await pinecone_retrieve(\n",
        "-            query=state.query,\n",
        "-            index_name=configuration.index_name,  # Add this to AgentConfiguration\n",
        "-            top_k=configuration.top_k  # Add this to AgentConfiguration\n",
        "-        )\n",
        "-        \n",
        "-        # Add query metadata\n",
        "-        for doc in docs:\n",
        "-            doc.metadata[\"query\"] = state.query\n",
        "-            \n",
        "-        return {\"documents\": docs}\n",
        "-        \n",
        "-    except Exception as e:\n",
        "-        logger.error(f\"Error retrieving documents: {str(e)}\")\n",
        "-        return {\"documents\": []}\n",
        "-\n",
        "-def retrieve_in_parallel(state: ResearcherState) -> list[Send]:\n",
        "-    \"\"\"Create parallel retrieval tasks for each query.\"\"\"\n",
        "-    return [\n",
        "-        Send(\"retrieve_documents\", QueryState(query=query)) \n",
        "-        for query in state.queries\n",
        "-    ]\n",
        "-\n",
        "-# Define the graph\n",
        "-builder = StateGraph(ResearcherState)\n",
        "-\n",
        "-# Add nodes\n",
        "-builder.add_node(\"generate_queries\", generate_queries)\n",
        "-builder.add_node(\"retrieve_documents\", retrieve_documents)\n",
        "-\n",
        "-# Add edges\n",
        "-builder.add_edge(START, \"generate_queries\")\n",
        "-builder.add_conditional_edges(\n",
        "-    \"generate_queries\",\n",
        "-    retrieve_in_parallel,\n",
        "-    path_map=[\"retrieve_documents\"],\n",
        "-)\n",
        "-builder.add_edge(\"retrieve_documents\", END)\n",
        "-\n",
        "-# Compile graph\n",
        "-graph = builder.compile()\n",
        "-graph.name = \"ResearcherGraph\"\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "src/rag/research_agent/state.py",
      "status": "removed",
      "additions": 0,
      "deletions": 30,
      "patch": "@@ -1,30 +0,0 @@\n-\"\"\"State management for the researcher graph.\n-\n-This module defines the state structures used in the researcher graph.\n-\"\"\"\n-\n-from dataclasses import dataclass, field\n-from typing import Annotated\n-\n-from langchain_core.documents import Document\n-\n-from rag.utils import reduce_docs\n-\n-\n-@dataclass(kw_only=True)\n-class QueryState:\n-    \"\"\"Private state for the retrieve_documents node in the researcher graph.\"\"\"\n-\n-    query: str\n-\n-\n-@dataclass(kw_only=True)\n-class ResearcherState:\n-    \"\"\"State of the researcher graph / agent.\"\"\"\n-\n-    question: str\n-    \"\"\"A step in the research plan generated by the retriever agent.\"\"\"\n-    queries: list[str] = field(default_factory=list)\n-    \"\"\"A list of search queries based on the question that the researcher generates.\"\"\"\n-    documents: Annotated[list[Document], reduce_docs] = field(default_factory=list)\n-    \"\"\"Populated by the retriever. This is a list of documents that the agent can reference.\"\"\"\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -1,30 +0,0 @@\n",
        "-\"\"\"State management for the researcher graph.\n",
        "-\n",
        "-This module defines the state structures used in the researcher graph.\n",
        "-\"\"\"\n",
        "-\n",
        "-from dataclasses import dataclass, field\n",
        "-from typing import Annotated\n",
        "-\n",
        "-from langchain_core.documents import Document\n",
        "-\n",
        "-from rag.utils import reduce_docs\n",
        "-\n",
        "-\n",
        "-@dataclass(kw_only=True)\n",
        "-class QueryState:\n",
        "-    \"\"\"Private state for the retrieve_documents node in the researcher graph.\"\"\"\n",
        "-\n",
        "-    query: str\n",
        "-\n",
        "-\n",
        "-@dataclass(kw_only=True)\n",
        "-class ResearcherState:\n",
        "-    \"\"\"State of the researcher graph / agent.\"\"\"\n",
        "-\n",
        "-    question: str\n",
        "-    \"\"\"A step in the research plan generated by the retriever agent.\"\"\"\n",
        "-    queries: list[str] = field(default_factory=list)\n",
        "-    \"\"\"A list of search queries based on the question that the researcher generates.\"\"\"\n",
        "-    documents: Annotated[list[Document], reduce_docs] = field(default_factory=list)\n",
        "-    \"\"\"Populated by the retriever. This is a list of documents that the agent can reference.\"\"\"\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "src/rag/retrieval_agent/__init__.py",
      "status": "removed",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "src/rag/retrieval_agent/configuration.py",
      "status": "removed",
      "additions": 0,
      "deletions": 62,
      "patch": "@@ -1,62 +0,0 @@\n-\"\"\"Define the configurable parameters for the agent.\"\"\"\n-\n-from __future__ import annotations\n-from dataclasses import dataclass, field\n-from rag.retrieval_agent import prompts\n-from rag.configuration import AgentConfiguration as BaseConfig\n-\n-@dataclass(kw_only=True)\n-class AgentConfiguration(BaseConfig):\n-    \"\"\"Configuration for the retrieval agent.\"\"\"\n-    \n-    # Model Configuration\n-    query_model: str = field(\n-        default=\"openai/gpt-4o-mini\",\n-        metadata={\n-            \"description\": \"Model for query processing and refinement\"\n-        }\n-    )\n-    \n-    response_model: str = field(\n-        default=\"openai/gpt-4o-mini\",\n-        metadata={\n-            \"description\": \"Model for response generation\"\n-        }\n-    )\n-\n-    # Prompt Configuration\n-    router_system_prompt: str = field(\n-        default=prompts.ROUTER_SYSTEM_PROMPT,\n-        metadata={\n-            \"description\": \"System prompt for query routing\"\n-        }\n-    )\n-\n-    more_info_system_prompt: str = field(\n-        default=prompts.MORE_INFO_SYSTEM_PROMPT,\n-        metadata={\n-            \"description\": \"System prompt for requesting clarification\"\n-        }\n-    )\n-\n-\n-    research_plan_system_prompt: str = field(\n-        default=prompts.RESEARCH_PLAN_SYSTEM_PROMPT,\n-        metadata={\n-            \"description\": \"System prompt for research planning\"\n-        }\n-    )\n-\n-    generate_queries_system_prompt: str = field(\n-        default=prompts.GENERATE_QUERIES_SYSTEM_PROMPT,\n-        metadata={\n-            \"description\": \"System prompt for query generation\"\n-        }\n-    )\n-\n-    response_system_prompt: str = field(\n-        default=prompts.RESPONSE_SYSTEM_PROMPT,\n-        metadata={\n-            \"description\": \"System prompt for response generation\"\n-        }\n-    )\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -1,62 +0,0 @@\n",
        "-\"\"\"Define the configurable parameters for the agent.\"\"\"\n",
        "-\n",
        "-from __future__ import annotations\n",
        "-from dataclasses import dataclass, field\n",
        "-from rag.retrieval_agent import prompts\n",
        "-from rag.configuration import AgentConfiguration as BaseConfig\n",
        "-\n",
        "-@dataclass(kw_only=True)\n",
        "-class AgentConfiguration(BaseConfig):\n",
        "-    \"\"\"Configuration for the retrieval agent.\"\"\"\n",
        "-    \n",
        "-    # Model Configuration\n",
        "-    query_model: str = field(\n",
        "-        default=\"openai/gpt-4o-mini\",\n",
        "-        metadata={\n",
        "-            \"description\": \"Model for query processing and refinement\"\n",
        "-        }\n",
        "-    )\n",
        "-    \n",
        "-    response_model: str = field(\n",
        "-        default=\"openai/gpt-4o-mini\",\n",
        "-        metadata={\n",
        "-            \"description\": \"Model for response generation\"\n",
        "-        }\n",
        "-    )\n",
        "-\n",
        "-    # Prompt Configuration\n",
        "-    router_system_prompt: str = field(\n",
        "-        default=prompts.ROUTER_SYSTEM_PROMPT,\n",
        "-        metadata={\n",
        "-            \"description\": \"System prompt for query routing\"\n",
        "-        }\n",
        "-    )\n",
        "-\n",
        "-    more_info_system_prompt: str = field(\n",
        "-        default=prompts.MORE_INFO_SYSTEM_PROMPT,\n",
        "-        metadata={\n",
        "-            \"description\": \"System prompt for requesting clarification\"\n",
        "-        }\n",
        "-    )\n",
        "-\n",
        "-\n",
        "-    research_plan_system_prompt: str = field(\n",
        "-        default=prompts.RESEARCH_PLAN_SYSTEM_PROMPT,\n",
        "-        metadata={\n",
        "-            \"description\": \"System prompt for research planning\"\n",
        "-        }\n",
        "-    )\n",
        "-\n",
        "-    generate_queries_system_prompt: str = field(\n",
        "-        default=prompts.GENERATE_QUERIES_SYSTEM_PROMPT,\n",
        "-        metadata={\n",
        "-            \"description\": \"System prompt for query generation\"\n",
        "-        }\n",
        "-    )\n",
        "-\n",
        "-    response_system_prompt: str = field(\n",
        "-        default=prompts.RESPONSE_SYSTEM_PROMPT,\n",
        "-        metadata={\n",
        "-            \"description\": \"System prompt for response generation\"\n",
        "-        }\n",
        "-    )\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "src/rag/retrieval_agent/graph.py",
      "status": "removed",
      "additions": 0,
      "deletions": 175,
      "patch": "@@ -1,175 +0,0 @@\n-\"\"\"Main entrypoint for the conversational retrieval graph.\"\"\"\n-\n-from typing import Any, Literal, TypedDict, cast\n-from langchain_core.messages import BaseMessage\n-from langchain_core.runnables import RunnableConfig\n-from langgraph.graph import END, START, StateGraph\n-\n-from rag.retrieval_agent.configuration import AgentConfiguration\n-from rag.research_agent.graph import graph as researcher_graph\n-from rag.retrieval_agent.state import AgentState, InputState, Router\n-from rag.utils import format_docs, load_chat_model\n-\n-import logging\n-logger = logging.getLogger(__name__)\n-\n-async def analyze_and_route_query(\n-    state: AgentState, \n-    *, \n-    config: RunnableConfig\n-) -> dict[str, Router]:\n-    \"\"\"Analyze and route the user's query.\"\"\"\n-    # Skip router for testing\n-    if state.router and state.router[\"logic\"]:\n-        return {\"router\": state.router}\n-\n-    configuration = AgentConfiguration.from_runnable_config(config)\n-    model = load_chat_model(configuration.query_model)\n-    messages = [\n-        {\"role\": \"system\", \"content\": configuration.router_system_prompt}\n-    ] + state.messages\n-    \n-    response = cast(Router, await model.with_structured_output(Router).ainvoke(messages))\n-    return {\"router\": response}\n-\n-def route_query(\n-    state: AgentState,\n-) -> Literal[\"create_research_plan\", \"ask_for_more_info\"]:\n-    \"\"\"Route query based on classification.\"\"\"\n-    _type = state.router[\"type\"]\n-    if _type == \"documents\":\n-        return \"create_research_plan\"\n-    elif _type == \"more-info\":\n-        return \"ask_for_more_info\"\n-    else:\n-        raise ValueError(f\"Unknown router type {_type}\")\n-\n-async def ask_for_more_info(\n-    state: AgentState, \n-    *, \n-    config: RunnableConfig\n-) -> dict[str, list[BaseMessage]]:\n-    \"\"\"Ask user for more information.\"\"\"\n-    configuration = AgentConfiguration.from_runnable_config(config)\n-    model = load_chat_model(configuration.query_model)\n-    \n-    system_prompt = configuration.more_info_system_prompt.format(\n-        logic=state.router[\"logic\"]\n-    )\n-    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state.messages\n-    \n-    response = await model.ainvoke(messages)\n-    return {\"messages\": [response]}\n-\n-async def respond_to_general_query(\n-    state: AgentState, \n-    *, \n-    config: RunnableConfig\n-) -> dict[str, list[BaseMessage]]:\n-    \"\"\"Respond to general queries.\"\"\"\n-    configuration = AgentConfiguration.from_runnable_config(config)\n-    model = load_chat_model(configuration.query_model)\n-    \n-    system_prompt = configuration.general_system_prompt.format(\n-        logic=state.router[\"logic\"]\n-    )\n-    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state.messages\n-    \n-    response = await model.ainvoke(messages)\n-    return {\"messages\": [response]}\n-\n-async def create_research_plan(\n-    state: AgentState, \n-    *, \n-    config: RunnableConfig\n-) -> dict[str, list[str]]:\n-    \"\"\"Create research plan.\"\"\"\n-    class Plan(TypedDict):\n-        steps: list[str]\n-\n-    configuration = AgentConfiguration.from_runnable_config(config)\n-    model = load_chat_model(configuration.query_model).with_structured_output(Plan)\n-    \n-    messages = [\n-        {\"role\": \"system\", \"content\": configuration.research_plan_system_prompt}\n-    ] + state.messages\n-    \n-    response = cast(Plan, await model.ainvoke(messages))\n-    return {\n-        \"steps\": response[\"steps\"],\n-        \"documents\": \"delete\",\n-        \"query\": state.query,\n-    }\n-\n-async def conduct_research(state: AgentState, *, config: RunnableConfig) -> dict[str, Any]:\n-    \"\"\"Execute research plan steps.\"\"\"\n-    try:\n-        result = await researcher_graph.ainvoke({\"question\": state.steps[0]}, config=config)\n-        return {\n-            \"documents\": result[\"documents\"], \n-            \"steps\": state.steps[1:]\n-        }\n-    except Exception as e:\n-        logger.error(f\"Error conducting research: {str(e)}\")\n-        return {\n-            \"documents\": [],\n-            \"steps\": state.steps[1:]\n-        }\n-\n-def check_finished(state: AgentState) -> Literal[\"respond\", \"conduct_research\"]:\n-    \"\"\"Check if research is complete.\"\"\"\n-    if len(state.steps or []) > 0:\n-        return \"conduct_research\"\n-    return \"respond\"\n-\n-async def respond(\n-    state: AgentState, \n-    *, \n-    config: RunnableConfig\n-) -> dict[str, list[BaseMessage]]:\n-    \"\"\"Generate final response.\"\"\"\n-    configuration = AgentConfiguration.from_runnable_config(config)\n-    model = load_chat_model(configuration.response_model)\n-    \n-    # TODO: add re-ranker\n-    top_k = 20\n-    context = format_docs(state.documents[:top_k])\n-    prompt = configuration.response_system_prompt.format(context=context)\n-    \n-    messages = [{\"role\": \"system\", \"content\": prompt}] + state.messages\n-    response = await model.ainvoke(messages)\n-    \n-    return {\n-        \"messages\": [response],\n-        \"answer\": response.content\n-    }\n-\n-# Define graph\n-builder = StateGraph(AgentState, input=InputState, config_schema=AgentConfiguration)\n-\n-# Add nodes\n-# builder.add_node(\"analyze_and_route_query\", analyze_and_route_query)\n-builder.add_node(\"create_research_plan\", create_research_plan)\n-builder.add_node(\"ask_for_more_info\", ask_for_more_info)\n-builder.add_node(\"conduct_research\", conduct_research)\n-builder.add_node(\"respond\", respond)\n-\n-# Add edges\n-builder.add_edge(START, \"create_research_plan\")\n-# builder.add_conditional_edges(\n-#     \"create_research_plan\",\n-#     route_query,\n-#     {\n-#         \"create_research_plan\": \"create_research_plan\",\n-#         \"ask_for_more_info\": \"ask_for_more_info\",\n-#     }\n-# )\n-builder.add_edge(\"create_research_plan\", \"conduct_research\")\n-builder.add_conditional_edges(\"conduct_research\", check_finished)\n-builder.add_edge(\"respond\", END)\n-builder.add_edge(\"ask_for_more_info\", END)\n-\n-# Compile graph\n-graph = builder.compile()\n-graph.name = \"RetrievalGraph\" \n-",
      "patch_lines": [
        "@@ -1,175 +0,0 @@\n",
        "-\"\"\"Main entrypoint for the conversational retrieval graph.\"\"\"\n",
        "-\n",
        "-from typing import Any, Literal, TypedDict, cast\n",
        "-from langchain_core.messages import BaseMessage\n",
        "-from langchain_core.runnables import RunnableConfig\n",
        "-from langgraph.graph import END, START, StateGraph\n",
        "-\n",
        "-from rag.retrieval_agent.configuration import AgentConfiguration\n",
        "-from rag.research_agent.graph import graph as researcher_graph\n",
        "-from rag.retrieval_agent.state import AgentState, InputState, Router\n",
        "-from rag.utils import format_docs, load_chat_model\n",
        "-\n",
        "-import logging\n",
        "-logger = logging.getLogger(__name__)\n",
        "-\n",
        "-async def analyze_and_route_query(\n",
        "-    state: AgentState, \n",
        "-    *, \n",
        "-    config: RunnableConfig\n",
        "-) -> dict[str, Router]:\n",
        "-    \"\"\"Analyze and route the user's query.\"\"\"\n",
        "-    # Skip router for testing\n",
        "-    if state.router and state.router[\"logic\"]:\n",
        "-        return {\"router\": state.router}\n",
        "-\n",
        "-    configuration = AgentConfiguration.from_runnable_config(config)\n",
        "-    model = load_chat_model(configuration.query_model)\n",
        "-    messages = [\n",
        "-        {\"role\": \"system\", \"content\": configuration.router_system_prompt}\n",
        "-    ] + state.messages\n",
        "-    \n",
        "-    response = cast(Router, await model.with_structured_output(Router).ainvoke(messages))\n",
        "-    return {\"router\": response}\n",
        "-\n",
        "-def route_query(\n",
        "-    state: AgentState,\n",
        "-) -> Literal[\"create_research_plan\", \"ask_for_more_info\"]:\n",
        "-    \"\"\"Route query based on classification.\"\"\"\n",
        "-    _type = state.router[\"type\"]\n",
        "-    if _type == \"documents\":\n",
        "-        return \"create_research_plan\"\n",
        "-    elif _type == \"more-info\":\n",
        "-        return \"ask_for_more_info\"\n",
        "-    else:\n",
        "-        raise ValueError(f\"Unknown router type {_type}\")\n",
        "-\n",
        "-async def ask_for_more_info(\n",
        "-    state: AgentState, \n",
        "-    *, \n",
        "-    config: RunnableConfig\n",
        "-) -> dict[str, list[BaseMessage]]:\n",
        "-    \"\"\"Ask user for more information.\"\"\"\n",
        "-    configuration = AgentConfiguration.from_runnable_config(config)\n",
        "-    model = load_chat_model(configuration.query_model)\n",
        "-    \n",
        "-    system_prompt = configuration.more_info_system_prompt.format(\n",
        "-        logic=state.router[\"logic\"]\n",
        "-    )\n",
        "-    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state.messages\n",
        "-    \n",
        "-    response = await model.ainvoke(messages)\n",
        "-    return {\"messages\": [response]}\n",
        "-\n",
        "-async def respond_to_general_query(\n",
        "-    state: AgentState, \n",
        "-    *, \n",
        "-    config: RunnableConfig\n",
        "-) -> dict[str, list[BaseMessage]]:\n",
        "-    \"\"\"Respond to general queries.\"\"\"\n",
        "-    configuration = AgentConfiguration.from_runnable_config(config)\n",
        "-    model = load_chat_model(configuration.query_model)\n",
        "-    \n",
        "-    system_prompt = configuration.general_system_prompt.format(\n",
        "-        logic=state.router[\"logic\"]\n",
        "-    )\n",
        "-    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state.messages\n",
        "-    \n",
        "-    response = await model.ainvoke(messages)\n",
        "-    return {\"messages\": [response]}\n",
        "-\n",
        "-async def create_research_plan(\n",
        "-    state: AgentState, \n",
        "-    *, \n",
        "-    config: RunnableConfig\n",
        "-) -> dict[str, list[str]]:\n",
        "-    \"\"\"Create research plan.\"\"\"\n",
        "-    class Plan(TypedDict):\n",
        "-        steps: list[str]\n",
        "-\n",
        "-    configuration = AgentConfiguration.from_runnable_config(config)\n",
        "-    model = load_chat_model(configuration.query_model).with_structured_output(Plan)\n",
        "-    \n",
        "-    messages = [\n",
        "-        {\"role\": \"system\", \"content\": configuration.research_plan_system_prompt}\n",
        "-    ] + state.messages\n",
        "-    \n",
        "-    response = cast(Plan, await model.ainvoke(messages))\n",
        "-    return {\n",
        "-        \"steps\": response[\"steps\"],\n",
        "-        \"documents\": \"delete\",\n",
        "-        \"query\": state.query,\n",
        "-    }\n",
        "-\n",
        "-async def conduct_research(state: AgentState, *, config: RunnableConfig) -> dict[str, Any]:\n",
        "-    \"\"\"Execute research plan steps.\"\"\"\n",
        "-    try:\n",
        "-        result = await researcher_graph.ainvoke({\"question\": state.steps[0]}, config=config)\n",
        "-        return {\n",
        "-            \"documents\": result[\"documents\"], \n",
        "-            \"steps\": state.steps[1:]\n",
        "-        }\n",
        "-    except Exception as e:\n",
        "-        logger.error(f\"Error conducting research: {str(e)}\")\n",
        "-        return {\n",
        "-            \"documents\": [],\n",
        "-            \"steps\": state.steps[1:]\n",
        "-        }\n",
        "-\n",
        "-def check_finished(state: AgentState) -> Literal[\"respond\", \"conduct_research\"]:\n",
        "-    \"\"\"Check if research is complete.\"\"\"\n",
        "-    if len(state.steps or []) > 0:\n",
        "-        return \"conduct_research\"\n",
        "-    return \"respond\"\n",
        "-\n",
        "-async def respond(\n",
        "-    state: AgentState, \n",
        "-    *, \n",
        "-    config: RunnableConfig\n",
        "-) -> dict[str, list[BaseMessage]]:\n",
        "-    \"\"\"Generate final response.\"\"\"\n",
        "-    configuration = AgentConfiguration.from_runnable_config(config)\n",
        "-    model = load_chat_model(configuration.response_model)\n",
        "-    \n",
        "-    # TODO: add re-ranker\n",
        "-    top_k = 20\n",
        "-    context = format_docs(state.documents[:top_k])\n",
        "-    prompt = configuration.response_system_prompt.format(context=context)\n",
        "-    \n",
        "-    messages = [{\"role\": \"system\", \"content\": prompt}] + state.messages\n",
        "-    response = await model.ainvoke(messages)\n",
        "-    \n",
        "-    return {\n",
        "-        \"messages\": [response],\n",
        "-        \"answer\": response.content\n",
        "-    }\n",
        "-\n",
        "-# Define graph\n",
        "-builder = StateGraph(AgentState, input=InputState, config_schema=AgentConfiguration)\n",
        "-\n",
        "-# Add nodes\n",
        "-# builder.add_node(\"analyze_and_route_query\", analyze_and_route_query)\n",
        "-builder.add_node(\"create_research_plan\", create_research_plan)\n",
        "-builder.add_node(\"ask_for_more_info\", ask_for_more_info)\n",
        "-builder.add_node(\"conduct_research\", conduct_research)\n",
        "-builder.add_node(\"respond\", respond)\n",
        "-\n",
        "-# Add edges\n",
        "-builder.add_edge(START, \"create_research_plan\")\n",
        "-# builder.add_conditional_edges(\n",
        "-#     \"create_research_plan\",\n",
        "-#     route_query,\n",
        "-#     {\n",
        "-#         \"create_research_plan\": \"create_research_plan\",\n",
        "-#         \"ask_for_more_info\": \"ask_for_more_info\",\n",
        "-#     }\n",
        "-# )\n",
        "-builder.add_edge(\"create_research_plan\", \"conduct_research\")\n",
        "-builder.add_conditional_edges(\"conduct_research\", check_finished)\n",
        "-builder.add_edge(\"respond\", END)\n",
        "-builder.add_edge(\"ask_for_more_info\", END)\n",
        "-\n",
        "-# Compile graph\n",
        "-graph = builder.compile()\n",
        "-graph.name = \"RetrievalGraph\" \n",
        "-\n"
      ]
    },
    {
      "path": "src/rag/retrieval_agent/prompts.py",
      "status": "removed",
      "additions": 0,
      "deletions": 30,
      "patch": "@@ -1,30 +0,0 @@\n-\"\"\"System prompts for the retrieval agent.\"\"\"\n-\n-ROUTER_SYSTEM_PROMPT = \"\"\"Analyze the user's question and classify it into one of these categories:\n-- documents: Questions about documents that need research\n-- more-info: Questions that need clarification\n-\n-Output Format:\n-{\n-    \"type\": \"category\",\n-    \"logic\": \"Explanation of why this category was chosen\"\n-}\"\"\"\n-\n-MORE_INFO_SYSTEM_PROMPT = \"\"\"The user's question needs clarification. {logic}\n-Ask for specific details that would help provide a better answer.\"\"\"\n-\n-GENERAL_SYSTEM_PROMPT = \"\"\"This is a general question. {logic}\n-Provide a helpful response based on general knowledge.\"\"\"\n-\n-RESEARCH_PLAN_SYSTEM_PROMPT = \"\"\"Create a step-by-step research plan to answer the user's question.\n-Each step should be a specific query or research task.\n-Focus on breaking down complex questions into smaller, searchable components.\"\"\"\n-\n-GENERATE_QUERIES_SYSTEM_PROMPT = \"\"\"Generate 3-5 diverse search queries to help answer the given question.\n-Focus on different aspects of the question to get comprehensive results.\n-Make queries specific and targeted.\"\"\"\n-\n-RESPONSE_SYSTEM_PROMPT = \"\"\"Use the following context to answer the user's question:\n-{context}\n-\n-Provide a comprehensive answer that directly addresses the question. If code is required, provide it in a code block.\"\"\"\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -1,30 +0,0 @@\n",
        "-\"\"\"System prompts for the retrieval agent.\"\"\"\n",
        "-\n",
        "-ROUTER_SYSTEM_PROMPT = \"\"\"Analyze the user's question and classify it into one of these categories:\n",
        "-- documents: Questions about documents that need research\n",
        "-- more-info: Questions that need clarification\n",
        "-\n",
        "-Output Format:\n",
        "-{\n",
        "-    \"type\": \"category\",\n",
        "-    \"logic\": \"Explanation of why this category was chosen\"\n",
        "-}\"\"\"\n",
        "-\n",
        "-MORE_INFO_SYSTEM_PROMPT = \"\"\"The user's question needs clarification. {logic}\n",
        "-Ask for specific details that would help provide a better answer.\"\"\"\n",
        "-\n",
        "-GENERAL_SYSTEM_PROMPT = \"\"\"This is a general question. {logic}\n",
        "-Provide a helpful response based on general knowledge.\"\"\"\n",
        "-\n",
        "-RESEARCH_PLAN_SYSTEM_PROMPT = \"\"\"Create a step-by-step research plan to answer the user's question.\n",
        "-Each step should be a specific query or research task.\n",
        "-Focus on breaking down complex questions into smaller, searchable components.\"\"\"\n",
        "-\n",
        "-GENERATE_QUERIES_SYSTEM_PROMPT = \"\"\"Generate 3-5 diverse search queries to help answer the given question.\n",
        "-Focus on different aspects of the question to get comprehensive results.\n",
        "-Make queries specific and targeted.\"\"\"\n",
        "-\n",
        "-RESPONSE_SYSTEM_PROMPT = \"\"\"Use the following context to answer the user's question:\n",
        "-{context}\n",
        "-\n",
        "-Provide a comprehensive answer that directly addresses the question. If code is required, provide it in a code block.\"\"\"\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "src/rag/retrieval_agent/state.py",
      "status": "removed",
      "additions": 0,
      "deletions": 24,
      "patch": "@@ -1,24 +0,0 @@\n-\"\"\"State management for the retrieval agent.\"\"\"\n-\n-from dataclasses import dataclass, field\n-from typing import Any, List, Optional, TypedDict, Literal\n-from langchain_core.messages import BaseMessage\n-from langchain_core.documents import Document\n-\n-class Router(TypedDict):\n-    \"\"\"Router type and logic.\"\"\"\n-    type: str\n-    logic: str\n-\n-class InputState(TypedDict):\n-    \"\"\"Input state for the retrieval agent.\"\"\"\n-    messages: list[BaseMessage]\n-\n-@dataclass\n-class AgentState:\n-    \"\"\"State for the retrieval agent.\"\"\"\n-    messages: list[BaseMessage]\n-    router: Optional[Router] = None\n-    steps: Optional[list[str]] = None\n-    documents: Optional[list[Document]] = None\n-    query: Optional[str] = None \n\\ No newline at end of file",
      "patch_lines": [
        "@@ -1,24 +0,0 @@\n",
        "-\"\"\"State management for the retrieval agent.\"\"\"\n",
        "-\n",
        "-from dataclasses import dataclass, field\n",
        "-from typing import Any, List, Optional, TypedDict, Literal\n",
        "-from langchain_core.messages import BaseMessage\n",
        "-from langchain_core.documents import Document\n",
        "-\n",
        "-class Router(TypedDict):\n",
        "-    \"\"\"Router type and logic.\"\"\"\n",
        "-    type: str\n",
        "-    logic: str\n",
        "-\n",
        "-class InputState(TypedDict):\n",
        "-    \"\"\"Input state for the retrieval agent.\"\"\"\n",
        "-    messages: list[BaseMessage]\n",
        "-\n",
        "-@dataclass\n",
        "-class AgentState:\n",
        "-    \"\"\"State for the retrieval agent.\"\"\"\n",
        "-    messages: list[BaseMessage]\n",
        "-    router: Optional[Router] = None\n",
        "-    steps: Optional[list[str]] = None\n",
        "-    documents: Optional[list[Document]] = None\n",
        "-    query: Optional[str] = None \n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "src/rag/retriever.py",
      "status": "modified",
      "additions": 17,
      "deletions": 17,
      "patch": "@@ -7,6 +7,8 @@\n from langchain_pinecone import PineconeVectorStore\n from pinecone.grpc import PineconeGRPC\n import logging\n+from langchain_core.retrievers import BaseRetriever\n+from langchain_core.runnables import RunnableConfig\n from dotenv import load_dotenv\n \n # Add logging to check if .env is loaded\n@@ -79,20 +81,18 @@ def make_pinecone_retriever(\n         raise\n \n # Usage example\n-async def retrieve_documents(\n-    query: str,\n-    index_name: str,\n-    top_k: int = 4\n-) -> List[Document]:\n-    \"\"\"Retrieve relevant documents for a query\"\"\"\n-    try:\n-        with make_pinecone_retriever(\n-            index_name=index_name,\n-            top_k=top_k\n-        ) as retriever:\n-            docs = await retriever.ainvoke(query)\n-            return docs\n-            \n-    except Exception as e:\n-        logger.error(f\"Error retrieving documents: {str(e)}\")\n-        raise \n\\ No newline at end of file\n+@contextmanager\n+def make_retriever(config: RunnableConfig) -> Iterator[BaseRetriever]:\n+    \"\"\"Create a retriever based on the current configuration\"\"\"\n+    from rag.chat_agent.backend.retrieval_graph.configuration import AgentConfiguration\n+    \n+    configuration = AgentConfiguration.from_runnable_config(config)\n+    embedding_model = make_embeddings()\n+    \n+    logger.info(f\"Creating retriever for doc: {configuration.index_name}\")\n+    \n+    with make_pinecone_retriever(\n+        index_name=configuration.index_name,\n+        embedding_model=embedding_model\n+    ) as retriever:\n+        yield retriever\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -7,6 +7,8 @@\n",
        " from langchain_pinecone import PineconeVectorStore\n",
        " from pinecone.grpc import PineconeGRPC\n",
        " import logging\n",
        "+from langchain_core.retrievers import BaseRetriever\n",
        "+from langchain_core.runnables import RunnableConfig\n",
        " from dotenv import load_dotenv\n",
        " \n",
        " # Add logging to check if .env is loaded\n",
        "@@ -79,20 +81,18 @@ def make_pinecone_retriever(\n",
        "         raise\n",
        " \n",
        " # Usage example\n",
        "-async def retrieve_documents(\n",
        "-    query: str,\n",
        "-    index_name: str,\n",
        "-    top_k: int = 4\n",
        "-) -> List[Document]:\n",
        "-    \"\"\"Retrieve relevant documents for a query\"\"\"\n",
        "-    try:\n",
        "-        with make_pinecone_retriever(\n",
        "-            index_name=index_name,\n",
        "-            top_k=top_k\n",
        "-        ) as retriever:\n",
        "-            docs = await retriever.ainvoke(query)\n",
        "-            return docs\n",
        "-            \n",
        "-    except Exception as e:\n",
        "-        logger.error(f\"Error retrieving documents: {str(e)}\")\n",
        "-        raise \n",
        "\\ No newline at end of file\n",
        "+@contextmanager\n",
        "+def make_retriever(config: RunnableConfig) -> Iterator[BaseRetriever]:\n",
        "+    \"\"\"Create a retriever based on the current configuration\"\"\"\n",
        "+    from rag.chat_agent.backend.retrieval_graph.configuration import AgentConfiguration\n",
        "+    \n",
        "+    configuration = AgentConfiguration.from_runnable_config(config)\n",
        "+    embedding_model = make_embeddings()\n",
        "+    \n",
        "+    logger.info(f\"Creating retriever for doc: {configuration.index_name}\")\n",
        "+    \n",
        "+    with make_pinecone_retriever(\n",
        "+        index_name=configuration.index_name,\n",
        "+        embedding_model=embedding_model\n",
        "+    ) as retriever:\n",
        "+        yield retriever\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "src/record_manager_cache.sql",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "src/server.py",
      "status": "modified",
      "additions": 69,
      "deletions": 17,
      "patch": "@@ -1,23 +1,27 @@\n from contextlib import asynccontextmanager\n from fastapi import FastAPI, HTTPException, BackgroundTasks, Response\n from pydantic import BaseModel, HttpUrl, Field\n-from typing import Optional, List, Dict, cast\n+from typing import Optional, List, Dict, cast, Any\n from model.types import ContentType\n-from rag import retrieve_documents\n+from rag import retriever\n+from rag.chat_agent.backend.retrieval_graph.researcher_graph.graph import graph\n from config import settings, LLMProvider\n from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n from langchain_anthropic import ChatAnthropic\n from indexer.web_indexer import WebIndexer\n from sse_starlette.sse import EventSourceResponse\n from fastapi.responses import StreamingResponse\n from fastapi.middleware.cors import CORSMiddleware\n+from rag.chat_agent.backend.retrieval_graph.configuration import AgentConfiguration\n import logging\n import asyncio\n import json\n+import traceback\n+from langchain_core.documents import Document\n+from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, BaseMessage\n \n logger = logging.getLogger(__name__)\n \n-\n # Initialize FastAPI with lifespan\n app = FastAPI()\n \n@@ -52,6 +56,7 @@ async def lifespan(app: FastAPI):\n             await app.state.web_indexer.cleanup()\n \n app = FastAPI(lifespan=lifespan)\n+\n class URLInput(BaseModel):\n     url: HttpUrl\n     content_type: Optional[ContentType] = None\n@@ -62,6 +67,9 @@ class RAGRequest(BaseModel):\n     question: str\n     index_name: str\n     top_k: int = 4\n+    thread_id: str = \"default\"\n+    user_id: str = \"default\"\n+\n     # thread_id: str = \"default\"\n     # llm_provider: LLMProvider = LLMProvider.ANTHROPIC\n     # return_sources: bool = False\n@@ -85,7 +93,36 @@ def get_llm(provider: LLMProvider):\n         )\n     else:\n         raise HTTPException(400, f\"Unsupported LLM provider: {provider}\")\n-    \n+\n+def serialize_message(msg: BaseMessage) -> dict:\n+    \"\"\"Convert a LangChain message object to a JSON-serializable dictionary\"\"\"\n+    return {\n+        \"content\": msg.content,\n+        \"type\": msg.__class__.__name__.lower(),\n+        \"additional_kwargs\": msg.additional_kwargs\n+    }\n+\n+def serialize_document(doc: Document) -> dict:\n+    \"\"\"Convert a Document object to a JSON-serializable dictionary\"\"\"\n+    return {\n+        \"page_content\": doc.page_content,\n+        \"metadata\": doc.metadata,\n+        \"type\": \"document\"\n+    }\n+\n+def serialize_response(response: Any) -> Any:\n+    \"\"\"Recursively serialize response objects to JSON-serializable format\"\"\"\n+    if isinstance(response, Document):\n+        return serialize_document(response)\n+    elif isinstance(response, (AIMessage, HumanMessage, SystemMessage)):\n+        return serialize_message(response)\n+    elif isinstance(response, list):\n+        return [serialize_response(item) for item in response]\n+    elif isinstance(response, dict):\n+        return {k: serialize_response(v) for k, v in response.items()}\n+    elif hasattr(response, \"dict\"):  # Handle Pydantic models\n+        return serialize_response(response.dict())\n+    return response\n \n @app.post(\"/index_url\")\n async def index_url(url_input: URLInput, background_tasks: BackgroundTasks):\n@@ -192,20 +229,35 @@ async def event_generator():\n @app.post(\"/query\")\n async def query(request: RAGRequest):\n     \"\"\"Query endpoint with content type filtering\"\"\"\n-    try:\n-        response = await retrieve_documents(\n-            query=request.question,\n-            index_name=request.index_name,\n-            top_k=request.top_k\n-        )\n-        return response\n+    async def generate_responses():\n+        try:\n+            config = {\n+                \"index_name\": request.index_name,\n+                \"top_k\": request.top_k,\n+                \"thread_id\": request.thread_id,\n+                \"user_id\": request.user_id,\n+            }\n             \n-    except Exception as e:\n-        logger.error(f\"Error in query endpoint: {str(e)}\")\n-        raise HTTPException(\n-            status_code=500, \n-            detail=f\"Error processing query: {str(e)}\"\n-        )\n+            from rag.chat_agent.backend.retrieval_graph.graph import graph\n+            state = {\"messages\": [{\"role\": \"user\", \"content\": request.question}]}\n+            async for response in graph.astream(state, config, stream_mode=\"values\"):\n+                # Serialize and convert response to JSON\n+                serialized_response = serialize_response(response)\n+                yield json.dumps(serialized_response) + \"\\n\"\n+                \n+        except Exception as e:\n+            logger.error(f\"Error in query endpoint: {str(e)}\")\n+            logger.error(traceback.format_exc())\n+            error_response = {\n+                \"error\": str(e),\n+                \"detail\": \"Error processing query\"\n+            }\n+            yield json.dumps(error_response) + \"\\n\"\n+    \n+    return StreamingResponse(\n+        generate_responses(),\n+        media_type=\"application/x-ndjson\"  # Use NDJSON format for streaming JSON\n+    )\n \n @app.get(\"/indexing_status/{doc_name}\")\n async def get_indexing_status(doc_name: str):",
      "patch_lines": [
        "@@ -1,23 +1,27 @@\n",
        " from contextlib import asynccontextmanager\n",
        " from fastapi import FastAPI, HTTPException, BackgroundTasks, Response\n",
        " from pydantic import BaseModel, HttpUrl, Field\n",
        "-from typing import Optional, List, Dict, cast\n",
        "+from typing import Optional, List, Dict, cast, Any\n",
        " from model.types import ContentType\n",
        "-from rag import retrieve_documents\n",
        "+from rag import retriever\n",
        "+from rag.chat_agent.backend.retrieval_graph.researcher_graph.graph import graph\n",
        " from config import settings, LLMProvider\n",
        " from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        " from langchain_anthropic import ChatAnthropic\n",
        " from indexer.web_indexer import WebIndexer\n",
        " from sse_starlette.sse import EventSourceResponse\n",
        " from fastapi.responses import StreamingResponse\n",
        " from fastapi.middleware.cors import CORSMiddleware\n",
        "+from rag.chat_agent.backend.retrieval_graph.configuration import AgentConfiguration\n",
        " import logging\n",
        " import asyncio\n",
        " import json\n",
        "+import traceback\n",
        "+from langchain_core.documents import Document\n",
        "+from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, BaseMessage\n",
        " \n",
        " logger = logging.getLogger(__name__)\n",
        " \n",
        "-\n",
        " # Initialize FastAPI with lifespan\n",
        " app = FastAPI()\n",
        " \n",
        "@@ -52,6 +56,7 @@ async def lifespan(app: FastAPI):\n",
        "             await app.state.web_indexer.cleanup()\n",
        " \n",
        " app = FastAPI(lifespan=lifespan)\n",
        "+\n",
        " class URLInput(BaseModel):\n",
        "     url: HttpUrl\n",
        "     content_type: Optional[ContentType] = None\n",
        "@@ -62,6 +67,9 @@ class RAGRequest(BaseModel):\n",
        "     question: str\n",
        "     index_name: str\n",
        "     top_k: int = 4\n",
        "+    thread_id: str = \"default\"\n",
        "+    user_id: str = \"default\"\n",
        "+\n",
        "     # thread_id: str = \"default\"\n",
        "     # llm_provider: LLMProvider = LLMProvider.ANTHROPIC\n",
        "     # return_sources: bool = False\n",
        "@@ -85,7 +93,36 @@ def get_llm(provider: LLMProvider):\n",
        "         )\n",
        "     else:\n",
        "         raise HTTPException(400, f\"Unsupported LLM provider: {provider}\")\n",
        "-    \n",
        "+\n",
        "+def serialize_message(msg: BaseMessage) -> dict:\n",
        "+    \"\"\"Convert a LangChain message object to a JSON-serializable dictionary\"\"\"\n",
        "+    return {\n",
        "+        \"content\": msg.content,\n",
        "+        \"type\": msg.__class__.__name__.lower(),\n",
        "+        \"additional_kwargs\": msg.additional_kwargs\n",
        "+    }\n",
        "+\n",
        "+def serialize_document(doc: Document) -> dict:\n",
        "+    \"\"\"Convert a Document object to a JSON-serializable dictionary\"\"\"\n",
        "+    return {\n",
        "+        \"page_content\": doc.page_content,\n",
        "+        \"metadata\": doc.metadata,\n",
        "+        \"type\": \"document\"\n",
        "+    }\n",
        "+\n",
        "+def serialize_response(response: Any) -> Any:\n",
        "+    \"\"\"Recursively serialize response objects to JSON-serializable format\"\"\"\n",
        "+    if isinstance(response, Document):\n",
        "+        return serialize_document(response)\n",
        "+    elif isinstance(response, (AIMessage, HumanMessage, SystemMessage)):\n",
        "+        return serialize_message(response)\n",
        "+    elif isinstance(response, list):\n",
        "+        return [serialize_response(item) for item in response]\n",
        "+    elif isinstance(response, dict):\n",
        "+        return {k: serialize_response(v) for k, v in response.items()}\n",
        "+    elif hasattr(response, \"dict\"):  # Handle Pydantic models\n",
        "+        return serialize_response(response.dict())\n",
        "+    return response\n",
        " \n",
        " @app.post(\"/index_url\")\n",
        " async def index_url(url_input: URLInput, background_tasks: BackgroundTasks):\n",
        "@@ -192,20 +229,35 @@ async def event_generator():\n",
        " @app.post(\"/query\")\n",
        " async def query(request: RAGRequest):\n",
        "     \"\"\"Query endpoint with content type filtering\"\"\"\n",
        "-    try:\n",
        "-        response = await retrieve_documents(\n",
        "-            query=request.question,\n",
        "-            index_name=request.index_name,\n",
        "-            top_k=request.top_k\n",
        "-        )\n",
        "-        return response\n",
        "+    async def generate_responses():\n",
        "+        try:\n",
        "+            config = {\n",
        "+                \"index_name\": request.index_name,\n",
        "+                \"top_k\": request.top_k,\n",
        "+                \"thread_id\": request.thread_id,\n",
        "+                \"user_id\": request.user_id,\n",
        "+            }\n",
        "             \n",
        "-    except Exception as e:\n",
        "-        logger.error(f\"Error in query endpoint: {str(e)}\")\n",
        "-        raise HTTPException(\n",
        "-            status_code=500, \n",
        "-            detail=f\"Error processing query: {str(e)}\"\n",
        "-        )\n",
        "+            from rag.chat_agent.backend.retrieval_graph.graph import graph\n",
        "+            state = {\"messages\": [{\"role\": \"user\", \"content\": request.question}]}\n",
        "+            async for response in graph.astream(state, config, stream_mode=\"values\"):\n",
        "+                # Serialize and convert response to JSON\n",
        "+                serialized_response = serialize_response(response)\n",
        "+                yield json.dumps(serialized_response) + \"\\n\"\n",
        "+                \n",
        "+        except Exception as e:\n",
        "+            logger.error(f\"Error in query endpoint: {str(e)}\")\n",
        "+            logger.error(traceback.format_exc())\n",
        "+            error_response = {\n",
        "+                \"error\": str(e),\n",
        "+                \"detail\": \"Error processing query\"\n",
        "+            }\n",
        "+            yield json.dumps(error_response) + \"\\n\"\n",
        "+    \n",
        "+    return StreamingResponse(\n",
        "+        generate_responses(),\n",
        "+        media_type=\"application/x-ndjson\"  # Use NDJSON format for streaming JSON\n",
        "+    )\n",
        " \n",
        " @app.get(\"/indexing_status/{doc_name}\")\n",
        " async def get_indexing_status(doc_name: str):\n"
      ]
    }
  ]
}
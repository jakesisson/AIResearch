{
  "project": "Research Data/agents",
  "repo": "danny-avila/agents",
  "prior_commit": "ce19a8b87353f038fb99122ed66f0bbd2a9f977f",
  "researched_commit": "399dbb01547de86c1b159c30c2e139093628988f",
  "compare_url": "https://github.com/danny-avila/agents/compare/ce19a8b87353f038fb99122ed66f0bbd2a9f977f...399dbb01547de86c1b159c30c2e139093628988f",
  "ahead_by": 1,
  "behind_by": 0,
  "changed_files": [
    {
      "path": ".eslintignore",
      "status": "removed",
      "additions": 0,
      "deletions": 19,
      "patch": "@@ -1,19 +0,0 @@\n-# Ignore build artifacts\n-dist/\n-node_modules/\n-\n-# Ignore configuration files\n-config/\n-routes/\n-\n-# Ignore specific directories\n-src/proto/\n-src/scripts/\n-\n-# Ignore specific files\n-*.js\n-*.mjs\n-script_docs.ts\n-\n-# Ignore the specific test file\n-src/llm/anthropic/llm.spec.ts",
      "patch_lines": [
        "@@ -1,19 +0,0 @@\n",
        "-# Ignore build artifacts\n",
        "-dist/\n",
        "-node_modules/\n",
        "-\n",
        "-# Ignore configuration files\n",
        "-config/\n",
        "-routes/\n",
        "-\n",
        "-# Ignore specific directories\n",
        "-src/proto/\n",
        "-src/scripts/\n",
        "-\n",
        "-# Ignore specific files\n",
        "-*.js\n",
        "-*.mjs\n",
        "-script_docs.ts\n",
        "-\n",
        "-# Ignore the specific test file\n",
        "-src/llm/anthropic/llm.spec.ts\n"
      ]
    },
    {
      "path": ".eslintrc.json",
      "status": "removed",
      "additions": 0,
      "deletions": 65,
      "patch": "@@ -1,65 +0,0 @@\n-{\n-  \"parser\": \"@typescript-eslint/parser\",\n-  \"plugins\": [\"@typescript-eslint\", \"import\"],\n-  \"extends\": [\n-    \"eslint:recommended\",\n-    \"plugin:@typescript-eslint/recommended\",\n-    \"plugin:import/errors\",\n-    \"plugin:import/warnings\",\n-    \"plugin:import/typescript\"\n-  ],\n-  \"env\": {\n-    \"node\": true,\n-    \"es2021\": true\n-  },\n-  \"parserOptions\": {\n-    \"ecmaVersion\": 2021,\n-    \"sourceType\": \"module\",\n-    \"project\": \"./tsconfig.json\"\n-  },\n-  \"settings\": {\n-    \"import/resolver\": {\n-      \"typescript\": {\n-        \"alwaysTryTypes\": true,\n-        \"project\": \"./tsconfig.json\"\n-      }\n-    }\n-  },\n-  \"ignorePatterns\": [\n-    \"dist/**/*\",\n-    \"config/**/*\",\n-    \"routes/**/*\",\n-    \"*.js\",\n-    \"*.mjs\",\n-    \"src/proto/\",\n-    \"src/scripts/\",\n-    \"src/llm/anthropic/llm.spec.ts\"\n-  ],\n-  \"rules\": {\n-    \"no-trailing-spaces\": \"error\",\n-    \"indent\": [\"error\", 2],\n-    \"linebreak-style\": [\"error\", \"unix\"],\n-    \"quotes\": [\"error\", \"single\"],\n-    \"semi\": [\"error\", \"always\"],\n-    \"no-multiple-empty-lines\": [\"error\", { \"max\": 1, \"maxEOF\": 0 }],\n-    \"no-console\": \"warn\",\n-    \"prefer-const\": \"error\",\n-    \"@typescript-eslint/no-unused-vars\": [\n-      \"error\",\n-      { \"argsIgnorePattern\": \"^_\" }\n-    ],\n-    \"@typescript-eslint/consistent-type-assertions\": \"error\",\n-    \"@typescript-eslint/explicit-function-return-type\": \"error\",\n-    \"@typescript-eslint/no-explicit-any\": \"error\",\n-    \"@typescript-eslint/no-unnecessary-condition\": \"warn\",\n-    \"@typescript-eslint/strict-boolean-expressions\": \"warn\"\n-  },\n-  \"overrides\": [\n-    {\n-      \"files\": [\"src/stream.ts\", \"src/utils/logging.ts\"],\n-      \"rules\": {\n-        \"no-console\": \"off\"\n-      }\n-    }\n-  ]\n-}",
      "patch_lines": [
        "@@ -1,65 +0,0 @@\n",
        "-{\n",
        "-  \"parser\": \"@typescript-eslint/parser\",\n",
        "-  \"plugins\": [\"@typescript-eslint\", \"import\"],\n",
        "-  \"extends\": [\n",
        "-    \"eslint:recommended\",\n",
        "-    \"plugin:@typescript-eslint/recommended\",\n",
        "-    \"plugin:import/errors\",\n",
        "-    \"plugin:import/warnings\",\n",
        "-    \"plugin:import/typescript\"\n",
        "-  ],\n",
        "-  \"env\": {\n",
        "-    \"node\": true,\n",
        "-    \"es2021\": true\n",
        "-  },\n",
        "-  \"parserOptions\": {\n",
        "-    \"ecmaVersion\": 2021,\n",
        "-    \"sourceType\": \"module\",\n",
        "-    \"project\": \"./tsconfig.json\"\n",
        "-  },\n",
        "-  \"settings\": {\n",
        "-    \"import/resolver\": {\n",
        "-      \"typescript\": {\n",
        "-        \"alwaysTryTypes\": true,\n",
        "-        \"project\": \"./tsconfig.json\"\n",
        "-      }\n",
        "-    }\n",
        "-  },\n",
        "-  \"ignorePatterns\": [\n",
        "-    \"dist/**/*\",\n",
        "-    \"config/**/*\",\n",
        "-    \"routes/**/*\",\n",
        "-    \"*.js\",\n",
        "-    \"*.mjs\",\n",
        "-    \"src/proto/\",\n",
        "-    \"src/scripts/\",\n",
        "-    \"src/llm/anthropic/llm.spec.ts\"\n",
        "-  ],\n",
        "-  \"rules\": {\n",
        "-    \"no-trailing-spaces\": \"error\",\n",
        "-    \"indent\": [\"error\", 2],\n",
        "-    \"linebreak-style\": [\"error\", \"unix\"],\n",
        "-    \"quotes\": [\"error\", \"single\"],\n",
        "-    \"semi\": [\"error\", \"always\"],\n",
        "-    \"no-multiple-empty-lines\": [\"error\", { \"max\": 1, \"maxEOF\": 0 }],\n",
        "-    \"no-console\": \"warn\",\n",
        "-    \"prefer-const\": \"error\",\n",
        "-    \"@typescript-eslint/no-unused-vars\": [\n",
        "-      \"error\",\n",
        "-      { \"argsIgnorePattern\": \"^_\" }\n",
        "-    ],\n",
        "-    \"@typescript-eslint/consistent-type-assertions\": \"error\",\n",
        "-    \"@typescript-eslint/explicit-function-return-type\": \"error\",\n",
        "-    \"@typescript-eslint/no-explicit-any\": \"error\",\n",
        "-    \"@typescript-eslint/no-unnecessary-condition\": \"warn\",\n",
        "-    \"@typescript-eslint/strict-boolean-expressions\": \"warn\"\n",
        "-  },\n",
        "-  \"overrides\": [\n",
        "-    {\n",
        "-      \"files\": [\"src/stream.ts\", \"src/utils/logging.ts\"],\n",
        "-      \"rules\": {\n",
        "-        \"no-console\": \"off\"\n",
        "-      }\n",
        "-    }\n",
        "-  ]\n",
        "-}\n"
      ]
    },
    {
      "path": ".gitattributes",
      "status": "added",
      "additions": 66,
      "deletions": 0,
      "patch": "@@ -0,0 +1,66 @@\n+# Set default behavior to automatically normalize line endings to LF\n+* text=auto eol=lf\n+\n+# Denote all files that are truly binary and should not be modified.\n+*.png binary\n+*.jpg binary\n+*.jpeg binary\n+*.gif binary\n+*.ico binary\n+*.pdf binary\n+*.woff binary\n+*.woff2 binary\n+*.ttf binary\n+*.eot binary\n+*.zip binary\n+*.tar binary\n+*.gz binary\n+*.7z binary\n+*.mp3 binary\n+*.mp4 binary\n+*.avi binary\n+*.mov binary\n+\n+# Force specific files to use LF\n+*.js text eol=lf\n+*.jsx text eol=lf\n+*.ts text eol=lf\n+*.tsx text eol=lf\n+*.json text eol=lf\n+*.md text eol=lf\n+*.yml text eol=lf\n+*.yaml text eol=lf\n+*.css text eol=lf\n+*.scss text eol=lf\n+*.sass text eol=lf\n+*.less text eol=lf\n+*.html text eol=lf\n+*.xml text eol=lf\n+*.svg text eol=lf\n+*.sh text eol=lf\n+*.bash text eol=lf\n+*.zsh text eol=lf\n+*.fish text eol=lf\n+*.ps1 text eol=lf\n+*.psm1 text eol=lf\n+*.psd1 text eol=lf\n+*.env text eol=lf\n+.gitignore text eol=lf\n+.gitattributes text eol=lf\n+.editorconfig text eol=lf\n+.eslintrc text eol=lf\n+.prettierrc text eol=lf\n+LICENSE text eol=lf\n+README text eol=lf\n+Dockerfile text eol=lf\n+Makefile text eol=lf\n+\n+# Windows-specific files that should keep CRLF\n+*.bat text eol=crlf\n+*.cmd text eol=crlf\n+\n+# Lockfiles should not be modified\n+package-lock.json binary\n+yarn.lock binary\n+pnpm-lock.yaml binary\n+bun.lockb binary",
      "patch_lines": [
        "@@ -0,0 +1,66 @@\n",
        "+# Set default behavior to automatically normalize line endings to LF\n",
        "+* text=auto eol=lf\n",
        "+\n",
        "+# Denote all files that are truly binary and should not be modified.\n",
        "+*.png binary\n",
        "+*.jpg binary\n",
        "+*.jpeg binary\n",
        "+*.gif binary\n",
        "+*.ico binary\n",
        "+*.pdf binary\n",
        "+*.woff binary\n",
        "+*.woff2 binary\n",
        "+*.ttf binary\n",
        "+*.eot binary\n",
        "+*.zip binary\n",
        "+*.tar binary\n",
        "+*.gz binary\n",
        "+*.7z binary\n",
        "+*.mp3 binary\n",
        "+*.mp4 binary\n",
        "+*.avi binary\n",
        "+*.mov binary\n",
        "+\n",
        "+# Force specific files to use LF\n",
        "+*.js text eol=lf\n",
        "+*.jsx text eol=lf\n",
        "+*.ts text eol=lf\n",
        "+*.tsx text eol=lf\n",
        "+*.json text eol=lf\n",
        "+*.md text eol=lf\n",
        "+*.yml text eol=lf\n",
        "+*.yaml text eol=lf\n",
        "+*.css text eol=lf\n",
        "+*.scss text eol=lf\n",
        "+*.sass text eol=lf\n",
        "+*.less text eol=lf\n",
        "+*.html text eol=lf\n",
        "+*.xml text eol=lf\n",
        "+*.svg text eol=lf\n",
        "+*.sh text eol=lf\n",
        "+*.bash text eol=lf\n",
        "+*.zsh text eol=lf\n",
        "+*.fish text eol=lf\n",
        "+*.ps1 text eol=lf\n",
        "+*.psm1 text eol=lf\n",
        "+*.psd1 text eol=lf\n",
        "+*.env text eol=lf\n",
        "+.gitignore text eol=lf\n",
        "+.gitattributes text eol=lf\n",
        "+.editorconfig text eol=lf\n",
        "+.eslintrc text eol=lf\n",
        "+.prettierrc text eol=lf\n",
        "+LICENSE text eol=lf\n",
        "+README text eol=lf\n",
        "+Dockerfile text eol=lf\n",
        "+Makefile text eol=lf\n",
        "+\n",
        "+# Windows-specific files that should keep CRLF\n",
        "+*.bat text eol=crlf\n",
        "+*.cmd text eol=crlf\n",
        "+\n",
        "+# Lockfiles should not be modified\n",
        "+package-lock.json binary\n",
        "+yarn.lock binary\n",
        "+pnpm-lock.yaml binary\n",
        "+bun.lockb binary\n"
      ]
    },
    {
      "path": "bun.lockb",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "docs/multi-agent-patterns.md",
      "status": "added",
      "additions": 153,
      "deletions": 0,
      "patch": "@@ -0,0 +1,153 @@\n+# Multi-Agent Patterns in Agentus\n+\n+This document explains the different multi-agent patterns supported by the `MultiAgentGraph` class.\n+\n+## Edge Types\n+\n+The `MultiAgentGraph` supports two types of edges between agents:\n+\n+### 1. Handoff Edges (Dynamic Routing)\n+\n+**Use Case**: When an agent needs to dynamically decide which agent to call next based on the conversation context.\n+\n+**How it works**: Creates transfer tools that agents can use to explicitly hand off control to another agent.\n+\n+**Example**:\n+\n+```typescript\n+const edges: t.GraphEdge[] = [\n+  {\n+    from: 'classifier',\n+    to: ['technical_expert', 'business_expert', 'general_assistant'],\n+    description: 'Route to appropriate expert based on query type',\n+    edgeType: 'handoff', // Optional - this is the default for conditional edges\n+    condition: (state) => {\n+      // Dynamic routing logic\n+      if (state.messages[0].content.includes('technical')) {\n+        return 'technical_expert';\n+      }\n+      // ... more logic\n+    },\n+  },\n+];\n+```\n+\n+**Default behavior**:\n+\n+- Single-to-single edges default to handoff\n+- Edges with conditions are always handoff\n+- Edges with `edgeType: 'handoff'` are handoff\n+\n+### 2. Parallel Edges (Automatic Fan-out/Fan-in)\n+\n+**Use Case**: When you want multiple agents to process simultaneously without explicit handoff logic.\n+\n+**How it works**: Creates direct graph edges that cause automatic parallel execution.\n+\n+**Example**:\n+\n+```typescript\n+const edges: t.GraphEdge[] = [\n+  {\n+    from: 'researcher',\n+    to: ['analyst1', 'analyst2', 'analyst3'], // Fan-out\n+    description: 'Distribute to all analysts for parallel processing',\n+    edgeType: 'direct', // Explicit parallel execution\n+  },\n+  {\n+    from: ['analyst1', 'analyst2', 'analyst3'], // Fan-in\n+    to: 'summarizer',\n+    description: 'Aggregate results from all analysts',\n+    edgeType: 'direct',\n+  },\n+];\n+```\n+\n+**Default behavior**:\n+\n+- Single-to-multiple edges default to parallel (fan-out)\n+- Multiple-to-single edges should explicitly set `edgeType: 'direct'` for fan-in\n+\n+## Common Patterns\n+\n+### 1. Sequential Handoffs\n+\n+```typescript\n+// Flight assistant can transfer to hotel assistant and vice versa\n+const edges = [\n+  { from: 'flight_assistant', to: 'hotel_assistant' },\n+  { from: 'hotel_assistant', to: 'flight_assistant' },\n+];\n+```\n+\n+### 2. Supervisor Pattern (Handoff)\n+\n+```typescript\n+// Supervisor decides which expert to route to\n+const edges = [\n+  {\n+    from: 'supervisor',\n+    to: ['expert1', 'expert2', 'expert3'],\n+    condition: (state) => decideExpert(state),\n+  },\n+  { from: 'expert1', to: 'supervisor' },\n+  { from: 'expert2', to: 'supervisor' },\n+  { from: 'expert3', to: 'supervisor' },\n+];\n+```\n+\n+### 3. Map-Reduce Pattern (Parallel)\n+\n+```typescript\n+// Distribute work and aggregate results\n+const edges = [\n+  {\n+    from: 'coordinator',\n+    to: ['worker1', 'worker2', 'worker3'],\n+    edgeType: 'direct', // Fan-out\n+  },\n+  {\n+    from: ['worker1', 'worker2', 'worker3'],\n+    to: 'aggregator',\n+    edgeType: 'direct', // Fan-in\n+  },\n+];\n+```\n+\n+### 4. Hybrid Pattern\n+\n+```typescript\n+// Mix of handoff and parallel\n+const edges = [\n+  // Classifier uses handoff to route\n+  {\n+    from: 'classifier',\n+    to: ['path_a', 'path_b'],\n+    condition: (state) => choosePath(state),\n+  },\n+  // Path A uses parallel processing\n+  {\n+    from: 'path_a',\n+    to: ['processor1', 'processor2'],\n+    edgeType: 'direct',\n+  },\n+  // Processors converge\n+  {\n+    from: ['processor1', 'processor2'],\n+    to: 'finalizer',\n+    edgeType: 'direct',\n+  },\n+];\n+```\n+\n+## Important Notes\n+\n+1. **Event Streaming**: When using parallel edges, you may see \"Run ID not found in run map\" errors in the console. These are harmless and can be ignored - they occur because LangGraph creates new run IDs for parallel executions that the event stream handler doesn't track.\n+\n+2. **State Management**: All agents share the same state (messages). Parallel agents see the same state snapshot and their updates are merged.\n+\n+3. **Tool Creation**:\n+   - Handoff edges create transfer tools (e.g., `transfer_to_agent_name`)\n+   - Parallel edges create direct graph connections (no tools)\n+\n+4. **Performance**: Parallel execution can significantly speed up processing when agents perform independent work.",
      "patch_lines": [
        "@@ -0,0 +1,153 @@\n",
        "+# Multi-Agent Patterns in Agentus\n",
        "+\n",
        "+This document explains the different multi-agent patterns supported by the `MultiAgentGraph` class.\n",
        "+\n",
        "+## Edge Types\n",
        "+\n",
        "+The `MultiAgentGraph` supports two types of edges between agents:\n",
        "+\n",
        "+### 1. Handoff Edges (Dynamic Routing)\n",
        "+\n",
        "+**Use Case**: When an agent needs to dynamically decide which agent to call next based on the conversation context.\n",
        "+\n",
        "+**How it works**: Creates transfer tools that agents can use to explicitly hand off control to another agent.\n",
        "+\n",
        "+**Example**:\n",
        "+\n",
        "+```typescript\n",
        "+const edges: t.GraphEdge[] = [\n",
        "+  {\n",
        "+    from: 'classifier',\n",
        "+    to: ['technical_expert', 'business_expert', 'general_assistant'],\n",
        "+    description: 'Route to appropriate expert based on query type',\n",
        "+    edgeType: 'handoff', // Optional - this is the default for conditional edges\n",
        "+    condition: (state) => {\n",
        "+      // Dynamic routing logic\n",
        "+      if (state.messages[0].content.includes('technical')) {\n",
        "+        return 'technical_expert';\n",
        "+      }\n",
        "+      // ... more logic\n",
        "+    },\n",
        "+  },\n",
        "+];\n",
        "+```\n",
        "+\n",
        "+**Default behavior**:\n",
        "+\n",
        "+- Single-to-single edges default to handoff\n",
        "+- Edges with conditions are always handoff\n",
        "+- Edges with `edgeType: 'handoff'` are handoff\n",
        "+\n",
        "+### 2. Parallel Edges (Automatic Fan-out/Fan-in)\n",
        "+\n",
        "+**Use Case**: When you want multiple agents to process simultaneously without explicit handoff logic.\n",
        "+\n",
        "+**How it works**: Creates direct graph edges that cause automatic parallel execution.\n",
        "+\n",
        "+**Example**:\n",
        "+\n",
        "+```typescript\n",
        "+const edges: t.GraphEdge[] = [\n",
        "+  {\n",
        "+    from: 'researcher',\n",
        "+    to: ['analyst1', 'analyst2', 'analyst3'], // Fan-out\n",
        "+    description: 'Distribute to all analysts for parallel processing',\n",
        "+    edgeType: 'direct', // Explicit parallel execution\n",
        "+  },\n",
        "+  {\n",
        "+    from: ['analyst1', 'analyst2', 'analyst3'], // Fan-in\n",
        "+    to: 'summarizer',\n",
        "+    description: 'Aggregate results from all analysts',\n",
        "+    edgeType: 'direct',\n",
        "+  },\n",
        "+];\n",
        "+```\n",
        "+\n",
        "+**Default behavior**:\n",
        "+\n",
        "+- Single-to-multiple edges default to parallel (fan-out)\n",
        "+- Multiple-to-single edges should explicitly set `edgeType: 'direct'` for fan-in\n",
        "+\n",
        "+## Common Patterns\n",
        "+\n",
        "+### 1. Sequential Handoffs\n",
        "+\n",
        "+```typescript\n",
        "+// Flight assistant can transfer to hotel assistant and vice versa\n",
        "+const edges = [\n",
        "+  { from: 'flight_assistant', to: 'hotel_assistant' },\n",
        "+  { from: 'hotel_assistant', to: 'flight_assistant' },\n",
        "+];\n",
        "+```\n",
        "+\n",
        "+### 2. Supervisor Pattern (Handoff)\n",
        "+\n",
        "+```typescript\n",
        "+// Supervisor decides which expert to route to\n",
        "+const edges = [\n",
        "+  {\n",
        "+    from: 'supervisor',\n",
        "+    to: ['expert1', 'expert2', 'expert3'],\n",
        "+    condition: (state) => decideExpert(state),\n",
        "+  },\n",
        "+  { from: 'expert1', to: 'supervisor' },\n",
        "+  { from: 'expert2', to: 'supervisor' },\n",
        "+  { from: 'expert3', to: 'supervisor' },\n",
        "+];\n",
        "+```\n",
        "+\n",
        "+### 3. Map-Reduce Pattern (Parallel)\n",
        "+\n",
        "+```typescript\n",
        "+// Distribute work and aggregate results\n",
        "+const edges = [\n",
        "+  {\n",
        "+    from: 'coordinator',\n",
        "+    to: ['worker1', 'worker2', 'worker3'],\n",
        "+    edgeType: 'direct', // Fan-out\n",
        "+  },\n",
        "+  {\n",
        "+    from: ['worker1', 'worker2', 'worker3'],\n",
        "+    to: 'aggregator',\n",
        "+    edgeType: 'direct', // Fan-in\n",
        "+  },\n",
        "+];\n",
        "+```\n",
        "+\n",
        "+### 4. Hybrid Pattern\n",
        "+\n",
        "+```typescript\n",
        "+// Mix of handoff and parallel\n",
        "+const edges = [\n",
        "+  // Classifier uses handoff to route\n",
        "+  {\n",
        "+    from: 'classifier',\n",
        "+    to: ['path_a', 'path_b'],\n",
        "+    condition: (state) => choosePath(state),\n",
        "+  },\n",
        "+  // Path A uses parallel processing\n",
        "+  {\n",
        "+    from: 'path_a',\n",
        "+    to: ['processor1', 'processor2'],\n",
        "+    edgeType: 'direct',\n",
        "+  },\n",
        "+  // Processors converge\n",
        "+  {\n",
        "+    from: ['processor1', 'processor2'],\n",
        "+    to: 'finalizer',\n",
        "+    edgeType: 'direct',\n",
        "+  },\n",
        "+];\n",
        "+```\n",
        "+\n",
        "+## Important Notes\n",
        "+\n",
        "+1. **Event Streaming**: When using parallel edges, you may see \"Run ID not found in run map\" errors in the console. These are harmless and can be ignored - they occur because LangGraph creates new run IDs for parallel executions that the event stream handler doesn't track.\n",
        "+\n",
        "+2. **State Management**: All agents share the same state (messages). Parallel agents see the same state snapshot and their updates are merged.\n",
        "+\n",
        "+3. **Tool Creation**:\n",
        "+   - Handoff edges create transfer tools (e.g., `transfer_to_agent_name`)\n",
        "+   - Parallel edges create direct graph connections (no tools)\n",
        "+\n",
        "+4. **Performance**: Parallel execution can significantly speed up processing when agents perform independent work.\n"
      ]
    },
    {
      "path": "docs/prompt-runnable-feature.md",
      "status": "added",
      "additions": 96,
      "deletions": 0,
      "patch": "@@ -0,0 +1,96 @@\n+# Prompt Runnable Feature\n+\n+## Overview\n+\n+The `promptInstructions` feature allows agents to dynamically append a human message after all state messages, ensuring AI agents respond appropriately in complex multi-agent scenarios.\n+\n+## Problem Solved\n+\n+In parallel multi-agent systems, when multiple AI agents send messages to a downstream agent (like a summarizer), the downstream agent may not respond because it only sees AI messages without a human prompt. This feature adds a dynamic human message to trigger a response.\n+\n+## Implementation\n+\n+### 1. Agent Configuration\n+\n+Add `promptInstructions` to your agent configuration:\n+\n+```typescript\n+const agent: AgentInputs = {\n+  agentId: 'summarizer',\n+  provider: Providers.ANTHROPIC,\n+  instructions: 'You are a summary expert...',\n+  // Static prompt\n+  promptInstructions: 'Please provide a summary of the above analyses.',\n+  // OR dynamic prompt based on message state\n+  promptInstructions: (messages) => {\n+    const analystCount = messages.filter(\n+      (msg) => msg._getType() === 'ai' && msg.name?.includes('analyst')\n+    ).length;\n+\n+    if (analystCount >= 3) {\n+      return 'Please synthesize the three analyses above.';\n+    }\n+    return undefined; // No prompt yet\n+  },\n+};\n+```\n+\n+### 2. How It Works\n+\n+1. The `promptInstructions` is converted to a `promptRunnable` during agent initialization\n+2. This runnable is applied after all message processing, including system messages\n+3. If the instructions are a function, it receives the current messages and can decide whether to add a prompt\n+4. If a prompt is returned, it's appended as a `HumanMessage`\n+\n+### 3. Use Cases\n+\n+#### Parallel Processing Aggregation\n+\n+When multiple agents process in parallel and send results to an aggregator:\n+\n+```typescript\n+promptInstructions: (messages) => {\n+  const inputCount = countInputMessages(messages);\n+  if (inputCount >= expectedCount) {\n+    return 'Based on all inputs above, please provide your synthesis.';\n+  }\n+  return undefined;\n+};\n+```\n+\n+#### Conditional Prompting\n+\n+Add prompts only under certain conditions:\n+\n+```typescript\n+promptInstructions: (messages) => {\n+  const lastMessage = messages[messages.length - 1];\n+  if (needsFollowUp(lastMessage)) {\n+    return 'Please elaborate on your previous response.';\n+  }\n+  return undefined;\n+};\n+```\n+\n+#### Static Prompts\n+\n+For simple cases where you always want to add a prompt:\n+\n+```typescript\n+promptInstructions: 'Please provide your analysis based on the conversation above.';\n+```\n+\n+## Benefits\n+\n+1. **Flexibility**: Use static strings or dynamic functions\n+2. **Context Awareness**: Functions can inspect the full message history\n+3. **Clean Separation**: Keeps prompt logic separate from agent instructions\n+4. **Runnable Pattern**: Consistent with existing `systemRunnable` pattern\n+5. **Composability**: Runnables can be chained and composed\n+\n+## Technical Details\n+\n+- Implemented as a `RunnableLambda` that processes messages\n+- Applied in `Graph.createCallModel()` after message pruning\n+- Async to support dynamic imports\n+- Returns messages unchanged if no prompt is needed",
      "patch_lines": [
        "@@ -0,0 +1,96 @@\n",
        "+# Prompt Runnable Feature\n",
        "+\n",
        "+## Overview\n",
        "+\n",
        "+The `promptInstructions` feature allows agents to dynamically append a human message after all state messages, ensuring AI agents respond appropriately in complex multi-agent scenarios.\n",
        "+\n",
        "+## Problem Solved\n",
        "+\n",
        "+In parallel multi-agent systems, when multiple AI agents send messages to a downstream agent (like a summarizer), the downstream agent may not respond because it only sees AI messages without a human prompt. This feature adds a dynamic human message to trigger a response.\n",
        "+\n",
        "+## Implementation\n",
        "+\n",
        "+### 1. Agent Configuration\n",
        "+\n",
        "+Add `promptInstructions` to your agent configuration:\n",
        "+\n",
        "+```typescript\n",
        "+const agent: AgentInputs = {\n",
        "+  agentId: 'summarizer',\n",
        "+  provider: Providers.ANTHROPIC,\n",
        "+  instructions: 'You are a summary expert...',\n",
        "+  // Static prompt\n",
        "+  promptInstructions: 'Please provide a summary of the above analyses.',\n",
        "+  // OR dynamic prompt based on message state\n",
        "+  promptInstructions: (messages) => {\n",
        "+    const analystCount = messages.filter(\n",
        "+      (msg) => msg._getType() === 'ai' && msg.name?.includes('analyst')\n",
        "+    ).length;\n",
        "+\n",
        "+    if (analystCount >= 3) {\n",
        "+      return 'Please synthesize the three analyses above.';\n",
        "+    }\n",
        "+    return undefined; // No prompt yet\n",
        "+  },\n",
        "+};\n",
        "+```\n",
        "+\n",
        "+### 2. How It Works\n",
        "+\n",
        "+1. The `promptInstructions` is converted to a `promptRunnable` during agent initialization\n",
        "+2. This runnable is applied after all message processing, including system messages\n",
        "+3. If the instructions are a function, it receives the current messages and can decide whether to add a prompt\n",
        "+4. If a prompt is returned, it's appended as a `HumanMessage`\n",
        "+\n",
        "+### 3. Use Cases\n",
        "+\n",
        "+#### Parallel Processing Aggregation\n",
        "+\n",
        "+When multiple agents process in parallel and send results to an aggregator:\n",
        "+\n",
        "+```typescript\n",
        "+promptInstructions: (messages) => {\n",
        "+  const inputCount = countInputMessages(messages);\n",
        "+  if (inputCount >= expectedCount) {\n",
        "+    return 'Based on all inputs above, please provide your synthesis.';\n",
        "+  }\n",
        "+  return undefined;\n",
        "+};\n",
        "+```\n",
        "+\n",
        "+#### Conditional Prompting\n",
        "+\n",
        "+Add prompts only under certain conditions:\n",
        "+\n",
        "+```typescript\n",
        "+promptInstructions: (messages) => {\n",
        "+  const lastMessage = messages[messages.length - 1];\n",
        "+  if (needsFollowUp(lastMessage)) {\n",
        "+    return 'Please elaborate on your previous response.';\n",
        "+  }\n",
        "+  return undefined;\n",
        "+};\n",
        "+```\n",
        "+\n",
        "+#### Static Prompts\n",
        "+\n",
        "+For simple cases where you always want to add a prompt:\n",
        "+\n",
        "+```typescript\n",
        "+promptInstructions: 'Please provide your analysis based on the conversation above.';\n",
        "+```\n",
        "+\n",
        "+## Benefits\n",
        "+\n",
        "+1. **Flexibility**: Use static strings or dynamic functions\n",
        "+2. **Context Awareness**: Functions can inspect the full message history\n",
        "+3. **Clean Separation**: Keeps prompt logic separate from agent instructions\n",
        "+4. **Runnable Pattern**: Consistent with existing `systemRunnable` pattern\n",
        "+5. **Composability**: Runnables can be chained and composed\n",
        "+\n",
        "+## Technical Details\n",
        "+\n",
        "+- Implemented as a `RunnableLambda` that processes messages\n",
        "+- Applied in `Graph.createCallModel()` after message pruning\n",
        "+- Async to support dynamic imports\n",
        "+- Returns messages unchanged if no prompt is needed\n"
      ]
    },
    {
      "path": "eslint.config.mjs",
      "status": "modified",
      "additions": 3,
      "deletions": 0,
      "patch": "@@ -82,6 +82,9 @@ export default defineConfig([globalIgnores([\n \n         \"@typescript-eslint/no-unused-vars\": [\"error\", {\n             argsIgnorePattern: \"^_\",\n+            varsIgnorePattern: \"^_\",\n+            caughtErrorsIgnorePattern: \"^_\",\n+            destructuredArrayIgnorePattern: \"^_\"\n         }],\n \n         \"@typescript-eslint/consistent-type-assertions\": \"error\",",
      "patch_lines": [
        "@@ -82,6 +82,9 @@ export default defineConfig([globalIgnores([\n",
        " \n",
        "         \"@typescript-eslint/no-unused-vars\": [\"error\", {\n",
        "             argsIgnorePattern: \"^_\",\n",
        "+            varsIgnorePattern: \"^_\",\n",
        "+            caughtErrorsIgnorePattern: \"^_\",\n",
        "+            destructuredArrayIgnorePattern: \"^_\"\n",
        "         }],\n",
        " \n",
        "         \"@typescript-eslint/consistent-type-assertions\": \"error\",\n"
      ]
    },
    {
      "path": "fix-line-endings.sh",
      "status": "added",
      "additions": 68,
      "deletions": 0,
      "patch": "@@ -0,0 +1,68 @@\n+#!/bin/bash\n+\n+# Script to convert all CRLF line endings to LF in the project\n+\n+echo \"Fixing line endings from CRLF to LF...\"\n+echo\n+\n+# Function to convert files\n+convert_files() {\n+    local pattern=$1\n+    local description=$2\n+    \n+    echo \"Converting $description files...\"\n+    find . -type f -name \"$pattern\" \\\n+        -not -path \"./node_modules/*\" \\\n+        -not -path \"./dist/*\" \\\n+        -not -path \"./.git/*\" \\\n+        -not -path \"./coverage/*\" \\\n+        -not -path \"./types/*\" \\\n+        -print0 | while IFS= read -r -d '' file; do\n+        # Check if file has CRLF endings\n+        if file \"$file\" | grep -q \"CRLF\"; then\n+            echo \"  Converting: $file\"\n+            # Convert CRLF to LF\n+            sed -i 's/\\r$//' \"$file\"\n+        fi\n+    done\n+}\n+\n+# Convert TypeScript and JavaScript files\n+convert_files \"*.ts\" \"TypeScript\"\n+convert_files \"*.tsx\" \"TypeScript JSX\"\n+convert_files \"*.js\" \"JavaScript\"\n+convert_files \"*.jsx\" \"JavaScript JSX\"\n+convert_files \"*.mjs\" \"JavaScript module\"\n+\n+# Convert configuration files\n+convert_files \"*.json\" \"JSON\"\n+convert_files \"*.yml\" \"YAML\"\n+convert_files \"*.yaml\" \"YAML\"\n+convert_files \".eslintrc*\" \"ESLint config\"\n+convert_files \".prettierrc*\" \"Prettier config\"\n+convert_files \"*.md\" \"Markdown\"\n+\n+# Convert other common files\n+convert_files \"*.sh\" \"Shell script\"\n+convert_files \"Dockerfile*\" \"Docker\"\n+convert_files \".env*\" \"Environment\"\n+convert_files \".gitignore\" \"Git ignore\"\n+convert_files \"LICENSE\" \"License\"\n+\n+echo\n+echo \"Line ending conversion complete!\"\n+\n+# Show statistics\n+echo\n+echo \"Checking for any remaining CRLF files...\"\n+remaining=$(find . -type f \\( -name \"*.ts\" -o -name \"*.tsx\" -o -name \"*.js\" -o -name \"*.jsx\" -o -name \"*.json\" \\) \\\n+    -not -path \"./node_modules/*\" \\\n+    -not -path \"./dist/*\" \\\n+    -not -path \"./.git/*\" \\\n+    -exec file {} \\; | grep -c \"CRLF\")\n+\n+if [ \"$remaining\" -eq 0 ]; then\n+    echo \"\u2705 All files now use LF line endings!\"\n+else\n+    echo \"\u26a0\ufe0f  Found $remaining files still with CRLF endings\"\n+fi",
      "patch_lines": [
        "@@ -0,0 +1,68 @@\n",
        "+#!/bin/bash\n",
        "+\n",
        "+# Script to convert all CRLF line endings to LF in the project\n",
        "+\n",
        "+echo \"Fixing line endings from CRLF to LF...\"\n",
        "+echo\n",
        "+\n",
        "+# Function to convert files\n",
        "+convert_files() {\n",
        "+    local pattern=$1\n",
        "+    local description=$2\n",
        "+    \n",
        "+    echo \"Converting $description files...\"\n",
        "+    find . -type f -name \"$pattern\" \\\n",
        "+        -not -path \"./node_modules/*\" \\\n",
        "+        -not -path \"./dist/*\" \\\n",
        "+        -not -path \"./.git/*\" \\\n",
        "+        -not -path \"./coverage/*\" \\\n",
        "+        -not -path \"./types/*\" \\\n",
        "+        -print0 | while IFS= read -r -d '' file; do\n",
        "+        # Check if file has CRLF endings\n",
        "+        if file \"$file\" | grep -q \"CRLF\"; then\n",
        "+            echo \"  Converting: $file\"\n",
        "+            # Convert CRLF to LF\n",
        "+            sed -i 's/\\r$//' \"$file\"\n",
        "+        fi\n",
        "+    done\n",
        "+}\n",
        "+\n",
        "+# Convert TypeScript and JavaScript files\n",
        "+convert_files \"*.ts\" \"TypeScript\"\n",
        "+convert_files \"*.tsx\" \"TypeScript JSX\"\n",
        "+convert_files \"*.js\" \"JavaScript\"\n",
        "+convert_files \"*.jsx\" \"JavaScript JSX\"\n",
        "+convert_files \"*.mjs\" \"JavaScript module\"\n",
        "+\n",
        "+# Convert configuration files\n",
        "+convert_files \"*.json\" \"JSON\"\n",
        "+convert_files \"*.yml\" \"YAML\"\n",
        "+convert_files \"*.yaml\" \"YAML\"\n",
        "+convert_files \".eslintrc*\" \"ESLint config\"\n",
        "+convert_files \".prettierrc*\" \"Prettier config\"\n",
        "+convert_files \"*.md\" \"Markdown\"\n",
        "+\n",
        "+# Convert other common files\n",
        "+convert_files \"*.sh\" \"Shell script\"\n",
        "+convert_files \"Dockerfile*\" \"Docker\"\n",
        "+convert_files \".env*\" \"Environment\"\n",
        "+convert_files \".gitignore\" \"Git ignore\"\n",
        "+convert_files \"LICENSE\" \"License\"\n",
        "+\n",
        "+echo\n",
        "+echo \"Line ending conversion complete!\"\n",
        "+\n",
        "+# Show statistics\n",
        "+echo\n",
        "+echo \"Checking for any remaining CRLF files...\"\n",
        "+remaining=$(find . -type f \\( -name \"*.ts\" -o -name \"*.tsx\" -o -name \"*.js\" -o -name \"*.jsx\" -o -name \"*.json\" \\) \\\n",
        "+    -not -path \"./node_modules/*\" \\\n",
        "+    -not -path \"./dist/*\" \\\n",
        "+    -not -path \"./.git/*\" \\\n",
        "+    -exec file {} \\; | grep -c \"CRLF\")\n",
        "+\n",
        "+if [ \"$remaining\" -eq 0 ]; then\n",
        "+    echo \"\u2705 All files now use LF line endings!\"\n",
        "+else\n",
        "+    echo \"\u26a0\ufe0f  Found $remaining files still with CRLF endings\"\n",
        "+fi\n"
      ]
    },
    {
      "path": "jest.config.mjs",
      "status": "modified",
      "additions": 12,
      "deletions": 1,
      "patch": "@@ -20,7 +20,18 @@ const config = {\n     env: {\n       NODE_ENV: 'test'\n     }\n-  }\n+  },\n+  // Limit concurrent test execution to avoid rate limits\n+  maxWorkers: 7,  // Number of worker processes\n+  maxConcurrency: 1,  // Number of tests that can run in each worker\n+  // Alternative: use percentage of available CPUs\n+  // maxWorkers: '50%',\n+  \n+  // Optional: increase timeout for network requests\n+  testTimeout: 30000,  // 30 seconds (default is 5 seconds)\n+  \n+  // Optional: run tests serially (one at a time) - uncomment if needed\n+  // runInBand: true,\n };\n \n export default config;\n\\ No newline at end of file",
      "patch_lines": [
        "@@ -20,7 +20,18 @@ const config = {\n",
        "     env: {\n",
        "       NODE_ENV: 'test'\n",
        "     }\n",
        "-  }\n",
        "+  },\n",
        "+  // Limit concurrent test execution to avoid rate limits\n",
        "+  maxWorkers: 7,  // Number of worker processes\n",
        "+  maxConcurrency: 1,  // Number of tests that can run in each worker\n",
        "+  // Alternative: use percentage of available CPUs\n",
        "+  // maxWorkers: '50%',\n",
        "+  \n",
        "+  // Optional: increase timeout for network requests\n",
        "+  testTimeout: 30000,  // 30 seconds (default is 5 seconds)\n",
        "+  \n",
        "+  // Optional: run tests serially (one at a time) - uncomment if needed\n",
        "+  // runInBand: true,\n",
        " };\n",
        " \n",
        " export default config;\n",
        "\\ No newline at end of file\n"
      ]
    },
    {
      "path": "lint_analyzer.sh",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "patch": "",
      "patch_lines": []
    },
    {
      "path": "package-lock.json",
      "status": "modified",
      "additions": 51,
      "deletions": 23,
      "patch": "@@ -1,22 +1,22 @@\n {\n   \"name\": \"@librechat/agents\",\n-  \"version\": \"2.4.90\",\n+  \"version\": \"3.0.00-rc1\",\n   \"lockfileVersion\": 3,\n   \"requires\": true,\n   \"packages\": {\n     \"\": {\n       \"name\": \"@librechat/agents\",\n-      \"version\": \"2.4.90\",\n+      \"version\": \"3.0.00-rc1\",\n       \"license\": \"MIT\",\n       \"dependencies\": {\n         \"@langchain/anthropic\": \"^0.3.26\",\n         \"@langchain/aws\": \"^0.1.12\",\n         \"@langchain/community\": \"^0.3.47\",\n-        \"@langchain/core\": \"^0.3.62\",\n+        \"@langchain/core\": \"^0.3.72\",\n         \"@langchain/deepseek\": \"^0.0.2\",\n         \"@langchain/google-genai\": \"^0.2.13\",\n         \"@langchain/google-vertexai\": \"^0.2.13\",\n-        \"@langchain/langgraph\": \"^0.3.4\",\n+        \"@langchain/langgraph\": \"^0.4.9\",\n         \"@langchain/mistralai\": \"^0.2.1\",\n         \"@langchain/ollama\": \"^0.2.3\",\n         \"@langchain/openai\": \"0.5.18\",\n@@ -4345,17 +4345,17 @@\n       }\n     },\n     \"node_modules/@langchain/core\": {\n-      \"version\": \"0.3.62\",\n-      \"resolved\": \"https://registry.npmjs.org/@langchain/core/-/core-0.3.62.tgz\",\n-      \"integrity\": \"sha512-GqRTcoUPnozGRMUcA6QkP7LHL/OvanGdB51Jgb0w7IIPDI3wFugxMHZ4gphnGDtxsD1tQY5ykyEpYNxFK8kl1w==\",\n+      \"version\": \"0.3.72\",\n+      \"resolved\": \"https://registry.npmjs.org/@langchain/core/-/core-0.3.72.tgz\",\n+      \"integrity\": \"sha512-WsGWVZYnlKffj2eEfDocPNiaTRoxyYiLSQdQ7oxZvxGZBqo/90vpjbC33UGK1uPNBM4kT+pkdaol/MnvKUh8TQ==\",\n       \"license\": \"MIT\",\n       \"dependencies\": {\n         \"@cfworker/json-schema\": \"^4.0.2\",\n         \"ansi-styles\": \"^5.0.0\",\n         \"camelcase\": \"6\",\n         \"decamelize\": \"1.2.0\",\n         \"js-tiktoken\": \"^1.0.12\",\n-        \"langsmith\": \"^0.3.33\",\n+        \"langsmith\": \"^0.3.46\",\n         \"mustache\": \"^4.2.0\",\n         \"p-queue\": \"^6.6.2\",\n         \"p-retry\": \"4\",\n@@ -4368,9 +4368,9 @@\n       }\n     },\n     \"node_modules/@langchain/core/node_modules/langsmith\": {\n-      \"version\": \"0.3.33\",\n-      \"resolved\": \"https://registry.npmjs.org/langsmith/-/langsmith-0.3.33.tgz\",\n-      \"integrity\": \"sha512-imNIaBL6+ElE5eMzNHYwFxo6W/6rHlqcaUjCYoIeGdCYWlARxE3CTGKul5DJnaUgGP2CTLFeNXyvRx5HWC/4KQ==\",\n+      \"version\": \"0.3.66\",\n+      \"resolved\": \"https://registry.npmjs.org/langsmith/-/langsmith-0.3.66.tgz\",\n+      \"integrity\": \"sha512-d50FJ25HPAT2e/6u7oPAYFYH7uvVhxf7vThAOE5tP6YFIUHwLMBmJj8R4Z7APp5jF4/m8upvbK4J4jP5UIN+Eg==\",\n       \"license\": \"MIT\",\n       \"dependencies\": {\n         \"@types/uuid\": \"^10.0.0\",\n@@ -4382,9 +4382,21 @@\n         \"uuid\": \"^10.0.0\"\n       },\n       \"peerDependencies\": {\n+        \"@opentelemetry/api\": \"*\",\n+        \"@opentelemetry/exporter-trace-otlp-proto\": \"*\",\n+        \"@opentelemetry/sdk-trace-base\": \"*\",\n         \"openai\": \"*\"\n       },\n       \"peerDependenciesMeta\": {\n+        \"@opentelemetry/api\": {\n+          \"optional\": true\n+        },\n+        \"@opentelemetry/exporter-trace-otlp-proto\": {\n+          \"optional\": true\n+        },\n+        \"@opentelemetry/sdk-trace-base\": {\n+          \"optional\": true\n+        },\n         \"openai\": {\n           \"optional\": true\n         }\n@@ -4506,13 +4518,13 @@\n       }\n     },\n     \"node_modules/@langchain/langgraph\": {\n-      \"version\": \"0.3.4\",\n-      \"resolved\": \"https://registry.npmjs.org/@langchain/langgraph/-/langgraph-0.3.4.tgz\",\n-      \"integrity\": \"sha512-Vuja8Qtu3Zjx7k4fK7Cnw+p8gtvIRPciWp9btPhAs3aUo6aBgOJOZVcK5Ii3mHfEHK/aQmRElR0x/u/YwykOrg==\",\n+      \"version\": \"0.4.9\",\n+      \"resolved\": \"https://registry.npmjs.org/@langchain/langgraph/-/langgraph-0.4.9.tgz\",\n+      \"integrity\": \"sha512-+rcdTGi4Ium4X/VtIX3Zw4RhxEkYWpwUyz806V6rffjHOAMamg6/WZDxpJbrP33RV/wJG1GH12Z29oX3Pqq3Aw==\",\n       \"license\": \"MIT\",\n       \"dependencies\": {\n-        \"@langchain/langgraph-checkpoint\": \"~0.0.18\",\n-        \"@langchain/langgraph-sdk\": \"~0.0.32\",\n+        \"@langchain/langgraph-checkpoint\": \"^0.1.1\",\n+        \"@langchain/langgraph-sdk\": \"~0.1.0\",\n         \"uuid\": \"^10.0.0\",\n         \"zod\": \"^3.25.32\"\n       },\n@@ -4530,9 +4542,9 @@\n       }\n     },\n     \"node_modules/@langchain/langgraph-checkpoint\": {\n-      \"version\": \"0.0.18\",\n-      \"resolved\": \"https://registry.npmjs.org/@langchain/langgraph-checkpoint/-/langgraph-checkpoint-0.0.18.tgz\",\n-      \"integrity\": \"sha512-IS7zJj36VgY+4pf8ZjsVuUWef7oTwt1y9ylvwu0aLuOn1d0fg05Om9DLm3v2GZ2Df6bhLV1kfWAM0IAl9O5rQQ==\",\n+      \"version\": \"0.1.1\",\n+      \"resolved\": \"https://registry.npmjs.org/@langchain/langgraph-checkpoint/-/langgraph-checkpoint-0.1.1.tgz\",\n+      \"integrity\": \"sha512-h2bP0RUikQZu0Um1ZUPErQLXyhzroJqKRbRcxYRTAh49oNlsfeq4A3K4YEDRbGGuyPZI/Jiqwhks1wZwY73AZw==\",\n       \"license\": \"MIT\",\n       \"dependencies\": {\n         \"uuid\": \"^10.0.0\"\n@@ -4541,7 +4553,7 @@\n         \"node\": \">=18\"\n       },\n       \"peerDependencies\": {\n-        \"@langchain/core\": \">=0.2.31 <0.4.0\"\n+        \"@langchain/core\": \">=0.2.31 <0.4.0 || ^1.0.0-alpha\"\n       }\n     },\n     \"node_modules/@langchain/langgraph-checkpoint/node_modules/uuid\": {\n@@ -4558,15 +4570,31 @@\n       }\n     },\n     \"node_modules/@langchain/langgraph-sdk\": {\n-      \"version\": \"0.0.32\",\n-      \"resolved\": \"https://registry.npmjs.org/@langchain/langgraph-sdk/-/langgraph-sdk-0.0.32.tgz\",\n-      \"integrity\": \"sha512-KQyM9kLO7T6AxwNrceajH7JOybP3pYpvUPnhiI2rrVndI1WyZUJ1eVC1e722BVRAPi6o+WcoTT4uMSZVinPOtA==\",\n+      \"version\": \"0.1.0\",\n+      \"resolved\": \"https://registry.npmjs.org/@langchain/langgraph-sdk/-/langgraph-sdk-0.1.0.tgz\",\n+      \"integrity\": \"sha512-1EKwzwJpgpNqLcRuGG+kLvvhAaPiFWZ9shl/obhL8qDKtYdbR67WCYE+2jUObZ8vKQuCoul16ewJ78g5VrZlKA==\",\n       \"license\": \"MIT\",\n       \"dependencies\": {\n         \"@types/json-schema\": \"^7.0.15\",\n         \"p-queue\": \"^6.6.2\",\n         \"p-retry\": \"4\",\n         \"uuid\": \"^9.0.0\"\n+      },\n+      \"peerDependencies\": {\n+        \"@langchain/core\": \">=0.2.31 <0.4.0\",\n+        \"react\": \"^18 || ^19\",\n+        \"react-dom\": \"^18 || ^19\"\n+      },\n+      \"peerDependenciesMeta\": {\n+        \"@langchain/core\": {\n+          \"optional\": true\n+        },\n+        \"react\": {\n+          \"optional\": true\n+        },\n+        \"react-dom\": {\n+          \"optional\": true\n+        }\n       }\n     },\n     \"node_modules/@langchain/langgraph/node_modules/uuid\": {",
      "patch_lines": [
        "@@ -1,22 +1,22 @@\n",
        " {\n",
        "   \"name\": \"@librechat/agents\",\n",
        "-  \"version\": \"2.4.90\",\n",
        "+  \"version\": \"3.0.00-rc1\",\n",
        "   \"lockfileVersion\": 3,\n",
        "   \"requires\": true,\n",
        "   \"packages\": {\n",
        "     \"\": {\n",
        "       \"name\": \"@librechat/agents\",\n",
        "-      \"version\": \"2.4.90\",\n",
        "+      \"version\": \"3.0.00-rc1\",\n",
        "       \"license\": \"MIT\",\n",
        "       \"dependencies\": {\n",
        "         \"@langchain/anthropic\": \"^0.3.26\",\n",
        "         \"@langchain/aws\": \"^0.1.12\",\n",
        "         \"@langchain/community\": \"^0.3.47\",\n",
        "-        \"@langchain/core\": \"^0.3.62\",\n",
        "+        \"@langchain/core\": \"^0.3.72\",\n",
        "         \"@langchain/deepseek\": \"^0.0.2\",\n",
        "         \"@langchain/google-genai\": \"^0.2.13\",\n",
        "         \"@langchain/google-vertexai\": \"^0.2.13\",\n",
        "-        \"@langchain/langgraph\": \"^0.3.4\",\n",
        "+        \"@langchain/langgraph\": \"^0.4.9\",\n",
        "         \"@langchain/mistralai\": \"^0.2.1\",\n",
        "         \"@langchain/ollama\": \"^0.2.3\",\n",
        "         \"@langchain/openai\": \"0.5.18\",\n",
        "@@ -4345,17 +4345,17 @@\n",
        "       }\n",
        "     },\n",
        "     \"node_modules/@langchain/core\": {\n",
        "-      \"version\": \"0.3.62\",\n",
        "-      \"resolved\": \"https://registry.npmjs.org/@langchain/core/-/core-0.3.62.tgz\",\n",
        "-      \"integrity\": \"sha512-GqRTcoUPnozGRMUcA6QkP7LHL/OvanGdB51Jgb0w7IIPDI3wFugxMHZ4gphnGDtxsD1tQY5ykyEpYNxFK8kl1w==\",\n",
        "+      \"version\": \"0.3.72\",\n",
        "+      \"resolved\": \"https://registry.npmjs.org/@langchain/core/-/core-0.3.72.tgz\",\n",
        "+      \"integrity\": \"sha512-WsGWVZYnlKffj2eEfDocPNiaTRoxyYiLSQdQ7oxZvxGZBqo/90vpjbC33UGK1uPNBM4kT+pkdaol/MnvKUh8TQ==\",\n",
        "       \"license\": \"MIT\",\n",
        "       \"dependencies\": {\n",
        "         \"@cfworker/json-schema\": \"^4.0.2\",\n",
        "         \"ansi-styles\": \"^5.0.0\",\n",
        "         \"camelcase\": \"6\",\n",
        "         \"decamelize\": \"1.2.0\",\n",
        "         \"js-tiktoken\": \"^1.0.12\",\n",
        "-        \"langsmith\": \"^0.3.33\",\n",
        "+        \"langsmith\": \"^0.3.46\",\n",
        "         \"mustache\": \"^4.2.0\",\n",
        "         \"p-queue\": \"^6.6.2\",\n",
        "         \"p-retry\": \"4\",\n",
        "@@ -4368,9 +4368,9 @@\n",
        "       }\n",
        "     },\n",
        "     \"node_modules/@langchain/core/node_modules/langsmith\": {\n",
        "-      \"version\": \"0.3.33\",\n",
        "-      \"resolved\": \"https://registry.npmjs.org/langsmith/-/langsmith-0.3.33.tgz\",\n",
        "-      \"integrity\": \"sha512-imNIaBL6+ElE5eMzNHYwFxo6W/6rHlqcaUjCYoIeGdCYWlARxE3CTGKul5DJnaUgGP2CTLFeNXyvRx5HWC/4KQ==\",\n",
        "+      \"version\": \"0.3.66\",\n",
        "+      \"resolved\": \"https://registry.npmjs.org/langsmith/-/langsmith-0.3.66.tgz\",\n",
        "+      \"integrity\": \"sha512-d50FJ25HPAT2e/6u7oPAYFYH7uvVhxf7vThAOE5tP6YFIUHwLMBmJj8R4Z7APp5jF4/m8upvbK4J4jP5UIN+Eg==\",\n",
        "       \"license\": \"MIT\",\n",
        "       \"dependencies\": {\n",
        "         \"@types/uuid\": \"^10.0.0\",\n",
        "@@ -4382,9 +4382,21 @@\n",
        "         \"uuid\": \"^10.0.0\"\n",
        "       },\n",
        "       \"peerDependencies\": {\n",
        "+        \"@opentelemetry/api\": \"*\",\n",
        "+        \"@opentelemetry/exporter-trace-otlp-proto\": \"*\",\n",
        "+        \"@opentelemetry/sdk-trace-base\": \"*\",\n",
        "         \"openai\": \"*\"\n",
        "       },\n",
        "       \"peerDependenciesMeta\": {\n",
        "+        \"@opentelemetry/api\": {\n",
        "+          \"optional\": true\n",
        "+        },\n",
        "+        \"@opentelemetry/exporter-trace-otlp-proto\": {\n",
        "+          \"optional\": true\n",
        "+        },\n",
        "+        \"@opentelemetry/sdk-trace-base\": {\n",
        "+          \"optional\": true\n",
        "+        },\n",
        "         \"openai\": {\n",
        "           \"optional\": true\n",
        "         }\n",
        "@@ -4506,13 +4518,13 @@\n",
        "       }\n",
        "     },\n",
        "     \"node_modules/@langchain/langgraph\": {\n",
        "-      \"version\": \"0.3.4\",\n",
        "-      \"resolved\": \"https://registry.npmjs.org/@langchain/langgraph/-/langgraph-0.3.4.tgz\",\n",
        "-      \"integrity\": \"sha512-Vuja8Qtu3Zjx7k4fK7Cnw+p8gtvIRPciWp9btPhAs3aUo6aBgOJOZVcK5Ii3mHfEHK/aQmRElR0x/u/YwykOrg==\",\n",
        "+      \"version\": \"0.4.9\",\n",
        "+      \"resolved\": \"https://registry.npmjs.org/@langchain/langgraph/-/langgraph-0.4.9.tgz\",\n",
        "+      \"integrity\": \"sha512-+rcdTGi4Ium4X/VtIX3Zw4RhxEkYWpwUyz806V6rffjHOAMamg6/WZDxpJbrP33RV/wJG1GH12Z29oX3Pqq3Aw==\",\n",
        "       \"license\": \"MIT\",\n",
        "       \"dependencies\": {\n",
        "-        \"@langchain/langgraph-checkpoint\": \"~0.0.18\",\n",
        "-        \"@langchain/langgraph-sdk\": \"~0.0.32\",\n",
        "+        \"@langchain/langgraph-checkpoint\": \"^0.1.1\",\n",
        "+        \"@langchain/langgraph-sdk\": \"~0.1.0\",\n",
        "         \"uuid\": \"^10.0.0\",\n",
        "         \"zod\": \"^3.25.32\"\n",
        "       },\n",
        "@@ -4530,9 +4542,9 @@\n",
        "       }\n",
        "     },\n",
        "     \"node_modules/@langchain/langgraph-checkpoint\": {\n",
        "-      \"version\": \"0.0.18\",\n",
        "-      \"resolved\": \"https://registry.npmjs.org/@langchain/langgraph-checkpoint/-/langgraph-checkpoint-0.0.18.tgz\",\n",
        "-      \"integrity\": \"sha512-IS7zJj36VgY+4pf8ZjsVuUWef7oTwt1y9ylvwu0aLuOn1d0fg05Om9DLm3v2GZ2Df6bhLV1kfWAM0IAl9O5rQQ==\",\n",
        "+      \"version\": \"0.1.1\",\n",
        "+      \"resolved\": \"https://registry.npmjs.org/@langchain/langgraph-checkpoint/-/langgraph-checkpoint-0.1.1.tgz\",\n",
        "+      \"integrity\": \"sha512-h2bP0RUikQZu0Um1ZUPErQLXyhzroJqKRbRcxYRTAh49oNlsfeq4A3K4YEDRbGGuyPZI/Jiqwhks1wZwY73AZw==\",\n",
        "       \"license\": \"MIT\",\n",
        "       \"dependencies\": {\n",
        "         \"uuid\": \"^10.0.0\"\n",
        "@@ -4541,7 +4553,7 @@\n",
        "         \"node\": \">=18\"\n",
        "       },\n",
        "       \"peerDependencies\": {\n",
        "-        \"@langchain/core\": \">=0.2.31 <0.4.0\"\n",
        "+        \"@langchain/core\": \">=0.2.31 <0.4.0 || ^1.0.0-alpha\"\n",
        "       }\n",
        "     },\n",
        "     \"node_modules/@langchain/langgraph-checkpoint/node_modules/uuid\": {\n",
        "@@ -4558,15 +4570,31 @@\n",
        "       }\n",
        "     },\n",
        "     \"node_modules/@langchain/langgraph-sdk\": {\n",
        "-      \"version\": \"0.0.32\",\n",
        "-      \"resolved\": \"https://registry.npmjs.org/@langchain/langgraph-sdk/-/langgraph-sdk-0.0.32.tgz\",\n",
        "-      \"integrity\": \"sha512-KQyM9kLO7T6AxwNrceajH7JOybP3pYpvUPnhiI2rrVndI1WyZUJ1eVC1e722BVRAPi6o+WcoTT4uMSZVinPOtA==\",\n",
        "+      \"version\": \"0.1.0\",\n",
        "+      \"resolved\": \"https://registry.npmjs.org/@langchain/langgraph-sdk/-/langgraph-sdk-0.1.0.tgz\",\n",
        "+      \"integrity\": \"sha512-1EKwzwJpgpNqLcRuGG+kLvvhAaPiFWZ9shl/obhL8qDKtYdbR67WCYE+2jUObZ8vKQuCoul16ewJ78g5VrZlKA==\",\n",
        "       \"license\": \"MIT\",\n",
        "       \"dependencies\": {\n",
        "         \"@types/json-schema\": \"^7.0.15\",\n",
        "         \"p-queue\": \"^6.6.2\",\n",
        "         \"p-retry\": \"4\",\n",
        "         \"uuid\": \"^9.0.0\"\n",
        "+      },\n",
        "+      \"peerDependencies\": {\n",
        "+        \"@langchain/core\": \">=0.2.31 <0.4.0\",\n",
        "+        \"react\": \"^18 || ^19\",\n",
        "+        \"react-dom\": \"^18 || ^19\"\n",
        "+      },\n",
        "+      \"peerDependenciesMeta\": {\n",
        "+        \"@langchain/core\": {\n",
        "+          \"optional\": true\n",
        "+        },\n",
        "+        \"react\": {\n",
        "+          \"optional\": true\n",
        "+        },\n",
        "+        \"react-dom\": {\n",
        "+          \"optional\": true\n",
        "+        }\n",
        "       }\n",
        "     },\n",
        "     \"node_modules/@langchain/langgraph/node_modules/uuid\": {\n"
      ]
    },
    {
      "path": "package.json",
      "status": "modified",
      "additions": 9,
      "deletions": 4,
      "patch": "@@ -1,6 +1,6 @@\n {\n   \"name\": \"@librechat/agents\",\n-  \"version\": \"2.4.90\",\n+  \"version\": \"3.0.00-rc1\",\n   \"main\": \"./dist/cjs/main.cjs\",\n   \"module\": \"./dist/esm/main.mjs\",\n   \"types\": \"./dist/types/index.d.ts\",\n@@ -47,7 +47,7 @@\n     \"image\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/image.ts --provider 'google' --name 'Jo' --location 'New York, NY'\",\n     \"code_exec_files\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/code_exec_files.ts --provider 'openAI' --name 'Jo' --location 'New York, NY'\",\n     \"code_exec_simple\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/code_exec_simple.ts --provider 'google' --name 'Jo' --location 'New York, NY'\",\n-    \"simple\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/simple.ts --provider 'openAI' --name 'Jo' --location 'New York, NY'\",\n+    \"simple\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/simple.ts --provider 'anthropic' --name 'Jo' --location 'New York, NY'\",\n     \"caching\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/caching.ts --name 'Jo' --location 'New York, NY'\",\n     \"thinking\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/thinking.ts --name 'Jo' --location 'New York, NY'\",\n     \"memory\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/memory.ts --provider 'openAI' --name 'Jo' --location 'New York, NY'\",\n@@ -56,6 +56,10 @@\n     \"ant_web_search\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/ant_web_search.ts --name 'Jo' --location 'New York, NY'\",\n     \"abort\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/abort.ts --provider 'openAI' --name 'Jo' --location 'New York, NY'\",\n     \"start:cli2\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/cli2.ts --provider 'anthropic' --name 'Jo' --location 'New York, NY'\",\n+    \"multi-agent-test\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/multi-agent-test.ts\",\n+    \"multi-agent-parallel\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/multi-agent-parallel.ts\",\n+    \"multi-agent-sequence\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/multi-agent-sequence.ts\",\n+    \"multi-agent-conditional\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/multi-agent-conditional.ts\",\n     \"script2\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/proto/example_test.ts\",\n     \"script3\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/proto/example_test_anthropic.ts\",\n     \"script4\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/cli4.ts --name 'Jo' --location 'New York, NY'\",\n@@ -66,6 +70,7 @@\n     \"start:collab\": \"node --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/main.ts\",\n     \"start:collab5\": \"node --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/collab_design_v5.ts\",\n     \"start:dev\": \"node --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/main.ts\",\n+    \"supervised\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/supervised.ts --provider anthropic --name Jo --location \\\"New York, NY\\\"\",\n     \"test\": \"jest\",\n     \"test:memory\": \"NODE_OPTIONS='--expose-gc' npx jest src/specs/title.memory-leak.test.ts\",\n     \"test:all\": \"npm test -- --testPathIgnorePatterns=title.memory-leak.test.ts && npm run test:memory\",\n@@ -84,11 +89,11 @@\n     \"@langchain/anthropic\": \"^0.3.26\",\n     \"@langchain/aws\": \"^0.1.12\",\n     \"@langchain/community\": \"^0.3.47\",\n-    \"@langchain/core\": \"^0.3.62\",\n+    \"@langchain/core\": \"^0.3.72\",\n     \"@langchain/deepseek\": \"^0.0.2\",\n     \"@langchain/google-genai\": \"^0.2.13\",\n     \"@langchain/google-vertexai\": \"^0.2.13\",\n-    \"@langchain/langgraph\": \"^0.3.4\",\n+    \"@langchain/langgraph\": \"^0.4.9\",\n     \"@langchain/mistralai\": \"^0.2.1\",\n     \"@langchain/ollama\": \"^0.2.3\",\n     \"@langchain/openai\": \"0.5.18\",",
      "patch_lines": [
        "@@ -1,6 +1,6 @@\n",
        " {\n",
        "   \"name\": \"@librechat/agents\",\n",
        "-  \"version\": \"2.4.90\",\n",
        "+  \"version\": \"3.0.00-rc1\",\n",
        "   \"main\": \"./dist/cjs/main.cjs\",\n",
        "   \"module\": \"./dist/esm/main.mjs\",\n",
        "   \"types\": \"./dist/types/index.d.ts\",\n",
        "@@ -47,7 +47,7 @@\n",
        "     \"image\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/image.ts --provider 'google' --name 'Jo' --location 'New York, NY'\",\n",
        "     \"code_exec_files\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/code_exec_files.ts --provider 'openAI' --name 'Jo' --location 'New York, NY'\",\n",
        "     \"code_exec_simple\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/code_exec_simple.ts --provider 'google' --name 'Jo' --location 'New York, NY'\",\n",
        "-    \"simple\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/simple.ts --provider 'openAI' --name 'Jo' --location 'New York, NY'\",\n",
        "+    \"simple\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/simple.ts --provider 'anthropic' --name 'Jo' --location 'New York, NY'\",\n",
        "     \"caching\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/caching.ts --name 'Jo' --location 'New York, NY'\",\n",
        "     \"thinking\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/thinking.ts --name 'Jo' --location 'New York, NY'\",\n",
        "     \"memory\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/memory.ts --provider 'openAI' --name 'Jo' --location 'New York, NY'\",\n",
        "@@ -56,6 +56,10 @@\n",
        "     \"ant_web_search\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/ant_web_search.ts --name 'Jo' --location 'New York, NY'\",\n",
        "     \"abort\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/abort.ts --provider 'openAI' --name 'Jo' --location 'New York, NY'\",\n",
        "     \"start:cli2\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/cli2.ts --provider 'anthropic' --name 'Jo' --location 'New York, NY'\",\n",
        "+    \"multi-agent-test\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/multi-agent-test.ts\",\n",
        "+    \"multi-agent-parallel\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/multi-agent-parallel.ts\",\n",
        "+    \"multi-agent-sequence\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/multi-agent-sequence.ts\",\n",
        "+    \"multi-agent-conditional\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/multi-agent-conditional.ts\",\n",
        "     \"script2\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/proto/example_test.ts\",\n",
        "     \"script3\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/proto/example_test_anthropic.ts\",\n",
        "     \"script4\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/cli4.ts --name 'Jo' --location 'New York, NY'\",\n",
        "@@ -66,6 +70,7 @@\n",
        "     \"start:collab\": \"node --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/main.ts\",\n",
        "     \"start:collab5\": \"node --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/collab_design_v5.ts\",\n",
        "     \"start:dev\": \"node --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/main.ts\",\n",
        "+    \"supervised\": \"node -r dotenv/config --loader ./tsconfig-paths-bootstrap.mjs --experimental-specifier-resolution=node ./src/scripts/supervised.ts --provider anthropic --name Jo --location \\\"New York, NY\\\"\",\n",
        "     \"test\": \"jest\",\n",
        "     \"test:memory\": \"NODE_OPTIONS='--expose-gc' npx jest src/specs/title.memory-leak.test.ts\",\n",
        "     \"test:all\": \"npm test -- --testPathIgnorePatterns=title.memory-leak.test.ts && npm run test:memory\",\n",
        "@@ -84,11 +89,11 @@\n",
        "     \"@langchain/anthropic\": \"^0.3.26\",\n",
        "     \"@langchain/aws\": \"^0.1.12\",\n",
        "     \"@langchain/community\": \"^0.3.47\",\n",
        "-    \"@langchain/core\": \"^0.3.62\",\n",
        "+    \"@langchain/core\": \"^0.3.72\",\n",
        "     \"@langchain/deepseek\": \"^0.0.2\",\n",
        "     \"@langchain/google-genai\": \"^0.2.13\",\n",
        "     \"@langchain/google-vertexai\": \"^0.2.13\",\n",
        "-    \"@langchain/langgraph\": \"^0.3.4\",\n",
        "+    \"@langchain/langgraph\": \"^0.4.9\",\n",
        "     \"@langchain/mistralai\": \"^0.2.1\",\n",
        "     \"@langchain/ollama\": \"^0.2.3\",\n",
        "     \"@langchain/openai\": \"0.5.18\",\n"
      ]
    },
    {
      "path": "remove-zone-identifiers.sh",
      "status": "added",
      "additions": 36,
      "deletions": 0,
      "patch": "@@ -0,0 +1,36 @@\n+#!/bin/bash\n+\n+# Script to remove all Zone.Identifier files\n+\n+echo \"Finding all Zone.Identifier files...\"\n+echo\n+\n+# Count the files\n+count=$(find . -type f -name \"*:Zone.Identifier\" 2>/dev/null | wc -l)\n+\n+if [ $count -eq 0 ]; then\n+    echo \"No Zone.Identifier files found.\"\n+    exit 0\n+fi\n+\n+echo \"Found $count Zone.Identifier files.\"\n+echo\n+echo \"Files to be removed:\"\n+echo \"===================\"\n+\n+# List all files\n+find . -type f -name \"*:Zone.Identifier\" 2>/dev/null | sort\n+\n+echo\n+echo \"===================\"\n+echo\n+read -p \"Do you want to remove all these files? (y/N): \" confirm\n+\n+if [[ $confirm =~ ^[Yy]$ ]]; then\n+    echo\n+    echo \"Removing Zone.Identifier files...\"\n+    find . -type f -name \"*:Zone.Identifier\" -delete 2>/dev/null\n+    echo \"Done! All Zone.Identifier files have been removed.\"\n+else\n+    echo \"Operation cancelled.\"\n+fi",
      "patch_lines": [
        "@@ -0,0 +1,36 @@\n",
        "+#!/bin/bash\n",
        "+\n",
        "+# Script to remove all Zone.Identifier files\n",
        "+\n",
        "+echo \"Finding all Zone.Identifier files...\"\n",
        "+echo\n",
        "+\n",
        "+# Count the files\n",
        "+count=$(find . -type f -name \"*:Zone.Identifier\" 2>/dev/null | wc -l)\n",
        "+\n",
        "+if [ $count -eq 0 ]; then\n",
        "+    echo \"No Zone.Identifier files found.\"\n",
        "+    exit 0\n",
        "+fi\n",
        "+\n",
        "+echo \"Found $count Zone.Identifier files.\"\n",
        "+echo\n",
        "+echo \"Files to be removed:\"\n",
        "+echo \"===================\"\n",
        "+\n",
        "+# List all files\n",
        "+find . -type f -name \"*:Zone.Identifier\" 2>/dev/null | sort\n",
        "+\n",
        "+echo\n",
        "+echo \"===================\"\n",
        "+echo\n",
        "+read -p \"Do you want to remove all these files? (y/N): \" confirm\n",
        "+\n",
        "+if [[ $confirm =~ ^[Yy]$ ]]; then\n",
        "+    echo\n",
        "+    echo \"Removing Zone.Identifier files...\"\n",
        "+    find . -type f -name \"*:Zone.Identifier\" -delete 2>/dev/null\n",
        "+    echo \"Done! All Zone.Identifier files have been removed.\"\n",
        "+else\n",
        "+    echo \"Operation cancelled.\"\n",
        "+fi\n"
      ]
    },
    {
      "path": "src/agents/AgentContext.ts",
      "status": "added",
      "additions": 315,
      "deletions": 0,
      "patch": "@@ -0,0 +1,315 @@\n+/* eslint-disable no-console */\n+// src/agents/AgentContext.ts\n+import { zodToJsonSchema } from 'zod-to-json-schema';\n+import { SystemMessage } from '@langchain/core/messages';\n+import { RunnableLambda } from '@langchain/core/runnables';\n+import type {\n+  UsageMetadata,\n+  BaseMessage,\n+  BaseMessageFields,\n+} from '@langchain/core/messages';\n+import type { RunnableConfig, Runnable } from '@langchain/core/runnables';\n+import type * as t from '@/types';\n+import type { createPruneMessages } from '@/messages';\n+import { ContentTypes, Providers } from '@/common';\n+\n+/**\n+ * Encapsulates agent-specific state that can vary between agents in a multi-agent system\n+ */\n+export class AgentContext {\n+  /**\n+   * Create an AgentContext from configuration with token accounting initialization\n+   */\n+  static fromConfig(\n+    agentConfig: t.AgentInputs,\n+    tokenCounter?: t.TokenCounter,\n+    indexTokenCountMap?: Record<string, number>\n+  ): AgentContext {\n+    const {\n+      agentId,\n+      provider,\n+      clientOptions,\n+      tools,\n+      toolMap,\n+      toolEnd,\n+      instructions,\n+      additional_instructions,\n+      streamBuffer,\n+      maxContextTokens,\n+      reasoningKey,\n+    } = agentConfig;\n+\n+    const agentContext = new AgentContext({\n+      agentId,\n+      provider,\n+      clientOptions,\n+      maxContextTokens,\n+      streamBuffer,\n+      tools,\n+      toolMap,\n+      instructions,\n+      additionalInstructions: additional_instructions,\n+      reasoningKey,\n+      toolEnd,\n+      instructionTokens: 0,\n+      tokenCounter,\n+    });\n+\n+    if (tokenCounter) {\n+      const tokenMap = indexTokenCountMap || {};\n+      agentContext.indexTokenCountMap = tokenMap;\n+      agentContext.tokenCalculationPromise = agentContext\n+        .calculateInstructionTokens(tokenCounter)\n+        .then(() => {\n+          // Update token map with instruction tokens\n+          agentContext.updateTokenMapWithInstructions(tokenMap);\n+        })\n+        .catch((err) => {\n+          console.error('Error calculating instruction tokens:', err);\n+        });\n+    } else if (indexTokenCountMap) {\n+      agentContext.indexTokenCountMap = indexTokenCountMap;\n+    }\n+\n+    return agentContext;\n+  }\n+\n+  /** Agent identifier */\n+  agentId: string;\n+  /** Provider for this specific agent */\n+  provider: Providers;\n+  /** Client options for this agent */\n+  clientOptions?: t.ClientOptions;\n+  /** Token count map indexed by message position */\n+  indexTokenCountMap: Record<string, number | undefined> = {};\n+  /** Maximum context tokens for this agent */\n+  maxContextTokens?: number;\n+  /** Current usage metadata for this agent */\n+  currentUsage?: Partial<UsageMetadata>;\n+  /** Prune messages function configured for this agent */\n+  pruneMessages?: ReturnType<typeof createPruneMessages>;\n+  /** Token counter function for this agent */\n+  tokenCounter?: t.TokenCounter;\n+  /** Instructions/system message token count */\n+  instructionTokens: number = 0;\n+  /** The amount of time that should pass before another consecutive API call */\n+  streamBuffer?: number;\n+  /** Last stream call timestamp for rate limiting */\n+  lastStreamCall?: number;\n+  /** Tools available to this agent */\n+  tools?: t.GraphTools;\n+  /** Tool map for this agent */\n+  toolMap?: t.ToolMap;\n+  /** Instructions for this agent */\n+  instructions?: string;\n+  /** Additional instructions for this agent */\n+  additionalInstructions?: string;\n+  /** Reasoning key for this agent */\n+  reasoningKey: 'reasoning_content' | 'reasoning' = 'reasoning_content';\n+  /** Last token for reasoning detection */\n+  lastToken?: string;\n+  /** Token type switch state */\n+  tokenTypeSwitch?: 'reasoning' | 'content';\n+  /** Current token type being processed */\n+  currentTokenType: ContentTypes.TEXT | ContentTypes.THINK | 'think_and_text' =\n+    ContentTypes.TEXT;\n+  /** Whether tools should end the workflow */\n+  toolEnd: boolean = false;\n+  /** System runnable for this agent */\n+  systemRunnable?: Runnable<\n+    BaseMessage[],\n+    (BaseMessage | SystemMessage)[],\n+    RunnableConfig<Record<string, unknown>>\n+  >;\n+  /** Promise for token calculation initialization */\n+  tokenCalculationPromise?: Promise<void>;\n+\n+  constructor({\n+    agentId,\n+    provider,\n+    clientOptions,\n+    maxContextTokens,\n+    streamBuffer,\n+    tokenCounter,\n+    tools,\n+    toolMap,\n+    instructions,\n+    additionalInstructions,\n+    reasoningKey,\n+    toolEnd,\n+    instructionTokens,\n+  }: {\n+    agentId: string;\n+    provider: Providers;\n+    clientOptions?: t.ClientOptions;\n+    maxContextTokens?: number;\n+    streamBuffer?: number;\n+    tokenCounter?: t.TokenCounter;\n+    tools?: t.GraphTools;\n+    toolMap?: t.ToolMap;\n+    instructions?: string;\n+    additionalInstructions?: string;\n+    reasoningKey?: 'reasoning_content' | 'reasoning';\n+    toolEnd?: boolean;\n+    instructionTokens?: number;\n+  }) {\n+    this.agentId = agentId;\n+    this.provider = provider;\n+    this.clientOptions = clientOptions;\n+    this.maxContextTokens = maxContextTokens;\n+    this.streamBuffer = streamBuffer;\n+    this.tokenCounter = tokenCounter;\n+    this.tools = tools;\n+    this.toolMap = toolMap;\n+    this.instructions = instructions;\n+    this.additionalInstructions = additionalInstructions;\n+    if (reasoningKey) {\n+      this.reasoningKey = reasoningKey;\n+    }\n+    if (toolEnd !== undefined) {\n+      this.toolEnd = toolEnd;\n+    }\n+    if (instructionTokens !== undefined) {\n+      this.instructionTokens = instructionTokens;\n+    }\n+\n+    this.systemRunnable = this.createSystemRunnable();\n+  }\n+\n+  /**\n+   * Create system runnable from instructions and calculate tokens if tokenCounter is available\n+   */\n+  private createSystemRunnable():\n+    | Runnable<\n+        BaseMessage[],\n+        (BaseMessage | SystemMessage)[],\n+        RunnableConfig<Record<string, unknown>>\n+      >\n+    | undefined {\n+    let finalInstructions: string | BaseMessageFields | undefined =\n+      this.instructions;\n+\n+    if (\n+      this.additionalInstructions != null &&\n+      this.additionalInstructions !== ''\n+    ) {\n+      finalInstructions =\n+        finalInstructions != null && finalInstructions\n+          ? `${finalInstructions}\\n\\n${this.additionalInstructions}`\n+          : this.additionalInstructions;\n+    }\n+\n+    // Handle Anthropic prompt caching\n+    if (\n+      finalInstructions != null &&\n+      finalInstructions !== '' &&\n+      this.provider === Providers.ANTHROPIC\n+    ) {\n+      const anthropicOptions = this.clientOptions as\n+        | t.AnthropicClientOptions\n+        | undefined;\n+      const defaultHeaders = anthropicOptions?.clientOptions?.defaultHeaders as\n+        | Record<string, string>\n+        | undefined;\n+      const anthropicBeta = defaultHeaders?.['anthropic-beta'];\n+      if (\n+        typeof anthropicBeta === 'string' &&\n+        anthropicBeta.includes('prompt-caching')\n+      ) {\n+        finalInstructions = {\n+          content: [\n+            {\n+              type: 'text',\n+              text: this.instructions,\n+              cache_control: { type: 'ephemeral' },\n+            },\n+          ],\n+        };\n+      }\n+    }\n+\n+    if (finalInstructions != null && finalInstructions !== '') {\n+      const systemMessage = new SystemMessage(finalInstructions);\n+\n+      if (this.tokenCounter) {\n+        this.instructionTokens += this.tokenCounter(systemMessage);\n+      }\n+\n+      return RunnableLambda.from((messages: BaseMessage[]) => {\n+        return [systemMessage, ...messages];\n+      }).withConfig({ runName: 'prompt' });\n+    }\n+\n+    return undefined;\n+  }\n+\n+  /**\n+   * Reset context for a new run\n+   */\n+  reset(): void {\n+    this.instructionTokens = 0;\n+    this.lastToken = undefined;\n+    this.indexTokenCountMap = {};\n+    this.currentUsage = undefined;\n+    this.pruneMessages = undefined;\n+    this.lastStreamCall = undefined;\n+    this.tokenTypeSwitch = undefined;\n+    this.currentTokenType = ContentTypes.TEXT;\n+  }\n+\n+  /**\n+   * Update the token count map with instruction tokens\n+   */\n+  updateTokenMapWithInstructions(baseTokenMap: Record<string, number>): void {\n+    if (this.instructionTokens > 0) {\n+      // Shift all indices by the instruction token count\n+      const shiftedMap: Record<string, number> = {};\n+      for (const [key, value] of Object.entries(baseTokenMap)) {\n+        const index = parseInt(key, 10);\n+        if (!isNaN(index)) {\n+          shiftedMap[String(index)] =\n+            value + (index === 0 ? this.instructionTokens : 0);\n+        }\n+      }\n+      this.indexTokenCountMap = shiftedMap;\n+    } else {\n+      this.indexTokenCountMap = { ...baseTokenMap };\n+    }\n+  }\n+\n+  /**\n+   * Calculate tool tokens and add to instruction tokens\n+   * Note: System message tokens are calculated during systemRunnable creation\n+   */\n+  async calculateInstructionTokens(\n+    tokenCounter: t.TokenCounter\n+  ): Promise<void> {\n+    let toolTokens = 0;\n+    if (this.tools && this.tools.length > 0) {\n+      for (const tool of this.tools) {\n+        const genericTool = tool as Record<string, unknown>;\n+        if (\n+          genericTool.schema != null &&\n+          typeof genericTool.schema === 'object'\n+        ) {\n+          const schema = genericTool.schema as {\n+            describe: (desc: string) => unknown;\n+          };\n+          const describedSchema = schema.describe(\n+            (genericTool.description as string) || ''\n+          );\n+          const jsonSchema = zodToJsonSchema(\n+            describedSchema as Parameters<typeof zodToJsonSchema>[0],\n+            (genericTool.name as string) || ''\n+          );\n+          toolTokens += tokenCounter(\n+            new SystemMessage(JSON.stringify(jsonSchema))\n+          );\n+        }\n+      }\n+    }\n+\n+    // Add tool tokens to existing instruction tokens (which may already include system message tokens)\n+    this.instructionTokens += toolTokens;\n+  }\n+}",
      "patch_lines": [
        "@@ -0,0 +1,315 @@\n",
        "+/* eslint-disable no-console */\n",
        "+// src/agents/AgentContext.ts\n",
        "+import { zodToJsonSchema } from 'zod-to-json-schema';\n",
        "+import { SystemMessage } from '@langchain/core/messages';\n",
        "+import { RunnableLambda } from '@langchain/core/runnables';\n",
        "+import type {\n",
        "+  UsageMetadata,\n",
        "+  BaseMessage,\n",
        "+  BaseMessageFields,\n",
        "+} from '@langchain/core/messages';\n",
        "+import type { RunnableConfig, Runnable } from '@langchain/core/runnables';\n",
        "+import type * as t from '@/types';\n",
        "+import type { createPruneMessages } from '@/messages';\n",
        "+import { ContentTypes, Providers } from '@/common';\n",
        "+\n",
        "+/**\n",
        "+ * Encapsulates agent-specific state that can vary between agents in a multi-agent system\n",
        "+ */\n",
        "+export class AgentContext {\n",
        "+  /**\n",
        "+   * Create an AgentContext from configuration with token accounting initialization\n",
        "+   */\n",
        "+  static fromConfig(\n",
        "+    agentConfig: t.AgentInputs,\n",
        "+    tokenCounter?: t.TokenCounter,\n",
        "+    indexTokenCountMap?: Record<string, number>\n",
        "+  ): AgentContext {\n",
        "+    const {\n",
        "+      agentId,\n",
        "+      provider,\n",
        "+      clientOptions,\n",
        "+      tools,\n",
        "+      toolMap,\n",
        "+      toolEnd,\n",
        "+      instructions,\n",
        "+      additional_instructions,\n",
        "+      streamBuffer,\n",
        "+      maxContextTokens,\n",
        "+      reasoningKey,\n",
        "+    } = agentConfig;\n",
        "+\n",
        "+    const agentContext = new AgentContext({\n",
        "+      agentId,\n",
        "+      provider,\n",
        "+      clientOptions,\n",
        "+      maxContextTokens,\n",
        "+      streamBuffer,\n",
        "+      tools,\n",
        "+      toolMap,\n",
        "+      instructions,\n",
        "+      additionalInstructions: additional_instructions,\n",
        "+      reasoningKey,\n",
        "+      toolEnd,\n",
        "+      instructionTokens: 0,\n",
        "+      tokenCounter,\n",
        "+    });\n",
        "+\n",
        "+    if (tokenCounter) {\n",
        "+      const tokenMap = indexTokenCountMap || {};\n",
        "+      agentContext.indexTokenCountMap = tokenMap;\n",
        "+      agentContext.tokenCalculationPromise = agentContext\n",
        "+        .calculateInstructionTokens(tokenCounter)\n",
        "+        .then(() => {\n",
        "+          // Update token map with instruction tokens\n",
        "+          agentContext.updateTokenMapWithInstructions(tokenMap);\n",
        "+        })\n",
        "+        .catch((err) => {\n",
        "+          console.error('Error calculating instruction tokens:', err);\n",
        "+        });\n",
        "+    } else if (indexTokenCountMap) {\n",
        "+      agentContext.indexTokenCountMap = indexTokenCountMap;\n",
        "+    }\n",
        "+\n",
        "+    return agentContext;\n",
        "+  }\n",
        "+\n",
        "+  /** Agent identifier */\n",
        "+  agentId: string;\n",
        "+  /** Provider for this specific agent */\n",
        "+  provider: Providers;\n",
        "+  /** Client options for this agent */\n",
        "+  clientOptions?: t.ClientOptions;\n",
        "+  /** Token count map indexed by message position */\n",
        "+  indexTokenCountMap: Record<string, number | undefined> = {};\n",
        "+  /** Maximum context tokens for this agent */\n",
        "+  maxContextTokens?: number;\n",
        "+  /** Current usage metadata for this agent */\n",
        "+  currentUsage?: Partial<UsageMetadata>;\n",
        "+  /** Prune messages function configured for this agent */\n",
        "+  pruneMessages?: ReturnType<typeof createPruneMessages>;\n",
        "+  /** Token counter function for this agent */\n",
        "+  tokenCounter?: t.TokenCounter;\n",
        "+  /** Instructions/system message token count */\n",
        "+  instructionTokens: number = 0;\n",
        "+  /** The amount of time that should pass before another consecutive API call */\n",
        "+  streamBuffer?: number;\n",
        "+  /** Last stream call timestamp for rate limiting */\n",
        "+  lastStreamCall?: number;\n",
        "+  /** Tools available to this agent */\n",
        "+  tools?: t.GraphTools;\n",
        "+  /** Tool map for this agent */\n",
        "+  toolMap?: t.ToolMap;\n",
        "+  /** Instructions for this agent */\n",
        "+  instructions?: string;\n",
        "+  /** Additional instructions for this agent */\n",
        "+  additionalInstructions?: string;\n",
        "+  /** Reasoning key for this agent */\n",
        "+  reasoningKey: 'reasoning_content' | 'reasoning' = 'reasoning_content';\n",
        "+  /** Last token for reasoning detection */\n",
        "+  lastToken?: string;\n",
        "+  /** Token type switch state */\n",
        "+  tokenTypeSwitch?: 'reasoning' | 'content';\n",
        "+  /** Current token type being processed */\n",
        "+  currentTokenType: ContentTypes.TEXT | ContentTypes.THINK | 'think_and_text' =\n",
        "+    ContentTypes.TEXT;\n",
        "+  /** Whether tools should end the workflow */\n",
        "+  toolEnd: boolean = false;\n",
        "+  /** System runnable for this agent */\n",
        "+  systemRunnable?: Runnable<\n",
        "+    BaseMessage[],\n",
        "+    (BaseMessage | SystemMessage)[],\n",
        "+    RunnableConfig<Record<string, unknown>>\n",
        "+  >;\n",
        "+  /** Promise for token calculation initialization */\n",
        "+  tokenCalculationPromise?: Promise<void>;\n",
        "+\n",
        "+  constructor({\n",
        "+    agentId,\n",
        "+    provider,\n",
        "+    clientOptions,\n",
        "+    maxContextTokens,\n",
        "+    streamBuffer,\n",
        "+    tokenCounter,\n",
        "+    tools,\n",
        "+    toolMap,\n",
        "+    instructions,\n",
        "+    additionalInstructions,\n",
        "+    reasoningKey,\n",
        "+    toolEnd,\n",
        "+    instructionTokens,\n",
        "+  }: {\n",
        "+    agentId: string;\n",
        "+    provider: Providers;\n",
        "+    clientOptions?: t.ClientOptions;\n",
        "+    maxContextTokens?: number;\n",
        "+    streamBuffer?: number;\n",
        "+    tokenCounter?: t.TokenCounter;\n",
        "+    tools?: t.GraphTools;\n",
        "+    toolMap?: t.ToolMap;\n",
        "+    instructions?: string;\n",
        "+    additionalInstructions?: string;\n",
        "+    reasoningKey?: 'reasoning_content' | 'reasoning';\n",
        "+    toolEnd?: boolean;\n",
        "+    instructionTokens?: number;\n",
        "+  }) {\n",
        "+    this.agentId = agentId;\n",
        "+    this.provider = provider;\n",
        "+    this.clientOptions = clientOptions;\n",
        "+    this.maxContextTokens = maxContextTokens;\n",
        "+    this.streamBuffer = streamBuffer;\n",
        "+    this.tokenCounter = tokenCounter;\n",
        "+    this.tools = tools;\n",
        "+    this.toolMap = toolMap;\n",
        "+    this.instructions = instructions;\n",
        "+    this.additionalInstructions = additionalInstructions;\n",
        "+    if (reasoningKey) {\n",
        "+      this.reasoningKey = reasoningKey;\n",
        "+    }\n",
        "+    if (toolEnd !== undefined) {\n",
        "+      this.toolEnd = toolEnd;\n",
        "+    }\n",
        "+    if (instructionTokens !== undefined) {\n",
        "+      this.instructionTokens = instructionTokens;\n",
        "+    }\n",
        "+\n",
        "+    this.systemRunnable = this.createSystemRunnable();\n",
        "+  }\n",
        "+\n",
        "+  /**\n",
        "+   * Create system runnable from instructions and calculate tokens if tokenCounter is available\n",
        "+   */\n",
        "+  private createSystemRunnable():\n",
        "+    | Runnable<\n",
        "+        BaseMessage[],\n",
        "+        (BaseMessage | SystemMessage)[],\n",
        "+        RunnableConfig<Record<string, unknown>>\n",
        "+      >\n",
        "+    | undefined {\n",
        "+    let finalInstructions: string | BaseMessageFields | undefined =\n",
        "+      this.instructions;\n",
        "+\n",
        "+    if (\n",
        "+      this.additionalInstructions != null &&\n",
        "+      this.additionalInstructions !== ''\n",
        "+    ) {\n",
        "+      finalInstructions =\n",
        "+        finalInstructions != null && finalInstructions\n",
        "+          ? `${finalInstructions}\\n\\n${this.additionalInstructions}`\n",
        "+          : this.additionalInstructions;\n",
        "+    }\n",
        "+\n",
        "+    // Handle Anthropic prompt caching\n",
        "+    if (\n",
        "+      finalInstructions != null &&\n",
        "+      finalInstructions !== '' &&\n",
        "+      this.provider === Providers.ANTHROPIC\n",
        "+    ) {\n",
        "+      const anthropicOptions = this.clientOptions as\n",
        "+        | t.AnthropicClientOptions\n",
        "+        | undefined;\n",
        "+      const defaultHeaders = anthropicOptions?.clientOptions?.defaultHeaders as\n",
        "+        | Record<string, string>\n",
        "+        | undefined;\n",
        "+      const anthropicBeta = defaultHeaders?.['anthropic-beta'];\n",
        "+      if (\n",
        "+        typeof anthropicBeta === 'string' &&\n",
        "+        anthropicBeta.includes('prompt-caching')\n",
        "+      ) {\n",
        "+        finalInstructions = {\n",
        "+          content: [\n",
        "+            {\n",
        "+              type: 'text',\n",
        "+              text: this.instructions,\n",
        "+              cache_control: { type: 'ephemeral' },\n",
        "+            },\n",
        "+          ],\n",
        "+        };\n",
        "+      }\n",
        "+    }\n",
        "+\n",
        "+    if (finalInstructions != null && finalInstructions !== '') {\n",
        "+      const systemMessage = new SystemMessage(finalInstructions);\n",
        "+\n",
        "+      if (this.tokenCounter) {\n",
        "+        this.instructionTokens += this.tokenCounter(systemMessage);\n",
        "+      }\n",
        "+\n",
        "+      return RunnableLambda.from((messages: BaseMessage[]) => {\n",
        "+        return [systemMessage, ...messages];\n",
        "+      }).withConfig({ runName: 'prompt' });\n",
        "+    }\n",
        "+\n",
        "+    return undefined;\n",
        "+  }\n",
        "+\n",
        "+  /**\n",
        "+   * Reset context for a new run\n",
        "+   */\n",
        "+  reset(): void {\n",
        "+    this.instructionTokens = 0;\n",
        "+    this.lastToken = undefined;\n",
        "+    this.indexTokenCountMap = {};\n",
        "+    this.currentUsage = undefined;\n",
        "+    this.pruneMessages = undefined;\n",
        "+    this.lastStreamCall = undefined;\n",
        "+    this.tokenTypeSwitch = undefined;\n",
        "+    this.currentTokenType = ContentTypes.TEXT;\n",
        "+  }\n",
        "+\n",
        "+  /**\n",
        "+   * Update the token count map with instruction tokens\n",
        "+   */\n",
        "+  updateTokenMapWithInstructions(baseTokenMap: Record<string, number>): void {\n",
        "+    if (this.instructionTokens > 0) {\n",
        "+      // Shift all indices by the instruction token count\n",
        "+      const shiftedMap: Record<string, number> = {};\n",
        "+      for (const [key, value] of Object.entries(baseTokenMap)) {\n",
        "+        const index = parseInt(key, 10);\n",
        "+        if (!isNaN(index)) {\n",
        "+          shiftedMap[String(index)] =\n",
        "+            value + (index === 0 ? this.instructionTokens : 0);\n",
        "+        }\n",
        "+      }\n",
        "+      this.indexTokenCountMap = shiftedMap;\n",
        "+    } else {\n",
        "+      this.indexTokenCountMap = { ...baseTokenMap };\n",
        "+    }\n",
        "+  }\n",
        "+\n",
        "+  /**\n",
        "+   * Calculate tool tokens and add to instruction tokens\n",
        "+   * Note: System message tokens are calculated during systemRunnable creation\n",
        "+   */\n",
        "+  async calculateInstructionTokens(\n",
        "+    tokenCounter: t.TokenCounter\n",
        "+  ): Promise<void> {\n",
        "+    let toolTokens = 0;\n",
        "+    if (this.tools && this.tools.length > 0) {\n",
        "+      for (const tool of this.tools) {\n",
        "+        const genericTool = tool as Record<string, unknown>;\n",
        "+        if (\n",
        "+          genericTool.schema != null &&\n",
        "+          typeof genericTool.schema === 'object'\n",
        "+        ) {\n",
        "+          const schema = genericTool.schema as {\n",
        "+            describe: (desc: string) => unknown;\n",
        "+          };\n",
        "+          const describedSchema = schema.describe(\n",
        "+            (genericTool.description as string) || ''\n",
        "+          );\n",
        "+          const jsonSchema = zodToJsonSchema(\n",
        "+            describedSchema as Parameters<typeof zodToJsonSchema>[0],\n",
        "+            (genericTool.name as string) || ''\n",
        "+          );\n",
        "+          toolTokens += tokenCounter(\n",
        "+            new SystemMessage(JSON.stringify(jsonSchema))\n",
        "+          );\n",
        "+        }\n",
        "+      }\n",
        "+    }\n",
        "+\n",
        "+    // Add tool tokens to existing instruction tokens (which may already include system message tokens)\n",
        "+    this.instructionTokens += toolTokens;\n",
        "+  }\n",
        "+}\n"
      ]
    },
    {
      "path": "src/common/enum.ts",
      "status": "modified",
      "additions": 4,
      "deletions": 3,
      "patch": "@@ -87,8 +87,9 @@ export enum Providers {\n }\n \n export enum GraphNodeKeys {\n-  TOOLS = 'tools',\n-  AGENT = 'agent',\n+  TOOLS = 'tools=',\n+  AGENT = 'agent=',\n+  ROUTER = 'router',\n   PRE_TOOLS = 'pre_tools',\n   POST_TOOLS = 'post_tools',\n }\n@@ -138,6 +139,7 @@ export enum Callback {\n   TOOL_ERROR = 'handleToolError',\n   TOOL_START = 'handleToolStart',\n   TOOL_END = 'handleToolEnd',\n+  CUSTOM_EVENT = 'handleCustomEvent',\n   /*\n   LLM_START = 'handleLLMStart',\n   LLM_NEW_TOKEN = 'handleLLMNewToken',\n@@ -153,7 +155,6 @@ export enum Callback {\n   RETRIEVER_START = 'handleRetrieverStart',\n   RETRIEVER_END = 'handleRetrieverEnd',\n   RETRIEVER_ERROR = 'handleRetrieverError',\n-  CUSTOM_EVENT = 'handleCustomEvent'\n   */\n }\n ",
      "patch_lines": [
        "@@ -87,8 +87,9 @@ export enum Providers {\n",
        " }\n",
        " \n",
        " export enum GraphNodeKeys {\n",
        "-  TOOLS = 'tools',\n",
        "-  AGENT = 'agent',\n",
        "+  TOOLS = 'tools=',\n",
        "+  AGENT = 'agent=',\n",
        "+  ROUTER = 'router',\n",
        "   PRE_TOOLS = 'pre_tools',\n",
        "   POST_TOOLS = 'post_tools',\n",
        " }\n",
        "@@ -138,6 +139,7 @@ export enum Callback {\n",
        "   TOOL_ERROR = 'handleToolError',\n",
        "   TOOL_START = 'handleToolStart',\n",
        "   TOOL_END = 'handleToolEnd',\n",
        "+  CUSTOM_EVENT = 'handleCustomEvent',\n",
        "   /*\n",
        "   LLM_START = 'handleLLMStart',\n",
        "   LLM_NEW_TOKEN = 'handleLLMNewToken',\n",
        "@@ -153,7 +155,6 @@ export enum Callback {\n",
        "   RETRIEVER_START = 'handleRetrieverStart',\n",
        "   RETRIEVER_END = 'handleRetrieverEnd',\n",
        "   RETRIEVER_ERROR = 'handleRetrieverError',\n",
        "-  CUSTOM_EVENT = 'handleCustomEvent'\n",
        "   */\n",
        " }\n",
        " \n"
      ]
    },
    {
      "path": "src/events.ts",
      "status": "modified",
      "additions": 17,
      "deletions": 11,
      "patch": "@@ -1,10 +1,11 @@\n /* eslint-disable no-console */\n // src/events.ts\n import type {\n+  ToolMessage,\n   UsageMetadata,\n   BaseMessageFields,\n } from '@langchain/core/messages';\n-import type { Graph } from '@/graphs';\n+import type { MultiAgentGraph, StandardGraph } from '@/graphs';\n import type * as t from '@/types';\n import { handleToolCalls } from '@/tools/handlers';\n import { Providers } from '@/common';\n@@ -30,12 +31,12 @@ export class ModelEndHandler implements t.EventHandler {\n     this.collectedUsage = collectedUsage;\n   }\n \n-  handle(\n+  async handle(\n     event: string,\n     data: t.ModelEndData,\n     metadata?: Record<string, unknown>,\n-    graph?: Graph\n-  ): void {\n+    graph?: StandardGraph | MultiAgentGraph\n+  ): Promise<void> {\n     if (!graph || !metadata) {\n       console.warn(`Graph or metadata not found in ${event} event`);\n       return;\n@@ -58,11 +59,16 @@ export class ModelEndHandler implements t.EventHandler {\n       { depth: null }\n     );\n \n-    if (metadata.provider !== Providers.GOOGLE) {\n+    const agentContext = graph.getAgentContext(metadata);\n+\n+    if (\n+      agentContext.provider !== Providers.GOOGLE &&\n+      agentContext.provider !== Providers.BEDROCK\n+    ) {\n       return;\n     }\n \n-    handleToolCalls(data?.output?.tool_calls, metadata, graph);\n+    await handleToolCalls(data?.output?.tool_calls, metadata, graph);\n   }\n }\n \n@@ -76,12 +82,12 @@ export class ToolEndHandler implements t.EventHandler {\n     this.callback = callback;\n     this.omitOutput = omitOutput;\n   }\n-  handle(\n+  async handle(\n     event: string,\n     data: t.StreamEventData | undefined,\n     metadata?: Record<string, unknown>,\n-    graph?: Graph\n-  ): void {\n+    graph?: StandardGraph | MultiAgentGraph\n+  ): Promise<void> {\n     if (!graph || !metadata) {\n       console.warn(`Graph or metadata not found in ${event} event`);\n       return;\n@@ -94,10 +100,10 @@ export class ToolEndHandler implements t.EventHandler {\n     }\n \n     this.callback?.(toolEndData, metadata);\n-    graph.handleToolCallCompleted(\n+    await graph.handleToolCallCompleted(\n       { input: toolEndData.input, output: toolEndData.output },\n       metadata,\n-      this.omitOutput?.(toolEndData.output.name)\n+      this.omitOutput?.((toolEndData.output as ToolMessage | undefined)?.name)\n     );\n   }\n }",
      "patch_lines": [
        "@@ -1,10 +1,11 @@\n",
        " /* eslint-disable no-console */\n",
        " // src/events.ts\n",
        " import type {\n",
        "+  ToolMessage,\n",
        "   UsageMetadata,\n",
        "   BaseMessageFields,\n",
        " } from '@langchain/core/messages';\n",
        "-import type { Graph } from '@/graphs';\n",
        "+import type { MultiAgentGraph, StandardGraph } from '@/graphs';\n",
        " import type * as t from '@/types';\n",
        " import { handleToolCalls } from '@/tools/handlers';\n",
        " import { Providers } from '@/common';\n",
        "@@ -30,12 +31,12 @@ export class ModelEndHandler implements t.EventHandler {\n",
        "     this.collectedUsage = collectedUsage;\n",
        "   }\n",
        " \n",
        "-  handle(\n",
        "+  async handle(\n",
        "     event: string,\n",
        "     data: t.ModelEndData,\n",
        "     metadata?: Record<string, unknown>,\n",
        "-    graph?: Graph\n",
        "-  ): void {\n",
        "+    graph?: StandardGraph | MultiAgentGraph\n",
        "+  ): Promise<void> {\n",
        "     if (!graph || !metadata) {\n",
        "       console.warn(`Graph or metadata not found in ${event} event`);\n",
        "       return;\n",
        "@@ -58,11 +59,16 @@ export class ModelEndHandler implements t.EventHandler {\n",
        "       { depth: null }\n",
        "     );\n",
        " \n",
        "-    if (metadata.provider !== Providers.GOOGLE) {\n",
        "+    const agentContext = graph.getAgentContext(metadata);\n",
        "+\n",
        "+    if (\n",
        "+      agentContext.provider !== Providers.GOOGLE &&\n",
        "+      agentContext.provider !== Providers.BEDROCK\n",
        "+    ) {\n",
        "       return;\n",
        "     }\n",
        " \n",
        "-    handleToolCalls(data?.output?.tool_calls, metadata, graph);\n",
        "+    await handleToolCalls(data?.output?.tool_calls, metadata, graph);\n",
        "   }\n",
        " }\n",
        " \n",
        "@@ -76,12 +82,12 @@ export class ToolEndHandler implements t.EventHandler {\n",
        "     this.callback = callback;\n",
        "     this.omitOutput = omitOutput;\n",
        "   }\n",
        "-  handle(\n",
        "+  async handle(\n",
        "     event: string,\n",
        "     data: t.StreamEventData | undefined,\n",
        "     metadata?: Record<string, unknown>,\n",
        "-    graph?: Graph\n",
        "-  ): void {\n",
        "+    graph?: StandardGraph | MultiAgentGraph\n",
        "+  ): Promise<void> {\n",
        "     if (!graph || !metadata) {\n",
        "       console.warn(`Graph or metadata not found in ${event} event`);\n",
        "       return;\n",
        "@@ -94,10 +100,10 @@ export class ToolEndHandler implements t.EventHandler {\n",
        "     }\n",
        " \n",
        "     this.callback?.(toolEndData, metadata);\n",
        "-    graph.handleToolCallCompleted(\n",
        "+    await graph.handleToolCallCompleted(\n",
        "       { input: toolEndData.input, output: toolEndData.output },\n",
        "       metadata,\n",
        "-      this.omitOutput?.(toolEndData.output.name)\n",
        "+      this.omitOutput?.((toolEndData.output as ToolMessage | undefined)?.name)\n",
        "     );\n",
        "   }\n",
        " }\n"
      ]
    },
    {
      "path": "src/graphs/Graph.ts",
      "status": "modified",
      "additions": 472,
      "deletions": 299,
      "patch": "@@ -4,37 +4,44 @@ import { nanoid } from 'nanoid';\n import { concat } from '@langchain/core/utils/stream';\n import { ToolNode } from '@langchain/langgraph/prebuilt';\n import { ChatVertexAI } from '@langchain/google-vertexai';\n-import { START, END, StateGraph } from '@langchain/langgraph';\n-import { Runnable, RunnableConfig } from '@langchain/core/runnables';\n-import { dispatchCustomEvent } from '@langchain/core/callbacks/dispatch';\n import {\n-  AIMessageChunk,\n+  START,\n+  END,\n+  Command,\n+  StateGraph,\n+  Annotation,\n+  messagesStateReducer,\n+} from '@langchain/langgraph';\n+import {\n+  Runnable,\n+  RunnableConfig,\n+  RunnableLambda,\n+} from '@langchain/core/runnables';\n+import {\n   ToolMessage,\n   SystemMessage,\n+  AIMessageChunk,\n } from '@langchain/core/messages';\n import type {\n-  BaseMessage,\n   BaseMessageFields,\n   UsageMetadata,\n+  BaseMessage,\n } from '@langchain/core/messages';\n+import type { ToolCall } from '@langchain/core/messages/tool';\n import type * as t from '@/types';\n import {\n-  Providers,\n-  GraphEvents,\n   GraphNodeKeys,\n-  StepTypes,\n-  Callback,\n   ContentTypes,\n+  GraphEvents,\n+  Providers,\n+  StepTypes,\n } from '@/common';\n-import type { ToolCall } from '@langchain/core/messages/tool';\n-import { getChatModelClass, manualToolStreamProviders } from '@/llm/providers';\n-import { ToolNode as CustomToolNode, toolsCondition } from '@/tools/ToolNode';\n import {\n-  createPruneMessages,\n+  formatAnthropicArtifactContent,\n+  convertMessagesToContent,\n   modifyDeltaProperties,\n   formatArtifactPayload,\n-  convertMessagesToContent,\n-  formatAnthropicArtifactContent,\n+  createPruneMessages,\n } from '@/messages';\n import {\n   resetIfNotEmpty,\n@@ -43,38 +50,37 @@ import {\n   joinKeys,\n   sleep,\n } from '@/utils';\n+import { getChatModelClass, manualToolStreamProviders } from '@/llm/providers';\n+import { ToolNode as CustomToolNode, toolsCondition } from '@/tools/ToolNode';\n import { ChatOpenAI, AzureChatOpenAI } from '@/llm/openai';\n+import { safeDispatchCustomEvent } from '@/utils/events';\n+import { AgentContext } from '@/agents/AgentContext';\n import { createFakeStreamingLLM } from '@/llm/fake';\n import { HandlerRegistry } from '@/events';\n \n const { AGENT, TOOLS } = GraphNodeKeys;\n-export type GraphNode = GraphNodeKeys | typeof START;\n-export type ClientCallback<T extends unknown[]> = (\n-  graph: StandardGraph,\n-  ...args: T\n-) => void;\n-export type ClientCallbacks = {\n-  [Callback.TOOL_ERROR]?: ClientCallback<[Error, string]>;\n-  [Callback.TOOL_START]?: ClientCallback<unknown[]>;\n-  [Callback.TOOL_END]?: ClientCallback<unknown[]>;\n-};\n-export type SystemCallbacks = {\n-  [K in keyof ClientCallbacks]: ClientCallbacks[K] extends ClientCallback<\n-    infer Args\n-  >\n-    ? (...args: Args) => void\n-    : never;\n-};\n \n export abstract class Graph<\n   T extends t.BaseGraphState = t.BaseGraphState,\n-  // eslint-disable-next-line @typescript-eslint/no-unused-vars\n-  TNodeName extends string = string,\n+  _TNodeName extends string = string,\n > {\n   abstract resetValues(): void;\n-  abstract createGraphState(): t.GraphStateChannels<T>;\n-  abstract initializeTools(): CustomToolNode<T> | ToolNode<T>;\n-  abstract initializeModel(): Runnable;\n+  abstract initializeTools({\n+    currentTools,\n+    currentToolMap,\n+  }: {\n+    currentTools?: t.GraphTools;\n+    currentToolMap?: t.ToolMap;\n+  }): CustomToolNode<T> | ToolNode<T>;\n+  abstract initializeModel({\n+    currentModel,\n+    tools,\n+    clientOptions,\n+  }: {\n+    currentModel?: t.ChatModel;\n+    tools?: t.GraphTools;\n+    clientOptions?: t.ClientOptions;\n+  }): Runnable;\n   abstract getRunMessages(): BaseMessage[] | undefined;\n   abstract getContentParts(): t.MessageContentComplex[] | undefined;\n   abstract generateStepId(stepKey: string): [string, number];\n@@ -85,29 +91,32 @@ export abstract class Graph<\n   abstract checkKeyList(keyList: (string | number | undefined)[]): boolean;\n   abstract getStepIdByKey(stepKey: string, index?: number): string;\n   abstract getRunStep(stepId: string): t.RunStep | undefined;\n-  abstract dispatchRunStep(stepKey: string, stepDetails: t.StepDetails): string;\n-  abstract dispatchRunStepDelta(id: string, delta: t.ToolCallDelta): void;\n-  abstract dispatchMessageDelta(id: string, delta: t.MessageDelta): void;\n+  abstract dispatchRunStep(\n+    stepKey: string,\n+    stepDetails: t.StepDetails\n+  ): Promise<string>;\n+  abstract dispatchRunStepDelta(\n+    id: string,\n+    delta: t.ToolCallDelta\n+  ): Promise<void>;\n+  abstract dispatchMessageDelta(\n+    id: string,\n+    delta: t.MessageDelta\n+  ): Promise<void>;\n   abstract dispatchReasoningDelta(\n     stepId: string,\n     delta: t.ReasoningDelta\n-  ): void;\n+  ): Promise<void>;\n   abstract handleToolCallCompleted(\n     data: t.ToolEndData,\n     metadata?: Record<string, unknown>,\n     omitOutput?: boolean\n-  ): void;\n+  ): Promise<void>;\n \n-  abstract createCallModel(): (\n-    state: T,\n-    config?: RunnableConfig\n-  ) => Promise<Partial<T>>;\n-  abstract createWorkflow(): t.CompiledWorkflow<T>;\n-  lastToken?: string;\n-  tokenTypeSwitch?: 'reasoning' | 'content';\n-  reasoningKey: 'reasoning_content' | 'reasoning' = 'reasoning_content';\n-  currentTokenType: ContentTypes.TEXT | ContentTypes.THINK | 'think_and_text' =\n-    ContentTypes.TEXT;\n+  abstract createCallModel(\n+    agentId?: string,\n+    currentModel?: t.ChatModel\n+  ): (state: T, config?: RunnableConfig) => Promise<Partial<T>>;\n   messageStepHasToolCalls: Map<string, boolean> = new Map();\n   messageIdsByStepKey: Map<string, string> = new Map();\n   prelimMessageIdsByStepKey: Map<string, string> = new Map();\n@@ -116,96 +125,52 @@ export abstract class Graph<\n   stepKeyIds: Map<string, string[]> = new Map<string, string[]>();\n   contentIndexMap: Map<string, number> = new Map();\n   toolCallStepIds: Map<string, string> = new Map();\n-  currentUsage: Partial<UsageMetadata> | undefined;\n-  indexTokenCountMap: Record<string, number | undefined> = {};\n-  maxContextTokens: number | undefined;\n-  pruneMessages?: ReturnType<typeof createPruneMessages>;\n-  /** The amount of time that should pass before another consecutive API call */\n-  streamBuffer: number | undefined;\n-  tokenCounter?: t.TokenCounter;\n   signal?: AbortSignal;\n   /** Set of invoked tool call IDs from non-message run steps completed mid-run, if any */\n   invokedToolIds?: Set<string>;\n   handlerRegistry: HandlerRegistry | undefined;\n }\n \n-export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n-  private graphState: t.GraphStateChannels<t.BaseGraphState>;\n-  clientOptions: t.ClientOptions;\n-  boundModel?: Runnable;\n-  /** The last recorded timestamp that a stream API call was invoked */\n-  lastStreamCall: number | undefined;\n-  systemMessage: SystemMessage | undefined;\n+export class StandardGraph extends Graph<t.BaseGraphState, t.GraphNode> {\n+  overrideModel?: t.ChatModel;\n+  /** Optional compile options passed into workflow.compile() */\n+  compileOptions?: t.CompileOptions | undefined;\n   messages: BaseMessage[] = [];\n   runId: string | undefined;\n-  tools?: t.GraphTools;\n-  toolMap?: t.ToolMap;\n   startIndex: number = 0;\n-  provider: Providers;\n-  toolEnd: boolean;\n   signal?: AbortSignal;\n+  /** Map of agent contexts by agent ID */\n+  agentContexts: Map<string, AgentContext> = new Map();\n+  /** Default agent ID to use */\n+  defaultAgentId: string;\n \n   constructor({\n+    // parent-level graph inputs\n     runId,\n-    tools,\n     signal,\n-    toolMap,\n-    provider,\n-    streamBuffer,\n-    instructions,\n-    reasoningKey,\n-    clientOptions,\n-    toolEnd = false,\n-    additional_instructions = '',\n+    agents,\n+    tokenCounter,\n+    indexTokenCountMap,\n   }: t.StandardGraphInput) {\n     super();\n     this.runId = runId;\n-    this.tools = tools;\n     this.signal = signal;\n-    this.toolEnd = toolEnd;\n-    this.toolMap = toolMap;\n-    this.provider = provider;\n-    this.streamBuffer = streamBuffer;\n-    this.clientOptions = clientOptions;\n-    this.graphState = this.createGraphState();\n-    this.boundModel = this.initializeModel();\n-    if (reasoningKey) {\n-      this.reasoningKey = reasoningKey;\n-    }\n \n-    let finalInstructions: string | BaseMessageFields | undefined =\n-      instructions;\n-    if (additional_instructions) {\n-      finalInstructions =\n-        finalInstructions != null && finalInstructions\n-          ? `${finalInstructions}\\n\\n${additional_instructions}`\n-          : additional_instructions;\n+    if (agents.length === 0) {\n+      throw new Error('At least one agent configuration is required');\n     }\n \n-    if (\n-      finalInstructions != null &&\n-      finalInstructions &&\n-      provider === Providers.ANTHROPIC &&\n-      ((\n-        (clientOptions as t.AnthropicClientOptions).clientOptions\n-          ?.defaultHeaders as Record<string, string> | undefined\n-      )?.['anthropic-beta']?.includes('prompt-caching') ??\n-        false)\n-    ) {\n-      finalInstructions = {\n-        content: [\n-          {\n-            type: 'text',\n-            text: instructions,\n-            cache_control: { type: 'ephemeral' },\n-          },\n-        ],\n-      };\n-    }\n+    for (const agentConfig of agents) {\n+      const agentContext = AgentContext.fromConfig(\n+        agentConfig,\n+        tokenCounter,\n+        indexTokenCountMap\n+      );\n \n-    if (finalInstructions != null && finalInstructions !== '') {\n-      this.systemMessage = new SystemMessage(finalInstructions);\n+      this.agentContexts.set(agentConfig.agentId, agentContext);\n     }\n+\n+    this.defaultAgentId = agents[0].agentId;\n   }\n \n   /* Init */\n@@ -224,24 +189,17 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n       new Map()\n     );\n     this.messageStepHasToolCalls = resetIfNotEmpty(\n-      this.prelimMessageIdsByStepKey,\n+      this.messageStepHasToolCalls,\n       new Map()\n     );\n     this.prelimMessageIdsByStepKey = resetIfNotEmpty(\n       this.prelimMessageIdsByStepKey,\n       new Map()\n     );\n-    this.currentTokenType = resetIfNotEmpty(\n-      this.currentTokenType,\n-      ContentTypes.TEXT\n-    );\n-    this.lastToken = resetIfNotEmpty(this.lastToken, undefined);\n-    this.tokenTypeSwitch = resetIfNotEmpty(this.tokenTypeSwitch, undefined);\n-    this.indexTokenCountMap = resetIfNotEmpty(this.indexTokenCountMap, {});\n-    this.currentUsage = resetIfNotEmpty(this.currentUsage, undefined);\n-    this.tokenCounter = resetIfNotEmpty(this.tokenCounter, undefined);\n-    this.maxContextTokens = resetIfNotEmpty(this.maxContextTokens, undefined);\n     this.invokedToolIds = resetIfNotEmpty(this.invokedToolIds, undefined);\n+    for (const context of this.agentContexts.values()) {\n+      context.reset();\n+    }\n   }\n \n   /* Run Step Processing */\n@@ -254,6 +212,33 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n     return undefined;\n   }\n \n+  getAgentContext(metadata: Record<string, unknown> | undefined): AgentContext {\n+    if (!metadata) {\n+      throw new Error('No metadata provided to retrieve agent context');\n+    }\n+\n+    const currentNode = metadata.langgraph_node as string;\n+    if (!currentNode) {\n+      throw new Error(\n+        'No langgraph_node in metadata to retrieve agent context'\n+      );\n+    }\n+\n+    let agentId: string | undefined;\n+    if (currentNode.startsWith(AGENT)) {\n+      agentId = currentNode.substring(AGENT.length);\n+    } else if (currentNode.startsWith(TOOLS)) {\n+      agentId = currentNode.substring(TOOLS.length);\n+    }\n+\n+    const agentContext = this.agentContexts.get(agentId ?? '');\n+    if (!agentContext) {\n+      throw new Error(`No agent context found for agent ID ${agentId}`);\n+    }\n+\n+    return agentContext;\n+  }\n+\n   getStepKey(metadata: Record<string, unknown> | undefined): string {\n     if (!metadata) return '';\n \n@@ -307,9 +292,11 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n       metadata.langgraph_step as number,\n       metadata.checkpoint_ns as string,\n     ];\n+\n+    const agentContext = this.getAgentContext(metadata);\n     if (\n-      this.currentTokenType === ContentTypes.THINK ||\n-      this.currentTokenType === 'think_and_text'\n+      agentContext.currentTokenType === ContentTypes.THINK ||\n+      agentContext.currentTokenType === 'think_and_text'\n     ) {\n       keyList.push('reasoning');\n     } else if (this.tokenTypeSwitch === 'content') {\n@@ -339,87 +326,127 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n \n   /* Graph */\n \n-  createGraphState(): t.GraphStateChannels<t.BaseGraphState> {\n-    return {\n-      messages: {\n-        value: (x: BaseMessage[], y: BaseMessage[]): BaseMessage[] => {\n-          if (!x.length) {\n-            if (this.systemMessage) {\n-              x.push(this.systemMessage);\n-            }\n+  createSystemRunnable({\n+    provider,\n+    clientOptions,\n+    instructions,\n+    additional_instructions,\n+  }: {\n+    provider?: Providers;\n+    clientOptions?: t.ClientOptions;\n+    instructions?: string;\n+    additional_instructions?: string;\n+  }): t.SystemRunnable | undefined {\n+    let finalInstructions: string | BaseMessageFields | undefined =\n+      instructions;\n+    if (additional_instructions != null && additional_instructions !== '') {\n+      finalInstructions =\n+        finalInstructions != null && finalInstructions\n+          ? `${finalInstructions}\\n\\n${additional_instructions}`\n+          : additional_instructions;\n+    }\n \n-            this.startIndex = x.length + y.length;\n-          }\n-          const current = x.concat(y);\n-          this.messages = current;\n-          return current;\n-        },\n-        default: () => [],\n-      },\n-    };\n+    if (\n+      finalInstructions != null &&\n+      finalInstructions &&\n+      provider === Providers.ANTHROPIC &&\n+      ((\n+        (clientOptions as t.AnthropicClientOptions).clientOptions\n+          ?.defaultHeaders as Record<string, string> | undefined\n+      )?.['anthropic-beta']?.includes('prompt-caching') ??\n+        false)\n+    ) {\n+      finalInstructions = {\n+        content: [\n+          {\n+            type: 'text',\n+            text: instructions,\n+            cache_control: { type: 'ephemeral' },\n+          },\n+        ],\n+      };\n+    }\n+\n+    if (finalInstructions != null && finalInstructions !== '') {\n+      const systemMessage = new SystemMessage(finalInstructions);\n+      return RunnableLambda.from((messages: BaseMessage[]) => {\n+        return [systemMessage, ...messages];\n+      }).withConfig({ runName: 'prompt' });\n+    }\n   }\n \n-  initializeTools():\n-    | CustomToolNode<t.BaseGraphState>\n-    | ToolNode<t.BaseGraphState> {\n+  initializeTools({\n+    currentTools,\n+    currentToolMap,\n+  }: {\n+    currentTools?: t.GraphTools;\n+    currentToolMap?: t.ToolMap;\n+  }): CustomToolNode<t.BaseGraphState> | ToolNode<t.BaseGraphState> {\n     // return new ToolNode<t.BaseGraphState>(this.tools);\n     return new CustomToolNode<t.BaseGraphState>({\n-      tools: (this.tools as t.GenericTool[] | undefined) || [],\n-      toolMap: this.toolMap,\n+      tools: (currentTools as t.GenericTool[] | undefined) ?? [],\n+      toolMap: currentToolMap,\n       toolCallStepIds: this.toolCallStepIds,\n       errorHandler: (data, metadata) =>\n         StandardGraph.handleToolCallErrorStatic(this, data, metadata),\n     });\n   }\n \n-  initializeModel(): Runnable {\n-    const ChatModelClass = getChatModelClass(this.provider);\n-    const model = new ChatModelClass(this.clientOptions);\n+  initializeModel({\n+    provider,\n+    tools,\n+    clientOptions,\n+  }: {\n+    provider: Providers;\n+    tools?: t.GraphTools;\n+    clientOptions?: t.ClientOptions;\n+  }): Runnable {\n+    const ChatModelClass = getChatModelClass(provider);\n+    const model = new ChatModelClass(clientOptions ?? {});\n \n     if (\n-      isOpenAILike(this.provider) &&\n+      isOpenAILike(provider) &&\n       (model instanceof ChatOpenAI || model instanceof AzureChatOpenAI)\n     ) {\n-      model.temperature = (this.clientOptions as t.OpenAIClientOptions)\n+      model.temperature = (clientOptions as t.OpenAIClientOptions)\n         .temperature as number;\n-      model.topP = (this.clientOptions as t.OpenAIClientOptions).topP as number;\n-      model.frequencyPenalty = (this.clientOptions as t.OpenAIClientOptions)\n+      model.topP = (clientOptions as t.OpenAIClientOptions).topP as number;\n+      model.frequencyPenalty = (clientOptions as t.OpenAIClientOptions)\n         .frequencyPenalty as number;\n-      model.presencePenalty = (this.clientOptions as t.OpenAIClientOptions)\n+      model.presencePenalty = (clientOptions as t.OpenAIClientOptions)\n         .presencePenalty as number;\n-      model.n = (this.clientOptions as t.OpenAIClientOptions).n as number;\n+      model.n = (clientOptions as t.OpenAIClientOptions).n as number;\n     } else if (\n-      this.provider === Providers.VERTEXAI &&\n+      provider === Providers.VERTEXAI &&\n       model instanceof ChatVertexAI\n     ) {\n-      model.temperature = (this.clientOptions as t.VertexAIClientOptions)\n+      model.temperature = (clientOptions as t.VertexAIClientOptions)\n         .temperature as number;\n-      model.topP = (this.clientOptions as t.VertexAIClientOptions)\n-        .topP as number;\n-      model.topK = (this.clientOptions as t.VertexAIClientOptions)\n-        .topK as number;\n-      model.topLogprobs = (this.clientOptions as t.VertexAIClientOptions)\n+      model.topP = (clientOptions as t.VertexAIClientOptions).topP as number;\n+      model.topK = (clientOptions as t.VertexAIClientOptions).topK as number;\n+      model.topLogprobs = (clientOptions as t.VertexAIClientOptions)\n         .topLogprobs as number;\n-      model.frequencyPenalty = (this.clientOptions as t.VertexAIClientOptions)\n+      model.frequencyPenalty = (clientOptions as t.VertexAIClientOptions)\n         .frequencyPenalty as number;\n-      model.presencePenalty = (this.clientOptions as t.VertexAIClientOptions)\n+      model.presencePenalty = (clientOptions as t.VertexAIClientOptions)\n         .presencePenalty as number;\n-      model.maxOutputTokens = (this.clientOptions as t.VertexAIClientOptions)\n+      model.maxOutputTokens = (clientOptions as t.VertexAIClientOptions)\n         .maxOutputTokens as number;\n     }\n \n-    if (!this.tools || this.tools.length === 0) {\n+    if (!tools || tools.length === 0) {\n       return model as unknown as Runnable;\n     }\n \n-    return (model as t.ModelWithTools).bindTools(this.tools);\n+    return (model as t.ModelWithTools).bindTools(tools);\n   }\n+\n   overrideTestModel(\n     responses: string[],\n     sleep?: number,\n     toolCalls?: ToolCall[]\n   ): void {\n-    this.boundModel = createFakeStreamingLLM({\n+    this.overrideModel = createFakeStreamingLLM({\n       responses,\n       sleep,\n       toolCalls,\n@@ -429,64 +456,113 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n   getNewModel({\n     provider,\n     clientOptions,\n-    omitOptions,\n   }: {\n     provider: Providers;\n     clientOptions?: t.ClientOptions;\n-    omitOptions?: Set<string>;\n   }): t.ChatModelInstance {\n     const ChatModelClass = getChatModelClass(provider);\n-    const options =\n-      omitOptions && clientOptions == null\n-        ? Object.assign(\n-          Object.fromEntries(\n-            Object.entries(this.clientOptions).filter(\n-              ([key]) => !omitOptions.has(key)\n-            )\n-          ),\n-          clientOptions\n-        )\n-        : (clientOptions ?? this.clientOptions);\n-    return new ChatModelClass(options);\n+    return new ChatModelClass(clientOptions ?? {});\n   }\n \n-  storeUsageMetadata(finalMessage?: BaseMessage): void {\n+  getUsageMetadata(\n+    finalMessage?: BaseMessage\n+  ): Partial<UsageMetadata> | undefined {\n     if (\n       finalMessage &&\n       'usage_metadata' in finalMessage &&\n       finalMessage.usage_metadata != null\n     ) {\n-      this.currentUsage = finalMessage.usage_metadata as Partial<UsageMetadata>;\n+      return finalMessage.usage_metadata as Partial<UsageMetadata>;\n+    }\n+  }\n+\n+  /** Execute model invocation with streaming support */\n+  private async attemptInvoke(\n+    {\n+      currentModel,\n+      finalMessages,\n+      provider,\n+      tools,\n+    }: {\n+      currentModel?: t.ChatModel;\n+      finalMessages: BaseMessage[];\n+      provider: Providers;\n+      tools?: t.GraphTools;\n+    },\n+    config?: RunnableConfig\n+  ): Promise<Partial<t.BaseGraphState>> {\n+    const model = this.overrideModel ?? currentModel;\n+    if (!model) {\n+      throw new Error('No model found');\n+    }\n+\n+    if ((tools?.length ?? 0) > 0 && manualToolStreamProviders.has(provider)) {\n+      if (!model.stream) {\n+        throw new Error('Model does not support stream');\n+      }\n+      const stream = await model.stream(finalMessages, config);\n+      let finalChunk: AIMessageChunk | undefined;\n+      for await (const chunk of stream) {\n+        await safeDispatchCustomEvent(\n+          GraphEvents.CHAT_MODEL_STREAM,\n+          { chunk, emitted: true },\n+          config\n+        );\n+        finalChunk = finalChunk ? concat(finalChunk, chunk) : chunk;\n+      }\n+      finalChunk = modifyDeltaProperties(provider, finalChunk);\n+      return { messages: [finalChunk as AIMessageChunk] };\n+    } else {\n+      const finalMessage = await model.invoke(finalMessages, config);\n+      if ((finalMessage.tool_calls?.length ?? 0) > 0) {\n+        finalMessage.tool_calls = finalMessage.tool_calls?.filter(\n+          (tool_call: ToolCall) => !!tool_call.name\n+        );\n+      }\n+      return { messages: [finalMessage] };\n     }\n   }\n \n-  cleanupSignalListener(): void {\n+  cleanupSignalListener(currentModel?: t.ChatModel): void {\n     if (!this.signal) {\n       return;\n     }\n-    if (!this.boundModel) {\n+    const model = this.overrideModel ?? currentModel;\n+    if (!model) {\n       return;\n     }\n-    const client = (this.boundModel as ChatOpenAI | undefined)?.exposedClient;\n+    const client = (model as ChatOpenAI | undefined)?.exposedClient;\n     if (!client?.abortHandler) {\n       return;\n     }\n     this.signal.removeEventListener('abort', client.abortHandler);\n     client.abortHandler = undefined;\n   }\n \n-  createCallModel() {\n+  createCallModel(agentId = 'default', currentModel?: t.ChatModel) {\n     return async (\n       state: t.BaseGraphState,\n       config?: RunnableConfig\n     ): Promise<Partial<t.BaseGraphState>> => {\n-      const { provider = '' } =\n-        (config?.configurable as t.GraphConfig | undefined) ?? {};\n-      if (this.boundModel == null) {\n+      /**\n+       * Get agent context - it must exist by this point\n+       */\n+      const agentContext = this.agentContexts.get(agentId);\n+      if (!agentContext) {\n+        throw new Error(`Agent context not found for agentId: ${agentId}`);\n+      }\n+\n+      const model = this.overrideModel ?? currentModel;\n+      if (!model) {\n         throw new Error('No Graph model found');\n       }\n-      if (!config || !provider) {\n-        throw new Error(`No ${config ? 'provider' : 'config'} provided`);\n+      if (!config) {\n+        throw new Error('No config provided');\n+      }\n+\n+      // Ensure token calculations are complete before proceeding\n+      if (agentContext.tokenCalculationPromise) {\n+        await agentContext.tokenCalculationPromise;\n       }\n       if (!config.signal) {\n         config.signal = this.signal;\n@@ -496,40 +572,40 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n \n       let messagesToUse = messages;\n       if (\n-        !this.pruneMessages &&\n-        this.tokenCounter &&\n-        this.maxContextTokens != null &&\n-        this.indexTokenCountMap[0] != null\n+        !agentContext.pruneMessages &&\n+        agentContext.tokenCounter &&\n+        agentContext.maxContextTokens != null &&\n+        agentContext.indexTokenCountMap[0] != null\n       ) {\n         const isAnthropicWithThinking =\n-          (this.provider === Providers.ANTHROPIC &&\n-            (this.clientOptions as t.AnthropicClientOptions).thinking !=\n+          (agentContext.provider === Providers.ANTHROPIC &&\n+            (agentContext.clientOptions as t.AnthropicClientOptions).thinking !=\n               null) ||\n-          (this.provider === Providers.BEDROCK &&\n-            (this.clientOptions as t.BedrockAnthropicInput)\n+          (agentContext.provider === Providers.BEDROCK &&\n+            (agentContext.clientOptions as t.BedrockAnthropicInput)\n               .additionalModelRequestFields?.['thinking'] != null) ||\n           (this.provider === Providers.OPENAI &&\n             (\n               (this.clientOptions as t.OpenAIClientOptions).modelKwargs\n                 ?.thinking as t.AnthropicClientOptions['thinking']\n             )?.type === 'enabled');\n \n-        this.pruneMessages = createPruneMessages({\n-          provider: this.provider,\n-          indexTokenCountMap: this.indexTokenCountMap,\n-          maxTokens: this.maxContextTokens,\n-          tokenCounter: this.tokenCounter,\n+        agentContext.pruneMessages = createPruneMessages({\n           startIndex: this.startIndex,\n+          provider: agentContext.provider,\n+          tokenCounter: agentContext.tokenCounter,\n+          maxTokens: agentContext.maxContextTokens,\n           thinkingEnabled: isAnthropicWithThinking,\n+          indexTokenCountMap: agentContext.indexTokenCountMap,\n         });\n       }\n-      if (this.pruneMessages) {\n-        const { context, indexTokenCountMap } = this.pruneMessages({\n+      if (agentContext.pruneMessages) {\n+        const { context, indexTokenCountMap } = agentContext.pruneMessages({\n           messages,\n-          usageMetadata: this.currentUsage,\n+          usageMetadata: agentContext.currentUsage,\n           // startOnMessageType: 'human',\n         });\n-        this.indexTokenCountMap = indexTokenCountMap;\n+        agentContext.indexTokenCountMap = indexTokenCountMap;\n         messagesToUse = context;\n       }\n \n@@ -544,7 +620,7 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n           : null;\n \n       if (\n-        provider === Providers.BEDROCK &&\n+        agentContext.provider === Providers.BEDROCK &&\n         lastMessageX instanceof AIMessageChunk &&\n         lastMessageY instanceof ToolMessage &&\n         typeof lastMessageX.content === 'string'\n@@ -554,95 +630,176 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n \n       const isLatestToolMessage = lastMessageY instanceof ToolMessage;\n \n-      if (isLatestToolMessage && provider === Providers.ANTHROPIC) {\n+      if (\n+        isLatestToolMessage &&\n+        agentContext.provider === Providers.ANTHROPIC\n+      ) {\n         formatAnthropicArtifactContent(finalMessages);\n       } else if (\n         isLatestToolMessage &&\n-        (isOpenAILike(provider) || isGoogleLike(provider))\n+        (isOpenAILike(agentContext.provider) ||\n+          isGoogleLike(agentContext.provider))\n       ) {\n         formatArtifactPayload(finalMessages);\n       }\n \n-      if (this.lastStreamCall != null && this.streamBuffer != null) {\n-        const timeSinceLastCall = Date.now() - this.lastStreamCall;\n-        if (timeSinceLastCall < this.streamBuffer) {\n+      if (\n+        agentContext.lastStreamCall != null &&\n+        agentContext.streamBuffer != null\n+      ) {\n+        const timeSinceLastCall = Date.now() - agentContext.lastStreamCall;\n+        if (timeSinceLastCall < agentContext.streamBuffer) {\n           const timeToWait =\n-            Math.ceil((this.streamBuffer - timeSinceLastCall) / 1000) * 1000;\n+            Math.ceil((agentContext.streamBuffer - timeSinceLastCall) / 1000) *\n+            1000;\n           await sleep(timeToWait);\n         }\n       }\n \n-      this.lastStreamCall = Date.now();\n+      agentContext.lastStreamCall = Date.now();\n \n-      let result: Partial<t.BaseGraphState>;\n-      if (\n-        (this.tools?.length ?? 0) > 0 &&\n-        manualToolStreamProviders.has(provider)\n-      ) {\n-        const stream = await this.boundModel.stream(finalMessages, config);\n-        let finalChunk: AIMessageChunk | undefined;\n-        for await (const chunk of stream) {\n-          dispatchCustomEvent(GraphEvents.CHAT_MODEL_STREAM, { chunk }, config);\n-          if (!finalChunk) {\n-            finalChunk = chunk;\n-          } else {\n-            finalChunk = concat(finalChunk, chunk);\n+      let result: Partial<t.BaseGraphState> | undefined;\n+      const fallbacks =\n+        (agentContext.clientOptions as t.LLMConfig | undefined)?.fallbacks ??\n+        [];\n+      try {\n+        result = await this.attemptInvoke(\n+          {\n+            currentModel: model,\n+            finalMessages,\n+            provider: agentContext.provider,\n+            tools: agentContext.tools,\n+          },\n+          config\n+        );\n+      } catch (primaryError) {\n+        let lastError: unknown = primaryError;\n+        for (const fb of fallbacks) {\n+          try {\n+            let model = this.getNewModel({\n+              provider: fb.provider,\n+              clientOptions: fb.clientOptions,\n+            });\n+            const bindableTools = agentContext.tools;\n+            model = (\n+              !bindableTools || bindableTools.length === 0\n+                ? model\n+                : model.bindTools(bindableTools)\n+            ) as t.ChatModelInstance;\n+            result = await this.attemptInvoke(\n+              {\n+                currentModel: model,\n+                finalMessages,\n+                provider: fb.provider,\n+                tools: agentContext.tools,\n+              },\n+              config\n+            );\n+            lastError = undefined;\n+            break;\n+          } catch (e) {\n+            lastError = e;\n+            continue;\n           }\n         }\n-\n-        finalChunk = modifyDeltaProperties(this.provider, finalChunk);\n-        result = { messages: [finalChunk as AIMessageChunk] };\n-      } else {\n-        const finalMessage = (await this.boundModel.invoke(\n-          finalMessages,\n-          config\n-        )) as AIMessageChunk;\n-        if ((finalMessage.tool_calls?.length ?? 0) > 0) {\n-          finalMessage.tool_calls = finalMessage.tool_calls?.filter(\n-            (tool_call) => {\n-              if (!tool_call.name) {\n-                return false;\n-              }\n-              return true;\n-            }\n-          );\n+        if (lastError !== undefined) {\n+          throw lastError;\n         }\n-        result = { messages: [finalMessage] };\n       }\n \n-      this.storeUsageMetadata(result.messages?.[0]);\n+      if (!result) {\n+        throw new Error('No result after model invocation');\n+      }\n+      agentContext.currentUsage = this.getUsageMetadata(result.messages?.[0]);\n       this.cleanupSignalListener();\n       return result;\n     };\n   }\n \n-  createWorkflow(): t.CompiledWorkflow<t.BaseGraphState> {\n+  createAgentNode(agentId: string): t.CompiledAgentWorfklow {\n+    const agentContext = this.agentContexts.get(agentId);\n+    if (!agentContext) {\n+      throw new Error(`Agent context not found for agentId: ${agentId}`);\n+    }\n+\n+    let currentModel = this.initializeModel({\n+      tools: agentContext.tools,\n+      provider: agentContext.provider,\n+      clientOptions: agentContext.clientOptions,\n+    });\n+\n+    if (agentContext.systemRunnable) {\n+      currentModel = agentContext.systemRunnable.pipe(currentModel);\n+    }\n+\n+    const agentNode = `${AGENT}${agentId}` as const;\n+    const toolNode = `${TOOLS}${agentId}` as const;\n+\n     const routeMessage = (\n       state: t.BaseGraphState,\n       config?: RunnableConfig\n     ): string => {\n       this.config = config;\n-      return toolsCondition(state, this.invokedToolIds);\n+      return toolsCondition(state, toolNode, this.invokedToolIds);\n     };\n \n-    const workflow = new StateGraph<t.BaseGraphState>({\n-      channels: this.graphState,\n-    })\n-      .addNode(AGENT, this.createCallModel())\n-      .addNode(TOOLS, this.initializeTools())\n-      .addEdge(START, AGENT)\n-      .addConditionalEdges(AGENT, routeMessage)\n-      .addEdge(TOOLS, this.toolEnd ? END : AGENT);\n+    const StateAnnotation = Annotation.Root({\n+      messages: Annotation<BaseMessage[]>({\n+        reducer: messagesStateReducer,\n+        default: () => [],\n+      }),\n+    });\n+\n+    const workflow = new StateGraph(StateAnnotation)\n+      .addNode(agentNode, this.createCallModel(agentId, currentModel))\n+      .addNode(\n+        toolNode,\n+        this.initializeTools({\n+          currentTools: agentContext.tools,\n+          currentToolMap: agentContext.toolMap,\n+        })\n+      )\n+      .addEdge(START, agentNode)\n+      .addConditionalEdges(agentNode, routeMessage)\n+      .addEdge(toolNode, agentContext.toolEnd ? END : agentNode);\n+\n+    // Cast to unknown to avoid tight coupling to external types; options are opt-in\n+    return workflow.compile(this.compileOptions as unknown as never);\n+  }\n+\n+  createWorkflow(): t.CompiledStateWorkflow {\n+    /** Use the default (first) agent for now */\n+    const agentNode = this.createAgentNode(this.defaultAgentId);\n+    const StateAnnotation = Annotation.Root({\n+      messages: Annotation<BaseMessage[]>({\n+        reducer: (a, b) => {\n+          if (!a.length) {\n+            this.startIndex = a.length + b.length;\n+          }\n+          const result = messagesStateReducer(a, b);\n+          this.messages = result;\n+          return result;\n+        },\n+        default: () => [],\n+      }),\n+    });\n+    const workflow = new StateGraph(StateAnnotation)\n+      .addNode(this.defaultAgentId, agentNode, { ends: [END] })\n+      .addEdge(START, this.defaultAgentId)\n+      .compile();\n \n-    return workflow.compile();\n+    return workflow;\n   }\n \n   /* Dispatchers */\n \n   /**\n    * Dispatches a run step to the client, returns the step ID\n    */\n-  dispatchRunStep(stepKey: string, stepDetails: t.StepDetails): string {\n+  async dispatchRunStep(\n+    stepKey: string,\n+    stepDetails: t.StepDetails\n+  ): Promise<string> {\n     if (!this.config) {\n       throw new Error('No config provided');\n     }\n@@ -674,15 +831,19 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n \n     this.contentData.push(runStep);\n     this.contentIndexMap.set(stepId, runStep.index);\n-    dispatchCustomEvent(GraphEvents.ON_RUN_STEP, runStep, this.config);\n+    await safeDispatchCustomEvent(\n+      GraphEvents.ON_RUN_STEP,\n+      runStep,\n+      this.config\n+    );\n     return stepId;\n   }\n \n-  handleToolCallCompleted(\n+  async handleToolCallCompleted(\n     data: t.ToolEndData,\n     metadata?: Record<string, unknown>,\n     omitOutput?: boolean\n-  ): void {\n+  ): Promise<void> {\n     if (!this.config) {\n       throw new Error('No config provided');\n     }\n@@ -691,7 +852,11 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n       return;\n     }\n \n-    const { input, output } = data;\n+    const { input, output: _output } = data;\n+    if ((_output as Command | undefined)?.lg_name === 'Command') {\n+      return;\n+    }\n+    const output = _output as ToolMessage;\n     const { tool_call_id } = output;\n     const stepId = this.toolCallStepIds.get(tool_call_id) ?? '';\n     if (!stepId) {\n@@ -717,29 +882,31 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n       progress: 1,\n     };\n \n-    this.handlerRegistry?.getHandler(GraphEvents.ON_RUN_STEP_COMPLETED)?.handle(\n-      GraphEvents.ON_RUN_STEP_COMPLETED,\n-      {\n-        result: {\n-          id: stepId,\n-          index: runStep.index,\n-          type: 'tool_call',\n-          tool_call,\n-        } as t.ToolCompleteEvent,\n-      },\n-      metadata,\n-      this\n-    );\n+    await this.handlerRegistry\n+      ?.getHandler(GraphEvents.ON_RUN_STEP_COMPLETED)\n+      ?.handle(\n+        GraphEvents.ON_RUN_STEP_COMPLETED,\n+        {\n+          result: {\n+            id: stepId,\n+            index: runStep.index,\n+            type: 'tool_call',\n+            tool_call,\n+          } as t.ToolCompleteEvent,\n+        },\n+        metadata,\n+        this\n+      );\n   }\n   /**\n    * Static version of handleToolCallError to avoid creating strong references\n    * that prevent garbage collection\n    */\n-  static handleToolCallErrorStatic(\n+  static async handleToolCallErrorStatic(\n     graph: StandardGraph,\n     data: t.ToolErrorData,\n     metadata?: Record<string, unknown>\n-  ): void {\n+  ): Promise<void> {\n     if (!graph.config) {\n       throw new Error('No config provided');\n     }\n@@ -769,7 +936,7 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n       progress: 1,\n     };\n \n-    graph.handlerRegistry\n+    await graph.handlerRegistry\n       ?.getHandler(GraphEvents.ON_RUN_STEP_COMPLETED)\n       ?.handle(\n         GraphEvents.ON_RUN_STEP_COMPLETED,\n@@ -790,14 +957,17 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n    * Instance method that delegates to the static method\n    * Kept for backward compatibility\n    */\n-  handleToolCallError(\n+  async handleToolCallError(\n     data: t.ToolErrorData,\n     metadata?: Record<string, unknown>\n-  ): void {\n-    StandardGraph.handleToolCallErrorStatic(this, data, metadata);\n+  ): Promise<void> {\n+    await StandardGraph.handleToolCallErrorStatic(this, data, metadata);\n   }\n \n-  dispatchRunStepDelta(id: string, delta: t.ToolCallDelta): void {\n+  async dispatchRunStepDelta(\n+    id: string,\n+    delta: t.ToolCallDelta\n+  ): Promise<void> {\n     if (!this.config) {\n       throw new Error('No config provided');\n     } else if (!id) {\n@@ -807,37 +977,40 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n       id,\n       delta,\n     };\n-    dispatchCustomEvent(\n+    await safeDispatchCustomEvent(\n       GraphEvents.ON_RUN_STEP_DELTA,\n       runStepDelta,\n       this.config\n     );\n   }\n \n-  dispatchMessageDelta(id: string, delta: t.MessageDelta): void {\n+  async dispatchMessageDelta(id: string, delta: t.MessageDelta): Promise<void> {\n     if (!this.config) {\n       throw new Error('No config provided');\n     }\n     const messageDelta: t.MessageDeltaEvent = {\n       id,\n       delta,\n     };\n-    dispatchCustomEvent(\n+    await safeDispatchCustomEvent(\n       GraphEvents.ON_MESSAGE_DELTA,\n       messageDelta,\n       this.config\n     );\n   }\n \n-  dispatchReasoningDelta = (stepId: string, delta: t.ReasoningDelta): void => {\n+  dispatchReasoningDelta = async (\n+    stepId: string,\n+    delta: t.ReasoningDelta\n+  ): Promise<void> => {\n     if (!this.config) {\n       throw new Error('No config provided');\n     }\n     const reasoningDelta: t.ReasoningDeltaEvent = {\n       id: stepId,\n       delta,\n     };\n-    dispatchCustomEvent(\n+    await safeDispatchCustomEvent(\n       GraphEvents.ON_REASONING_DELTA,\n       reasoningDelta,\n       this.config",
      "patch_lines": [
        "@@ -4,37 +4,44 @@ import { nanoid } from 'nanoid';\n",
        " import { concat } from '@langchain/core/utils/stream';\n",
        " import { ToolNode } from '@langchain/langgraph/prebuilt';\n",
        " import { ChatVertexAI } from '@langchain/google-vertexai';\n",
        "-import { START, END, StateGraph } from '@langchain/langgraph';\n",
        "-import { Runnable, RunnableConfig } from '@langchain/core/runnables';\n",
        "-import { dispatchCustomEvent } from '@langchain/core/callbacks/dispatch';\n",
        " import {\n",
        "-  AIMessageChunk,\n",
        "+  START,\n",
        "+  END,\n",
        "+  Command,\n",
        "+  StateGraph,\n",
        "+  Annotation,\n",
        "+  messagesStateReducer,\n",
        "+} from '@langchain/langgraph';\n",
        "+import {\n",
        "+  Runnable,\n",
        "+  RunnableConfig,\n",
        "+  RunnableLambda,\n",
        "+} from '@langchain/core/runnables';\n",
        "+import {\n",
        "   ToolMessage,\n",
        "   SystemMessage,\n",
        "+  AIMessageChunk,\n",
        " } from '@langchain/core/messages';\n",
        " import type {\n",
        "-  BaseMessage,\n",
        "   BaseMessageFields,\n",
        "   UsageMetadata,\n",
        "+  BaseMessage,\n",
        " } from '@langchain/core/messages';\n",
        "+import type { ToolCall } from '@langchain/core/messages/tool';\n",
        " import type * as t from '@/types';\n",
        " import {\n",
        "-  Providers,\n",
        "-  GraphEvents,\n",
        "   GraphNodeKeys,\n",
        "-  StepTypes,\n",
        "-  Callback,\n",
        "   ContentTypes,\n",
        "+  GraphEvents,\n",
        "+  Providers,\n",
        "+  StepTypes,\n",
        " } from '@/common';\n",
        "-import type { ToolCall } from '@langchain/core/messages/tool';\n",
        "-import { getChatModelClass, manualToolStreamProviders } from '@/llm/providers';\n",
        "-import { ToolNode as CustomToolNode, toolsCondition } from '@/tools/ToolNode';\n",
        " import {\n",
        "-  createPruneMessages,\n",
        "+  formatAnthropicArtifactContent,\n",
        "+  convertMessagesToContent,\n",
        "   modifyDeltaProperties,\n",
        "   formatArtifactPayload,\n",
        "-  convertMessagesToContent,\n",
        "-  formatAnthropicArtifactContent,\n",
        "+  createPruneMessages,\n",
        " } from '@/messages';\n",
        " import {\n",
        "   resetIfNotEmpty,\n",
        "@@ -43,38 +50,37 @@ import {\n",
        "   joinKeys,\n",
        "   sleep,\n",
        " } from '@/utils';\n",
        "+import { getChatModelClass, manualToolStreamProviders } from '@/llm/providers';\n",
        "+import { ToolNode as CustomToolNode, toolsCondition } from '@/tools/ToolNode';\n",
        " import { ChatOpenAI, AzureChatOpenAI } from '@/llm/openai';\n",
        "+import { safeDispatchCustomEvent } from '@/utils/events';\n",
        "+import { AgentContext } from '@/agents/AgentContext';\n",
        " import { createFakeStreamingLLM } from '@/llm/fake';\n",
        " import { HandlerRegistry } from '@/events';\n",
        " \n",
        " const { AGENT, TOOLS } = GraphNodeKeys;\n",
        "-export type GraphNode = GraphNodeKeys | typeof START;\n",
        "-export type ClientCallback<T extends unknown[]> = (\n",
        "-  graph: StandardGraph,\n",
        "-  ...args: T\n",
        "-) => void;\n",
        "-export type ClientCallbacks = {\n",
        "-  [Callback.TOOL_ERROR]?: ClientCallback<[Error, string]>;\n",
        "-  [Callback.TOOL_START]?: ClientCallback<unknown[]>;\n",
        "-  [Callback.TOOL_END]?: ClientCallback<unknown[]>;\n",
        "-};\n",
        "-export type SystemCallbacks = {\n",
        "-  [K in keyof ClientCallbacks]: ClientCallbacks[K] extends ClientCallback<\n",
        "-    infer Args\n",
        "-  >\n",
        "-    ? (...args: Args) => void\n",
        "-    : never;\n",
        "-};\n",
        " \n",
        " export abstract class Graph<\n",
        "   T extends t.BaseGraphState = t.BaseGraphState,\n",
        "-  // eslint-disable-next-line @typescript-eslint/no-unused-vars\n",
        "-  TNodeName extends string = string,\n",
        "+  _TNodeName extends string = string,\n",
        " > {\n",
        "   abstract resetValues(): void;\n",
        "-  abstract createGraphState(): t.GraphStateChannels<T>;\n",
        "-  abstract initializeTools(): CustomToolNode<T> | ToolNode<T>;\n",
        "-  abstract initializeModel(): Runnable;\n",
        "+  abstract initializeTools({\n",
        "+    currentTools,\n",
        "+    currentToolMap,\n",
        "+  }: {\n",
        "+    currentTools?: t.GraphTools;\n",
        "+    currentToolMap?: t.ToolMap;\n",
        "+  }): CustomToolNode<T> | ToolNode<T>;\n",
        "+  abstract initializeModel({\n",
        "+    currentModel,\n",
        "+    tools,\n",
        "+    clientOptions,\n",
        "+  }: {\n",
        "+    currentModel?: t.ChatModel;\n",
        "+    tools?: t.GraphTools;\n",
        "+    clientOptions?: t.ClientOptions;\n",
        "+  }): Runnable;\n",
        "   abstract getRunMessages(): BaseMessage[] | undefined;\n",
        "   abstract getContentParts(): t.MessageContentComplex[] | undefined;\n",
        "   abstract generateStepId(stepKey: string): [string, number];\n",
        "@@ -85,29 +91,32 @@ export abstract class Graph<\n",
        "   abstract checkKeyList(keyList: (string | number | undefined)[]): boolean;\n",
        "   abstract getStepIdByKey(stepKey: string, index?: number): string;\n",
        "   abstract getRunStep(stepId: string): t.RunStep | undefined;\n",
        "-  abstract dispatchRunStep(stepKey: string, stepDetails: t.StepDetails): string;\n",
        "-  abstract dispatchRunStepDelta(id: string, delta: t.ToolCallDelta): void;\n",
        "-  abstract dispatchMessageDelta(id: string, delta: t.MessageDelta): void;\n",
        "+  abstract dispatchRunStep(\n",
        "+    stepKey: string,\n",
        "+    stepDetails: t.StepDetails\n",
        "+  ): Promise<string>;\n",
        "+  abstract dispatchRunStepDelta(\n",
        "+    id: string,\n",
        "+    delta: t.ToolCallDelta\n",
        "+  ): Promise<void>;\n",
        "+  abstract dispatchMessageDelta(\n",
        "+    id: string,\n",
        "+    delta: t.MessageDelta\n",
        "+  ): Promise<void>;\n",
        "   abstract dispatchReasoningDelta(\n",
        "     stepId: string,\n",
        "     delta: t.ReasoningDelta\n",
        "-  ): void;\n",
        "+  ): Promise<void>;\n",
        "   abstract handleToolCallCompleted(\n",
        "     data: t.ToolEndData,\n",
        "     metadata?: Record<string, unknown>,\n",
        "     omitOutput?: boolean\n",
        "-  ): void;\n",
        "+  ): Promise<void>;\n",
        " \n",
        "-  abstract createCallModel(): (\n",
        "-    state: T,\n",
        "-    config?: RunnableConfig\n",
        "-  ) => Promise<Partial<T>>;\n",
        "-  abstract createWorkflow(): t.CompiledWorkflow<T>;\n",
        "-  lastToken?: string;\n",
        "-  tokenTypeSwitch?: 'reasoning' | 'content';\n",
        "-  reasoningKey: 'reasoning_content' | 'reasoning' = 'reasoning_content';\n",
        "-  currentTokenType: ContentTypes.TEXT | ContentTypes.THINK | 'think_and_text' =\n",
        "-    ContentTypes.TEXT;\n",
        "+  abstract createCallModel(\n",
        "+    agentId?: string,\n",
        "+    currentModel?: t.ChatModel\n",
        "+  ): (state: T, config?: RunnableConfig) => Promise<Partial<T>>;\n",
        "   messageStepHasToolCalls: Map<string, boolean> = new Map();\n",
        "   messageIdsByStepKey: Map<string, string> = new Map();\n",
        "   prelimMessageIdsByStepKey: Map<string, string> = new Map();\n",
        "@@ -116,96 +125,52 @@ export abstract class Graph<\n",
        "   stepKeyIds: Map<string, string[]> = new Map<string, string[]>();\n",
        "   contentIndexMap: Map<string, number> = new Map();\n",
        "   toolCallStepIds: Map<string, string> = new Map();\n",
        "-  currentUsage: Partial<UsageMetadata> | undefined;\n",
        "-  indexTokenCountMap: Record<string, number | undefined> = {};\n",
        "-  maxContextTokens: number | undefined;\n",
        "-  pruneMessages?: ReturnType<typeof createPruneMessages>;\n",
        "-  /** The amount of time that should pass before another consecutive API call */\n",
        "-  streamBuffer: number | undefined;\n",
        "-  tokenCounter?: t.TokenCounter;\n",
        "   signal?: AbortSignal;\n",
        "   /** Set of invoked tool call IDs from non-message run steps completed mid-run, if any */\n",
        "   invokedToolIds?: Set<string>;\n",
        "   handlerRegistry: HandlerRegistry | undefined;\n",
        " }\n",
        " \n",
        "-export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n",
        "-  private graphState: t.GraphStateChannels<t.BaseGraphState>;\n",
        "-  clientOptions: t.ClientOptions;\n",
        "-  boundModel?: Runnable;\n",
        "-  /** The last recorded timestamp that a stream API call was invoked */\n",
        "-  lastStreamCall: number | undefined;\n",
        "-  systemMessage: SystemMessage | undefined;\n",
        "+export class StandardGraph extends Graph<t.BaseGraphState, t.GraphNode> {\n",
        "+  overrideModel?: t.ChatModel;\n",
        "+  /** Optional compile options passed into workflow.compile() */\n",
        "+  compileOptions?: t.CompileOptions | undefined;\n",
        "   messages: BaseMessage[] = [];\n",
        "   runId: string | undefined;\n",
        "-  tools?: t.GraphTools;\n",
        "-  toolMap?: t.ToolMap;\n",
        "   startIndex: number = 0;\n",
        "-  provider: Providers;\n",
        "-  toolEnd: boolean;\n",
        "   signal?: AbortSignal;\n",
        "+  /** Map of agent contexts by agent ID */\n",
        "+  agentContexts: Map<string, AgentContext> = new Map();\n",
        "+  /** Default agent ID to use */\n",
        "+  defaultAgentId: string;\n",
        " \n",
        "   constructor({\n",
        "+    // parent-level graph inputs\n",
        "     runId,\n",
        "-    tools,\n",
        "     signal,\n",
        "-    toolMap,\n",
        "-    provider,\n",
        "-    streamBuffer,\n",
        "-    instructions,\n",
        "-    reasoningKey,\n",
        "-    clientOptions,\n",
        "-    toolEnd = false,\n",
        "-    additional_instructions = '',\n",
        "+    agents,\n",
        "+    tokenCounter,\n",
        "+    indexTokenCountMap,\n",
        "   }: t.StandardGraphInput) {\n",
        "     super();\n",
        "     this.runId = runId;\n",
        "-    this.tools = tools;\n",
        "     this.signal = signal;\n",
        "-    this.toolEnd = toolEnd;\n",
        "-    this.toolMap = toolMap;\n",
        "-    this.provider = provider;\n",
        "-    this.streamBuffer = streamBuffer;\n",
        "-    this.clientOptions = clientOptions;\n",
        "-    this.graphState = this.createGraphState();\n",
        "-    this.boundModel = this.initializeModel();\n",
        "-    if (reasoningKey) {\n",
        "-      this.reasoningKey = reasoningKey;\n",
        "-    }\n",
        " \n",
        "-    let finalInstructions: string | BaseMessageFields | undefined =\n",
        "-      instructions;\n",
        "-    if (additional_instructions) {\n",
        "-      finalInstructions =\n",
        "-        finalInstructions != null && finalInstructions\n",
        "-          ? `${finalInstructions}\\n\\n${additional_instructions}`\n",
        "-          : additional_instructions;\n",
        "+    if (agents.length === 0) {\n",
        "+      throw new Error('At least one agent configuration is required');\n",
        "     }\n",
        " \n",
        "-    if (\n",
        "-      finalInstructions != null &&\n",
        "-      finalInstructions &&\n",
        "-      provider === Providers.ANTHROPIC &&\n",
        "-      ((\n",
        "-        (clientOptions as t.AnthropicClientOptions).clientOptions\n",
        "-          ?.defaultHeaders as Record<string, string> | undefined\n",
        "-      )?.['anthropic-beta']?.includes('prompt-caching') ??\n",
        "-        false)\n",
        "-    ) {\n",
        "-      finalInstructions = {\n",
        "-        content: [\n",
        "-          {\n",
        "-            type: 'text',\n",
        "-            text: instructions,\n",
        "-            cache_control: { type: 'ephemeral' },\n",
        "-          },\n",
        "-        ],\n",
        "-      };\n",
        "-    }\n",
        "+    for (const agentConfig of agents) {\n",
        "+      const agentContext = AgentContext.fromConfig(\n",
        "+        agentConfig,\n",
        "+        tokenCounter,\n",
        "+        indexTokenCountMap\n",
        "+      );\n",
        " \n",
        "-    if (finalInstructions != null && finalInstructions !== '') {\n",
        "-      this.systemMessage = new SystemMessage(finalInstructions);\n",
        "+      this.agentContexts.set(agentConfig.agentId, agentContext);\n",
        "     }\n",
        "+\n",
        "+    this.defaultAgentId = agents[0].agentId;\n",
        "   }\n",
        " \n",
        "   /* Init */\n",
        "@@ -224,24 +189,17 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n",
        "       new Map()\n",
        "     );\n",
        "     this.messageStepHasToolCalls = resetIfNotEmpty(\n",
        "-      this.prelimMessageIdsByStepKey,\n",
        "+      this.messageStepHasToolCalls,\n",
        "       new Map()\n",
        "     );\n",
        "     this.prelimMessageIdsByStepKey = resetIfNotEmpty(\n",
        "       this.prelimMessageIdsByStepKey,\n",
        "       new Map()\n",
        "     );\n",
        "-    this.currentTokenType = resetIfNotEmpty(\n",
        "-      this.currentTokenType,\n",
        "-      ContentTypes.TEXT\n",
        "-    );\n",
        "-    this.lastToken = resetIfNotEmpty(this.lastToken, undefined);\n",
        "-    this.tokenTypeSwitch = resetIfNotEmpty(this.tokenTypeSwitch, undefined);\n",
        "-    this.indexTokenCountMap = resetIfNotEmpty(this.indexTokenCountMap, {});\n",
        "-    this.currentUsage = resetIfNotEmpty(this.currentUsage, undefined);\n",
        "-    this.tokenCounter = resetIfNotEmpty(this.tokenCounter, undefined);\n",
        "-    this.maxContextTokens = resetIfNotEmpty(this.maxContextTokens, undefined);\n",
        "     this.invokedToolIds = resetIfNotEmpty(this.invokedToolIds, undefined);\n",
        "+    for (const context of this.agentContexts.values()) {\n",
        "+      context.reset();\n",
        "+    }\n",
        "   }\n",
        " \n",
        "   /* Run Step Processing */\n",
        "@@ -254,6 +212,33 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n",
        "     return undefined;\n",
        "   }\n",
        " \n",
        "+  getAgentContext(metadata: Record<string, unknown> | undefined): AgentContext {\n",
        "+    if (!metadata) {\n",
        "+      throw new Error('No metadata provided to retrieve agent context');\n",
        "+    }\n",
        "+\n",
        "+    const currentNode = metadata.langgraph_node as string;\n",
        "+    if (!currentNode) {\n",
        "+      throw new Error(\n",
        "+        'No langgraph_node in metadata to retrieve agent context'\n",
        "+      );\n",
        "+    }\n",
        "+\n",
        "+    let agentId: string | undefined;\n",
        "+    if (currentNode.startsWith(AGENT)) {\n",
        "+      agentId = currentNode.substring(AGENT.length);\n",
        "+    } else if (currentNode.startsWith(TOOLS)) {\n",
        "+      agentId = currentNode.substring(TOOLS.length);\n",
        "+    }\n",
        "+\n",
        "+    const agentContext = this.agentContexts.get(agentId ?? '');\n",
        "+    if (!agentContext) {\n",
        "+      throw new Error(`No agent context found for agent ID ${agentId}`);\n",
        "+    }\n",
        "+\n",
        "+    return agentContext;\n",
        "+  }\n",
        "+\n",
        "   getStepKey(metadata: Record<string, unknown> | undefined): string {\n",
        "     if (!metadata) return '';\n",
        " \n",
        "@@ -307,9 +292,11 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n",
        "       metadata.langgraph_step as number,\n",
        "       metadata.checkpoint_ns as string,\n",
        "     ];\n",
        "+\n",
        "+    const agentContext = this.getAgentContext(metadata);\n",
        "     if (\n",
        "-      this.currentTokenType === ContentTypes.THINK ||\n",
        "-      this.currentTokenType === 'think_and_text'\n",
        "+      agentContext.currentTokenType === ContentTypes.THINK ||\n",
        "+      agentContext.currentTokenType === 'think_and_text'\n",
        "     ) {\n",
        "       keyList.push('reasoning');\n",
        "     } else if (this.tokenTypeSwitch === 'content') {\n",
        "@@ -339,87 +326,127 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n",
        " \n",
        "   /* Graph */\n",
        " \n",
        "-  createGraphState(): t.GraphStateChannels<t.BaseGraphState> {\n",
        "-    return {\n",
        "-      messages: {\n",
        "-        value: (x: BaseMessage[], y: BaseMessage[]): BaseMessage[] => {\n",
        "-          if (!x.length) {\n",
        "-            if (this.systemMessage) {\n",
        "-              x.push(this.systemMessage);\n",
        "-            }\n",
        "+  createSystemRunnable({\n",
        "+    provider,\n",
        "+    clientOptions,\n",
        "+    instructions,\n",
        "+    additional_instructions,\n",
        "+  }: {\n",
        "+    provider?: Providers;\n",
        "+    clientOptions?: t.ClientOptions;\n",
        "+    instructions?: string;\n",
        "+    additional_instructions?: string;\n",
        "+  }): t.SystemRunnable | undefined {\n",
        "+    let finalInstructions: string | BaseMessageFields | undefined =\n",
        "+      instructions;\n",
        "+    if (additional_instructions != null && additional_instructions !== '') {\n",
        "+      finalInstructions =\n",
        "+        finalInstructions != null && finalInstructions\n",
        "+          ? `${finalInstructions}\\n\\n${additional_instructions}`\n",
        "+          : additional_instructions;\n",
        "+    }\n",
        " \n",
        "-            this.startIndex = x.length + y.length;\n",
        "-          }\n",
        "-          const current = x.concat(y);\n",
        "-          this.messages = current;\n",
        "-          return current;\n",
        "-        },\n",
        "-        default: () => [],\n",
        "-      },\n",
        "-    };\n",
        "+    if (\n",
        "+      finalInstructions != null &&\n",
        "+      finalInstructions &&\n",
        "+      provider === Providers.ANTHROPIC &&\n",
        "+      ((\n",
        "+        (clientOptions as t.AnthropicClientOptions).clientOptions\n",
        "+          ?.defaultHeaders as Record<string, string> | undefined\n",
        "+      )?.['anthropic-beta']?.includes('prompt-caching') ??\n",
        "+        false)\n",
        "+    ) {\n",
        "+      finalInstructions = {\n",
        "+        content: [\n",
        "+          {\n",
        "+            type: 'text',\n",
        "+            text: instructions,\n",
        "+            cache_control: { type: 'ephemeral' },\n",
        "+          },\n",
        "+        ],\n",
        "+      };\n",
        "+    }\n",
        "+\n",
        "+    if (finalInstructions != null && finalInstructions !== '') {\n",
        "+      const systemMessage = new SystemMessage(finalInstructions);\n",
        "+      return RunnableLambda.from((messages: BaseMessage[]) => {\n",
        "+        return [systemMessage, ...messages];\n",
        "+      }).withConfig({ runName: 'prompt' });\n",
        "+    }\n",
        "   }\n",
        " \n",
        "-  initializeTools():\n",
        "-    | CustomToolNode<t.BaseGraphState>\n",
        "-    | ToolNode<t.BaseGraphState> {\n",
        "+  initializeTools({\n",
        "+    currentTools,\n",
        "+    currentToolMap,\n",
        "+  }: {\n",
        "+    currentTools?: t.GraphTools;\n",
        "+    currentToolMap?: t.ToolMap;\n",
        "+  }): CustomToolNode<t.BaseGraphState> | ToolNode<t.BaseGraphState> {\n",
        "     // return new ToolNode<t.BaseGraphState>(this.tools);\n",
        "     return new CustomToolNode<t.BaseGraphState>({\n",
        "-      tools: (this.tools as t.GenericTool[] | undefined) || [],\n",
        "-      toolMap: this.toolMap,\n",
        "+      tools: (currentTools as t.GenericTool[] | undefined) ?? [],\n",
        "+      toolMap: currentToolMap,\n",
        "       toolCallStepIds: this.toolCallStepIds,\n",
        "       errorHandler: (data, metadata) =>\n",
        "         StandardGraph.handleToolCallErrorStatic(this, data, metadata),\n",
        "     });\n",
        "   }\n",
        " \n",
        "-  initializeModel(): Runnable {\n",
        "-    const ChatModelClass = getChatModelClass(this.provider);\n",
        "-    const model = new ChatModelClass(this.clientOptions);\n",
        "+  initializeModel({\n",
        "+    provider,\n",
        "+    tools,\n",
        "+    clientOptions,\n",
        "+  }: {\n",
        "+    provider: Providers;\n",
        "+    tools?: t.GraphTools;\n",
        "+    clientOptions?: t.ClientOptions;\n",
        "+  }): Runnable {\n",
        "+    const ChatModelClass = getChatModelClass(provider);\n",
        "+    const model = new ChatModelClass(clientOptions ?? {});\n",
        " \n",
        "     if (\n",
        "-      isOpenAILike(this.provider) &&\n",
        "+      isOpenAILike(provider) &&\n",
        "       (model instanceof ChatOpenAI || model instanceof AzureChatOpenAI)\n",
        "     ) {\n",
        "-      model.temperature = (this.clientOptions as t.OpenAIClientOptions)\n",
        "+      model.temperature = (clientOptions as t.OpenAIClientOptions)\n",
        "         .temperature as number;\n",
        "-      model.topP = (this.clientOptions as t.OpenAIClientOptions).topP as number;\n",
        "-      model.frequencyPenalty = (this.clientOptions as t.OpenAIClientOptions)\n",
        "+      model.topP = (clientOptions as t.OpenAIClientOptions).topP as number;\n",
        "+      model.frequencyPenalty = (clientOptions as t.OpenAIClientOptions)\n",
        "         .frequencyPenalty as number;\n",
        "-      model.presencePenalty = (this.clientOptions as t.OpenAIClientOptions)\n",
        "+      model.presencePenalty = (clientOptions as t.OpenAIClientOptions)\n",
        "         .presencePenalty as number;\n",
        "-      model.n = (this.clientOptions as t.OpenAIClientOptions).n as number;\n",
        "+      model.n = (clientOptions as t.OpenAIClientOptions).n as number;\n",
        "     } else if (\n",
        "-      this.provider === Providers.VERTEXAI &&\n",
        "+      provider === Providers.VERTEXAI &&\n",
        "       model instanceof ChatVertexAI\n",
        "     ) {\n",
        "-      model.temperature = (this.clientOptions as t.VertexAIClientOptions)\n",
        "+      model.temperature = (clientOptions as t.VertexAIClientOptions)\n",
        "         .temperature as number;\n",
        "-      model.topP = (this.clientOptions as t.VertexAIClientOptions)\n",
        "-        .topP as number;\n",
        "-      model.topK = (this.clientOptions as t.VertexAIClientOptions)\n",
        "-        .topK as number;\n",
        "-      model.topLogprobs = (this.clientOptions as t.VertexAIClientOptions)\n",
        "+      model.topP = (clientOptions as t.VertexAIClientOptions).topP as number;\n",
        "+      model.topK = (clientOptions as t.VertexAIClientOptions).topK as number;\n",
        "+      model.topLogprobs = (clientOptions as t.VertexAIClientOptions)\n",
        "         .topLogprobs as number;\n",
        "-      model.frequencyPenalty = (this.clientOptions as t.VertexAIClientOptions)\n",
        "+      model.frequencyPenalty = (clientOptions as t.VertexAIClientOptions)\n",
        "         .frequencyPenalty as number;\n",
        "-      model.presencePenalty = (this.clientOptions as t.VertexAIClientOptions)\n",
        "+      model.presencePenalty = (clientOptions as t.VertexAIClientOptions)\n",
        "         .presencePenalty as number;\n",
        "-      model.maxOutputTokens = (this.clientOptions as t.VertexAIClientOptions)\n",
        "+      model.maxOutputTokens = (clientOptions as t.VertexAIClientOptions)\n",
        "         .maxOutputTokens as number;\n",
        "     }\n",
        " \n",
        "-    if (!this.tools || this.tools.length === 0) {\n",
        "+    if (!tools || tools.length === 0) {\n",
        "       return model as unknown as Runnable;\n",
        "     }\n",
        " \n",
        "-    return (model as t.ModelWithTools).bindTools(this.tools);\n",
        "+    return (model as t.ModelWithTools).bindTools(tools);\n",
        "   }\n",
        "+\n",
        "   overrideTestModel(\n",
        "     responses: string[],\n",
        "     sleep?: number,\n",
        "     toolCalls?: ToolCall[]\n",
        "   ): void {\n",
        "-    this.boundModel = createFakeStreamingLLM({\n",
        "+    this.overrideModel = createFakeStreamingLLM({\n",
        "       responses,\n",
        "       sleep,\n",
        "       toolCalls,\n",
        "@@ -429,64 +456,113 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n",
        "   getNewModel({\n",
        "     provider,\n",
        "     clientOptions,\n",
        "-    omitOptions,\n",
        "   }: {\n",
        "     provider: Providers;\n",
        "     clientOptions?: t.ClientOptions;\n",
        "-    omitOptions?: Set<string>;\n",
        "   }): t.ChatModelInstance {\n",
        "     const ChatModelClass = getChatModelClass(provider);\n",
        "-    const options =\n",
        "-      omitOptions && clientOptions == null\n",
        "-        ? Object.assign(\n",
        "-          Object.fromEntries(\n",
        "-            Object.entries(this.clientOptions).filter(\n",
        "-              ([key]) => !omitOptions.has(key)\n",
        "-            )\n",
        "-          ),\n",
        "-          clientOptions\n",
        "-        )\n",
        "-        : (clientOptions ?? this.clientOptions);\n",
        "-    return new ChatModelClass(options);\n",
        "+    return new ChatModelClass(clientOptions ?? {});\n",
        "   }\n",
        " \n",
        "-  storeUsageMetadata(finalMessage?: BaseMessage): void {\n",
        "+  getUsageMetadata(\n",
        "+    finalMessage?: BaseMessage\n",
        "+  ): Partial<UsageMetadata> | undefined {\n",
        "     if (\n",
        "       finalMessage &&\n",
        "       'usage_metadata' in finalMessage &&\n",
        "       finalMessage.usage_metadata != null\n",
        "     ) {\n",
        "-      this.currentUsage = finalMessage.usage_metadata as Partial<UsageMetadata>;\n",
        "+      return finalMessage.usage_metadata as Partial<UsageMetadata>;\n",
        "+    }\n",
        "+  }\n",
        "+\n",
        "+  /** Execute model invocation with streaming support */\n",
        "+  private async attemptInvoke(\n",
        "+    {\n",
        "+      currentModel,\n",
        "+      finalMessages,\n",
        "+      provider,\n",
        "+      tools,\n",
        "+    }: {\n",
        "+      currentModel?: t.ChatModel;\n",
        "+      finalMessages: BaseMessage[];\n",
        "+      provider: Providers;\n",
        "+      tools?: t.GraphTools;\n",
        "+    },\n",
        "+    config?: RunnableConfig\n",
        "+  ): Promise<Partial<t.BaseGraphState>> {\n",
        "+    const model = this.overrideModel ?? currentModel;\n",
        "+    if (!model) {\n",
        "+      throw new Error('No model found');\n",
        "+    }\n",
        "+\n",
        "+    if ((tools?.length ?? 0) > 0 && manualToolStreamProviders.has(provider)) {\n",
        "+      if (!model.stream) {\n",
        "+        throw new Error('Model does not support stream');\n",
        "+      }\n",
        "+      const stream = await model.stream(finalMessages, config);\n",
        "+      let finalChunk: AIMessageChunk | undefined;\n",
        "+      for await (const chunk of stream) {\n",
        "+        await safeDispatchCustomEvent(\n",
        "+          GraphEvents.CHAT_MODEL_STREAM,\n",
        "+          { chunk, emitted: true },\n",
        "+          config\n",
        "+        );\n",
        "+        finalChunk = finalChunk ? concat(finalChunk, chunk) : chunk;\n",
        "+      }\n",
        "+      finalChunk = modifyDeltaProperties(provider, finalChunk);\n",
        "+      return { messages: [finalChunk as AIMessageChunk] };\n",
        "+    } else {\n",
        "+      const finalMessage = await model.invoke(finalMessages, config);\n",
        "+      if ((finalMessage.tool_calls?.length ?? 0) > 0) {\n",
        "+        finalMessage.tool_calls = finalMessage.tool_calls?.filter(\n",
        "+          (tool_call: ToolCall) => !!tool_call.name\n",
        "+        );\n",
        "+      }\n",
        "+      return { messages: [finalMessage] };\n",
        "     }\n",
        "   }\n",
        " \n",
        "-  cleanupSignalListener(): void {\n",
        "+  cleanupSignalListener(currentModel?: t.ChatModel): void {\n",
        "     if (!this.signal) {\n",
        "       return;\n",
        "     }\n",
        "-    if (!this.boundModel) {\n",
        "+    const model = this.overrideModel ?? currentModel;\n",
        "+    if (!model) {\n",
        "       return;\n",
        "     }\n",
        "-    const client = (this.boundModel as ChatOpenAI | undefined)?.exposedClient;\n",
        "+    const client = (model as ChatOpenAI | undefined)?.exposedClient;\n",
        "     if (!client?.abortHandler) {\n",
        "       return;\n",
        "     }\n",
        "     this.signal.removeEventListener('abort', client.abortHandler);\n",
        "     client.abortHandler = undefined;\n",
        "   }\n",
        " \n",
        "-  createCallModel() {\n",
        "+  createCallModel(agentId = 'default', currentModel?: t.ChatModel) {\n",
        "     return async (\n",
        "       state: t.BaseGraphState,\n",
        "       config?: RunnableConfig\n",
        "     ): Promise<Partial<t.BaseGraphState>> => {\n",
        "-      const { provider = '' } =\n",
        "-        (config?.configurable as t.GraphConfig | undefined) ?? {};\n",
        "-      if (this.boundModel == null) {\n",
        "+      /**\n",
        "+       * Get agent context - it must exist by this point\n",
        "+       */\n",
        "+      const agentContext = this.agentContexts.get(agentId);\n",
        "+      if (!agentContext) {\n",
        "+        throw new Error(`Agent context not found for agentId: ${agentId}`);\n",
        "+      }\n",
        "+\n",
        "+      const model = this.overrideModel ?? currentModel;\n",
        "+      if (!model) {\n",
        "         throw new Error('No Graph model found');\n",
        "       }\n",
        "-      if (!config || !provider) {\n",
        "-        throw new Error(`No ${config ? 'provider' : 'config'} provided`);\n",
        "+      if (!config) {\n",
        "+        throw new Error('No config provided');\n",
        "+      }\n",
        "+\n",
        "+      // Ensure token calculations are complete before proceeding\n",
        "+      if (agentContext.tokenCalculationPromise) {\n",
        "+        await agentContext.tokenCalculationPromise;\n",
        "       }\n",
        "       if (!config.signal) {\n",
        "         config.signal = this.signal;\n",
        "@@ -496,40 +572,40 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n",
        " \n",
        "       let messagesToUse = messages;\n",
        "       if (\n",
        "-        !this.pruneMessages &&\n",
        "-        this.tokenCounter &&\n",
        "-        this.maxContextTokens != null &&\n",
        "-        this.indexTokenCountMap[0] != null\n",
        "+        !agentContext.pruneMessages &&\n",
        "+        agentContext.tokenCounter &&\n",
        "+        agentContext.maxContextTokens != null &&\n",
        "+        agentContext.indexTokenCountMap[0] != null\n",
        "       ) {\n",
        "         const isAnthropicWithThinking =\n",
        "-          (this.provider === Providers.ANTHROPIC &&\n",
        "-            (this.clientOptions as t.AnthropicClientOptions).thinking !=\n",
        "+          (agentContext.provider === Providers.ANTHROPIC &&\n",
        "+            (agentContext.clientOptions as t.AnthropicClientOptions).thinking !=\n",
        "               null) ||\n",
        "-          (this.provider === Providers.BEDROCK &&\n",
        "-            (this.clientOptions as t.BedrockAnthropicInput)\n",
        "+          (agentContext.provider === Providers.BEDROCK &&\n",
        "+            (agentContext.clientOptions as t.BedrockAnthropicInput)\n",
        "               .additionalModelRequestFields?.['thinking'] != null) ||\n",
        "           (this.provider === Providers.OPENAI &&\n",
        "             (\n",
        "               (this.clientOptions as t.OpenAIClientOptions).modelKwargs\n",
        "                 ?.thinking as t.AnthropicClientOptions['thinking']\n",
        "             )?.type === 'enabled');\n",
        " \n",
        "-        this.pruneMessages = createPruneMessages({\n",
        "-          provider: this.provider,\n",
        "-          indexTokenCountMap: this.indexTokenCountMap,\n",
        "-          maxTokens: this.maxContextTokens,\n",
        "-          tokenCounter: this.tokenCounter,\n",
        "+        agentContext.pruneMessages = createPruneMessages({\n",
        "           startIndex: this.startIndex,\n",
        "+          provider: agentContext.provider,\n",
        "+          tokenCounter: agentContext.tokenCounter,\n",
        "+          maxTokens: agentContext.maxContextTokens,\n",
        "           thinkingEnabled: isAnthropicWithThinking,\n",
        "+          indexTokenCountMap: agentContext.indexTokenCountMap,\n",
        "         });\n",
        "       }\n",
        "-      if (this.pruneMessages) {\n",
        "-        const { context, indexTokenCountMap } = this.pruneMessages({\n",
        "+      if (agentContext.pruneMessages) {\n",
        "+        const { context, indexTokenCountMap } = agentContext.pruneMessages({\n",
        "           messages,\n",
        "-          usageMetadata: this.currentUsage,\n",
        "+          usageMetadata: agentContext.currentUsage,\n",
        "           // startOnMessageType: 'human',\n",
        "         });\n",
        "-        this.indexTokenCountMap = indexTokenCountMap;\n",
        "+        agentContext.indexTokenCountMap = indexTokenCountMap;\n",
        "         messagesToUse = context;\n",
        "       }\n",
        " \n",
        "@@ -544,7 +620,7 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n",
        "           : null;\n",
        " \n",
        "       if (\n",
        "-        provider === Providers.BEDROCK &&\n",
        "+        agentContext.provider === Providers.BEDROCK &&\n",
        "         lastMessageX instanceof AIMessageChunk &&\n",
        "         lastMessageY instanceof ToolMessage &&\n",
        "         typeof lastMessageX.content === 'string'\n",
        "@@ -554,95 +630,176 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n",
        " \n",
        "       const isLatestToolMessage = lastMessageY instanceof ToolMessage;\n",
        " \n",
        "-      if (isLatestToolMessage && provider === Providers.ANTHROPIC) {\n",
        "+      if (\n",
        "+        isLatestToolMessage &&\n",
        "+        agentContext.provider === Providers.ANTHROPIC\n",
        "+      ) {\n",
        "         formatAnthropicArtifactContent(finalMessages);\n",
        "       } else if (\n",
        "         isLatestToolMessage &&\n",
        "-        (isOpenAILike(provider) || isGoogleLike(provider))\n",
        "+        (isOpenAILike(agentContext.provider) ||\n",
        "+          isGoogleLike(agentContext.provider))\n",
        "       ) {\n",
        "         formatArtifactPayload(finalMessages);\n",
        "       }\n",
        " \n",
        "-      if (this.lastStreamCall != null && this.streamBuffer != null) {\n",
        "-        const timeSinceLastCall = Date.now() - this.lastStreamCall;\n",
        "-        if (timeSinceLastCall < this.streamBuffer) {\n",
        "+      if (\n",
        "+        agentContext.lastStreamCall != null &&\n",
        "+        agentContext.streamBuffer != null\n",
        "+      ) {\n",
        "+        const timeSinceLastCall = Date.now() - agentContext.lastStreamCall;\n",
        "+        if (timeSinceLastCall < agentContext.streamBuffer) {\n",
        "           const timeToWait =\n",
        "-            Math.ceil((this.streamBuffer - timeSinceLastCall) / 1000) * 1000;\n",
        "+            Math.ceil((agentContext.streamBuffer - timeSinceLastCall) / 1000) *\n",
        "+            1000;\n",
        "           await sleep(timeToWait);\n",
        "         }\n",
        "       }\n",
        " \n",
        "-      this.lastStreamCall = Date.now();\n",
        "+      agentContext.lastStreamCall = Date.now();\n",
        " \n",
        "-      let result: Partial<t.BaseGraphState>;\n",
        "-      if (\n",
        "-        (this.tools?.length ?? 0) > 0 &&\n",
        "-        manualToolStreamProviders.has(provider)\n",
        "-      ) {\n",
        "-        const stream = await this.boundModel.stream(finalMessages, config);\n",
        "-        let finalChunk: AIMessageChunk | undefined;\n",
        "-        for await (const chunk of stream) {\n",
        "-          dispatchCustomEvent(GraphEvents.CHAT_MODEL_STREAM, { chunk }, config);\n",
        "-          if (!finalChunk) {\n",
        "-            finalChunk = chunk;\n",
        "-          } else {\n",
        "-            finalChunk = concat(finalChunk, chunk);\n",
        "+      let result: Partial<t.BaseGraphState> | undefined;\n",
        "+      const fallbacks =\n",
        "+        (agentContext.clientOptions as t.LLMConfig | undefined)?.fallbacks ??\n",
        "+        [];\n",
        "+      try {\n",
        "+        result = await this.attemptInvoke(\n",
        "+          {\n",
        "+            currentModel: model,\n",
        "+            finalMessages,\n",
        "+            provider: agentContext.provider,\n",
        "+            tools: agentContext.tools,\n",
        "+          },\n",
        "+          config\n",
        "+        );\n",
        "+      } catch (primaryError) {\n",
        "+        let lastError: unknown = primaryError;\n",
        "+        for (const fb of fallbacks) {\n",
        "+          try {\n",
        "+            let model = this.getNewModel({\n",
        "+              provider: fb.provider,\n",
        "+              clientOptions: fb.clientOptions,\n",
        "+            });\n",
        "+            const bindableTools = agentContext.tools;\n",
        "+            model = (\n",
        "+              !bindableTools || bindableTools.length === 0\n",
        "+                ? model\n",
        "+                : model.bindTools(bindableTools)\n",
        "+            ) as t.ChatModelInstance;\n",
        "+            result = await this.attemptInvoke(\n",
        "+              {\n",
        "+                currentModel: model,\n",
        "+                finalMessages,\n",
        "+                provider: fb.provider,\n",
        "+                tools: agentContext.tools,\n",
        "+              },\n",
        "+              config\n",
        "+            );\n",
        "+            lastError = undefined;\n",
        "+            break;\n",
        "+          } catch (e) {\n",
        "+            lastError = e;\n",
        "+            continue;\n",
        "           }\n",
        "         }\n",
        "-\n",
        "-        finalChunk = modifyDeltaProperties(this.provider, finalChunk);\n",
        "-        result = { messages: [finalChunk as AIMessageChunk] };\n",
        "-      } else {\n",
        "-        const finalMessage = (await this.boundModel.invoke(\n",
        "-          finalMessages,\n",
        "-          config\n",
        "-        )) as AIMessageChunk;\n",
        "-        if ((finalMessage.tool_calls?.length ?? 0) > 0) {\n",
        "-          finalMessage.tool_calls = finalMessage.tool_calls?.filter(\n",
        "-            (tool_call) => {\n",
        "-              if (!tool_call.name) {\n",
        "-                return false;\n",
        "-              }\n",
        "-              return true;\n",
        "-            }\n",
        "-          );\n",
        "+        if (lastError !== undefined) {\n",
        "+          throw lastError;\n",
        "         }\n",
        "-        result = { messages: [finalMessage] };\n",
        "       }\n",
        " \n",
        "-      this.storeUsageMetadata(result.messages?.[0]);\n",
        "+      if (!result) {\n",
        "+        throw new Error('No result after model invocation');\n",
        "+      }\n",
        "+      agentContext.currentUsage = this.getUsageMetadata(result.messages?.[0]);\n",
        "       this.cleanupSignalListener();\n",
        "       return result;\n",
        "     };\n",
        "   }\n",
        " \n",
        "-  createWorkflow(): t.CompiledWorkflow<t.BaseGraphState> {\n",
        "+  createAgentNode(agentId: string): t.CompiledAgentWorfklow {\n",
        "+    const agentContext = this.agentContexts.get(agentId);\n",
        "+    if (!agentContext) {\n",
        "+      throw new Error(`Agent context not found for agentId: ${agentId}`);\n",
        "+    }\n",
        "+\n",
        "+    let currentModel = this.initializeModel({\n",
        "+      tools: agentContext.tools,\n",
        "+      provider: agentContext.provider,\n",
        "+      clientOptions: agentContext.clientOptions,\n",
        "+    });\n",
        "+\n",
        "+    if (agentContext.systemRunnable) {\n",
        "+      currentModel = agentContext.systemRunnable.pipe(currentModel);\n",
        "+    }\n",
        "+\n",
        "+    const agentNode = `${AGENT}${agentId}` as const;\n",
        "+    const toolNode = `${TOOLS}${agentId}` as const;\n",
        "+\n",
        "     const routeMessage = (\n",
        "       state: t.BaseGraphState,\n",
        "       config?: RunnableConfig\n",
        "     ): string => {\n",
        "       this.config = config;\n",
        "-      return toolsCondition(state, this.invokedToolIds);\n",
        "+      return toolsCondition(state, toolNode, this.invokedToolIds);\n",
        "     };\n",
        " \n",
        "-    const workflow = new StateGraph<t.BaseGraphState>({\n",
        "-      channels: this.graphState,\n",
        "-    })\n",
        "-      .addNode(AGENT, this.createCallModel())\n",
        "-      .addNode(TOOLS, this.initializeTools())\n",
        "-      .addEdge(START, AGENT)\n",
        "-      .addConditionalEdges(AGENT, routeMessage)\n",
        "-      .addEdge(TOOLS, this.toolEnd ? END : AGENT);\n",
        "+    const StateAnnotation = Annotation.Root({\n",
        "+      messages: Annotation<BaseMessage[]>({\n",
        "+        reducer: messagesStateReducer,\n",
        "+        default: () => [],\n",
        "+      }),\n",
        "+    });\n",
        "+\n",
        "+    const workflow = new StateGraph(StateAnnotation)\n",
        "+      .addNode(agentNode, this.createCallModel(agentId, currentModel))\n",
        "+      .addNode(\n",
        "+        toolNode,\n",
        "+        this.initializeTools({\n",
        "+          currentTools: agentContext.tools,\n",
        "+          currentToolMap: agentContext.toolMap,\n",
        "+        })\n",
        "+      )\n",
        "+      .addEdge(START, agentNode)\n",
        "+      .addConditionalEdges(agentNode, routeMessage)\n",
        "+      .addEdge(toolNode, agentContext.toolEnd ? END : agentNode);\n",
        "+\n",
        "+    // Cast to unknown to avoid tight coupling to external types; options are opt-in\n",
        "+    return workflow.compile(this.compileOptions as unknown as never);\n",
        "+  }\n",
        "+\n",
        "+  createWorkflow(): t.CompiledStateWorkflow {\n",
        "+    /** Use the default (first) agent for now */\n",
        "+    const agentNode = this.createAgentNode(this.defaultAgentId);\n",
        "+    const StateAnnotation = Annotation.Root({\n",
        "+      messages: Annotation<BaseMessage[]>({\n",
        "+        reducer: (a, b) => {\n",
        "+          if (!a.length) {\n",
        "+            this.startIndex = a.length + b.length;\n",
        "+          }\n",
        "+          const result = messagesStateReducer(a, b);\n",
        "+          this.messages = result;\n",
        "+          return result;\n",
        "+        },\n",
        "+        default: () => [],\n",
        "+      }),\n",
        "+    });\n",
        "+    const workflow = new StateGraph(StateAnnotation)\n",
        "+      .addNode(this.defaultAgentId, agentNode, { ends: [END] })\n",
        "+      .addEdge(START, this.defaultAgentId)\n",
        "+      .compile();\n",
        " \n",
        "-    return workflow.compile();\n",
        "+    return workflow;\n",
        "   }\n",
        " \n",
        "   /* Dispatchers */\n",
        " \n",
        "   /**\n",
        "    * Dispatches a run step to the client, returns the step ID\n",
        "    */\n",
        "-  dispatchRunStep(stepKey: string, stepDetails: t.StepDetails): string {\n",
        "+  async dispatchRunStep(\n",
        "+    stepKey: string,\n",
        "+    stepDetails: t.StepDetails\n",
        "+  ): Promise<string> {\n",
        "     if (!this.config) {\n",
        "       throw new Error('No config provided');\n",
        "     }\n",
        "@@ -674,15 +831,19 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n",
        " \n",
        "     this.contentData.push(runStep);\n",
        "     this.contentIndexMap.set(stepId, runStep.index);\n",
        "-    dispatchCustomEvent(GraphEvents.ON_RUN_STEP, runStep, this.config);\n",
        "+    await safeDispatchCustomEvent(\n",
        "+      GraphEvents.ON_RUN_STEP,\n",
        "+      runStep,\n",
        "+      this.config\n",
        "+    );\n",
        "     return stepId;\n",
        "   }\n",
        " \n",
        "-  handleToolCallCompleted(\n",
        "+  async handleToolCallCompleted(\n",
        "     data: t.ToolEndData,\n",
        "     metadata?: Record<string, unknown>,\n",
        "     omitOutput?: boolean\n",
        "-  ): void {\n",
        "+  ): Promise<void> {\n",
        "     if (!this.config) {\n",
        "       throw new Error('No config provided');\n",
        "     }\n",
        "@@ -691,7 +852,11 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n",
        "       return;\n",
        "     }\n",
        " \n",
        "-    const { input, output } = data;\n",
        "+    const { input, output: _output } = data;\n",
        "+    if ((_output as Command | undefined)?.lg_name === 'Command') {\n",
        "+      return;\n",
        "+    }\n",
        "+    const output = _output as ToolMessage;\n",
        "     const { tool_call_id } = output;\n",
        "     const stepId = this.toolCallStepIds.get(tool_call_id) ?? '';\n",
        "     if (!stepId) {\n",
        "@@ -717,29 +882,31 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n",
        "       progress: 1,\n",
        "     };\n",
        " \n",
        "-    this.handlerRegistry?.getHandler(GraphEvents.ON_RUN_STEP_COMPLETED)?.handle(\n",
        "-      GraphEvents.ON_RUN_STEP_COMPLETED,\n",
        "-      {\n",
        "-        result: {\n",
        "-          id: stepId,\n",
        "-          index: runStep.index,\n",
        "-          type: 'tool_call',\n",
        "-          tool_call,\n",
        "-        } as t.ToolCompleteEvent,\n",
        "-      },\n",
        "-      metadata,\n",
        "-      this\n",
        "-    );\n",
        "+    await this.handlerRegistry\n",
        "+      ?.getHandler(GraphEvents.ON_RUN_STEP_COMPLETED)\n",
        "+      ?.handle(\n",
        "+        GraphEvents.ON_RUN_STEP_COMPLETED,\n",
        "+        {\n",
        "+          result: {\n",
        "+            id: stepId,\n",
        "+            index: runStep.index,\n",
        "+            type: 'tool_call',\n",
        "+            tool_call,\n",
        "+          } as t.ToolCompleteEvent,\n",
        "+        },\n",
        "+        metadata,\n",
        "+        this\n",
        "+      );\n",
        "   }\n",
        "   /**\n",
        "    * Static version of handleToolCallError to avoid creating strong references\n",
        "    * that prevent garbage collection\n",
        "    */\n",
        "-  static handleToolCallErrorStatic(\n",
        "+  static async handleToolCallErrorStatic(\n",
        "     graph: StandardGraph,\n",
        "     data: t.ToolErrorData,\n",
        "     metadata?: Record<string, unknown>\n",
        "-  ): void {\n",
        "+  ): Promise<void> {\n",
        "     if (!graph.config) {\n",
        "       throw new Error('No config provided');\n",
        "     }\n",
        "@@ -769,7 +936,7 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n",
        "       progress: 1,\n",
        "     };\n",
        " \n",
        "-    graph.handlerRegistry\n",
        "+    await graph.handlerRegistry\n",
        "       ?.getHandler(GraphEvents.ON_RUN_STEP_COMPLETED)\n",
        "       ?.handle(\n",
        "         GraphEvents.ON_RUN_STEP_COMPLETED,\n",
        "@@ -790,14 +957,17 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n",
        "    * Instance method that delegates to the static method\n",
        "    * Kept for backward compatibility\n",
        "    */\n",
        "-  handleToolCallError(\n",
        "+  async handleToolCallError(\n",
        "     data: t.ToolErrorData,\n",
        "     metadata?: Record<string, unknown>\n",
        "-  ): void {\n",
        "-    StandardGraph.handleToolCallErrorStatic(this, data, metadata);\n",
        "+  ): Promise<void> {\n",
        "+    await StandardGraph.handleToolCallErrorStatic(this, data, metadata);\n",
        "   }\n",
        " \n",
        "-  dispatchRunStepDelta(id: string, delta: t.ToolCallDelta): void {\n",
        "+  async dispatchRunStepDelta(\n",
        "+    id: string,\n",
        "+    delta: t.ToolCallDelta\n",
        "+  ): Promise<void> {\n",
        "     if (!this.config) {\n",
        "       throw new Error('No config provided');\n",
        "     } else if (!id) {\n",
        "@@ -807,37 +977,40 @@ export class StandardGraph extends Graph<t.BaseGraphState, GraphNode> {\n",
        "       id,\n",
        "       delta,\n",
        "     };\n",
        "-    dispatchCustomEvent(\n",
        "+    await safeDispatchCustomEvent(\n",
        "       GraphEvents.ON_RUN_STEP_DELTA,\n",
        "       runStepDelta,\n",
        "       this.config\n",
        "     );\n",
        "   }\n",
        " \n",
        "-  dispatchMessageDelta(id: string, delta: t.MessageDelta): void {\n",
        "+  async dispatchMessageDelta(id: string, delta: t.MessageDelta): Promise<void> {\n",
        "     if (!this.config) {\n",
        "       throw new Error('No config provided');\n",
        "     }\n",
        "     const messageDelta: t.MessageDeltaEvent = {\n",
        "       id,\n",
        "       delta,\n",
        "     };\n",
        "-    dispatchCustomEvent(\n",
        "+    await safeDispatchCustomEvent(\n",
        "       GraphEvents.ON_MESSAGE_DELTA,\n",
        "       messageDelta,\n",
        "       this.config\n",
        "     );\n",
        "   }\n",
        " \n",
        "-  dispatchReasoningDelta = (stepId: string, delta: t.ReasoningDelta): void => {\n",
        "+  dispatchReasoningDelta = async (\n",
        "+    stepId: string,\n",
        "+    delta: t.ReasoningDelta\n",
        "+  ): Promise<void> => {\n",
        "     if (!this.config) {\n",
        "       throw new Error('No config provided');\n",
        "     }\n",
        "     const reasoningDelta: t.ReasoningDeltaEvent = {\n",
        "       id: stepId,\n",
        "       delta,\n",
        "     };\n",
        "-    dispatchCustomEvent(\n",
        "+    await safeDispatchCustomEvent(\n",
        "       GraphEvents.ON_REASONING_DELTA,\n",
        "       reasoningDelta,\n",
        "       this.config\n"
      ]
    },
    {
      "path": "src/graphs/MultiAgentGraph.ts",
      "status": "added",
      "additions": 381,
      "deletions": 0,
      "patch": "@@ -0,0 +1,381 @@\n+import { z } from 'zod';\n+import { tool } from '@langchain/core/tools';\n+import { ToolMessage, HumanMessage } from '@langchain/core/messages';\n+import {\n+  END,\n+  START,\n+  Command,\n+  StateGraph,\n+  Annotation,\n+  getCurrentTaskInput,\n+  messagesStateReducer,\n+} from '@langchain/langgraph';\n+import type { ToolRunnableConfig } from '@langchain/core/tools';\n+import type { BaseMessage } from '@langchain/core/messages';\n+import type * as t from '@/types';\n+import { StandardGraph } from './Graph';\n+\n+/**\n+ * MultiAgentGraph extends StandardGraph to support dynamic multi-agent workflows\n+ * with handoffs, fan-in/fan-out, and other composable patterns\n+ */\n+export class MultiAgentGraph extends StandardGraph {\n+  private edges: t.GraphEdge[];\n+  private startingNodes: Set<string> = new Set();\n+  private directEdges: t.GraphEdge[] = [];\n+  private handoffEdges: t.GraphEdge[] = [];\n+\n+  constructor(input: t.MultiAgentGraphInput) {\n+    super(input);\n+    this.edges = input.edges;\n+    this.categorizeEdges();\n+    this.analyzeGraph();\n+    this.createHandoffTools();\n+  }\n+\n+  /**\n+   * Categorize edges into handoff and direct types\n+   */\n+  private categorizeEdges(): void {\n+    for (const edge of this.edges) {\n+      // Default behavior: edges with conditions or explicit 'handoff' type are handoff edges\n+      // Edges with explicit 'direct' type or multi-destination without conditions are direct edges\n+      if (edge.edgeType === 'direct') {\n+        this.directEdges.push(edge);\n+      } else if (edge.edgeType === 'handoff' || edge.condition != null) {\n+        this.handoffEdges.push(edge);\n+      } else {\n+        // Default: single-to-single edges are handoff, single-to-multiple are direct\n+        const destinations = Array.isArray(edge.to) ? edge.to : [edge.to];\n+        const sources = Array.isArray(edge.from) ? edge.from : [edge.from];\n+\n+        if (sources.length === 1 && destinations.length > 1) {\n+          // Fan-out pattern defaults to direct\n+          this.directEdges.push(edge);\n+        } else {\n+          // Everything else defaults to handoff\n+          this.handoffEdges.push(edge);\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Analyze graph structure to determine starting nodes and connections\n+   */\n+  private analyzeGraph(): void {\n+    const hasIncomingEdge = new Set<string>();\n+\n+    // Track all nodes that have incoming edges\n+    for (const edge of this.edges) {\n+      const destinations = Array.isArray(edge.to) ? edge.to : [edge.to];\n+      destinations.forEach((dest) => hasIncomingEdge.add(dest));\n+    }\n+\n+    // Starting nodes are those without incoming edges\n+    for (const agentId of this.agentContexts.keys()) {\n+      if (!hasIncomingEdge.has(agentId)) {\n+        this.startingNodes.add(agentId);\n+      }\n+    }\n+\n+    // If no starting nodes found, use the first agent\n+    if (this.startingNodes.size === 0 && this.agentContexts.size > 0) {\n+      this.startingNodes.add(this.agentContexts.keys().next().value!);\n+    }\n+  }\n+\n+  /**\n+   * Create handoff tools for agents based on handoff edges only\n+   */\n+  private createHandoffTools(): void {\n+    // Group handoff edges by source agent(s)\n+    const handoffsByAgent = new Map<string, t.GraphEdge[]>();\n+\n+    // Only process handoff edges for tool creation\n+    for (const edge of this.handoffEdges) {\n+      const sources = Array.isArray(edge.from) ? edge.from : [edge.from];\n+      sources.forEach((source) => {\n+        if (!handoffsByAgent.has(source)) {\n+          handoffsByAgent.set(source, []);\n+        }\n+        handoffsByAgent.get(source)!.push(edge);\n+      });\n+    }\n+\n+    // Create handoff tools for each agent\n+    for (const [agentId, edges] of handoffsByAgent) {\n+      const agentContext = this.agentContexts.get(agentId);\n+      if (!agentContext) continue;\n+\n+      // Create handoff tools for this agent's outgoing edges\n+      const handoffTools: t.GenericTool[] = [];\n+      for (const edge of edges) {\n+        handoffTools.push(...this.createHandoffToolsForEdge(edge));\n+      }\n+\n+      // Add handoff tools to the agent's existing tools\n+      if (!agentContext.tools) {\n+        agentContext.tools = [];\n+      }\n+      agentContext.tools.push(...handoffTools);\n+\n+      // Update tool map\n+      for (const tool of handoffTools) {\n+        if (!agentContext.toolMap) {\n+          agentContext.toolMap = new Map();\n+        }\n+        agentContext.toolMap.set(tool.name, tool);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Create handoff tools for an edge (handles multiple destinations)\n+   */\n+  private createHandoffToolsForEdge(edge: t.GraphEdge): t.GenericTool[] {\n+    const tools: t.GenericTool[] = [];\n+    const destinations = Array.isArray(edge.to) ? edge.to : [edge.to];\n+\n+    // If there's a condition, create a single conditional handoff tool\n+    if (edge.condition != null) {\n+      const toolName = 'conditional_transfer';\n+      const toolDescription =\n+        edge.description ?? 'Conditionally transfer control based on state';\n+\n+      tools.push(\n+        tool(\n+          async (_, config) => {\n+            const state = getCurrentTaskInput() as t.BaseGraphState;\n+            const toolCallId =\n+              (config as ToolRunnableConfig | undefined)?.toolCall?.id ??\n+              'unknown';\n+\n+            // Evaluate condition\n+            const result = edge.condition!(state);\n+            let destination: string;\n+\n+            if (typeof result === 'boolean') {\n+              // If true, use first destination; if false, don't transfer\n+              if (!result) return null;\n+              destination = destinations[0];\n+            } else if (typeof result === 'string') {\n+              destination = result;\n+            } else {\n+              // Array of destinations - for now, use the first\n+              destination = Array.isArray(result) ? result[0] : destinations[0];\n+            }\n+\n+            const toolMessage = new ToolMessage({\n+              content: `Conditionally transferred to ${destination}`,\n+              name: toolName,\n+              tool_call_id: toolCallId,\n+            });\n+\n+            return new Command({\n+              goto: destination,\n+              update: { messages: state.messages.concat(toolMessage) },\n+              graph: Command.PARENT,\n+            });\n+          },\n+          {\n+            name: toolName,\n+            schema: z.object({}),\n+            description: toolDescription,\n+          }\n+        )\n+      );\n+    } else {\n+      // Create individual tools for each destination\n+      for (const destination of destinations) {\n+        const toolName = `transfer_to_${destination}`;\n+        const toolDescription =\n+          edge.description ?? `Transfer control to agent '${destination}'`;\n+\n+        tools.push(\n+          tool(\n+            async (_, config) => {\n+              const toolCallId =\n+                (config as ToolRunnableConfig | undefined)?.toolCall?.id ??\n+                'unknown';\n+              const toolMessage = new ToolMessage({\n+                content: `Successfully transferred to ${destination}`,\n+                name: toolName,\n+                tool_call_id: toolCallId,\n+              });\n+\n+              const state = getCurrentTaskInput() as t.BaseGraphState;\n+\n+              return new Command({\n+                goto: destination,\n+                update: { messages: state.messages.concat(toolMessage) },\n+                graph: Command.PARENT,\n+              });\n+            },\n+            {\n+              name: toolName,\n+              schema: z.object({}),\n+              description: toolDescription,\n+            }\n+          )\n+        );\n+      }\n+    }\n+\n+    return tools;\n+  }\n+\n+  /**\n+   * Create a complete agent subgraph (similar to createReactAgent)\n+   */\n+  private createAgentSubgraph(agentId: string): t.CompiledAgentWorfklow {\n+    // This is essentially the same as createAgentNode from StandardGraph\n+    return this.createAgentNode(agentId);\n+  }\n+\n+  /**\n+   * Create the multi-agent workflow with dynamic handoffs\n+   */\n+  override createWorkflow(): t.CompiledStateWorkflow {\n+    const StateAnnotation = Annotation.Root({\n+      messages: Annotation<BaseMessage[]>({\n+        reducer: (a, b) => {\n+          if (!a.length) {\n+            this.startIndex = a.length + b.length;\n+          }\n+          const result = messagesStateReducer(a, b);\n+          this.messages = result;\n+          return result;\n+        },\n+        default: () => [],\n+      }),\n+    });\n+\n+    const builder = new StateGraph(StateAnnotation);\n+\n+    // Add all agents as complete subgraphs\n+    for (const [agentId] of this.agentContexts) {\n+      // Get all possible destinations for this agent\n+      const handoffDestinations = new Set<string>();\n+      const directDestinations = new Set<string>();\n+\n+      // Check handoff edges for destinations\n+      for (const edge of this.handoffEdges) {\n+        const sources = Array.isArray(edge.from) ? edge.from : [edge.from];\n+        if (sources.includes(agentId) === true) {\n+          const dests = Array.isArray(edge.to) ? edge.to : [edge.to];\n+          dests.forEach((dest) => handoffDestinations.add(dest));\n+        }\n+      }\n+\n+      // Check direct edges for destinations\n+      for (const edge of this.directEdges) {\n+        const sources = Array.isArray(edge.from) ? edge.from : [edge.from];\n+        if (sources.includes(agentId) === true) {\n+          const dests = Array.isArray(edge.to) ? edge.to : [edge.to];\n+          dests.forEach((dest) => directDestinations.add(dest));\n+        }\n+      }\n+\n+      // If agent has handoff destinations, add END to possible ends\n+      // If agent only has direct destinations, it naturally ends without explicit END\n+      const destinations = new Set([...handoffDestinations]);\n+      if (handoffDestinations.size > 0 || directDestinations.size === 0) {\n+        destinations.add(END);\n+      }\n+\n+      // Create the agent subgraph (includes agent + tools)\n+      const agentSubgraph = this.createAgentSubgraph(agentId);\n+\n+      // Add the agent as a node with its possible destinations\n+      builder.addNode(agentId, agentSubgraph, {\n+        ends: Array.from(destinations),\n+      });\n+    }\n+\n+    // Add starting edges for all starting nodes\n+    for (const startNode of this.startingNodes) {\n+      // eslint-disable-next-line @typescript-eslint/ban-ts-comment\n+      /** @ts-ignore */\n+      builder.addEdge(START, startNode);\n+    }\n+\n+    /** Add direct edges for automatic transitions\n+     * Group edges by destination to handle fan-in scenarios\n+     */\n+    const edgesByDestination = new Map<string, t.GraphEdge[]>();\n+\n+    for (const edge of this.directEdges) {\n+      const destinations = Array.isArray(edge.to) ? edge.to : [edge.to];\n+      for (const destination of destinations) {\n+        if (!edgesByDestination.has(destination)) {\n+          edgesByDestination.set(destination, []);\n+        }\n+        edgesByDestination.get(destination)!.push(edge);\n+      }\n+    }\n+\n+    for (const [destination, edges] of edgesByDestination) {\n+      /** Checks if this is a fan-in scenario with prompt instructions */\n+      const edgesWithPrompt = edges.filter(\n+        (edge) =>\n+          edge.promptInstructions != null && edge.promptInstructions !== ''\n+      );\n+\n+      if (edgesWithPrompt.length > 0) {\n+        // Fan-in with prompt: create a single wrapper node for this destination\n+        const wrapperNodeId = `fan_in_${destination}_prompt`;\n+\n+        // Use the first edge's prompt instructions (they should all be the same for fan-in)\n+        const promptInstructions = edgesWithPrompt[0].promptInstructions;\n+\n+        builder.addNode(wrapperNodeId, async (state: t.BaseGraphState) => {\n+          let promptText: string | undefined;\n+\n+          if (typeof promptInstructions === 'function') {\n+            promptText = promptInstructions(state.messages);\n+          } else {\n+            promptText = promptInstructions;\n+          }\n+\n+          if (promptText != null && promptText !== '') {\n+            // Return state with the prompt message added\n+            return {\n+              messages: [...state.messages, new HumanMessage(promptText)],\n+            };\n+          }\n+\n+          // No prompt needed, return empty update\n+          return {};\n+        });\n+\n+        // Add edges from all sources to the wrapper, then wrapper to destination\n+        for (const edge of edges) {\n+          const sources = Array.isArray(edge.from) ? edge.from : [edge.from];\n+          for (const source of sources) {\n+            // eslint-disable-next-line @typescript-eslint/ban-ts-comment\n+            /** @ts-ignore */\n+            builder.addEdge(source, wrapperNodeId);\n+          }\n+        }\n+\n+        // Single edge from wrapper to destination\n+        // eslint-disable-next-line @typescript-eslint/ban-ts-comment\n+        /** @ts-ignore */\n+        builder.addEdge(wrapperNodeId, destination);\n+      } else {\n+        // No prompt instructions, add direct edges\n+        for (const edge of edges) {\n+          const sources = Array.isArray(edge.from) ? edge.from : [edge.from];\n+          for (const source of sources) {\n+            // eslint-disable-next-line @typescript-eslint/ban-ts-comment\n+            /** @ts-ignore */\n+            builder.addEdge(source, destination);\n+          }\n+        }\n+      }\n+    }\n+\n+    return builder.compile(this.compileOptions as unknown as never);\n+  }\n+}",
      "patch_lines": [
        "@@ -0,0 +1,381 @@\n",
        "+import { z } from 'zod';\n",
        "+import { tool } from '@langchain/core/tools';\n",
        "+import { ToolMessage, HumanMessage } from '@langchain/core/messages';\n",
        "+import {\n",
        "+  END,\n",
        "+  START,\n",
        "+  Command,\n",
        "+  StateGraph,\n",
        "+  Annotation,\n",
        "+  getCurrentTaskInput,\n",
        "+  messagesStateReducer,\n",
        "+} from '@langchain/langgraph';\n",
        "+import type { ToolRunnableConfig } from '@langchain/core/tools';\n",
        "+import type { BaseMessage } from '@langchain/core/messages';\n",
        "+import type * as t from '@/types';\n",
        "+import { StandardGraph } from './Graph';\n",
        "+\n",
        "+/**\n",
        "+ * MultiAgentGraph extends StandardGraph to support dynamic multi-agent workflows\n",
        "+ * with handoffs, fan-in/fan-out, and other composable patterns\n",
        "+ */\n",
        "+export class MultiAgentGraph extends StandardGraph {\n",
        "+  private edges: t.GraphEdge[];\n",
        "+  private startingNodes: Set<string> = new Set();\n",
        "+  private directEdges: t.GraphEdge[] = [];\n",
        "+  private handoffEdges: t.GraphEdge[] = [];\n",
        "+\n",
        "+  constructor(input: t.MultiAgentGraphInput) {\n",
        "+    super(input);\n",
        "+    this.edges = input.edges;\n",
        "+    this.categorizeEdges();\n",
        "+    this.analyzeGraph();\n",
        "+    this.createHandoffTools();\n",
        "+  }\n",
        "+\n",
        "+  /**\n",
        "+   * Categorize edges into handoff and direct types\n",
        "+   */\n",
        "+  private categorizeEdges(): void {\n",
        "+    for (const edge of this.edges) {\n",
        "+      // Default behavior: edges with conditions or explicit 'handoff' type are handoff edges\n",
        "+      // Edges with explicit 'direct' type or multi-destination without conditions are direct edges\n",
        "+      if (edge.edgeType === 'direct') {\n",
        "+        this.directEdges.push(edge);\n",
        "+      } else if (edge.edgeType === 'handoff' || edge.condition != null) {\n",
        "+        this.handoffEdges.push(edge);\n",
        "+      } else {\n",
        "+        // Default: single-to-single edges are handoff, single-to-multiple are direct\n",
        "+        const destinations = Array.isArray(edge.to) ? edge.to : [edge.to];\n",
        "+        const sources = Array.isArray(edge.from) ? edge.from : [edge.from];\n",
        "+\n",
        "+        if (sources.length === 1 && destinations.length > 1) {\n",
        "+          // Fan-out pattern defaults to direct\n",
        "+          this.directEdges.push(edge);\n",
        "+        } else {\n",
        "+          // Everything else defaults to handoff\n",
        "+          this.handoffEdges.push(edge);\n",
        "+        }\n",
        "+      }\n",
        "+    }\n",
        "+  }\n",
        "+\n",
        "+  /**\n",
        "+   * Analyze graph structure to determine starting nodes and connections\n",
        "+   */\n",
        "+  private analyzeGraph(): void {\n",
        "+    const hasIncomingEdge = new Set<string>();\n",
        "+\n",
        "+    // Track all nodes that have incoming edges\n",
        "+    for (const edge of this.edges) {\n",
        "+      const destinations = Array.isArray(edge.to) ? edge.to : [edge.to];\n",
        "+      destinations.forEach((dest) => hasIncomingEdge.add(dest));\n",
        "+    }\n",
        "+\n",
        "+    // Starting nodes are those without incoming edges\n",
        "+    for (const agentId of this.agentContexts.keys()) {\n",
        "+      if (!hasIncomingEdge.has(agentId)) {\n",
        "+        this.startingNodes.add(agentId);\n",
        "+      }\n",
        "+    }\n",
        "+\n",
        "+    // If no starting nodes found, use the first agent\n",
        "+    if (this.startingNodes.size === 0 && this.agentContexts.size > 0) {\n",
        "+      this.startingNodes.add(this.agentContexts.keys().next().value!);\n",
        "+    }\n",
        "+  }\n",
        "+\n",
        "+  /**\n",
        "+   * Create handoff tools for agents based on handoff edges only\n",
        "+   */\n",
        "+  private createHandoffTools(): void {\n",
        "+    // Group handoff edges by source agent(s)\n",
        "+    const handoffsByAgent = new Map<string, t.GraphEdge[]>();\n",
        "+\n",
        "+    // Only process handoff edges for tool creation\n",
        "+    for (const edge of this.handoffEdges) {\n",
        "+      const sources = Array.isArray(edge.from) ? edge.from : [edge.from];\n",
        "+      sources.forEach((source) => {\n",
        "+        if (!handoffsByAgent.has(source)) {\n",
        "+          handoffsByAgent.set(source, []);\n",
        "+        }\n",
        "+        handoffsByAgent.get(source)!.push(edge);\n",
        "+      });\n",
        "+    }\n",
        "+\n",
        "+    // Create handoff tools for each agent\n",
        "+    for (const [agentId, edges] of handoffsByAgent) {\n",
        "+      const agentContext = this.agentContexts.get(agentId);\n",
        "+      if (!agentContext) continue;\n",
        "+\n",
        "+      // Create handoff tools for this agent's outgoing edges\n",
        "+      const handoffTools: t.GenericTool[] = [];\n",
        "+      for (const edge of edges) {\n",
        "+        handoffTools.push(...this.createHandoffToolsForEdge(edge));\n",
        "+      }\n",
        "+\n",
        "+      // Add handoff tools to the agent's existing tools\n",
        "+      if (!agentContext.tools) {\n",
        "+        agentContext.tools = [];\n",
        "+      }\n",
        "+      agentContext.tools.push(...handoffTools);\n",
        "+\n",
        "+      // Update tool map\n",
        "+      for (const tool of handoffTools) {\n",
        "+        if (!agentContext.toolMap) {\n",
        "+          agentContext.toolMap = new Map();\n",
        "+        }\n",
        "+        agentContext.toolMap.set(tool.name, tool);\n",
        "+      }\n",
        "+    }\n",
        "+  }\n",
        "+\n",
        "+  /**\n",
        "+   * Create handoff tools for an edge (handles multiple destinations)\n",
        "+   */\n",
        "+  private createHandoffToolsForEdge(edge: t.GraphEdge): t.GenericTool[] {\n",
        "+    const tools: t.GenericTool[] = [];\n",
        "+    const destinations = Array.isArray(edge.to) ? edge.to : [edge.to];\n",
        "+\n",
        "+    // If there's a condition, create a single conditional handoff tool\n",
        "+    if (edge.condition != null) {\n",
        "+      const toolName = 'conditional_transfer';\n",
        "+      const toolDescription =\n",
        "+        edge.description ?? 'Conditionally transfer control based on state';\n",
        "+\n",
        "+      tools.push(\n",
        "+        tool(\n",
        "+          async (_, config) => {\n",
        "+            const state = getCurrentTaskInput() as t.BaseGraphState;\n",
        "+            const toolCallId =\n",
        "+              (config as ToolRunnableConfig | undefined)?.toolCall?.id ??\n",
        "+              'unknown';\n",
        "+\n",
        "+            // Evaluate condition\n",
        "+            const result = edge.condition!(state);\n",
        "+            let destination: string;\n",
        "+\n",
        "+            if (typeof result === 'boolean') {\n",
        "+              // If true, use first destination; if false, don't transfer\n",
        "+              if (!result) return null;\n",
        "+              destination = destinations[0];\n",
        "+            } else if (typeof result === 'string') {\n",
        "+              destination = result;\n",
        "+            } else {\n",
        "+              // Array of destinations - for now, use the first\n",
        "+              destination = Array.isArray(result) ? result[0] : destinations[0];\n",
        "+            }\n",
        "+\n",
        "+            const toolMessage = new ToolMessage({\n",
        "+              content: `Conditionally transferred to ${destination}`,\n",
        "+              name: toolName,\n",
        "+              tool_call_id: toolCallId,\n",
        "+            });\n",
        "+\n",
        "+            return new Command({\n",
        "+              goto: destination,\n",
        "+              update: { messages: state.messages.concat(toolMessage) },\n",
        "+              graph: Command.PARENT,\n",
        "+            });\n",
        "+          },\n",
        "+          {\n",
        "+            name: toolName,\n",
        "+            schema: z.object({}),\n",
        "+            description: toolDescription,\n",
        "+          }\n",
        "+        )\n",
        "+      );\n",
        "+    } else {\n",
        "+      // Create individual tools for each destination\n",
        "+      for (const destination of destinations) {\n",
        "+        const toolName = `transfer_to_${destination}`;\n",
        "+        const toolDescription =\n",
        "+          edge.description ?? `Transfer control to agent '${destination}'`;\n",
        "+\n",
        "+        tools.push(\n",
        "+          tool(\n",
        "+            async (_, config) => {\n",
        "+              const toolCallId =\n",
        "+                (config as ToolRunnableConfig | undefined)?.toolCall?.id ??\n",
        "+                'unknown';\n",
        "+              const toolMessage = new ToolMessage({\n",
        "+                content: `Successfully transferred to ${destination}`,\n",
        "+                name: toolName,\n",
        "+                tool_call_id: toolCallId,\n",
        "+              });\n",
        "+\n",
        "+              const state = getCurrentTaskInput() as t.BaseGraphState;\n",
        "+\n",
        "+              return new Command({\n",
        "+                goto: destination,\n",
        "+                update: { messages: state.messages.concat(toolMessage) },\n",
        "+                graph: Command.PARENT,\n",
        "+              });\n",
        "+            },\n",
        "+            {\n",
        "+              name: toolName,\n",
        "+              schema: z.object({}),\n",
        "+              description: toolDescription,\n",
        "+            }\n",
        "+          )\n",
        "+        );\n",
        "+      }\n",
        "+    }\n",
        "+\n",
        "+    return tools;\n",
        "+  }\n",
        "+\n",
        "+  /**\n",
        "+   * Create a complete agent subgraph (similar to createReactAgent)\n",
        "+   */\n",
        "+  private createAgentSubgraph(agentId: string): t.CompiledAgentWorfklow {\n",
        "+    // This is essentially the same as createAgentNode from StandardGraph\n",
        "+    return this.createAgentNode(agentId);\n",
        "+  }\n",
        "+\n",
        "+  /**\n",
        "+   * Create the multi-agent workflow with dynamic handoffs\n",
        "+   */\n",
        "+  override createWorkflow(): t.CompiledStateWorkflow {\n",
        "+    const StateAnnotation = Annotation.Root({\n",
        "+      messages: Annotation<BaseMessage[]>({\n",
        "+        reducer: (a, b) => {\n",
        "+          if (!a.length) {\n",
        "+            this.startIndex = a.length + b.length;\n",
        "+          }\n",
        "+          const result = messagesStateReducer(a, b);\n",
        "+          this.messages = result;\n",
        "+          return result;\n",
        "+        },\n",
        "+        default: () => [],\n",
        "+      }),\n",
        "+    });\n",
        "+\n",
        "+    const builder = new StateGraph(StateAnnotation);\n",
        "+\n",
        "+    // Add all agents as complete subgraphs\n",
        "+    for (const [agentId] of this.agentContexts) {\n",
        "+      // Get all possible destinations for this agent\n",
        "+      const handoffDestinations = new Set<string>();\n",
        "+      const directDestinations = new Set<string>();\n",
        "+\n",
        "+      // Check handoff edges for destinations\n",
        "+      for (const edge of this.handoffEdges) {\n",
        "+        const sources = Array.isArray(edge.from) ? edge.from : [edge.from];\n",
        "+        if (sources.includes(agentId) === true) {\n",
        "+          const dests = Array.isArray(edge.to) ? edge.to : [edge.to];\n",
        "+          dests.forEach((dest) => handoffDestinations.add(dest));\n",
        "+        }\n",
        "+      }\n",
        "+\n",
        "+      // Check direct edges for destinations\n",
        "+      for (const edge of this.directEdges) {\n",
        "+        const sources = Array.isArray(edge.from) ? edge.from : [edge.from];\n",
        "+        if (sources.includes(agentId) === true) {\n",
        "+          const dests = Array.isArray(edge.to) ? edge.to : [edge.to];\n",
        "+          dests.forEach((dest) => directDestinations.add(dest));\n",
        "+        }\n",
        "+      }\n",
        "+\n",
        "+      // If agent has handoff destinations, add END to possible ends\n",
        "+      // If agent only has direct destinations, it naturally ends without explicit END\n",
        "+      const destinations = new Set([...handoffDestinations]);\n",
        "+      if (handoffDestinations.size > 0 || directDestinations.size === 0) {\n",
        "+        destinations.add(END);\n",
        "+      }\n",
        "+\n",
        "+      // Create the agent subgraph (includes agent + tools)\n",
        "+      const agentSubgraph = this.createAgentSubgraph(agentId);\n",
        "+\n",
        "+      // Add the agent as a node with its possible destinations\n",
        "+      builder.addNode(agentId, agentSubgraph, {\n",
        "+        ends: Array.from(destinations),\n",
        "+      });\n",
        "+    }\n",
        "+\n",
        "+    // Add starting edges for all starting nodes\n",
        "+    for (const startNode of this.startingNodes) {\n",
        "+      // eslint-disable-next-line @typescript-eslint/ban-ts-comment\n",
        "+      /** @ts-ignore */\n",
        "+      builder.addEdge(START, startNode);\n",
        "+    }\n",
        "+\n",
        "+    /** Add direct edges for automatic transitions\n",
        "+     * Group edges by destination to handle fan-in scenarios\n",
        "+     */\n",
        "+    const edgesByDestination = new Map<string, t.GraphEdge[]>();\n",
        "+\n",
        "+    for (const edge of this.directEdges) {\n",
        "+      const destinations = Array.isArray(edge.to) ? edge.to : [edge.to];\n",
        "+      for (const destination of destinations) {\n",
        "+        if (!edgesByDestination.has(destination)) {\n",
        "+          edgesByDestination.set(destination, []);\n",
        "+        }\n",
        "+        edgesByDestination.get(destination)!.push(edge);\n",
        "+      }\n",
        "+    }\n",
        "+\n",
        "+    for (const [destination, edges] of edgesByDestination) {\n",
        "+      /** Checks if this is a fan-in scenario with prompt instructions */\n",
        "+      const edgesWithPrompt = edges.filter(\n",
        "+        (edge) =>\n",
        "+          edge.promptInstructions != null && edge.promptInstructions !== ''\n",
        "+      );\n",
        "+\n",
        "+      if (edgesWithPrompt.length > 0) {\n",
        "+        // Fan-in with prompt: create a single wrapper node for this destination\n",
        "+        const wrapperNodeId = `fan_in_${destination}_prompt`;\n",
        "+\n",
        "+        // Use the first edge's prompt instructions (they should all be the same for fan-in)\n",
        "+        const promptInstructions = edgesWithPrompt[0].promptInstructions;\n",
        "+\n",
        "+        builder.addNode(wrapperNodeId, async (state: t.BaseGraphState) => {\n",
        "+          let promptText: string | undefined;\n",
        "+\n",
        "+          if (typeof promptInstructions === 'function') {\n",
        "+            promptText = promptInstructions(state.messages);\n",
        "+          } else {\n",
        "+            promptText = promptInstructions;\n",
        "+          }\n",
        "+\n",
        "+          if (promptText != null && promptText !== '') {\n",
        "+            // Return state with the prompt message added\n",
        "+            return {\n",
        "+              messages: [...state.messages, new HumanMessage(promptText)],\n",
        "+            };\n",
        "+          }\n",
        "+\n",
        "+          // No prompt needed, return empty update\n",
        "+          return {};\n",
        "+        });\n",
        "+\n",
        "+        // Add edges from all sources to the wrapper, then wrapper to destination\n",
        "+        for (const edge of edges) {\n",
        "+          const sources = Array.isArray(edge.from) ? edge.from : [edge.from];\n",
        "+          for (const source of sources) {\n",
        "+            // eslint-disable-next-line @typescript-eslint/ban-ts-comment\n",
        "+            /** @ts-ignore */\n",
        "+            builder.addEdge(source, wrapperNodeId);\n",
        "+          }\n",
        "+        }\n",
        "+\n",
        "+        // Single edge from wrapper to destination\n",
        "+        // eslint-disable-next-line @typescript-eslint/ban-ts-comment\n",
        "+        /** @ts-ignore */\n",
        "+        builder.addEdge(wrapperNodeId, destination);\n",
        "+      } else {\n",
        "+        // No prompt instructions, add direct edges\n",
        "+        for (const edge of edges) {\n",
        "+          const sources = Array.isArray(edge.from) ? edge.from : [edge.from];\n",
        "+          for (const source of sources) {\n",
        "+            // eslint-disable-next-line @typescript-eslint/ban-ts-comment\n",
        "+            /** @ts-ignore */\n",
        "+            builder.addEdge(source, destination);\n",
        "+          }\n",
        "+        }\n",
        "+      }\n",
        "+    }\n",
        "+\n",
        "+    return builder.compile(this.compileOptions as unknown as never);\n",
        "+  }\n",
        "+}\n"
      ]
    },
    {
      "path": "src/graphs/index.ts",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "patch": "@@ -1 +1,2 @@\n-export * from './Graph';\n\\ No newline at end of file\n+export * from './Graph';\n+export * from './MultiAgentGraph';",
      "patch_lines": [
        "@@ -1 +1,2 @@\n",
        "-export * from './Graph';\n",
        "\\ No newline at end of file\n",
        "+export * from './Graph';\n",
        "+export * from './MultiAgentGraph';\n"
      ]
    },
    {
      "path": "src/llm/google/utils/zod_to_genai_parameters.ts",
      "status": "modified",
      "additions": 2,
      "deletions": 4,
      "patch": "@@ -1,5 +1,3 @@\n-/* eslint-disable @typescript-eslint/no-unused-vars */\n-\n import {\n   type FunctionDeclarationSchema as GenerativeAIFunctionDeclarationSchema,\n   type SchemaType as FunctionDeclarationSchemaType,\n@@ -67,7 +65,7 @@ export function schemaToGenerativeAIParameters<\n   const jsonSchema = removeAdditionalProperties(\n     isInteropZodSchema(schema) ? toJsonSchema(schema) : schema\n   );\n-  const { $schema, ...rest } = jsonSchema;\n+  const { $schema: _s, ...rest } = jsonSchema;\n \n   return rest as GenerativeAIFunctionDeclarationSchema;\n }\n@@ -82,7 +80,7 @@ export function jsonSchemaToGeminiParameters(\n   const jsonSchema = removeAdditionalProperties(\n     schema as GenerativeAIJsonSchemaDirty\n   );\n-  const { $schema, ...rest } = jsonSchema;\n+  const { $schema: _s, ...rest } = jsonSchema;\n \n   return rest as GenerativeAIFunctionDeclarationSchema;\n }",
      "patch_lines": [
        "@@ -1,5 +1,3 @@\n",
        "-/* eslint-disable @typescript-eslint/no-unused-vars */\n",
        "-\n",
        " import {\n",
        "   type FunctionDeclarationSchema as GenerativeAIFunctionDeclarationSchema,\n",
        "   type SchemaType as FunctionDeclarationSchemaType,\n",
        "@@ -67,7 +65,7 @@ export function schemaToGenerativeAIParameters<\n",
        "   const jsonSchema = removeAdditionalProperties(\n",
        "     isInteropZodSchema(schema) ? toJsonSchema(schema) : schema\n",
        "   );\n",
        "-  const { $schema, ...rest } = jsonSchema;\n",
        "+  const { $schema: _s, ...rest } = jsonSchema;\n",
        " \n",
        "   return rest as GenerativeAIFunctionDeclarationSchema;\n",
        " }\n",
        "@@ -82,7 +80,7 @@ export function jsonSchemaToGeminiParameters(\n",
        "   const jsonSchema = removeAdditionalProperties(\n",
        "     schema as GenerativeAIJsonSchemaDirty\n",
        "   );\n",
        "-  const { $schema, ...rest } = jsonSchema;\n",
        "+  const { $schema: _s, ...rest } = jsonSchema;\n",
        " \n",
        "   return rest as GenerativeAIFunctionDeclarationSchema;\n",
        " }\n"
      ]
    },
    {
      "path": "src/messages/reducer.ts",
      "status": "added",
      "additions": 80,
      "deletions": 0,
      "patch": "@@ -0,0 +1,80 @@\n+import {\n+  BaseMessage,\n+  BaseMessageLike,\n+  coerceMessageLikeToMessage,\n+} from '@langchain/core/messages';\n+import { v4 } from 'uuid';\n+\n+export const REMOVE_ALL_MESSAGES = '__remove_all__';\n+\n+export type Messages =\n+  | Array<BaseMessage | BaseMessageLike>\n+  | BaseMessage\n+  | BaseMessageLike;\n+\n+/**\n+ * Prebuilt reducer that combines returned messages.\n+ * Can handle standard messages and special modifiers like {@link RemoveMessage}\n+ * instances.\n+ */\n+export function messagesStateReducer(\n+  left: Messages,\n+  right: Messages\n+): BaseMessage[] {\n+  const leftArray = Array.isArray(left) ? left : [left];\n+  const rightArray = Array.isArray(right) ? right : [right];\n+  // coerce to message\n+  const leftMessages = (leftArray as BaseMessageLike[]).map(\n+    coerceMessageLikeToMessage\n+  );\n+  const rightMessages = (rightArray as BaseMessageLike[]).map(\n+    coerceMessageLikeToMessage\n+  );\n+  // assign missing ids\n+  for (const m of leftMessages) {\n+    if (m.id === null || m.id === undefined) {\n+      m.id = v4();\n+      m.lc_kwargs.id = m.id;\n+    }\n+  }\n+\n+  let removeAllIdx: number | undefined;\n+  for (let i = 0; i < rightMessages.length; i += 1) {\n+    const m = rightMessages[i];\n+    if (m.id === null || m.id === undefined) {\n+      m.id = v4();\n+      m.lc_kwargs.id = m.id;\n+    }\n+\n+    if (m.getType() === 'remove' && m.id === REMOVE_ALL_MESSAGES) {\n+      removeAllIdx = i;\n+    }\n+  }\n+\n+  if (removeAllIdx != null) return rightMessages.slice(removeAllIdx + 1);\n+\n+  // merge\n+  const merged = [...leftMessages];\n+  const mergedById = new Map(merged.map((m, i) => [m.id, i]));\n+  const idsToRemove = new Set();\n+  for (const m of rightMessages) {\n+    const existingIdx = mergedById.get(m.id);\n+    if (existingIdx !== undefined) {\n+      if (m.getType() === 'remove') {\n+        idsToRemove.add(m.id);\n+      } else {\n+        idsToRemove.delete(m.id);\n+        merged[existingIdx] = m;\n+      }\n+    } else {\n+      if (m.getType() === 'remove') {\n+        throw new Error(\n+          `Attempting to delete a message with an ID that doesn't exist ('${m.id}')`\n+        );\n+      }\n+      mergedById.set(m.id, merged.length);\n+      merged.push(m);\n+    }\n+  }\n+  return merged.filter((m) => !idsToRemove.has(m.id));\n+}",
      "patch_lines": [
        "@@ -0,0 +1,80 @@\n",
        "+import {\n",
        "+  BaseMessage,\n",
        "+  BaseMessageLike,\n",
        "+  coerceMessageLikeToMessage,\n",
        "+} from '@langchain/core/messages';\n",
        "+import { v4 } from 'uuid';\n",
        "+\n",
        "+export const REMOVE_ALL_MESSAGES = '__remove_all__';\n",
        "+\n",
        "+export type Messages =\n",
        "+  | Array<BaseMessage | BaseMessageLike>\n",
        "+  | BaseMessage\n",
        "+  | BaseMessageLike;\n",
        "+\n",
        "+/**\n",
        "+ * Prebuilt reducer that combines returned messages.\n",
        "+ * Can handle standard messages and special modifiers like {@link RemoveMessage}\n",
        "+ * instances.\n",
        "+ */\n",
        "+export function messagesStateReducer(\n",
        "+  left: Messages,\n",
        "+  right: Messages\n",
        "+): BaseMessage[] {\n",
        "+  const leftArray = Array.isArray(left) ? left : [left];\n",
        "+  const rightArray = Array.isArray(right) ? right : [right];\n",
        "+  // coerce to message\n",
        "+  const leftMessages = (leftArray as BaseMessageLike[]).map(\n",
        "+    coerceMessageLikeToMessage\n",
        "+  );\n",
        "+  const rightMessages = (rightArray as BaseMessageLike[]).map(\n",
        "+    coerceMessageLikeToMessage\n",
        "+  );\n",
        "+  // assign missing ids\n",
        "+  for (const m of leftMessages) {\n",
        "+    if (m.id === null || m.id === undefined) {\n",
        "+      m.id = v4();\n",
        "+      m.lc_kwargs.id = m.id;\n",
        "+    }\n",
        "+  }\n",
        "+\n",
        "+  let removeAllIdx: number | undefined;\n",
        "+  for (let i = 0; i < rightMessages.length; i += 1) {\n",
        "+    const m = rightMessages[i];\n",
        "+    if (m.id === null || m.id === undefined) {\n",
        "+      m.id = v4();\n",
        "+      m.lc_kwargs.id = m.id;\n",
        "+    }\n",
        "+\n",
        "+    if (m.getType() === 'remove' && m.id === REMOVE_ALL_MESSAGES) {\n",
        "+      removeAllIdx = i;\n",
        "+    }\n",
        "+  }\n",
        "+\n",
        "+  if (removeAllIdx != null) return rightMessages.slice(removeAllIdx + 1);\n",
        "+\n",
        "+  // merge\n",
        "+  const merged = [...leftMessages];\n",
        "+  const mergedById = new Map(merged.map((m, i) => [m.id, i]));\n",
        "+  const idsToRemove = new Set();\n",
        "+  for (const m of rightMessages) {\n",
        "+    const existingIdx = mergedById.get(m.id);\n",
        "+    if (existingIdx !== undefined) {\n",
        "+      if (m.getType() === 'remove') {\n",
        "+        idsToRemove.add(m.id);\n",
        "+      } else {\n",
        "+        idsToRemove.delete(m.id);\n",
        "+        merged[existingIdx] = m;\n",
        "+      }\n",
        "+    } else {\n",
        "+      if (m.getType() === 'remove') {\n",
        "+        throw new Error(\n",
        "+          `Attempting to delete a message with an ID that doesn't exist ('${m.id}')`\n",
        "+        );\n",
        "+      }\n",
        "+      mergedById.set(m.id, merged.length);\n",
        "+      merged.push(m);\n",
        "+    }\n",
        "+  }\n",
        "+  return merged.filter((m) => !idsToRemove.has(m.id));\n",
        "+}\n"
      ]
    },
    {
      "path": "src/run.ts",
      "status": "modified",
      "additions": 158,
      "deletions": 103,
      "patch": "@@ -1,9 +1,7 @@\n // src/run.ts\n import './instrumentation';\n-import { zodToJsonSchema } from 'zod-to-json-schema';\n import { CallbackHandler } from '@langfuse/langchain';\n import { PromptTemplate } from '@langchain/core/prompts';\n-import { SystemMessage } from '@langchain/core/messages';\n import { RunnableLambda } from '@langchain/core/runnables';\n import { AzureChatOpenAI, ChatOpenAI } from '@langchain/openai';\n import type {\n@@ -12,15 +10,13 @@ import type {\n } from '@langchain/core/messages';\n import type { StringPromptValue } from '@langchain/core/prompt_values';\n import type { RunnableConfig } from '@langchain/core/runnables';\n-import type { ClientCallbacks, SystemCallbacks } from '@/graphs/Graph';\n import type * as t from '@/types';\n-import { GraphEvents, Providers, Callback, TitleMethod } from '@/common';\n-import { manualToolStreamProviders } from '@/llm/providers';\n-import { shiftIndexTokenCountMap } from '@/messages/format';\n import {\n-  createTitleRunnable,\n   createCompletionTitleRunnable,\n+  createTitleRunnable,\n } from '@/utils/title';\n+import { GraphEvents, Callback, TitleMethod } from '@/common';\n+import { MultiAgentGraph } from '@/graphs/MultiAgentGraph';\n import { createTokenCounter } from '@/utils/tokens';\n import { StandardGraph } from '@/graphs/Graph';\n import { HandlerRegistry } from '@/events';\n@@ -40,14 +36,13 @@ export const defaultOmitOptions = new Set([\n   'additionalModelRequestFields',\n ]);\n \n-export class Run<T extends t.BaseGraphState> {\n-  graphRunnable?: t.CompiledWorkflow<T, Partial<T>, string>;\n-  // private collab!: CollabGraph;\n-  // private taskManager!: TaskManager;\n-  private handlerRegistry: HandlerRegistry;\n+export class Run<_T extends t.BaseGraphState> {\n   id: string;\n-  Graph: StandardGraph | undefined;\n-  provider: Providers | undefined;\n+  private tokenCounter?: t.TokenCounter;\n+  private handlerRegistry: HandlerRegistry;\n+  private indexTokenCountMap?: Record<string, number>;\n+  graphRunnable?: t.CompiledStateWorkflow;\n+  Graph: StandardGraph | MultiAgentGraph | undefined;\n   returnContent: boolean = false;\n \n   private constructor(config: Partial<t.RunConfig>) {\n@@ -57,6 +52,8 @@ export class Run<T extends t.BaseGraphState> {\n     }\n \n     this.id = runId;\n+    this.tokenCounter = config.tokenCounter;\n+    this.indexTokenCountMap = config.indexTokenCountMap;\n \n     const handlerRegistry = new HandlerRegistry();\n \n@@ -74,39 +71,89 @@ export class Run<T extends t.BaseGraphState> {\n       throw new Error('Graph config not provided');\n     }\n \n-    if (config.graphConfig.type === 'standard' || !config.graphConfig.type) {\n-      this.provider = config.graphConfig.llmConfig.provider;\n-      this.graphRunnable = this.createStandardGraph(\n-        config.graphConfig\n-      ) as unknown as t.CompiledWorkflow<T, Partial<T>, string>;\n+    /** Handle different graph types */\n+    if (config.graphConfig.type === 'multi-agent') {\n+      this.graphRunnable = this.createMultiAgentGraph(config.graphConfig);\n+      if (this.Graph) {\n+        this.Graph.handlerRegistry = handlerRegistry;\n+      }\n+    } else {\n+      // Default to legacy graph for 'standard' or undefined type\n+      this.graphRunnable = this.createLegacyGraph(config.graphConfig);\n       if (this.Graph) {\n+        this.Graph.compileOptions =\n+          config.graphConfig.compileOptions ?? this.Graph.compileOptions;\n         this.Graph.handlerRegistry = handlerRegistry;\n       }\n     }\n \n     this.returnContent = config.returnContent ?? false;\n   }\n \n-  private createStandardGraph(\n-    config: t.StandardGraphConfig\n-  ): t.CompiledWorkflow<t.IState, Partial<t.IState>, string> {\n-    const { llmConfig, tools = [], ...graphInput } = config;\n+  private createLegacyGraph(\n+    config: t.LegacyGraphConfig\n+  ): t.CompiledStateWorkflow {\n+    const {\n+      type: _type,\n+      llmConfig,\n+      signal,\n+      tools = [],\n+      ...agentInputs\n+    } = config;\n     const { provider, ...clientOptions } = llmConfig;\n \n-    const standardGraph = new StandardGraph({\n+    /** TEMP: Create agent configuration for the single agent */\n+    const agentConfig: t.AgentInputs = {\n+      ...agentInputs,\n       tools,\n       provider,\n       clientOptions,\n-      ...graphInput,\n+      agentId: 'default',\n+    };\n+\n+    const standardGraph = new StandardGraph({\n+      signal,\n       runId: this.id,\n+      agents: [agentConfig],\n+      tokenCounter: this.tokenCounter,\n+      indexTokenCountMap: this.indexTokenCountMap,\n     });\n+    // propagate compile options from graph config\n+    standardGraph.compileOptions = (\n+      config as t.LegacyGraphConfig\n+    ).compileOptions;\n     this.Graph = standardGraph;\n     return standardGraph.createWorkflow();\n   }\n \n+  private createMultiAgentGraph(\n+    config: t.MultiAgentGraphConfig\n+  ): t.CompiledStateWorkflow {\n+    const { agents, edges, compileOptions } = config;\n+\n+    const multiAgentGraph = new MultiAgentGraph({\n+      runId: this.id,\n+      agents,\n+      edges,\n+      tokenCounter: this.tokenCounter,\n+      indexTokenCountMap: this.indexTokenCountMap,\n+    });\n+\n+    if (compileOptions != null) {\n+      multiAgentGraph.compileOptions = compileOptions;\n+    }\n+\n+    this.Graph = multiAgentGraph;\n+    return multiAgentGraph.createWorkflow();\n+  }\n+\n   static async create<T extends t.BaseGraphState>(\n     config: t.RunConfig\n   ): Promise<Run<T>> {\n+    // Create tokenCounter if indexTokenCountMap is provided but tokenCounter is not\n+    if (config.indexTokenCountMap && !config.tokenCounter) {\n+      config.tokenCounter = await createTokenCounter();\n+    }\n     return new Run<T>(config);\n   }\n \n@@ -119,12 +166,49 @@ export class Run<T extends t.BaseGraphState> {\n     return this.Graph.getRunMessages();\n   }\n \n+  /**\n+   * Creates a custom event callback handler that intercepts custom events\n+   * and processes them through our handler registry instead of EventStreamCallbackHandler\n+   */\n+  private createCustomEventCallback() {\n+    return async (\n+      eventName: string,\n+      data: unknown,\n+      runId: string,\n+      tags?: string[],\n+      metadata?: Record<string, unknown>\n+    ): Promise<void> => {\n+      if (\n+        (data as t.StreamEventData)['emitted'] === true &&\n+        eventName === GraphEvents.CHAT_MODEL_STREAM\n+      ) {\n+        return;\n+      }\n+      const handler = this.handlerRegistry.getHandler(eventName);\n+      if (handler && this.Graph) {\n+        await handler.handle(\n+          eventName,\n+          data as\n+            | t.StreamEventData\n+            | t.ModelEndData\n+            | t.RunStep\n+            | t.RunStepDeltaEvent\n+            | t.MessageDeltaEvent\n+            | t.ReasoningDeltaEvent\n+            | { result: t.ToolEndEvent },\n+          metadata,\n+          this.Graph\n+        );\n+      }\n+    };\n+  }\n+\n   async processStream(\n     inputs: t.IState,\n     config: Partial<RunnableConfig> & { version: 'v1' | 'v2'; run_id?: string },\n     streamOptions?: t.EventStreamOptions\n   ): Promise<MessageContentComplex[] | undefined> {\n-    if (!this.graphRunnable) {\n+    if (this.graphRunnable == null) {\n       throw new Error(\n         'Run not initialized. Make sure to use Run.create() to instantiate the Run.'\n       );\n@@ -136,15 +220,18 @@ export class Run<T extends t.BaseGraphState> {\n     }\n \n     this.Graph.resetValues(streamOptions?.keepContent);\n-    const provider = this.Graph.provider;\n-    const hasTools = this.Graph.tools ? this.Graph.tools.length > 0 : false;\n-    if (streamOptions?.callbacks) {\n-      /* TODO: conflicts with callback manager */\n-      const callbacks = (config.callbacks as t.ProvidedCallbacks) ?? [];\n-      config.callbacks = callbacks.concat(\n-        this.getCallbacks(streamOptions.callbacks)\n-      );\n-    }\n+\n+    /** Custom event callback to intercept and handle custom events */\n+    const customEventCallback = this.createCustomEventCallback();\n+\n+    const baseCallbacks = (config.callbacks as t.ProvidedCallbacks) ?? [];\n+    const streamCallbacks = streamOptions?.callbacks\n+      ? this.getCallbacks(streamOptions.callbacks)\n+      : [];\n+\n+    config.callbacks = baseCallbacks.concat(streamCallbacks).concat({\n+      [Callback.CUSTOM_EVENT]: customEventCallback,\n+    });\n \n     if (\n       isPresent(process.env.LANGFUSE_SECRET_KEY) &&\n@@ -171,79 +258,28 @@ export class Run<T extends t.BaseGraphState> {\n       throw new Error('Run ID not provided');\n     }\n \n-    const tokenCounter =\n-      streamOptions?.tokenCounter ??\n-      (streamOptions?.indexTokenCountMap\n-        ? await createTokenCounter()\n-        : undefined);\n-    const tools = this.Graph.tools as\n-      | Array<t.GenericTool | undefined>\n-      | undefined;\n-    const toolTokens = tokenCounter\n-      ? (tools?.reduce((acc, tool) => {\n-        if (!(tool as Partial<t.GenericTool>).schema) {\n-          return acc;\n-        }\n-\n-        const jsonSchema = zodToJsonSchema(\n-          (tool?.schema as t.ZodObjectAny).describe(tool?.description ?? ''),\n-          tool?.name ?? ''\n-        );\n-        return (\n-          acc + tokenCounter(new SystemMessage(JSON.stringify(jsonSchema)))\n-        );\n-      }, 0) ?? 0)\n-      : 0;\n-    let instructionTokens = toolTokens;\n-    if (this.Graph.systemMessage && tokenCounter) {\n-      instructionTokens += tokenCounter(this.Graph.systemMessage);\n-    }\n-    const tokenMap = streamOptions?.indexTokenCountMap ?? {};\n-    if (this.Graph.systemMessage && instructionTokens > 0) {\n-      this.Graph.indexTokenCountMap = shiftIndexTokenCountMap(\n-        tokenMap,\n-        instructionTokens\n-      );\n-    } else if (instructionTokens > 0) {\n-      tokenMap[0] = tokenMap[0] + instructionTokens;\n-      this.Graph.indexTokenCountMap = tokenMap;\n-    } else {\n-      this.Graph.indexTokenCountMap = tokenMap;\n-    }\n-\n-    this.Graph.maxContextTokens = streamOptions?.maxContextTokens;\n-    this.Graph.tokenCounter = tokenCounter;\n-\n     config.run_id = this.id;\n     config.configurable = Object.assign(config.configurable ?? {}, {\n       run_id: this.id,\n-      provider: this.provider,\n     });\n \n     const stream = this.graphRunnable.streamEvents(inputs, config, {\n       raiseError: true,\n     });\n \n     for await (const event of stream) {\n-      const { data, name, metadata, ...info } = event;\n+      const { data, metadata, ...info } = event;\n \n-      let eventName: t.EventName = info.event;\n-      if (\n-        hasTools &&\n-        manualToolStreamProviders.has(provider) &&\n-        eventName === GraphEvents.CHAT_MODEL_STREAM\n-      ) {\n-        /* Skipping CHAT_MODEL_STREAM event due to double-call edge case */\n-        continue;\n-      }\n+      const eventName: t.EventName = info.event;\n \n-      if (eventName && eventName === GraphEvents.ON_CUSTOM_EVENT) {\n-        eventName = name;\n+      /** Skip custom events as they're handled by our callback */\n+      if (eventName === GraphEvents.ON_CUSTOM_EVENT) {\n+        continue;\n       }\n \n       const handler = this.handlerRegistry.getHandler(eventName);\n       if (handler) {\n-        handler.handle(eventName, data, metadata, this.Graph);\n+        await handler.handle(eventName, data, metadata, this.Graph);\n       }\n     }\n \n@@ -252,19 +288,19 @@ export class Run<T extends t.BaseGraphState> {\n     }\n   }\n \n-  private createSystemCallback<K extends keyof ClientCallbacks>(\n-    clientCallbacks: ClientCallbacks,\n+  private createSystemCallback<K extends keyof t.ClientCallbacks>(\n+    clientCallbacks: t.ClientCallbacks,\n     key: K\n-  ): SystemCallbacks[K] {\n+  ): t.SystemCallbacks[K] {\n     return ((...args: unknown[]) => {\n       const clientCallback = clientCallbacks[key];\n       if (clientCallback && this.Graph) {\n         (clientCallback as (...args: unknown[]) => void)(this.Graph, ...args);\n       }\n-    }) as SystemCallbacks[K];\n+    }) as t.SystemCallbacks[K];\n   }\n \n-  getCallbacks(clientCallbacks: ClientCallbacks): SystemCallbacks {\n+  getCallbacks(clientCallbacks: t.ClientCallbacks): t.SystemCallbacks {\n     return {\n       [Callback.TOOL_ERROR]: this.createSystemCallback(\n         clientCallbacks,\n@@ -289,7 +325,6 @@ export class Run<T extends t.BaseGraphState> {\n     clientOptions,\n     chainOptions,\n     skipLanguage,\n-    omitOptions = defaultOmitOptions,\n     titleMethod = TitleMethod.COMPLETION,\n     titlePromptTemplate,\n   }: t.RunTitleOptions): Promise<{ language?: string; title?: string }> {\n@@ -327,7 +362,6 @@ export class Run<T extends t.BaseGraphState> {\n \n     const model = this.Graph?.getNewModel({\n       provider,\n-      omitOptions,\n       clientOptions,\n     });\n     if (!model) {\n@@ -373,9 +407,30 @@ export class Run<T extends t.BaseGraphState> {\n       .pipe(titleChain)\n       .withConfig({ runName: 'TitleChain' });\n \n-    return await fullChain.invoke(\n-      { input: inputText, output: response },\n-      chainOptions\n-    );\n+    const invokeConfig = Object.assign({}, chainOptions, {\n+      run_id: this.id,\n+      runId: this.id,\n+    });\n+\n+    try {\n+      return await fullChain.invoke(\n+        { input: inputText, output: response },\n+        invokeConfig\n+      );\n+    } catch (_e) {\n+      // Fallback: strip callbacks to avoid EventStream tracer errors in certain environments\n+      // But preserve langfuse handler if it exists\n+      const langfuseHandler = (invokeConfig.callbacks as t.ProvidedCallbacks)?.find(\n+        (cb) => cb instanceof CallbackHandler\n+      );\n+      const { callbacks: _cb, ...rest } = invokeConfig;\n+      const safeConfig = Object.assign({}, rest, {\n+        callbacks: langfuseHandler ? [langfuseHandler] : [],\n+      });\n+      return await fullChain.invoke(\n+        { input: inputText, output: response },\n+        safeConfig as Partial<RunnableConfig>\n+      );\n+    }\n   }\n }",
      "patch_lines": [
        "@@ -1,9 +1,7 @@\n",
        " // src/run.ts\n",
        " import './instrumentation';\n",
        "-import { zodToJsonSchema } from 'zod-to-json-schema';\n",
        " import { CallbackHandler } from '@langfuse/langchain';\n",
        " import { PromptTemplate } from '@langchain/core/prompts';\n",
        "-import { SystemMessage } from '@langchain/core/messages';\n",
        " import { RunnableLambda } from '@langchain/core/runnables';\n",
        " import { AzureChatOpenAI, ChatOpenAI } from '@langchain/openai';\n",
        " import type {\n",
        "@@ -12,15 +10,13 @@ import type {\n",
        " } from '@langchain/core/messages';\n",
        " import type { StringPromptValue } from '@langchain/core/prompt_values';\n",
        " import type { RunnableConfig } from '@langchain/core/runnables';\n",
        "-import type { ClientCallbacks, SystemCallbacks } from '@/graphs/Graph';\n",
        " import type * as t from '@/types';\n",
        "-import { GraphEvents, Providers, Callback, TitleMethod } from '@/common';\n",
        "-import { manualToolStreamProviders } from '@/llm/providers';\n",
        "-import { shiftIndexTokenCountMap } from '@/messages/format';\n",
        " import {\n",
        "-  createTitleRunnable,\n",
        "   createCompletionTitleRunnable,\n",
        "+  createTitleRunnable,\n",
        " } from '@/utils/title';\n",
        "+import { GraphEvents, Callback, TitleMethod } from '@/common';\n",
        "+import { MultiAgentGraph } from '@/graphs/MultiAgentGraph';\n",
        " import { createTokenCounter } from '@/utils/tokens';\n",
        " import { StandardGraph } from '@/graphs/Graph';\n",
        " import { HandlerRegistry } from '@/events';\n",
        "@@ -40,14 +36,13 @@ export const defaultOmitOptions = new Set([\n",
        "   'additionalModelRequestFields',\n",
        " ]);\n",
        " \n",
        "-export class Run<T extends t.BaseGraphState> {\n",
        "-  graphRunnable?: t.CompiledWorkflow<T, Partial<T>, string>;\n",
        "-  // private collab!: CollabGraph;\n",
        "-  // private taskManager!: TaskManager;\n",
        "-  private handlerRegistry: HandlerRegistry;\n",
        "+export class Run<_T extends t.BaseGraphState> {\n",
        "   id: string;\n",
        "-  Graph: StandardGraph | undefined;\n",
        "-  provider: Providers | undefined;\n",
        "+  private tokenCounter?: t.TokenCounter;\n",
        "+  private handlerRegistry: HandlerRegistry;\n",
        "+  private indexTokenCountMap?: Record<string, number>;\n",
        "+  graphRunnable?: t.CompiledStateWorkflow;\n",
        "+  Graph: StandardGraph | MultiAgentGraph | undefined;\n",
        "   returnContent: boolean = false;\n",
        " \n",
        "   private constructor(config: Partial<t.RunConfig>) {\n",
        "@@ -57,6 +52,8 @@ export class Run<T extends t.BaseGraphState> {\n",
        "     }\n",
        " \n",
        "     this.id = runId;\n",
        "+    this.tokenCounter = config.tokenCounter;\n",
        "+    this.indexTokenCountMap = config.indexTokenCountMap;\n",
        " \n",
        "     const handlerRegistry = new HandlerRegistry();\n",
        " \n",
        "@@ -74,39 +71,89 @@ export class Run<T extends t.BaseGraphState> {\n",
        "       throw new Error('Graph config not provided');\n",
        "     }\n",
        " \n",
        "-    if (config.graphConfig.type === 'standard' || !config.graphConfig.type) {\n",
        "-      this.provider = config.graphConfig.llmConfig.provider;\n",
        "-      this.graphRunnable = this.createStandardGraph(\n",
        "-        config.graphConfig\n",
        "-      ) as unknown as t.CompiledWorkflow<T, Partial<T>, string>;\n",
        "+    /** Handle different graph types */\n",
        "+    if (config.graphConfig.type === 'multi-agent') {\n",
        "+      this.graphRunnable = this.createMultiAgentGraph(config.graphConfig);\n",
        "+      if (this.Graph) {\n",
        "+        this.Graph.handlerRegistry = handlerRegistry;\n",
        "+      }\n",
        "+    } else {\n",
        "+      // Default to legacy graph for 'standard' or undefined type\n",
        "+      this.graphRunnable = this.createLegacyGraph(config.graphConfig);\n",
        "       if (this.Graph) {\n",
        "+        this.Graph.compileOptions =\n",
        "+          config.graphConfig.compileOptions ?? this.Graph.compileOptions;\n",
        "         this.Graph.handlerRegistry = handlerRegistry;\n",
        "       }\n",
        "     }\n",
        " \n",
        "     this.returnContent = config.returnContent ?? false;\n",
        "   }\n",
        " \n",
        "-  private createStandardGraph(\n",
        "-    config: t.StandardGraphConfig\n",
        "-  ): t.CompiledWorkflow<t.IState, Partial<t.IState>, string> {\n",
        "-    const { llmConfig, tools = [], ...graphInput } = config;\n",
        "+  private createLegacyGraph(\n",
        "+    config: t.LegacyGraphConfig\n",
        "+  ): t.CompiledStateWorkflow {\n",
        "+    const {\n",
        "+      type: _type,\n",
        "+      llmConfig,\n",
        "+      signal,\n",
        "+      tools = [],\n",
        "+      ...agentInputs\n",
        "+    } = config;\n",
        "     const { provider, ...clientOptions } = llmConfig;\n",
        " \n",
        "-    const standardGraph = new StandardGraph({\n",
        "+    /** TEMP: Create agent configuration for the single agent */\n",
        "+    const agentConfig: t.AgentInputs = {\n",
        "+      ...agentInputs,\n",
        "       tools,\n",
        "       provider,\n",
        "       clientOptions,\n",
        "-      ...graphInput,\n",
        "+      agentId: 'default',\n",
        "+    };\n",
        "+\n",
        "+    const standardGraph = new StandardGraph({\n",
        "+      signal,\n",
        "       runId: this.id,\n",
        "+      agents: [agentConfig],\n",
        "+      tokenCounter: this.tokenCounter,\n",
        "+      indexTokenCountMap: this.indexTokenCountMap,\n",
        "     });\n",
        "+    // propagate compile options from graph config\n",
        "+    standardGraph.compileOptions = (\n",
        "+      config as t.LegacyGraphConfig\n",
        "+    ).compileOptions;\n",
        "     this.Graph = standardGraph;\n",
        "     return standardGraph.createWorkflow();\n",
        "   }\n",
        " \n",
        "+  private createMultiAgentGraph(\n",
        "+    config: t.MultiAgentGraphConfig\n",
        "+  ): t.CompiledStateWorkflow {\n",
        "+    const { agents, edges, compileOptions } = config;\n",
        "+\n",
        "+    const multiAgentGraph = new MultiAgentGraph({\n",
        "+      runId: this.id,\n",
        "+      agents,\n",
        "+      edges,\n",
        "+      tokenCounter: this.tokenCounter,\n",
        "+      indexTokenCountMap: this.indexTokenCountMap,\n",
        "+    });\n",
        "+\n",
        "+    if (compileOptions != null) {\n",
        "+      multiAgentGraph.compileOptions = compileOptions;\n",
        "+    }\n",
        "+\n",
        "+    this.Graph = multiAgentGraph;\n",
        "+    return multiAgentGraph.createWorkflow();\n",
        "+  }\n",
        "+\n",
        "   static async create<T extends t.BaseGraphState>(\n",
        "     config: t.RunConfig\n",
        "   ): Promise<Run<T>> {\n",
        "+    // Create tokenCounter if indexTokenCountMap is provided but tokenCounter is not\n",
        "+    if (config.indexTokenCountMap && !config.tokenCounter) {\n",
        "+      config.tokenCounter = await createTokenCounter();\n",
        "+    }\n",
        "     return new Run<T>(config);\n",
        "   }\n",
        " \n",
        "@@ -119,12 +166,49 @@ export class Run<T extends t.BaseGraphState> {\n",
        "     return this.Graph.getRunMessages();\n",
        "   }\n",
        " \n",
        "+  /**\n",
        "+   * Creates a custom event callback handler that intercepts custom events\n",
        "+   * and processes them through our handler registry instead of EventStreamCallbackHandler\n",
        "+   */\n",
        "+  private createCustomEventCallback() {\n",
        "+    return async (\n",
        "+      eventName: string,\n",
        "+      data: unknown,\n",
        "+      runId: string,\n",
        "+      tags?: string[],\n",
        "+      metadata?: Record<string, unknown>\n",
        "+    ): Promise<void> => {\n",
        "+      if (\n",
        "+        (data as t.StreamEventData)['emitted'] === true &&\n",
        "+        eventName === GraphEvents.CHAT_MODEL_STREAM\n",
        "+      ) {\n",
        "+        return;\n",
        "+      }\n",
        "+      const handler = this.handlerRegistry.getHandler(eventName);\n",
        "+      if (handler && this.Graph) {\n",
        "+        await handler.handle(\n",
        "+          eventName,\n",
        "+          data as\n",
        "+            | t.StreamEventData\n",
        "+            | t.ModelEndData\n",
        "+            | t.RunStep\n",
        "+            | t.RunStepDeltaEvent\n",
        "+            | t.MessageDeltaEvent\n",
        "+            | t.ReasoningDeltaEvent\n",
        "+            | { result: t.ToolEndEvent },\n",
        "+          metadata,\n",
        "+          this.Graph\n",
        "+        );\n",
        "+      }\n",
        "+    };\n",
        "+  }\n",
        "+\n",
        "   async processStream(\n",
        "     inputs: t.IState,\n",
        "     config: Partial<RunnableConfig> & { version: 'v1' | 'v2'; run_id?: string },\n",
        "     streamOptions?: t.EventStreamOptions\n",
        "   ): Promise<MessageContentComplex[] | undefined> {\n",
        "-    if (!this.graphRunnable) {\n",
        "+    if (this.graphRunnable == null) {\n",
        "       throw new Error(\n",
        "         'Run not initialized. Make sure to use Run.create() to instantiate the Run.'\n",
        "       );\n",
        "@@ -136,15 +220,18 @@ export class Run<T extends t.BaseGraphState> {\n",
        "     }\n",
        " \n",
        "     this.Graph.resetValues(streamOptions?.keepContent);\n",
        "-    const provider = this.Graph.provider;\n",
        "-    const hasTools = this.Graph.tools ? this.Graph.tools.length > 0 : false;\n",
        "-    if (streamOptions?.callbacks) {\n",
        "-      /* TODO: conflicts with callback manager */\n",
        "-      const callbacks = (config.callbacks as t.ProvidedCallbacks) ?? [];\n",
        "-      config.callbacks = callbacks.concat(\n",
        "-        this.getCallbacks(streamOptions.callbacks)\n",
        "-      );\n",
        "-    }\n",
        "+\n",
        "+    /** Custom event callback to intercept and handle custom events */\n",
        "+    const customEventCallback = this.createCustomEventCallback();\n",
        "+\n",
        "+    const baseCallbacks = (config.callbacks as t.ProvidedCallbacks) ?? [];\n",
        "+    const streamCallbacks = streamOptions?.callbacks\n",
        "+      ? this.getCallbacks(streamOptions.callbacks)\n",
        "+      : [];\n",
        "+\n",
        "+    config.callbacks = baseCallbacks.concat(streamCallbacks).concat({\n",
        "+      [Callback.CUSTOM_EVENT]: customEventCallback,\n",
        "+    });\n",
        " \n",
        "     if (\n",
        "       isPresent(process.env.LANGFUSE_SECRET_KEY) &&\n",
        "@@ -171,79 +258,28 @@ export class Run<T extends t.BaseGraphState> {\n",
        "       throw new Error('Run ID not provided');\n",
        "     }\n",
        " \n",
        "-    const tokenCounter =\n",
        "-      streamOptions?.tokenCounter ??\n",
        "-      (streamOptions?.indexTokenCountMap\n",
        "-        ? await createTokenCounter()\n",
        "-        : undefined);\n",
        "-    const tools = this.Graph.tools as\n",
        "-      | Array<t.GenericTool | undefined>\n",
        "-      | undefined;\n",
        "-    const toolTokens = tokenCounter\n",
        "-      ? (tools?.reduce((acc, tool) => {\n",
        "-        if (!(tool as Partial<t.GenericTool>).schema) {\n",
        "-          return acc;\n",
        "-        }\n",
        "-\n",
        "-        const jsonSchema = zodToJsonSchema(\n",
        "-          (tool?.schema as t.ZodObjectAny).describe(tool?.description ?? ''),\n",
        "-          tool?.name ?? ''\n",
        "-        );\n",
        "-        return (\n",
        "-          acc + tokenCounter(new SystemMessage(JSON.stringify(jsonSchema)))\n",
        "-        );\n",
        "-      }, 0) ?? 0)\n",
        "-      : 0;\n",
        "-    let instructionTokens = toolTokens;\n",
        "-    if (this.Graph.systemMessage && tokenCounter) {\n",
        "-      instructionTokens += tokenCounter(this.Graph.systemMessage);\n",
        "-    }\n",
        "-    const tokenMap = streamOptions?.indexTokenCountMap ?? {};\n",
        "-    if (this.Graph.systemMessage && instructionTokens > 0) {\n",
        "-      this.Graph.indexTokenCountMap = shiftIndexTokenCountMap(\n",
        "-        tokenMap,\n",
        "-        instructionTokens\n",
        "-      );\n",
        "-    } else if (instructionTokens > 0) {\n",
        "-      tokenMap[0] = tokenMap[0] + instructionTokens;\n",
        "-      this.Graph.indexTokenCountMap = tokenMap;\n",
        "-    } else {\n",
        "-      this.Graph.indexTokenCountMap = tokenMap;\n",
        "-    }\n",
        "-\n",
        "-    this.Graph.maxContextTokens = streamOptions?.maxContextTokens;\n",
        "-    this.Graph.tokenCounter = tokenCounter;\n",
        "-\n",
        "     config.run_id = this.id;\n",
        "     config.configurable = Object.assign(config.configurable ?? {}, {\n",
        "       run_id: this.id,\n",
        "-      provider: this.provider,\n",
        "     });\n",
        " \n",
        "     const stream = this.graphRunnable.streamEvents(inputs, config, {\n",
        "       raiseError: true,\n",
        "     });\n",
        " \n",
        "     for await (const event of stream) {\n",
        "-      const { data, name, metadata, ...info } = event;\n",
        "+      const { data, metadata, ...info } = event;\n",
        " \n",
        "-      let eventName: t.EventName = info.event;\n",
        "-      if (\n",
        "-        hasTools &&\n",
        "-        manualToolStreamProviders.has(provider) &&\n",
        "-        eventName === GraphEvents.CHAT_MODEL_STREAM\n",
        "-      ) {\n",
        "-        /* Skipping CHAT_MODEL_STREAM event due to double-call edge case */\n",
        "-        continue;\n",
        "-      }\n",
        "+      const eventName: t.EventName = info.event;\n",
        " \n",
        "-      if (eventName && eventName === GraphEvents.ON_CUSTOM_EVENT) {\n",
        "-        eventName = name;\n",
        "+      /** Skip custom events as they're handled by our callback */\n",
        "+      if (eventName === GraphEvents.ON_CUSTOM_EVENT) {\n",
        "+        continue;\n",
        "       }\n",
        " \n",
        "       const handler = this.handlerRegistry.getHandler(eventName);\n",
        "       if (handler) {\n",
        "-        handler.handle(eventName, data, metadata, this.Graph);\n",
        "+        await handler.handle(eventName, data, metadata, this.Graph);\n",
        "       }\n",
        "     }\n",
        " \n",
        "@@ -252,19 +288,19 @@ export class Run<T extends t.BaseGraphState> {\n",
        "     }\n",
        "   }\n",
        " \n",
        "-  private createSystemCallback<K extends keyof ClientCallbacks>(\n",
        "-    clientCallbacks: ClientCallbacks,\n",
        "+  private createSystemCallback<K extends keyof t.ClientCallbacks>(\n",
        "+    clientCallbacks: t.ClientCallbacks,\n",
        "     key: K\n",
        "-  ): SystemCallbacks[K] {\n",
        "+  ): t.SystemCallbacks[K] {\n",
        "     return ((...args: unknown[]) => {\n",
        "       const clientCallback = clientCallbacks[key];\n",
        "       if (clientCallback && this.Graph) {\n",
        "         (clientCallback as (...args: unknown[]) => void)(this.Graph, ...args);\n",
        "       }\n",
        "-    }) as SystemCallbacks[K];\n",
        "+    }) as t.SystemCallbacks[K];\n",
        "   }\n",
        " \n",
        "-  getCallbacks(clientCallbacks: ClientCallbacks): SystemCallbacks {\n",
        "+  getCallbacks(clientCallbacks: t.ClientCallbacks): t.SystemCallbacks {\n",
        "     return {\n",
        "       [Callback.TOOL_ERROR]: this.createSystemCallback(\n",
        "         clientCallbacks,\n",
        "@@ -289,7 +325,6 @@ export class Run<T extends t.BaseGraphState> {\n",
        "     clientOptions,\n",
        "     chainOptions,\n",
        "     skipLanguage,\n",
        "-    omitOptions = defaultOmitOptions,\n",
        "     titleMethod = TitleMethod.COMPLETION,\n",
        "     titlePromptTemplate,\n",
        "   }: t.RunTitleOptions): Promise<{ language?: string; title?: string }> {\n",
        "@@ -327,7 +362,6 @@ export class Run<T extends t.BaseGraphState> {\n",
        " \n",
        "     const model = this.Graph?.getNewModel({\n",
        "       provider,\n",
        "-      omitOptions,\n",
        "       clientOptions,\n",
        "     });\n",
        "     if (!model) {\n",
        "@@ -373,9 +407,30 @@ export class Run<T extends t.BaseGraphState> {\n",
        "       .pipe(titleChain)\n",
        "       .withConfig({ runName: 'TitleChain' });\n",
        " \n",
        "-    return await fullChain.invoke(\n",
        "-      { input: inputText, output: response },\n",
        "-      chainOptions\n",
        "-    );\n",
        "+    const invokeConfig = Object.assign({}, chainOptions, {\n",
        "+      run_id: this.id,\n",
        "+      runId: this.id,\n",
        "+    });\n",
        "+\n",
        "+    try {\n",
        "+      return await fullChain.invoke(\n",
        "+        { input: inputText, output: response },\n",
        "+        invokeConfig\n",
        "+      );\n",
        "+    } catch (_e) {\n",
        "+      // Fallback: strip callbacks to avoid EventStream tracer errors in certain environments\n",
        "+      // But preserve langfuse handler if it exists\n",
        "+      const langfuseHandler = (invokeConfig.callbacks as t.ProvidedCallbacks)?.find(\n",
        "+        (cb) => cb instanceof CallbackHandler\n",
        "+      );\n",
        "+      const { callbacks: _cb, ...rest } = invokeConfig;\n",
        "+      const safeConfig = Object.assign({}, rest, {\n",
        "+        callbacks: langfuseHandler ? [langfuseHandler] : [],\n",
        "+      });\n",
        "+      return await fullChain.invoke(\n",
        "+        { input: inputText, output: response },\n",
        "+        safeConfig as Partial<RunnableConfig>\n",
        "+      );\n",
        "+    }\n",
        "   }\n",
        " }\n"
      ]
    },
    {
      "path": "src/scripts/cli4.ts",
      "status": "modified",
      "additions": 29,
      "deletions": 21,
      "patch": "@@ -8,7 +8,6 @@ import type * as t from '@/types';\n import { ModelEndHandler, ToolEndHandler } from '@/events';\n import { ChatModelStreamHandler } from '@/stream';\n \n-\n import { getArgs } from '@/scripts/args';\n import { Run } from '@/run';\n import { GraphEvents, Callback, Providers } from '@/common';\n@@ -25,31 +24,35 @@ async function testStandardStreaming(): Promise<void> {\n       handle: (_event: string, data: t.StreamEventData): void => {\n         console.log('====== ON_RUN_STEP_COMPLETED ======');\n         console.dir(data, { depth: null });\n-      }\n+      },\n     },\n     [GraphEvents.ON_RUN_STEP]: {\n       handle: (_event: string, data: t.StreamEventData): void => {\n         console.log('====== ON_RUN_STEP ======');\n         console.dir(data, { depth: null });\n-      }\n+      },\n     },\n     [GraphEvents.ON_RUN_STEP_DELTA]: {\n       handle: (_event: string, data: t.StreamEventData): void => {\n         console.log('====== ON_RUN_STEP_DELTA ======');\n         console.dir(data, { depth: null });\n-      }\n+      },\n     },\n     [GraphEvents.ON_MESSAGE_DELTA]: {\n       handle: (_event: string, data: t.StreamEventData): void => {\n         console.log('====== ON_MESSAGE_DELTA ======');\n         console.dir(data, { depth: null });\n-      }\n+      },\n     },\n     [GraphEvents.TOOL_START]: {\n-      handle: (_event: string, data: t.StreamEventData, metadata?: Record<string, unknown>): void => {\n+      handle: (\n+        _event: string,\n+        data: t.StreamEventData,\n+        metadata?: Record<string, unknown>\n+      ): void => {\n         console.log('====== TOOL_START ======');\n         console.dir(data, { depth: null });\n-      }\n+      },\n     },\n     // [GraphEvents.LLM_STREAM]: new LLMStreamHandler(),\n     // [GraphEvents.LLM_START]: {\n@@ -90,11 +93,12 @@ async function testStandardStreaming(): Promise<void> {\n   // const llmConfig = getLLMConfig(provider);\n   let llmConfig = getLLMConfig(Providers.OPENAI);\n \n-  const graphConfig: t.StandardGraphConfig  = {\n+  const graphConfig: t.LegacyGraphConfig = {\n     type: 'standard',\n     llmConfig,\n     tools: [new TavilySearchResults()],\n-    instructions: 'You are a friendly AI assistant. Always address the user by their name.',\n+    instructions:\n+      'You are a friendly AI assistant. Always address the user by their name.',\n     additional_instructions: `The user's name is ${userName} and they are located in ${location}.`,\n   };\n \n@@ -116,21 +120,25 @@ async function testStandardStreaming(): Promise<void> {\n   console.log(' Test 1: OpenAI Tool Usage');\n \n   // conversationHistory.push(new HumanMessage(`Hi I'm ${userName}.`));\n-  conversationHistory.push(new HumanMessage(`search for good sunrise hikes near ${location}\n-then search weather in ${location} for today which is ${currentDate}`));\n+  conversationHistory.push(\n+    new HumanMessage(`search for good sunrise hikes near ${location}\n+then search weather in ${location} for today which is ${currentDate}`)\n+  );\n   let inputs = {\n     messages: conversationHistory,\n   };\n-  const contentParts = await run.processStream(inputs, config,\n-  //   {\n-  //   [Callback.TOOL_START]: (graph, ...args) => {\n-  //       console.log('TOOL_START callback');\n-  //   },\n-  //   [Callback.TOOL_END]: (graph, ...args) => {\n-  //       console.log('TOOL_END callback');\n-  //   },\n-  // }\n-);\n+  const contentParts = await run.processStream(\n+    inputs,\n+    config\n+    //   {\n+    //   [Callback.TOOL_START]: (graph, ...args) => {\n+    //       console.log('TOOL_START callback');\n+    //   },\n+    //   [Callback.TOOL_END]: (graph, ...args) => {\n+    //       console.log('TOOL_END callback');\n+    //   },\n+    // }\n+  );\n   const finalMessages = run.getRunMessages();\n   if (finalMessages) {\n     conversationHistory.push(...finalMessages);",
      "patch_lines": [
        "@@ -8,7 +8,6 @@ import type * as t from '@/types';\n",
        " import { ModelEndHandler, ToolEndHandler } from '@/events';\n",
        " import { ChatModelStreamHandler } from '@/stream';\n",
        " \n",
        "-\n",
        " import { getArgs } from '@/scripts/args';\n",
        " import { Run } from '@/run';\n",
        " import { GraphEvents, Callback, Providers } from '@/common';\n",
        "@@ -25,31 +24,35 @@ async function testStandardStreaming(): Promise<void> {\n",
        "       handle: (_event: string, data: t.StreamEventData): void => {\n",
        "         console.log('====== ON_RUN_STEP_COMPLETED ======');\n",
        "         console.dir(data, { depth: null });\n",
        "-      }\n",
        "+      },\n",
        "     },\n",
        "     [GraphEvents.ON_RUN_STEP]: {\n",
        "       handle: (_event: string, data: t.StreamEventData): void => {\n",
        "         console.log('====== ON_RUN_STEP ======');\n",
        "         console.dir(data, { depth: null });\n",
        "-      }\n",
        "+      },\n",
        "     },\n",
        "     [GraphEvents.ON_RUN_STEP_DELTA]: {\n",
        "       handle: (_event: string, data: t.StreamEventData): void => {\n",
        "         console.log('====== ON_RUN_STEP_DELTA ======');\n",
        "         console.dir(data, { depth: null });\n",
        "-      }\n",
        "+      },\n",
        "     },\n",
        "     [GraphEvents.ON_MESSAGE_DELTA]: {\n",
        "       handle: (_event: string, data: t.StreamEventData): void => {\n",
        "         console.log('====== ON_MESSAGE_DELTA ======');\n",
        "         console.dir(data, { depth: null });\n",
        "-      }\n",
        "+      },\n",
        "     },\n",
        "     [GraphEvents.TOOL_START]: {\n",
        "-      handle: (_event: string, data: t.StreamEventData, metadata?: Record<string, unknown>): void => {\n",
        "+      handle: (\n",
        "+        _event: string,\n",
        "+        data: t.StreamEventData,\n",
        "+        metadata?: Record<string, unknown>\n",
        "+      ): void => {\n",
        "         console.log('====== TOOL_START ======');\n",
        "         console.dir(data, { depth: null });\n",
        "-      }\n",
        "+      },\n",
        "     },\n",
        "     // [GraphEvents.LLM_STREAM]: new LLMStreamHandler(),\n",
        "     // [GraphEvents.LLM_START]: {\n",
        "@@ -90,11 +93,12 @@ async function testStandardStreaming(): Promise<void> {\n",
        "   // const llmConfig = getLLMConfig(provider);\n",
        "   let llmConfig = getLLMConfig(Providers.OPENAI);\n",
        " \n",
        "-  const graphConfig: t.StandardGraphConfig  = {\n",
        "+  const graphConfig: t.LegacyGraphConfig = {\n",
        "     type: 'standard',\n",
        "     llmConfig,\n",
        "     tools: [new TavilySearchResults()],\n",
        "-    instructions: 'You are a friendly AI assistant. Always address the user by their name.',\n",
        "+    instructions:\n",
        "+      'You are a friendly AI assistant. Always address the user by their name.',\n",
        "     additional_instructions: `The user's name is ${userName} and they are located in ${location}.`,\n",
        "   };\n",
        " \n",
        "@@ -116,21 +120,25 @@ async function testStandardStreaming(): Promise<void> {\n",
        "   console.log(' Test 1: OpenAI Tool Usage');\n",
        " \n",
        "   // conversationHistory.push(new HumanMessage(`Hi I'm ${userName}.`));\n",
        "-  conversationHistory.push(new HumanMessage(`search for good sunrise hikes near ${location}\n",
        "-then search weather in ${location} for today which is ${currentDate}`));\n",
        "+  conversationHistory.push(\n",
        "+    new HumanMessage(`search for good sunrise hikes near ${location}\n",
        "+then search weather in ${location} for today which is ${currentDate}`)\n",
        "+  );\n",
        "   let inputs = {\n",
        "     messages: conversationHistory,\n",
        "   };\n",
        "-  const contentParts = await run.processStream(inputs, config,\n",
        "-  //   {\n",
        "-  //   [Callback.TOOL_START]: (graph, ...args) => {\n",
        "-  //       console.log('TOOL_START callback');\n",
        "-  //   },\n",
        "-  //   [Callback.TOOL_END]: (graph, ...args) => {\n",
        "-  //       console.log('TOOL_END callback');\n",
        "-  //   },\n",
        "-  // }\n",
        "-);\n",
        "+  const contentParts = await run.processStream(\n",
        "+    inputs,\n",
        "+    config\n",
        "+    //   {\n",
        "+    //   [Callback.TOOL_START]: (graph, ...args) => {\n",
        "+    //       console.log('TOOL_START callback');\n",
        "+    //   },\n",
        "+    //   [Callback.TOOL_END]: (graph, ...args) => {\n",
        "+    //       console.log('TOOL_END callback');\n",
        "+    //   },\n",
        "+    // }\n",
        "+  );\n",
        "   const finalMessages = run.getRunMessages();\n",
        "   if (finalMessages) {\n",
        "     conversationHistory.push(...finalMessages);\n"
      ]
    },
    {
      "path": "src/scripts/cli5.ts",
      "status": "modified",
      "additions": 29,
      "deletions": 21,
      "patch": "@@ -8,7 +8,6 @@ import type * as t from '@/types';\n import { ModelEndHandler, ToolEndHandler } from '@/events';\n import { ChatModelStreamHandler } from '@/stream';\n \n-\n import { getArgs } from '@/scripts/args';\n import { Run } from '@/run';\n import { GraphEvents, Callback, Providers } from '@/common';\n@@ -25,31 +24,35 @@ async function testStandardStreaming(): Promise<void> {\n       handle: (_event: string, data: t.StreamEventData): void => {\n         console.log('====== ON_RUN_STEP_COMPLETED ======');\n         console.dir(data, { depth: null });\n-      }\n+      },\n     },\n     [GraphEvents.ON_RUN_STEP]: {\n       handle: (_event: string, data: t.StreamEventData): void => {\n         console.log('====== ON_RUN_STEP ======');\n         console.dir(data, { depth: null });\n-      }\n+      },\n     },\n     [GraphEvents.ON_RUN_STEP_DELTA]: {\n       handle: (_event: string, data: t.StreamEventData): void => {\n         console.log('====== ON_RUN_STEP_DELTA ======');\n         console.dir(data, { depth: null });\n-      }\n+      },\n     },\n     [GraphEvents.ON_MESSAGE_DELTA]: {\n       handle: (_event: string, data: t.StreamEventData): void => {\n         console.log('====== ON_MESSAGE_DELTA ======');\n         console.dir(data, { depth: null });\n-      }\n+      },\n     },\n     [GraphEvents.TOOL_START]: {\n-      handle: (_event: string, data: t.StreamEventData, metadata?: Record<string, unknown>): void => {\n+      handle: (\n+        _event: string,\n+        data: t.StreamEventData,\n+        metadata?: Record<string, unknown>\n+      ): void => {\n         console.log('====== TOOL_START ======');\n         console.dir(data, { depth: null });\n-      }\n+      },\n     },\n     // [GraphEvents.LLM_STREAM]: new LLMStreamHandler(),\n     // [GraphEvents.LLM_START]: {\n@@ -90,11 +93,12 @@ async function testStandardStreaming(): Promise<void> {\n   // const llmConfig = getLLMConfig(provider);\n   let llmConfig = getLLMConfig(Providers.ANTHROPIC);\n \n-  const graphConfig: t.StandardGraphConfig  = {\n+  const graphConfig: t.LegacyGraphConfig = {\n     type: 'standard',\n     llmConfig,\n     tools: [new TavilySearchResults()],\n-    instructions: 'You are a friendly AI assistant. Always address the user by their name.',\n+    instructions:\n+      'You are a friendly AI assistant. Always address the user by their name.',\n     additional_instructions: `The user's name is ${userName} and they are located in ${location}.`,\n   };\n \n@@ -116,21 +120,25 @@ async function testStandardStreaming(): Promise<void> {\n   console.log(' Test 1: Anthropic Tool Usage');\n \n   // conversationHistory.push(new HumanMessage(`Hi I'm ${userName}.`));\n-  conversationHistory.push(new HumanMessage(`search for good sunrise hikes near ${location}\n-then search weather in ${location} for today which is ${currentDate}`));\n+  conversationHistory.push(\n+    new HumanMessage(`search for good sunrise hikes near ${location}\n+then search weather in ${location} for today which is ${currentDate}`)\n+  );\n   let inputs = {\n     messages: conversationHistory,\n   };\n-  const contentParts = await run.processStream(inputs, config,\n-  //   {\n-  //   [Callback.TOOL_START]: (graph, ...args) => {\n-  //       console.log('TOOL_START callback');\n-  //   },\n-  //   [Callback.TOOL_END]: (graph, ...args) => {\n-  //       console.log('TOOL_END callback');\n-  //   },\n-  // }\n-);\n+  const contentParts = await run.processStream(\n+    inputs,\n+    config\n+    //   {\n+    //   [Callback.TOOL_START]: (graph, ...args) => {\n+    //       console.log('TOOL_START callback');\n+    //   },\n+    //   [Callback.TOOL_END]: (graph, ...args) => {\n+    //       console.log('TOOL_END callback');\n+    //   },\n+    // }\n+  );\n   const finalMessages = run.getRunMessages();\n   if (finalMessages) {\n     conversationHistory.push(...finalMessages);",
      "patch_lines": [
        "@@ -8,7 +8,6 @@ import type * as t from '@/types';\n",
        " import { ModelEndHandler, ToolEndHandler } from '@/events';\n",
        " import { ChatModelStreamHandler } from '@/stream';\n",
        " \n",
        "-\n",
        " import { getArgs } from '@/scripts/args';\n",
        " import { Run } from '@/run';\n",
        " import { GraphEvents, Callback, Providers } from '@/common';\n",
        "@@ -25,31 +24,35 @@ async function testStandardStreaming(): Promise<void> {\n",
        "       handle: (_event: string, data: t.StreamEventData): void => {\n",
        "         console.log('====== ON_RUN_STEP_COMPLETED ======');\n",
        "         console.dir(data, { depth: null });\n",
        "-      }\n",
        "+      },\n",
        "     },\n",
        "     [GraphEvents.ON_RUN_STEP]: {\n",
        "       handle: (_event: string, data: t.StreamEventData): void => {\n",
        "         console.log('====== ON_RUN_STEP ======');\n",
        "         console.dir(data, { depth: null });\n",
        "-      }\n",
        "+      },\n",
        "     },\n",
        "     [GraphEvents.ON_RUN_STEP_DELTA]: {\n",
        "       handle: (_event: string, data: t.StreamEventData): void => {\n",
        "         console.log('====== ON_RUN_STEP_DELTA ======');\n",
        "         console.dir(data, { depth: null });\n",
        "-      }\n",
        "+      },\n",
        "     },\n",
        "     [GraphEvents.ON_MESSAGE_DELTA]: {\n",
        "       handle: (_event: string, data: t.StreamEventData): void => {\n",
        "         console.log('====== ON_MESSAGE_DELTA ======');\n",
        "         console.dir(data, { depth: null });\n",
        "-      }\n",
        "+      },\n",
        "     },\n",
        "     [GraphEvents.TOOL_START]: {\n",
        "-      handle: (_event: string, data: t.StreamEventData, metadata?: Record<string, unknown>): void => {\n",
        "+      handle: (\n",
        "+        _event: string,\n",
        "+        data: t.StreamEventData,\n",
        "+        metadata?: Record<string, unknown>\n",
        "+      ): void => {\n",
        "         console.log('====== TOOL_START ======');\n",
        "         console.dir(data, { depth: null });\n",
        "-      }\n",
        "+      },\n",
        "     },\n",
        "     // [GraphEvents.LLM_STREAM]: new LLMStreamHandler(),\n",
        "     // [GraphEvents.LLM_START]: {\n",
        "@@ -90,11 +93,12 @@ async function testStandardStreaming(): Promise<void> {\n",
        "   // const llmConfig = getLLMConfig(provider);\n",
        "   let llmConfig = getLLMConfig(Providers.ANTHROPIC);\n",
        " \n",
        "-  const graphConfig: t.StandardGraphConfig  = {\n",
        "+  const graphConfig: t.LegacyGraphConfig = {\n",
        "     type: 'standard',\n",
        "     llmConfig,\n",
        "     tools: [new TavilySearchResults()],\n",
        "-    instructions: 'You are a friendly AI assistant. Always address the user by their name.',\n",
        "+    instructions:\n",
        "+      'You are a friendly AI assistant. Always address the user by their name.',\n",
        "     additional_instructions: `The user's name is ${userName} and they are located in ${location}.`,\n",
        "   };\n",
        " \n",
        "@@ -116,21 +120,25 @@ async function testStandardStreaming(): Promise<void> {\n",
        "   console.log(' Test 1: Anthropic Tool Usage');\n",
        " \n",
        "   // conversationHistory.push(new HumanMessage(`Hi I'm ${userName}.`));\n",
        "-  conversationHistory.push(new HumanMessage(`search for good sunrise hikes near ${location}\n",
        "-then search weather in ${location} for today which is ${currentDate}`));\n",
        "+  conversationHistory.push(\n",
        "+    new HumanMessage(`search for good sunrise hikes near ${location}\n",
        "+then search weather in ${location} for today which is ${currentDate}`)\n",
        "+  );\n",
        "   let inputs = {\n",
        "     messages: conversationHistory,\n",
        "   };\n",
        "-  const contentParts = await run.processStream(inputs, config,\n",
        "-  //   {\n",
        "-  //   [Callback.TOOL_START]: (graph, ...args) => {\n",
        "-  //       console.log('TOOL_START callback');\n",
        "-  //   },\n",
        "-  //   [Callback.TOOL_END]: (graph, ...args) => {\n",
        "-  //       console.log('TOOL_END callback');\n",
        "-  //   },\n",
        "-  // }\n",
        "-);\n",
        "+  const contentParts = await run.processStream(\n",
        "+    inputs,\n",
        "+    config\n",
        "+    //   {\n",
        "+    //   [Callback.TOOL_START]: (graph, ...args) => {\n",
        "+    //       console.log('TOOL_START callback');\n",
        "+    //   },\n",
        "+    //   [Callback.TOOL_END]: (graph, ...args) => {\n",
        "+    //       console.log('TOOL_END callback');\n",
        "+    //   },\n",
        "+    // }\n",
        "+  );\n",
        "   const finalMessages = run.getRunMessages();\n",
        "   if (finalMessages) {\n",
        "     conversationHistory.push(...finalMessages);\n"
      ]
    },
    {
      "path": "src/scripts/code_exec.ts",
      "status": "modified",
      "additions": 5,
      "deletions": 5,
      "patch": "@@ -54,8 +54,8 @@ async function testCodeExecution(): Promise<void> {\n         event: GraphEvents.ON_RUN_STEP_DELTA,\n         data: t.StreamEventData\n       ): void => {\n-        console.log('====== ON_RUN_STEP_DELTA ======');\n-        console.dir(data, { depth: null });\n+        // console.log('====== ON_RUN_STEP_DELTA ======');\n+        // console.dir(data, { depth: null });\n         aggregateContent({ event, data: data as t.RunStepDeltaEvent });\n       },\n     },\n@@ -64,8 +64,8 @@ async function testCodeExecution(): Promise<void> {\n         event: GraphEvents.ON_MESSAGE_DELTA,\n         data: t.StreamEventData\n       ): void => {\n-        console.log('====== ON_MESSAGE_DELTA ======');\n-        console.dir(data, { depth: null });\n+        // console.log('====== ON_MESSAGE_DELTA ======');\n+        // console.dir(data, { depth: null });\n         aggregateContent({ event, data: data as t.MessageDeltaEvent });\n       },\n     },\n@@ -176,7 +176,7 @@ async function testCodeExecution(): Promise<void> {\n     conversationHistory.push(...finalMessages2);\n   }\n   console.log('\\n\\n====================\\n\\n');\n-  console.dir(contentParts, { depth: null });\n+  console.dir(finalContentParts2, { depth: null });\n \n   const { handleLLMEnd, collected } = createMetadataAggregator();\n   const titleResult = await run.generateTitle({",
      "patch_lines": [
        "@@ -54,8 +54,8 @@ async function testCodeExecution(): Promise<void> {\n",
        "         event: GraphEvents.ON_RUN_STEP_DELTA,\n",
        "         data: t.StreamEventData\n",
        "       ): void => {\n",
        "-        console.log('====== ON_RUN_STEP_DELTA ======');\n",
        "-        console.dir(data, { depth: null });\n",
        "+        // console.log('====== ON_RUN_STEP_DELTA ======');\n",
        "+        // console.dir(data, { depth: null });\n",
        "         aggregateContent({ event, data: data as t.RunStepDeltaEvent });\n",
        "       },\n",
        "     },\n",
        "@@ -64,8 +64,8 @@ async function testCodeExecution(): Promise<void> {\n",
        "         event: GraphEvents.ON_MESSAGE_DELTA,\n",
        "         data: t.StreamEventData\n",
        "       ): void => {\n",
        "-        console.log('====== ON_MESSAGE_DELTA ======');\n",
        "-        console.dir(data, { depth: null });\n",
        "+        // console.log('====== ON_MESSAGE_DELTA ======');\n",
        "+        // console.dir(data, { depth: null });\n",
        "         aggregateContent({ event, data: data as t.MessageDeltaEvent });\n",
        "       },\n",
        "     },\n",
        "@@ -176,7 +176,7 @@ async function testCodeExecution(): Promise<void> {\n",
        "     conversationHistory.push(...finalMessages2);\n",
        "   }\n",
        "   console.log('\\n\\n====================\\n\\n');\n",
        "-  console.dir(contentParts, { depth: null });\n",
        "+  console.dir(finalContentParts2, { depth: null });\n",
        " \n",
        "   const { handleLLMEnd, collected } = createMetadataAggregator();\n",
        "   const titleResult = await run.generateTitle({\n"
      ]
    },
    {
      "path": "src/scripts/code_exec_simple.ts",
      "status": "modified",
      "additions": 46,
      "deletions": 27,
      "patch": "@@ -23,45 +23,74 @@ async function testCodeExecution(): Promise<void> {\n     [GraphEvents.CHAT_MODEL_END]: new ModelEndHandler(),\n     [GraphEvents.CHAT_MODEL_STREAM]: new ChatModelStreamHandler(),\n     [GraphEvents.ON_RUN_STEP_COMPLETED]: {\n-      handle: (event: GraphEvents.ON_RUN_STEP_COMPLETED, data: t.StreamEventData): void => {\n+      handle: (\n+        event: GraphEvents.ON_RUN_STEP_COMPLETED,\n+        data: t.StreamEventData\n+      ): void => {\n         console.log('====== ON_RUN_STEP_COMPLETED ======');\n         console.dir(data, { depth: null });\n-        aggregateContent({ event, data: data as unknown as { result: t.ToolEndEvent } });\n-      }\n+        aggregateContent({\n+          event,\n+          data: data as unknown as { result: t.ToolEndEvent },\n+        });\n+      },\n     },\n     [GraphEvents.ON_RUN_STEP]: {\n-      handle: (event: GraphEvents.ON_RUN_STEP, data: t.StreamEventData): void => {\n+      handle: (\n+        event: GraphEvents.ON_RUN_STEP,\n+        data: t.StreamEventData\n+      ): void => {\n         console.log('====== ON_RUN_STEP ======');\n         console.dir(data, { depth: null });\n         aggregateContent({ event, data: data as t.RunStep });\n-      }\n+      },\n     },\n     [GraphEvents.ON_RUN_STEP_DELTA]: {\n-      handle: (event: GraphEvents.ON_RUN_STEP_DELTA, data: t.StreamEventData): void => {\n+      handle: (\n+        event: GraphEvents.ON_RUN_STEP_DELTA,\n+        data: t.StreamEventData\n+      ): void => {\n         console.log('====== ON_RUN_STEP_DELTA ======');\n         console.dir(data, { depth: null });\n         aggregateContent({ event, data: data as t.RunStepDeltaEvent });\n-      }\n+      },\n     },\n     [GraphEvents.ON_MESSAGE_DELTA]: {\n-      handle: (event: GraphEvents.ON_MESSAGE_DELTA, data: t.StreamEventData): void => {\n+      handle: (\n+        event: GraphEvents.ON_MESSAGE_DELTA,\n+        data: t.StreamEventData\n+      ): void => {\n         console.log('====== ON_MESSAGE_DELTA ======');\n         console.dir(data, { depth: null });\n         aggregateContent({ event, data: data as t.MessageDeltaEvent });\n-      }\n+      },\n     },\n     [GraphEvents.TOOL_START]: {\n-      handle: (_event: string, data: t.StreamEventData, metadata?: Record<string, unknown>): void => {\n+      handle: (\n+        _event: string,\n+        data: t.StreamEventData,\n+        metadata?: Record<string, unknown>\n+      ): void => {\n         console.log('====== TOOL_START ======');\n         console.dir(data, { depth: null });\n-      }\n+      },\n     },\n   };\n \n   const llmConfig = getLLMConfig(provider);\n-  const instructions = 'You are a friendly AI assistant with coding capabilities. Always address the user by their name.';\n+  const instructions =\n+    'You are a friendly AI assistant with coding capabilities. Always address the user by their name.';\n   const additional_instructions = `The user's name is ${userName} and they are located in ${location}.`;\n \n+  // const userMessage1 = `how much memory is this (its in bytes) in MB? 31192000`;\n+  // const userMessage1 = `can you show me a good use case for rscript by running some code`;\n+  const userMessage1 = `Run hello world in french and in english, using python. please run 2 parallel code executions.`;\n+  const humanMessage = new HumanMessage(userMessage1);\n+  const tokenCounter = await createTokenCounter();\n+  const indexTokenCountMap = {\n+    0: tokenCounter(humanMessage),\n+  };\n+\n   const runConfig: t.RunConfig = {\n     runId: 'message-num-1',\n     graphConfig: {\n@@ -70,9 +99,12 @@ async function testCodeExecution(): Promise<void> {\n       tools: [new TavilySearchResults(), createCodeExecutionTool()],\n       instructions,\n       additional_instructions,\n+      maxContextTokens: 8000,\n     },\n     returnContent: true,\n     customHandlers,\n+    indexTokenCountMap,\n+    tokenCounter,\n   };\n   const run = await Run.create<t.IState>(runConfig);\n \n@@ -87,25 +119,12 @@ async function testCodeExecution(): Promise<void> {\n \n   console.log('Test 1: Simple Code Execution');\n \n-  // const userMessage1 = `how much memory is this (its in bytes) in MB? 31192000`;\n-  // const userMessage1 = `can you show me a good use case for rscript by running some code`;\n-  const userMessage1 = `Run hello world in french and in english, using python. please run 2 parallel code executions.`;\n-  const humanMessage = new HumanMessage(userMessage1);\n-  const tokenCounter = await createTokenCounter();\n-  const indexTokenCountMap = {\n-    0: tokenCounter(humanMessage),\n-  };\n-\n   conversationHistory.push(humanMessage);\n \n   let inputs = {\n     messages: conversationHistory,\n   };\n-  const finalContentParts1 = await run.processStream(inputs, config, {\n-    maxContextTokens: 8000,\n-    indexTokenCountMap,\n-    tokenCounter,\n-  });\n+  const finalContentParts1 = await run.processStream(inputs, config);\n   const finalMessages1 = run.getRunMessages();\n   if (finalMessages1) {\n     conversationHistory.push(...finalMessages1);\n@@ -126,4 +145,4 @@ testCodeExecution().catch((err) => {\n   console.log('Conversation history:');\n   console.dir(conversationHistory, { depth: null });\n   process.exit(1);\n-});\n\\ No newline at end of file\n+});",
      "patch_lines": [
        "@@ -23,45 +23,74 @@ async function testCodeExecution(): Promise<void> {\n",
        "     [GraphEvents.CHAT_MODEL_END]: new ModelEndHandler(),\n",
        "     [GraphEvents.CHAT_MODEL_STREAM]: new ChatModelStreamHandler(),\n",
        "     [GraphEvents.ON_RUN_STEP_COMPLETED]: {\n",
        "-      handle: (event: GraphEvents.ON_RUN_STEP_COMPLETED, data: t.StreamEventData): void => {\n",
        "+      handle: (\n",
        "+        event: GraphEvents.ON_RUN_STEP_COMPLETED,\n",
        "+        data: t.StreamEventData\n",
        "+      ): void => {\n",
        "         console.log('====== ON_RUN_STEP_COMPLETED ======');\n",
        "         console.dir(data, { depth: null });\n",
        "-        aggregateContent({ event, data: data as unknown as { result: t.ToolEndEvent } });\n",
        "-      }\n",
        "+        aggregateContent({\n",
        "+          event,\n",
        "+          data: data as unknown as { result: t.ToolEndEvent },\n",
        "+        });\n",
        "+      },\n",
        "     },\n",
        "     [GraphEvents.ON_RUN_STEP]: {\n",
        "-      handle: (event: GraphEvents.ON_RUN_STEP, data: t.StreamEventData): void => {\n",
        "+      handle: (\n",
        "+        event: GraphEvents.ON_RUN_STEP,\n",
        "+        data: t.StreamEventData\n",
        "+      ): void => {\n",
        "         console.log('====== ON_RUN_STEP ======');\n",
        "         console.dir(data, { depth: null });\n",
        "         aggregateContent({ event, data: data as t.RunStep });\n",
        "-      }\n",
        "+      },\n",
        "     },\n",
        "     [GraphEvents.ON_RUN_STEP_DELTA]: {\n",
        "-      handle: (event: GraphEvents.ON_RUN_STEP_DELTA, data: t.StreamEventData): void => {\n",
        "+      handle: (\n",
        "+        event: GraphEvents.ON_RUN_STEP_DELTA,\n",
        "+        data: t.StreamEventData\n",
        "+      ): void => {\n",
        "         console.log('====== ON_RUN_STEP_DELTA ======');\n",
        "         console.dir(data, { depth: null });\n",
        "         aggregateContent({ event, data: data as t.RunStepDeltaEvent });\n",
        "-      }\n",
        "+      },\n",
        "     },\n",
        "     [GraphEvents.ON_MESSAGE_DELTA]: {\n",
        "-      handle: (event: GraphEvents.ON_MESSAGE_DELTA, data: t.StreamEventData): void => {\n",
        "+      handle: (\n",
        "+        event: GraphEvents.ON_MESSAGE_DELTA,\n",
        "+        data: t.StreamEventData\n",
        "+      ): void => {\n",
        "         console.log('====== ON_MESSAGE_DELTA ======');\n",
        "         console.dir(data, { depth: null });\n",
        "         aggregateContent({ event, data: data as t.MessageDeltaEvent });\n",
        "-      }\n",
        "+      },\n",
        "     },\n",
        "     [GraphEvents.TOOL_START]: {\n",
        "-      handle: (_event: string, data: t.StreamEventData, metadata?: Record<string, unknown>): void => {\n",
        "+      handle: (\n",
        "+        _event: string,\n",
        "+        data: t.StreamEventData,\n",
        "+        metadata?: Record<string, unknown>\n",
        "+      ): void => {\n",
        "         console.log('====== TOOL_START ======');\n",
        "         console.dir(data, { depth: null });\n",
        "-      }\n",
        "+      },\n",
        "     },\n",
        "   };\n",
        " \n",
        "   const llmConfig = getLLMConfig(provider);\n",
        "-  const instructions = 'You are a friendly AI assistant with coding capabilities. Always address the user by their name.';\n",
        "+  const instructions =\n",
        "+    'You are a friendly AI assistant with coding capabilities. Always address the user by their name.';\n",
        "   const additional_instructions = `The user's name is ${userName} and they are located in ${location}.`;\n",
        " \n",
        "+  // const userMessage1 = `how much memory is this (its in bytes) in MB? 31192000`;\n",
        "+  // const userMessage1 = `can you show me a good use case for rscript by running some code`;\n",
        "+  const userMessage1 = `Run hello world in french and in english, using python. please run 2 parallel code executions.`;\n",
        "+  const humanMessage = new HumanMessage(userMessage1);\n",
        "+  const tokenCounter = await createTokenCounter();\n",
        "+  const indexTokenCountMap = {\n",
        "+    0: tokenCounter(humanMessage),\n",
        "+  };\n",
        "+\n",
        "   const runConfig: t.RunConfig = {\n",
        "     runId: 'message-num-1',\n",
        "     graphConfig: {\n",
        "@@ -70,9 +99,12 @@ async function testCodeExecution(): Promise<void> {\n",
        "       tools: [new TavilySearchResults(), createCodeExecutionTool()],\n",
        "       instructions,\n",
        "       additional_instructions,\n",
        "+      maxContextTokens: 8000,\n",
        "     },\n",
        "     returnContent: true,\n",
        "     customHandlers,\n",
        "+    indexTokenCountMap,\n",
        "+    tokenCounter,\n",
        "   };\n",
        "   const run = await Run.create<t.IState>(runConfig);\n",
        " \n",
        "@@ -87,25 +119,12 @@ async function testCodeExecution(): Promise<void> {\n",
        " \n",
        "   console.log('Test 1: Simple Code Execution');\n",
        " \n",
        "-  // const userMessage1 = `how much memory is this (its in bytes) in MB? 31192000`;\n",
        "-  // const userMessage1 = `can you show me a good use case for rscript by running some code`;\n",
        "-  const userMessage1 = `Run hello world in french and in english, using python. please run 2 parallel code executions.`;\n",
        "-  const humanMessage = new HumanMessage(userMessage1);\n",
        "-  const tokenCounter = await createTokenCounter();\n",
        "-  const indexTokenCountMap = {\n",
        "-    0: tokenCounter(humanMessage),\n",
        "-  };\n",
        "-\n",
        "   conversationHistory.push(humanMessage);\n",
        " \n",
        "   let inputs = {\n",
        "     messages: conversationHistory,\n",
        "   };\n",
        "-  const finalContentParts1 = await run.processStream(inputs, config, {\n",
        "-    maxContextTokens: 8000,\n",
        "-    indexTokenCountMap,\n",
        "-    tokenCounter,\n",
        "-  });\n",
        "+  const finalContentParts1 = await run.processStream(inputs, config);\n",
        "   const finalMessages1 = run.getRunMessages();\n",
        "   if (finalMessages1) {\n",
        "     conversationHistory.push(...finalMessages1);\n",
        "@@ -126,4 +145,4 @@ testCodeExecution().catch((err) => {\n",
        "   console.log('Conversation history:');\n",
        "   console.dir(conversationHistory, { depth: null });\n",
        "   process.exit(1);\n",
        "-});\n",
        "\\ No newline at end of file\n",
        "+});\n"
      ]
    },
    {
      "path": "src/scripts/handoff-test.ts",
      "status": "added",
      "additions": 135,
      "deletions": 0,
      "patch": "@@ -0,0 +1,135 @@\n+import { config } from 'dotenv';\n+config();\n+\n+import { z } from 'zod';\n+import { tool } from '@langchain/core/tools';\n+import { ChatAnthropic } from '@langchain/anthropic';\n+import { createReactAgent } from '@langchain/langgraph/prebuilt';\n+import {\n+  StateGraph,\n+  MessagesAnnotation,\n+  Command,\n+  START,\n+  getCurrentTaskInput,\n+  END,\n+} from '@langchain/langgraph';\n+import { ToolMessage } from '@langchain/core/messages';\n+\n+interface CreateHandoffToolParams {\n+  agentName: string;\n+  description?: string;\n+}\n+\n+const createHandoffTool = ({\n+  agentName,\n+  description,\n+}: CreateHandoffToolParams) => {\n+  const toolName = `transfer_to_${agentName}`;\n+  const toolDescription = description || `Ask agent '${agentName}' for help`;\n+\n+  const handoffTool = tool(\n+    async (_, config) => {\n+      const toolMessage = new ToolMessage({\n+        content: `Successfully transferred to ${agentName}`,\n+        name: toolName,\n+        tool_call_id: config.toolCall.id,\n+      });\n+\n+      // inject the current agent state\n+      const state =\n+        getCurrentTaskInput() as (typeof MessagesAnnotation)['State'];\n+      return new Command({\n+        goto: agentName,\n+        update: { messages: state.messages.concat(toolMessage) },\n+        graph: Command.PARENT,\n+      });\n+    },\n+    {\n+      name: toolName,\n+      schema: z.object({}),\n+      description: toolDescription,\n+    }\n+  );\n+\n+  return handoffTool;\n+};\n+\n+const bookHotel = tool(\n+  async (input: { hotel_name: string }) => {\n+    return `Successfully booked a stay at ${input.hotel_name}.`;\n+  },\n+  {\n+    name: 'book_hotel',\n+    description: 'Book a hotel',\n+    schema: z.object({\n+      hotel_name: z.string().describe('The name of the hotel to book'),\n+    }),\n+  }\n+);\n+\n+const bookFlight = tool(\n+  async (input: { from_airport: string; to_airport: string }) => {\n+    return `Successfully booked a flight from ${input.from_airport} to ${input.to_airport}.`;\n+  },\n+  {\n+    name: 'book_flight',\n+    description: 'Book a flight',\n+    schema: z.object({\n+      from_airport: z.string().describe('The departure airport code'),\n+      to_airport: z.string().describe('The arrival airport code'),\n+    }),\n+  }\n+);\n+\n+const transferToHotelAssistant = createHandoffTool({\n+  agentName: 'hotel_assistant',\n+  description: 'Transfer user to the hotel-booking assistant.',\n+});\n+\n+const transferToFlightAssistant = createHandoffTool({\n+  agentName: 'flight_assistant',\n+  description: 'Transfer user to the flight-booking assistant.',\n+});\n+\n+const llm = new ChatAnthropic({\n+  modelName: 'claude-3-5-sonnet-latest',\n+  apiKey: process.env.ANTHROPIC_API_KEY,\n+});\n+\n+const flightAssistant = createReactAgent({\n+  llm,\n+  tools: [bookFlight, transferToHotelAssistant],\n+  prompt: 'You are a flight booking assistant',\n+  name: 'flight_assistant',\n+});\n+\n+const hotelAssistant = createReactAgent({\n+  llm,\n+  tools: [bookHotel, transferToFlightAssistant],\n+  prompt: 'You are a hotel booking assistant',\n+  name: 'hotel_assistant',\n+});\n+\n+const multiAgentGraph = new StateGraph(MessagesAnnotation)\n+  .addNode('flight_assistant', flightAssistant, {\n+    ends: ['hotel_assistant', END],\n+  })\n+  .addNode('hotel_assistant', hotelAssistant, {\n+    ends: ['flight_assistant', END],\n+  })\n+  .addEdge(START, 'flight_assistant')\n+  .compile();\n+\n+const stream = await multiAgentGraph.stream({\n+  messages: [\n+    {\n+      role: 'user',\n+      content: 'book a flight from BOS to JFK and a stay at McKittrick Hotel',\n+    },\n+  ],\n+});\n+\n+for await (const chunk of stream) {\n+  console.log(chunk);\n+  console.log('\\n');\n+}",
      "patch_lines": [
        "@@ -0,0 +1,135 @@\n",
        "+import { config } from 'dotenv';\n",
        "+config();\n",
        "+\n",
        "+import { z } from 'zod';\n",
        "+import { tool } from '@langchain/core/tools';\n",
        "+import { ChatAnthropic } from '@langchain/anthropic';\n",
        "+import { createReactAgent } from '@langchain/langgraph/prebuilt';\n",
        "+import {\n",
        "+  StateGraph,\n",
        "+  MessagesAnnotation,\n",
        "+  Command,\n",
        "+  START,\n",
        "+  getCurrentTaskInput,\n",
        "+  END,\n",
        "+} from '@langchain/langgraph';\n",
        "+import { ToolMessage } from '@langchain/core/messages';\n",
        "+\n",
        "+interface CreateHandoffToolParams {\n",
        "+  agentName: string;\n",
        "+  description?: string;\n",
        "+}\n",
        "+\n",
        "+const createHandoffTool = ({\n",
        "+  agentName,\n",
        "+  description,\n",
        "+}: CreateHandoffToolParams) => {\n",
        "+  const toolName = `transfer_to_${agentName}`;\n",
        "+  const toolDescription = description || `Ask agent '${agentName}' for help`;\n",
        "+\n",
        "+  const handoffTool = tool(\n",
        "+    async (_, config) => {\n",
        "+      const toolMessage = new ToolMessage({\n",
        "+        content: `Successfully transferred to ${agentName}`,\n",
        "+        name: toolName,\n",
        "+        tool_call_id: config.toolCall.id,\n",
        "+      });\n",
        "+\n",
        "+      // inject the current agent state\n",
        "+      const state =\n",
        "+        getCurrentTaskInput() as (typeof MessagesAnnotation)['State'];\n",
        "+      return new Command({\n",
        "+        goto: agentName,\n",
        "+        update: { messages: state.messages.concat(toolMessage) },\n",
        "+        graph: Command.PARENT,\n",
        "+      });\n",
        "+    },\n",
        "+    {\n",
        "+      name: toolName,\n",
        "+      schema: z.object({}),\n",
        "+      description: toolDescription,\n",
        "+    }\n",
        "+  );\n",
        "+\n",
        "+  return handoffTool;\n",
        "+};\n",
        "+\n",
        "+const bookHotel = tool(\n",
        "+  async (input: { hotel_name: string }) => {\n",
        "+    return `Successfully booked a stay at ${input.hotel_name}.`;\n",
        "+  },\n",
        "+  {\n",
        "+    name: 'book_hotel',\n",
        "+    description: 'Book a hotel',\n",
        "+    schema: z.object({\n",
        "+      hotel_name: z.string().describe('The name of the hotel to book'),\n",
        "+    }),\n",
        "+  }\n",
        "+);\n",
        "+\n",
        "+const bookFlight = tool(\n",
        "+  async (input: { from_airport: string; to_airport: string }) => {\n",
        "+    return `Successfully booked a flight from ${input.from_airport} to ${input.to_airport}.`;\n",
        "+  },\n",
        "+  {\n",
        "+    name: 'book_flight',\n",
        "+    description: 'Book a flight',\n",
        "+    schema: z.object({\n",
        "+      from_airport: z.string().describe('The departure airport code'),\n",
        "+      to_airport: z.string().describe('The arrival airport code'),\n",
        "+    }),\n",
        "+  }\n",
        "+);\n",
        "+\n",
        "+const transferToHotelAssistant = createHandoffTool({\n",
        "+  agentName: 'hotel_assistant',\n",
        "+  description: 'Transfer user to the hotel-booking assistant.',\n",
        "+});\n",
        "+\n",
        "+const transferToFlightAssistant = createHandoffTool({\n",
        "+  agentName: 'flight_assistant',\n",
        "+  description: 'Transfer user to the flight-booking assistant.',\n",
        "+});\n",
        "+\n",
        "+const llm = new ChatAnthropic({\n",
        "+  modelName: 'claude-3-5-sonnet-latest',\n",
        "+  apiKey: process.env.ANTHROPIC_API_KEY,\n",
        "+});\n",
        "+\n",
        "+const flightAssistant = createReactAgent({\n",
        "+  llm,\n",
        "+  tools: [bookFlight, transferToHotelAssistant],\n",
        "+  prompt: 'You are a flight booking assistant',\n",
        "+  name: 'flight_assistant',\n",
        "+});\n",
        "+\n",
        "+const hotelAssistant = createReactAgent({\n",
        "+  llm,\n",
        "+  tools: [bookHotel, transferToFlightAssistant],\n",
        "+  prompt: 'You are a hotel booking assistant',\n",
        "+  name: 'hotel_assistant',\n",
        "+});\n",
        "+\n",
        "+const multiAgentGraph = new StateGraph(MessagesAnnotation)\n",
        "+  .addNode('flight_assistant', flightAssistant, {\n",
        "+    ends: ['hotel_assistant', END],\n",
        "+  })\n",
        "+  .addNode('hotel_assistant', hotelAssistant, {\n",
        "+    ends: ['flight_assistant', END],\n",
        "+  })\n",
        "+  .addEdge(START, 'flight_assistant')\n",
        "+  .compile();\n",
        "+\n",
        "+const stream = await multiAgentGraph.stream({\n",
        "+  messages: [\n",
        "+    {\n",
        "+      role: 'user',\n",
        "+      content: 'book a flight from BOS to JFK and a stay at McKittrick Hotel',\n",
        "+    },\n",
        "+  ],\n",
        "+});\n",
        "+\n",
        "+for await (const chunk of stream) {\n",
        "+  console.log(chunk);\n",
        "+  console.log('\\n');\n",
        "+}\n"
      ]
    },
    {
      "path": "src/scripts/multi-agent-conditional.ts",
      "status": "added",
      "additions": 220,
      "deletions": 0,
      "patch": "@@ -0,0 +1,220 @@\n+import { config } from 'dotenv';\n+config();\n+\n+import { HumanMessage, BaseMessage } from '@langchain/core/messages';\n+import { Run } from '@/run';\n+import { Providers, GraphEvents } from '@/common';\n+import { ChatModelStreamHandler, createContentAggregator } from '@/stream';\n+import { ToolEndHandler, ModelEndHandler } from '@/events';\n+import type * as t from '@/types';\n+\n+const conversationHistory: BaseMessage[] = [];\n+\n+/**\n+ * Example of conditional multi-agent system\n+ *\n+ * Graph structure:\n+ * START -> classifier\n+ * classifier -> technical_expert (if technical question)\n+ * classifier -> business_expert (if business question)\n+ * classifier -> general_assistant (otherwise)\n+ * [all experts] -> END\n+ */\n+async function testConditionalMultiAgent() {\n+  console.log('Testing Conditional Multi-Agent System...\\n');\n+\n+  // Set up content aggregator\n+  const { contentParts, aggregateContent } = createContentAggregator();\n+\n+  // Define specialized agents\n+  const agents: t.AgentInputs[] = [\n+    {\n+      agentId: 'classifier',\n+      provider: Providers.ANTHROPIC,\n+      clientOptions: {\n+        modelName: 'claude-3-5-sonnet-latest',\n+        apiKey: process.env.ANTHROPIC_API_KEY,\n+      },\n+      instructions:\n+        'You are a query classifier. Analyze user questions and determine if they are technical, business-related, or general.',\n+      maxContextTokens: 8000,\n+    },\n+    {\n+      agentId: 'technical_expert',\n+      provider: Providers.ANTHROPIC,\n+      clientOptions: {\n+        modelName: 'claude-3-5-sonnet-latest',\n+        apiKey: process.env.ANTHROPIC_API_KEY,\n+      },\n+      instructions:\n+        'You are a technical expert. Provide detailed technical answers about programming, systems, and technology.',\n+      maxContextTokens: 8000,\n+    },\n+    {\n+      agentId: 'business_expert',\n+      provider: Providers.ANTHROPIC,\n+      clientOptions: {\n+        modelName: 'claude-3-5-sonnet-latest',\n+        apiKey: process.env.ANTHROPIC_API_KEY,\n+      },\n+      instructions:\n+        'You are a business expert. Provide insights on business strategy, operations, and management.',\n+      maxContextTokens: 8000,\n+    },\n+    {\n+      agentId: 'general_assistant',\n+      provider: Providers.ANTHROPIC,\n+      clientOptions: {\n+        modelName: 'claude-3-5-sonnet-latest',\n+        apiKey: process.env.ANTHROPIC_API_KEY,\n+      },\n+      instructions:\n+        'You are a helpful general assistant. Answer questions on a wide range of topics.',\n+      maxContextTokens: 8000,\n+    },\n+  ];\n+\n+  // Define conditional edges\n+  // These create handoff tools with conditional routing logic\n+  const edges: t.GraphEdge[] = [\n+    {\n+      from: 'classifier',\n+      to: ['technical_expert', 'business_expert', 'general_assistant'],\n+      description: 'Route to appropriate expert based on query type',\n+      condition: (state: t.BaseGraphState) => {\n+        // Simple keyword-based routing for demo\n+        // In a real system, this would use the classifier's analysis\n+        const lastMessage = state.messages[state.messages.length - 1];\n+        const content = lastMessage.content?.toString().toLowerCase() || '';\n+\n+        if (\n+          content.includes('code') ||\n+          content.includes('programming') ||\n+          content.includes('technical')\n+        ) {\n+          return 'technical_expert';\n+        } else if (\n+          content.includes('business') ||\n+          content.includes('strategy') ||\n+          content.includes('market')\n+        ) {\n+          return 'business_expert';\n+        } else {\n+          return 'general_assistant';\n+        }\n+      },\n+    },\n+  ];\n+\n+  // Track selected expert\n+  let selectedExpert = '';\n+\n+  // Create custom handlers\n+  const customHandlers = {\n+    [GraphEvents.TOOL_END]: new ToolEndHandler(),\n+    [GraphEvents.CHAT_MODEL_END]: new ModelEndHandler(),\n+    [GraphEvents.CHAT_MODEL_STREAM]: new ChatModelStreamHandler(),\n+    [GraphEvents.ON_RUN_STEP_COMPLETED]: {\n+      handle: (\n+        event: GraphEvents.ON_RUN_STEP_COMPLETED,\n+        data: t.StreamEventData\n+      ): void => {\n+        aggregateContent({\n+          event,\n+          data: data as unknown as { result: t.ToolEndEvent },\n+        });\n+      },\n+    },\n+    [GraphEvents.ON_RUN_STEP]: {\n+      handle: (\n+        event: GraphEvents.ON_RUN_STEP,\n+        data: t.StreamEventData\n+      ): void => {\n+        const runStepData = data as any;\n+        if (runStepData?.name && runStepData.name !== 'classifier') {\n+          selectedExpert = runStepData.name;\n+          console.log(`Routing to: ${selectedExpert}`);\n+        }\n+        aggregateContent({ event, data: data as t.RunStep });\n+      },\n+    },\n+    [GraphEvents.ON_RUN_STEP_DELTA]: {\n+      handle: (\n+        event: GraphEvents.ON_RUN_STEP_DELTA,\n+        data: t.StreamEventData\n+      ): void => {\n+        aggregateContent({ event, data: data as t.RunStepDeltaEvent });\n+      },\n+    },\n+    [GraphEvents.ON_MESSAGE_DELTA]: {\n+      handle: (\n+        event: GraphEvents.ON_MESSAGE_DELTA,\n+        data: t.StreamEventData\n+      ): void => {\n+        aggregateContent({ event, data: data as t.MessageDeltaEvent });\n+      },\n+    },\n+  };\n+\n+  // Create multi-agent run configuration\n+  const runConfig: t.RunConfig = {\n+    runId: `conditional-multi-agent-${Date.now()}`,\n+    graphConfig: {\n+      type: 'multi-agent',\n+      agents,\n+      edges,\n+    },\n+    customHandlers,\n+    returnContent: true,\n+  };\n+\n+  try {\n+    // Create and execute the run\n+    const run = await Run.create(runConfig);\n+\n+    // Test with different types of questions\n+    const testQuestions = [\n+      'How do I implement a binary search tree in Python?',\n+      'What are the key strategies for market expansion?',\n+      'What is the capital of France?',\n+    ];\n+\n+    const config = {\n+      configurable: {\n+        thread_id: 'conditional-conversation-1',\n+      },\n+      streamMode: 'values',\n+      version: 'v2' as const,\n+    };\n+\n+    for (const question of testQuestions) {\n+      console.log(`\\n--- Processing: \"${question}\" ---\\n`);\n+\n+      // Reset for each question\n+      selectedExpert = '';\n+      conversationHistory.length = 0;\n+      conversationHistory.push(new HumanMessage(question));\n+\n+      // Process with streaming\n+      const inputs = {\n+        messages: conversationHistory,\n+      };\n+\n+      const finalContentParts = await run.processStream(inputs, config);\n+      const finalMessages = run.getRunMessages();\n+\n+      if (finalMessages) {\n+        conversationHistory.push(...finalMessages);\n+      }\n+\n+      console.log(`\\n\\nExpert used: ${selectedExpert}`);\n+      console.log('Content parts:', contentParts.length);\n+      console.log('---');\n+    }\n+  } catch (error) {\n+    console.error('Error in conditional multi-agent test:', error);\n+  }\n+}\n+\n+// Run the test\n+testConditionalMultiAgent();",
      "patch_lines": [
        "@@ -0,0 +1,220 @@\n",
        "+import { config } from 'dotenv';\n",
        "+config();\n",
        "+\n",
        "+import { HumanMessage, BaseMessage } from '@langchain/core/messages';\n",
        "+import { Run } from '@/run';\n",
        "+import { Providers, GraphEvents } from '@/common';\n",
        "+import { ChatModelStreamHandler, createContentAggregator } from '@/stream';\n",
        "+import { ToolEndHandler, ModelEndHandler } from '@/events';\n",
        "+import type * as t from '@/types';\n",
        "+\n",
        "+const conversationHistory: BaseMessage[] = [];\n",
        "+\n",
        "+/**\n",
        "+ * Example of conditional multi-agent system\n",
        "+ *\n",
        "+ * Graph structure:\n",
        "+ * START -> classifier\n",
        "+ * classifier -> technical_expert (if technical question)\n",
        "+ * classifier -> business_expert (if business question)\n",
        "+ * classifier -> general_assistant (otherwise)\n",
        "+ * [all experts] -> END\n",
        "+ */\n",
        "+async function testConditionalMultiAgent() {\n",
        "+  console.log('Testing Conditional Multi-Agent System...\\n');\n",
        "+\n",
        "+  // Set up content aggregator\n",
        "+  const { contentParts, aggregateContent } = createContentAggregator();\n",
        "+\n",
        "+  // Define specialized agents\n",
        "+  const agents: t.AgentInputs[] = [\n",
        "+    {\n",
        "+      agentId: 'classifier',\n",
        "+      provider: Providers.ANTHROPIC,\n",
        "+      clientOptions: {\n",
        "+        modelName: 'claude-3-5-sonnet-latest',\n",
        "+        apiKey: process.env.ANTHROPIC_API_KEY,\n",
        "+      },\n",
        "+      instructions:\n",
        "+        'You are a query classifier. Analyze user questions and determine if they are technical, business-related, or general.',\n",
        "+      maxContextTokens: 8000,\n",
        "+    },\n",
        "+    {\n",
        "+      agentId: 'technical_expert',\n",
        "+      provider: Providers.ANTHROPIC,\n",
        "+      clientOptions: {\n",
        "+        modelName: 'claude-3-5-sonnet-latest',\n",
        "+        apiKey: process.env.ANTHROPIC_API_KEY,\n",
        "+      },\n",
        "+      instructions:\n",
        "+        'You are a technical expert. Provide detailed technical answers about programming, systems, and technology.',\n",
        "+      maxContextTokens: 8000,\n",
        "+    },\n",
        "+    {\n",
        "+      agentId: 'business_expert',\n",
        "+      provider: Providers.ANTHROPIC,\n",
        "+      clientOptions: {\n",
        "+        modelName: 'claude-3-5-sonnet-latest',\n",
        "+        apiKey: process.env.ANTHROPIC_API_KEY,\n",
        "+      },\n",
        "+      instructions:\n",
        "+        'You are a business expert. Provide insights on business strategy, operations, and management.',\n",
        "+      maxContextTokens: 8000,\n",
        "+    },\n",
        "+    {\n",
        "+      agentId: 'general_assistant',\n",
        "+      provider: Providers.ANTHROPIC,\n",
        "+      clientOptions: {\n",
        "+        modelName: 'claude-3-5-sonnet-latest',\n",
        "+        apiKey: process.env.ANTHROPIC_API_KEY,\n",
        "+      },\n",
        "+      instructions:\n",
        "+        'You are a helpful general assistant. Answer questions on a wide range of topics.',\n",
        "+      maxContextTokens: 8000,\n",
        "+    },\n",
        "+  ];\n",
        "+\n",
        "+  // Define conditional edges\n",
        "+  // These create handoff tools with conditional routing logic\n",
        "+  const edges: t.GraphEdge[] = [\n",
        "+    {\n",
        "+      from: 'classifier',\n",
        "+      to: ['technical_expert', 'business_expert', 'general_assistant'],\n",
        "+      description: 'Route to appropriate expert based on query type',\n",
        "+      condition: (state: t.BaseGraphState) => {\n",
        "+        // Simple keyword-based routing for demo\n",
        "+        // In a real system, this would use the classifier's analysis\n",
        "+        const lastMessage = state.messages[state.messages.length - 1];\n",
        "+        const content = lastMessage.content?.toString().toLowerCase() || '';\n",
        "+\n",
        "+        if (\n",
        "+          content.includes('code') ||\n",
        "+          content.includes('programming') ||\n",
        "+          content.includes('technical')\n",
        "+        ) {\n",
        "+          return 'technical_expert';\n",
        "+        } else if (\n",
        "+          content.includes('business') ||\n",
        "+          content.includes('strategy') ||\n",
        "+          content.includes('market')\n",
        "+        ) {\n",
        "+          return 'business_expert';\n",
        "+        } else {\n",
        "+          return 'general_assistant';\n",
        "+        }\n",
        "+      },\n",
        "+    },\n",
        "+  ];\n",
        "+\n",
        "+  // Track selected expert\n",
        "+  let selectedExpert = '';\n",
        "+\n",
        "+  // Create custom handlers\n",
        "+  const customHandlers = {\n",
        "+    [GraphEvents.TOOL_END]: new ToolEndHandler(),\n",
        "+    [GraphEvents.CHAT_MODEL_END]: new ModelEndHandler(),\n",
        "+    [GraphEvents.CHAT_MODEL_STREAM]: new ChatModelStreamHandler(),\n",
        "+    [GraphEvents.ON_RUN_STEP_COMPLETED]: {\n",
        "+      handle: (\n",
        "+        event: GraphEvents.ON_RUN_STEP_COMPLETED,\n",
        "+        data: t.StreamEventData\n",
        "+      ): void => {\n",
        "+        aggregateContent({\n",
        "+          event,\n",
        "+          data: data as unknown as { result: t.ToolEndEvent },\n",
        "+        });\n",
        "+      },\n",
        "+    },\n",
        "+    [GraphEvents.ON_RUN_STEP]: {\n",
        "+      handle: (\n",
        "+        event: GraphEvents.ON_RUN_STEP,\n",
        "+        data: t.StreamEventData\n",
        "+      ): void => {\n",
        "+        const runStepData = data as any;\n",
        "+        if (runStepData?.name && runStepData.name !== 'classifier') {\n",
        "+          selectedExpert = runStepData.name;\n",
        "+          console.log(`Routing to: ${selectedExpert}`);\n",
        "+        }\n",
        "+        aggregateContent({ event, data: data as t.RunStep });\n",
        "+      },\n",
        "+    },\n",
        "+    [GraphEvents.ON_RUN_STEP_DELTA]: {\n",
        "+      handle: (\n",
        "+        event: GraphEvents.ON_RUN_STEP_DELTA,\n",
        "+        data: t.StreamEventData\n",
        "+      ): void => {\n",
        "+        aggregateContent({ event, data: data as t.RunStepDeltaEvent });\n",
        "+      },\n",
        "+    },\n",
        "+    [GraphEvents.ON_MESSAGE_DELTA]: {\n",
        "+      handle: (\n",
        "+        event: GraphEvents.ON_MESSAGE_DELTA,\n",
        "+        data: t.StreamEventData\n",
        "+      ): void => {\n",
        "+        aggregateContent({ event, data: data as t.MessageDeltaEvent });\n",
        "+      },\n",
        "+    },\n",
        "+  };\n",
        "+\n",
        "+  // Create multi-agent run configuration\n",
        "+  const runConfig: t.RunConfig = {\n",
        "+    runId: `conditional-multi-agent-${Date.now()}`,\n",
        "+    graphConfig: {\n",
        "+      type: 'multi-agent',\n",
        "+      agents,\n",
        "+      edges,\n",
        "+    },\n",
        "+    customHandlers,\n",
        "+    returnContent: true,\n",
        "+  };\n",
        "+\n",
        "+  try {\n",
        "+    // Create and execute the run\n",
        "+    const run = await Run.create(runConfig);\n",
        "+\n",
        "+    // Test with different types of questions\n",
        "+    const testQuestions = [\n",
        "+      'How do I implement a binary search tree in Python?',\n",
        "+      'What are the key strategies for market expansion?',\n",
        "+      'What is the capital of France?',\n",
        "+    ];\n",
        "+\n",
        "+    const config = {\n",
        "+      configurable: {\n",
        "+        thread_id: 'conditional-conversation-1',\n",
        "+      },\n",
        "+      streamMode: 'values',\n",
        "+      version: 'v2' as const,\n",
        "+    };\n",
        "+\n",
        "+    for (const question of testQuestions) {\n",
        "+      console.log(`\\n--- Processing: \"${question}\" ---\\n`);\n",
        "+\n",
        "+      // Reset for each question\n",
        "+      selectedExpert = '';\n",
        "+      conversationHistory.length = 0;\n",
        "+      conversationHistory.push(new HumanMessage(question));\n",
        "+\n",
        "+      // Process with streaming\n",
        "+      const inputs = {\n",
        "+        messages: conversationHistory,\n",
        "+      };\n",
        "+\n",
        "+      const finalContentParts = await run.processStream(inputs, config);\n",
        "+      const finalMessages = run.getRunMessages();\n",
        "+\n",
        "+      if (finalMessages) {\n",
        "+        conversationHistory.push(...finalMessages);\n",
        "+      }\n",
        "+\n",
        "+      console.log(`\\n\\nExpert used: ${selectedExpert}`);\n",
        "+      console.log('Content parts:', contentParts.length);\n",
        "+      console.log('---');\n",
        "+    }\n",
        "+  } catch (error) {\n",
        "+    console.error('Error in conditional multi-agent test:', error);\n",
        "+  }\n",
        "+}\n",
        "+\n",
        "+// Run the test\n",
        "+testConditionalMultiAgent();\n"
      ]
    },
    {
      "path": "src/scripts/multi-agent-example-output.md",
      "status": "added",
      "additions": 110,
      "deletions": 0,
      "patch": "@@ -0,0 +1,110 @@\n+# Multi-Agent Test Scripts - Example Output\n+\n+## multi-agent-test.ts\n+\n+```\n+Testing Multi-Agent Handoff System...\n+\n+Invoking multi-agent graph...\n+\n+====== ON_RUN_STEP ======\n+{ name: 'flight_assistant', type: 'agent', ... }\n+[flight_assistant] Starting...\n+\n+====== CHAT_MODEL_STREAM ======\n+I'll help you book a flight from Boston to New York...\n+\n+====== TOOL_START ======\n+{ name: 'transfer_to_hotel_assistant', ... }\n+\n+====== ON_RUN_STEP_COMPLETED ======\n+[flight_assistant] Completed\n+\n+====== ON_RUN_STEP ======\n+{ name: 'hotel_assistant', type: 'agent', ... }\n+[hotel_assistant] Starting...\n+\n+====== CHAT_MODEL_STREAM ======\n+Great! I'll help you find a hotel near Times Square...\n+\n+Final content parts:\n+[\n+  { type: 'text', text: \"I'll help you book a flight...\" },\n+  { type: 'tool_use', name: 'transfer_to_hotel_assistant', ... },\n+  { type: 'text', text: \"Great! I'll help you find a hotel...\" }\n+]\n+```\n+\n+## multi-agent-parallel.ts\n+\n+```\n+Testing Parallel Multi-Agent System (Fan-in/Fan-out)...\n+\n+Invoking parallel multi-agent graph...\n+\n+====== ON_RUN_STEP ======\n+[researcher] Starting analysis...\n+\n+====== ON_RUN_STEP ======\n+[analyst1] Starting analysis...\n+[analyst2] Starting analysis...\n+[analyst3] Starting analysis...\n+\n+====== CHAT_MODEL_STREAM ======\n+[analyst1] From a financial perspective...\n+[analyst2] From a technical perspective...\n+[analyst3] From a market perspective...\n+\n+====== ON_RUN_STEP_COMPLETED ======\n+[analyst1] Completed analysis\n+[analyst2] Completed analysis\n+[analyst3] Completed analysis\n+\n+====== ON_RUN_STEP ======\n+[summarizer] Starting analysis...\n+\n+Final content parts: 5 parts\n+```\n+\n+## multi-agent-conditional.ts\n+\n+```\n+Testing Conditional Multi-Agent System...\n+\n+--- Processing: \"How do I implement a binary search tree in Python?\" ---\n+\n+====== ON_RUN_STEP ======\n+{ name: 'classifier', type: 'agent', ... }\n+\n+====== ON_RUN_STEP ======\n+{ name: 'technical_expert', type: 'agent', ... }\n+Routing to: technical_expert\n+\n+====== CHAT_MODEL_STREAM ======\n+To implement a binary search tree in Python, you'll need to create a Node class...\n+\n+Expert used: technical_expert\n+Content parts: 2\n+---\n+\n+--- Processing: \"What are the key strategies for market expansion?\" ---\n+\n+Routing to: business_expert\n+...\n+\n+--- Processing: \"What is the capital of France?\" ---\n+\n+Routing to: general_assistant\n+...\n+```\n+\n+## Key Features of Updated Scripts\n+\n+1. **Proper Event Handling**: All GraphEvents are properly handled with custom handlers\n+2. **Content Aggregation**: Using `createContentAggregator()` to collect all content parts\n+3. **Stream Output**: Real-time token streaming via `ChatModelStreamHandler`\n+4. **Debug Logging**: Comprehensive event logging for debugging multi-agent flows\n+5. **Conversation History**: Proper tracking of messages across agent interactions\n+6. **Return Content**: `returnContent: true` ensures content parts are returned\n+\n+The scripts now follow the same patterns as `simple.ts` and `tools.ts`, providing consistent behavior and comprehensive event handling across all multi-agent scenarios.",
      "patch_lines": [
        "@@ -0,0 +1,110 @@\n",
        "+# Multi-Agent Test Scripts - Example Output\n",
        "+\n",
        "+## multi-agent-test.ts\n",
        "+\n",
        "+```\n",
        "+Testing Multi-Agent Handoff System...\n",
        "+\n",
        "+Invoking multi-agent graph...\n",
        "+\n",
        "+====== ON_RUN_STEP ======\n",
        "+{ name: 'flight_assistant', type: 'agent', ... }\n",
        "+[flight_assistant] Starting...\n",
        "+\n",
        "+====== CHAT_MODEL_STREAM ======\n",
        "+I'll help you book a flight from Boston to New York...\n",
        "+\n",
        "+====== TOOL_START ======\n",
        "+{ name: 'transfer_to_hotel_assistant', ... }\n",
        "+\n",
        "+====== ON_RUN_STEP_COMPLETED ======\n",
        "+[flight_assistant] Completed\n",
        "+\n",
        "+====== ON_RUN_STEP ======\n",
        "+{ name: 'hotel_assistant', type: 'agent', ... }\n",
        "+[hotel_assistant] Starting...\n",
        "+\n",
        "+====== CHAT_MODEL_STREAM ======\n",
        "+Great! I'll help you find a hotel near Times Square...\n",
        "+\n",
        "+Final content parts:\n",
        "+[\n",
        "+  { type: 'text', text: \"I'll help you book a flight...\" },\n",
        "+  { type: 'tool_use', name: 'transfer_to_hotel_assistant', ... },\n",
        "+  { type: 'text', text: \"Great! I'll help you find a hotel...\" }\n",
        "+]\n",
        "+```\n",
        "+\n",
        "+## multi-agent-parallel.ts\n",
        "+\n",
        "+```\n",
        "+Testing Parallel Multi-Agent System (Fan-in/Fan-out)...\n",
        "+\n",
        "+Invoking parallel multi-agent graph...\n",
        "+\n",
        "+====== ON_RUN_STEP ======\n",
        "+[researcher] Starting analysis...\n",
        "+\n",
        "+====== ON_RUN_STEP ======\n",
        "+[analyst1] Starting analysis...\n",
        "+[analyst2] Starting analysis...\n",
        "+[analyst3] Starting analysis...\n",
        "+\n",
        "+====== CHAT_MODEL_STREAM ======\n",
        "+[analyst1] From a financial perspective...\n",
        "+[analyst2] From a technical perspective...\n",
        "+[analyst3] From a market perspective...\n",
        "+\n",
        "+====== ON_RUN_STEP_COMPLETED ======\n",
        "+[analyst1] Completed analysis\n",
        "+[analyst2] Completed analysis\n",
        "+[analyst3] Completed analysis\n",
        "+\n",
        "+====== ON_RUN_STEP ======\n",
        "+[summarizer] Starting analysis...\n",
        "+\n",
        "+Final content parts: 5 parts\n",
        "+```\n",
        "+\n",
        "+## multi-agent-conditional.ts\n",
        "+\n",
        "+```\n",
        "+Testing Conditional Multi-Agent System...\n",
        "+\n",
        "+--- Processing: \"How do I implement a binary search tree in Python?\" ---\n",
        "+\n",
        "+====== ON_RUN_STEP ======\n",
        "+{ name: 'classifier', type: 'agent', ... }\n",
        "+\n",
        "+====== ON_RUN_STEP ======\n",
        "+{ name: 'technical_expert', type: 'agent', ... }\n",
        "+Routing to: technical_expert\n",
        "+\n",
        "+====== CHAT_MODEL_STREAM ======\n",
        "+To implement a binary search tree in Python, you'll need to create a Node class...\n",
        "+\n",
        "+Expert used: technical_expert\n",
        "+Content parts: 2\n",
        "+---\n",
        "+\n",
        "+--- Processing: \"What are the key strategies for market expansion?\" ---\n",
        "+\n",
        "+Routing to: business_expert\n",
        "+...\n",
        "+\n",
        "+--- Processing: \"What is the capital of France?\" ---\n",
        "+\n",
        "+Routing to: general_assistant\n",
        "+...\n",
        "+```\n",
        "+\n",
        "+## Key Features of Updated Scripts\n",
        "+\n",
        "+1. **Proper Event Handling**: All GraphEvents are properly handled with custom handlers\n",
        "+2. **Content Aggregation**: Using `createContentAggregator()` to collect all content parts\n",
        "+3. **Stream Output**: Real-time token streaming via `ChatModelStreamHandler`\n",
        "+4. **Debug Logging**: Comprehensive event logging for debugging multi-agent flows\n",
        "+5. **Conversation History**: Proper tracking of messages across agent interactions\n",
        "+6. **Return Content**: `returnContent: true` ensures content parts are returned\n",
        "+\n",
        "+The scripts now follow the same patterns as `simple.ts` and `tools.ts`, providing consistent behavior and comprehensive event handling across all multi-agent scenarios.\n"
      ]
    },
    {
      "path": "src/scripts/multi-agent-parallel.ts",
      "status": "added",
      "additions": 337,
      "deletions": 0,
      "patch": "@@ -0,0 +1,337 @@\n+import { config } from 'dotenv';\n+config();\n+\n+import { HumanMessage, BaseMessage } from '@langchain/core/messages';\n+import { Run } from '@/run';\n+import { Providers, GraphEvents } from '@/common';\n+import { ChatModelStreamHandler, createContentAggregator } from '@/stream';\n+import { ToolEndHandler, ModelEndHandler } from '@/events';\n+import type * as t from '@/types';\n+\n+const conversationHistory: BaseMessage[] = [];\n+\n+/**\n+ * Example of parallel multi-agent system with fan-in/fan-out pattern\n+ *\n+ * Graph structure:\n+ * START -> researcher\n+ * researcher -> [analyst1, analyst2, analyst3] (fan-out)\n+ * [analyst1, analyst2, analyst3] -> summarizer (fan-in)\n+ * summarizer -> END\n+ */\n+async function testParallelMultiAgent() {\n+  console.log('Testing Parallel Multi-Agent System (Fan-in/Fan-out)...\\n');\n+\n+  // Note: You may see \"Run ID not found in run map\" errors during parallel execution.\n+  // This is a known issue with LangGraph's event streaming when nodes run in parallel.\n+  // The errors can be safely ignored - the parallel execution still works correctly.\n+\n+  // Set up content aggregator\n+  const { contentParts, aggregateContent } = createContentAggregator();\n+\n+  // Define specialized agents\n+  const agents: t.AgentInputs[] = [\n+    {\n+      agentId: 'researcher',\n+      provider: Providers.ANTHROPIC,\n+      clientOptions: {\n+        modelName: 'claude-3-5-sonnet-latest',\n+        apiKey: process.env.ANTHROPIC_API_KEY,\n+      },\n+      instructions: `You are a research coordinator in a multi-agent analysis workflow. Your sole job is to:\n+\n+  1. Analyze the incoming request and break it down into exactly 3 distinct research areas\n+  2. Create specific, actionable analysis tasks for each specialist team\n+  3. Format your output as clear directives (not analysis yourself)\n+  4. Keep your response under 200 words - you are NOT doing the analysis, just coordinating it\n+\n+  Example format:\n+  \"ANALYSIS COORDINATION:\n+  Financial Team: [specific financial analysis task]\n+  Technical Team: [specific technical analysis task] \n+  Market Team: [specific market analysis task]\n+\n+  Proceed with parallel analysis.\"\n+\n+  Do NOT provide any actual analysis - your job is purely coordination and task assignment.`,\n+    },\n+    {\n+      agentId: 'analyst1',\n+      provider: Providers.ANTHROPIC,\n+      clientOptions: {\n+        modelName: 'claude-3-5-sonnet-latest',\n+        apiKey: process.env.ANTHROPIC_API_KEY,\n+      },\n+      instructions: `You are a FINANCIAL AND ECONOMIC ANALYST in a parallel analysis workflow. \n+\n+  CRITICAL: You must provide detailed, substantive financial analysis (minimum 300 words). Focus specifically on:\n+\n+  \u2022 Economic impact metrics (GDP, productivity, employment effects)\n+  \u2022 Monetary policy implications and inflation considerations  \n+  \u2022 Investment flows and capital market disruptions\n+  \u2022 Financial sector transformation and banking impacts\n+  \u2022 Government fiscal policy and tax revenue effects\n+  \u2022 International trade and currency implications\n+  \u2022 Risk assessment and financial stability concerns\n+\n+  Provide concrete data, projections, and financial reasoning. Never give empty responses, brief acknowledgments, or defer to other analysts. This is your domain expertise - deliver comprehensive financial analysis.\n+\n+  Start your response with \"FINANCIAL ANALYSIS:\" and provide detailed insights.`,\n+    },\n+    {\n+      agentId: 'analyst2',\n+      provider: Providers.ANTHROPIC,\n+      clientOptions: {\n+        modelName: 'claude-3-5-sonnet-latest',\n+        apiKey: process.env.ANTHROPIC_API_KEY,\n+      },\n+      instructions: `You are a TECHNICAL AND IMPLEMENTATION ANALYST in a parallel analysis workflow.\n+\n+  CRITICAL: You must provide detailed, substantive technical analysis (minimum 300 words). Focus specifically on:\n+\n+  \u2022 AI infrastructure requirements and scalability challenges\n+  \u2022 Computing power, data center, and energy demands\n+  \u2022 Network connectivity and bandwidth requirements\n+  \u2022 Technical barriers to widespread AI adoption\n+  \u2022 Implementation timelines and deployment phases\n+  \u2022 Skills gap analysis and workforce technical requirements\n+  \u2022 Security vulnerabilities and technical risk mitigation\n+  \u2022 Integration challenges with existing systems\n+\n+  Provide concrete technical assessments, infrastructure projections, and implementation roadmaps. Never give empty responses, brief acknowledgments, or defer to other analysts. This is your technical expertise domain.\n+\n+  Start your response with \"TECHNICAL ANALYSIS:\" and provide detailed technical insights.`,\n+    },\n+    {\n+      agentId: 'analyst3',\n+      provider: Providers.ANTHROPIC,\n+      clientOptions: {\n+        modelName: 'claude-3-5-sonnet-latest',\n+        apiKey: process.env.ANTHROPIC_API_KEY,\n+      },\n+      instructions: `You are a MARKET AND INDUSTRY ANALYST in a parallel analysis workflow.\n+\n+  CRITICAL: You must provide detailed, substantive market analysis (minimum 300 words). Focus specifically on:\n+\n+  \u2022 Industry disruption patterns and transformation timelines\n+  \u2022 Competitive landscape shifts and market consolidation\n+  \u2022 Consumer adoption rates and behavior changes\n+  \u2022 New business models and market opportunities\n+  \u2022 Geographic market variations and regional impacts\n+  \u2022 Regulatory environment and policy implications\n+  \u2022 Market size projections and growth trajectories\n+  \u2022 Sector-specific impacts (healthcare, finance, manufacturing, etc.)\n+\n+  Provide concrete market assessments, competitive analysis, and industry projections. Never give empty responses, brief acknowledgments, or defer to other analysts. This is your market expertise domain.\n+\n+  Start your response with \"MARKET ANALYSIS:\" and provide detailed market insights.`,\n+    },\n+    {\n+      agentId: 'summarizer',\n+      provider: Providers.ANTHROPIC,\n+      clientOptions: {\n+        modelName: 'claude-3-5-sonnet-latest',\n+        apiKey: process.env.ANTHROPIC_API_KEY,\n+      },\n+      instructions: `You are the SYNTHESIS AND SUMMARY EXPERT in a multi-agent workflow.\n+\n+  Your job is to:\n+  1. Receive and analyze the outputs from all three specialist analysts\n+  2. Identify key themes, conflicts, and complementary insights across analyses\n+  3. Synthesize findings into a coherent, comprehensive executive summary\n+  4. Highlight critical interdependencies between financial, technical, and market factors\n+  5. Provide integrated conclusions and strategic recommendations\n+\n+  Structure your summary as:\n+  - EXECUTIVE SUMMARY (key findings)\n+  - INTEGRATED INSIGHTS (how the three analyses connect)\n+  - CRITICAL INTERDEPENDENCIES (financial-technical-market intersections)\n+  - STRATEGIC RECOMMENDATIONS (actionable next steps)\n+  - RISK ASSESSMENT (combined risk factors from all domains)\n+\n+  Minimum 400 words. Create value through integration, not just concatenation of the three analyses.`,\n+    },\n+  ];\n+\n+  // Define direct edges (fan-out and fan-in)\n+  const edges: t.GraphEdge[] = [\n+    {\n+      from: 'researcher',\n+      to: ['analyst1', 'analyst2', 'analyst3'], // Fan-out to multiple analysts\n+      description: 'Distribute research to specialist analysts',\n+      edgeType: 'direct', // Explicitly set as direct for automatic transition (enables parallel execution)\n+    },\n+    {\n+      from: ['analyst1', 'analyst2', 'analyst3'], // Fan-in from multiple sources\n+      to: 'summarizer',\n+      description: 'Aggregate analysis results',\n+      edgeType: 'direct', // Fan-in is also direct\n+      // Add prompt when all analysts have provided input\n+      promptInstructions: (messages) => {\n+        // Check if we have analysis content from all three analysts\n+        // Look for the specific headers each analyst uses\n+        const aiMessages = messages.filter((msg) => msg._getType() === 'ai');\n+        const messageContent = aiMessages.map((msg) => msg.content).join('\\n');\n+\n+        const hasFinancialAnalysis = messageContent.includes(\n+          'FINANCIAL ANALYSIS:'\n+        );\n+        const hasTechnicalAnalysis = messageContent.includes(\n+          'TECHNICAL ANALYSIS:'\n+        );\n+        const hasMarketAnalysis = messageContent.includes('MARKET ANALYSIS:');\n+\n+        console.log(\n+          `Checking for analyses - Financial: ${hasFinancialAnalysis}, Technical: ${hasTechnicalAnalysis}, Market: ${hasMarketAnalysis}`\n+        );\n+\n+        if (hasFinancialAnalysis && hasTechnicalAnalysis && hasMarketAnalysis) {\n+          return 'Based on the comprehensive analyses from all three specialist teams above, please synthesize their insights into a cohesive executive summary. Focus on the key findings, common themes, and strategic implications across the financial, technical, and market perspectives.';\n+        }\n+        return undefined; // No prompt if we haven't received all analyst inputs\n+      },\n+    },\n+  ];\n+\n+  // Track which agents are active\n+  const activeAgents = new Set<string>();\n+\n+  // Create custom handlers\n+  const customHandlers = {\n+    [GraphEvents.TOOL_END]: new ToolEndHandler(),\n+    [GraphEvents.CHAT_MODEL_END]: new ModelEndHandler(),\n+    [GraphEvents.CHAT_MODEL_STREAM]: new ChatModelStreamHandler(),\n+    [GraphEvents.ON_RUN_STEP_COMPLETED]: {\n+      handle: (\n+        event: GraphEvents.ON_RUN_STEP_COMPLETED,\n+        data: t.StreamEventData\n+      ): void => {\n+        console.log('====== ON_RUN_STEP_COMPLETED ======');\n+        const runStepData = data as any;\n+        if (runStepData?.name) {\n+          activeAgents.delete(runStepData.name);\n+          console.log(`[${runStepData.name}] Completed analysis`);\n+        }\n+        aggregateContent({\n+          event,\n+          data: data as unknown as { result: t.ToolEndEvent },\n+        });\n+      },\n+    },\n+    [GraphEvents.ON_RUN_STEP]: {\n+      handle: (\n+        event: GraphEvents.ON_RUN_STEP,\n+        data: t.StreamEventData\n+      ): void => {\n+        console.log('====== ON_RUN_STEP ======');\n+        const runStepData = data as any;\n+        if (runStepData?.name) {\n+          activeAgents.add(runStepData.name);\n+          console.log(`[${runStepData.name}] Starting analysis...`);\n+        }\n+        aggregateContent({ event, data: data as t.RunStep });\n+      },\n+    },\n+    [GraphEvents.ON_RUN_STEP_DELTA]: {\n+      handle: (\n+        event: GraphEvents.ON_RUN_STEP_DELTA,\n+        data: t.StreamEventData\n+      ): void => {\n+        aggregateContent({ event, data: data as t.RunStepDeltaEvent });\n+      },\n+    },\n+    [GraphEvents.ON_MESSAGE_DELTA]: {\n+      handle: (\n+        event: GraphEvents.ON_MESSAGE_DELTA,\n+        data: t.StreamEventData\n+      ): void => {\n+        console.log('====== ON_MESSAGE_DELTA ======');\n+        console.dir(data, { depth: null });\n+        aggregateContent({ event, data: data as t.MessageDeltaEvent });\n+      },\n+    },\n+  };\n+\n+  // Create multi-agent run configuration\n+  const runConfig: t.RunConfig = {\n+    runId: `parallel-multi-agent-${Date.now()}`,\n+    graphConfig: {\n+      type: 'multi-agent',\n+      agents,\n+      edges,\n+      // Add compile options to help with parallel execution\n+      compileOptions: {\n+        // checkpointer: new MemorySaver(), // Uncomment if needed\n+      },\n+    },\n+    customHandlers,\n+    returnContent: true,\n+  };\n+\n+  try {\n+    // Create and execute the run\n+    const run = await Run.create(runConfig);\n+\n+    // Debug: Log the graph structure\n+    console.log('=== DEBUG: Graph Structure ===');\n+    const graph = (run as any).Graph;\n+    console.log('Graph exists:', !!graph);\n+    if (graph) {\n+      console.log('Graph type:', graph.constructor.name);\n+      console.log('AgentContexts exists:', !!graph.agentContexts);\n+      if (graph.agentContexts) {\n+        console.log('AgentContexts size:', graph.agentContexts.size);\n+        for (const [agentId, context] of graph.agentContexts) {\n+          console.log(`\\nAgent: ${agentId}`);\n+          console.log(\n+            `Tools: ${context.tools?.map((t: any) => t.name || 'unnamed').join(', ') || 'none'}`\n+          );\n+        }\n+      }\n+    }\n+    console.log('=== END DEBUG ===\\n');\n+\n+    const userMessage = `EXECUTE PARALLEL ANALYSIS WORKFLOW:\n+\n+Step 1: Researcher - identify 3 key analysis areas for AI economic impact\n+Step 2: IMMEDIATELY send to ALL THREE analysts simultaneously:\n+  - analyst1 (financial): Provide detailed economic impact analysis\n+  - analyst2 (technical): Provide detailed technical implementation analysis  \n+  - analyst3 (market): Provide detailed market disruption analysis\n+Step 3: Summarizer - compile all three analyses\n+\n+IMPORTANT: Each analyst must produce substantive analysis (200+ words), not just acknowledgments or empty responses. This is a multi-agent workflow test requiring actual parallel execution.`;\n+    conversationHistory.push(new HumanMessage(userMessage));\n+\n+    console.log('Invoking parallel multi-agent graph...\\n');\n+\n+    const config = {\n+      configurable: {\n+        thread_id: 'parallel-conversation-1',\n+      },\n+      streamMode: 'values',\n+      version: 'v2' as const,\n+    };\n+\n+    // Process with streaming\n+    const inputs = {\n+      messages: conversationHistory,\n+    };\n+\n+    const finalContentParts = await run.processStream(inputs, config);\n+    const finalMessages = run.getRunMessages();\n+\n+    if (finalMessages) {\n+      conversationHistory.push(...finalMessages);\n+    }\n+\n+    console.log('\\n\\nActive agents during execution:', activeAgents.size);\n+    console.log('Final content parts:', contentParts.length, 'parts');\n+    console.dir(contentParts, { depth: null });\n+  } catch (error) {\n+    console.error('Error in parallel multi-agent test:', error);\n+  }\n+}\n+\n+// Run the test\n+testParallelMultiAgent();",
      "patch_lines": [
        "@@ -0,0 +1,337 @@\n",
        "+import { config } from 'dotenv';\n",
        "+config();\n",
        "+\n",
        "+import { HumanMessage, BaseMessage } from '@langchain/core/messages';\n",
        "+import { Run } from '@/run';\n",
        "+import { Providers, GraphEvents } from '@/common';\n",
        "+import { ChatModelStreamHandler, createContentAggregator } from '@/stream';\n",
        "+import { ToolEndHandler, ModelEndHandler } from '@/events';\n",
        "+import type * as t from '@/types';\n",
        "+\n",
        "+const conversationHistory: BaseMessage[] = [];\n",
        "+\n",
        "+/**\n",
        "+ * Example of parallel multi-agent system with fan-in/fan-out pattern\n",
        "+ *\n",
        "+ * Graph structure:\n",
        "+ * START -> researcher\n",
        "+ * researcher -> [analyst1, analyst2, analyst3] (fan-out)\n",
        "+ * [analyst1, analyst2, analyst3] -> summarizer (fan-in)\n",
        "+ * summarizer -> END\n",
        "+ */\n",
        "+async function testParallelMultiAgent() {\n",
        "+  console.log('Testing Parallel Multi-Agent System (Fan-in/Fan-out)...\\n');\n",
        "+\n",
        "+  // Note: You may see \"Run ID not found in run map\" errors during parallel execution.\n",
        "+  // This is a known issue with LangGraph's event streaming when nodes run in parallel.\n",
        "+  // The errors can be safely ignored - the parallel execution still works correctly.\n",
        "+\n",
        "+  // Set up content aggregator\n",
        "+  const { contentParts, aggregateContent } = createContentAggregator();\n",
        "+\n",
        "+  // Define specialized agents\n",
        "+  const agents: t.AgentInputs[] = [\n",
        "+    {\n",
        "+      agentId: 'researcher',\n",
        "+      provider: Providers.ANTHROPIC,\n",
        "+      clientOptions: {\n",
        "+        modelName: 'claude-3-5-sonnet-latest',\n",
        "+        apiKey: process.env.ANTHROPIC_API_KEY,\n",
        "+      },\n",
        "+      instructions: `You are a research coordinator in a multi-agent analysis workflow. Your sole job is to:\n",
        "+\n",
        "+  1. Analyze the incoming request and break it down into exactly 3 distinct research areas\n",
        "+  2. Create specific, actionable analysis tasks for each specialist team\n",
        "+  3. Format your output as clear directives (not analysis yourself)\n",
        "+  4. Keep your response under 200 words - you are NOT doing the analysis, just coordinating it\n",
        "+\n",
        "+  Example format:\n",
        "+  \"ANALYSIS COORDINATION:\n",
        "+  Financial Team: [specific financial analysis task]\n",
        "+  Technical Team: [specific technical analysis task] \n",
        "+  Market Team: [specific market analysis task]\n",
        "+\n",
        "+  Proceed with parallel analysis.\"\n",
        "+\n",
        "+  Do NOT provide any actual analysis - your job is purely coordination and task assignment.`,\n",
        "+    },\n",
        "+    {\n",
        "+      agentId: 'analyst1',\n",
        "+      provider: Providers.ANTHROPIC,\n",
        "+      clientOptions: {\n",
        "+        modelName: 'claude-3-5-sonnet-latest',\n",
        "+        apiKey: process.env.ANTHROPIC_API_KEY,\n",
        "+      },\n",
        "+      instructions: `You are a FINANCIAL AND ECONOMIC ANALYST in a parallel analysis workflow. \n",
        "+\n",
        "+  CRITICAL: You must provide detailed, substantive financial analysis (minimum 300 words). Focus specifically on:\n",
        "+\n",
        "+  \u2022 Economic impact metrics (GDP, productivity, employment effects)\n",
        "+  \u2022 Monetary policy implications and inflation considerations  \n",
        "+  \u2022 Investment flows and capital market disruptions\n",
        "+  \u2022 Financial sector transformation and banking impacts\n",
        "+  \u2022 Government fiscal policy and tax revenue effects\n",
        "+  \u2022 International trade and currency implications\n",
        "+  \u2022 Risk assessment and financial stability concerns\n",
        "+\n",
        "+  Provide concrete data, projections, and financial reasoning. Never give empty responses, brief acknowledgments, or defer to other analysts. This is your domain expertise - deliver comprehensive financial analysis.\n",
        "+\n",
        "+  Start your response with \"FINANCIAL ANALYSIS:\" and provide detailed insights.`,\n",
        "+    },\n",
        "+    {\n",
        "+      agentId: 'analyst2',\n",
        "+      provider: Providers.ANTHROPIC,\n",
        "+      clientOptions: {\n",
        "+        modelName: 'claude-3-5-sonnet-latest',\n",
        "+        apiKey: process.env.ANTHROPIC_API_KEY,\n",
        "+      },\n",
        "+      instructions: `You are a TECHNICAL AND IMPLEMENTATION ANALYST in a parallel analysis workflow.\n",
        "+\n",
        "+  CRITICAL: You must provide detailed, substantive technical analysis (minimum 300 words). Focus specifically on:\n",
        "+\n",
        "+  \u2022 AI infrastructure requirements and scalability challenges\n",
        "+  \u2022 Computing power, data center, and energy demands\n",
        "+  \u2022 Network connectivity and bandwidth requirements\n",
        "+  \u2022 Technical barriers to widespread AI adoption\n",
        "+  \u2022 Implementation timelines and deployment phases\n",
        "+  \u2022 Skills gap analysis and workforce technical requirements\n",
        "+  \u2022 Security vulnerabilities and technical risk mitigation\n",
        "+  \u2022 Integration challenges with existing systems\n",
        "+\n",
        "+  Provide concrete technical assessments, infrastructure projections, and implementation roadmaps. Never give empty responses, brief acknowledgments, or defer to other analysts. This is your technical expertise domain.\n",
        "+\n",
        "+  Start your response with \"TECHNICAL ANALYSIS:\" and provide detailed technical insights.`,\n",
        "+    },\n",
        "+    {\n",
        "+      agentId: 'analyst3',\n",
        "+      provider: Providers.ANTHROPIC,\n",
        "+      clientOptions: {\n",
        "+        modelName: 'claude-3-5-sonnet-latest',\n",
        "+        apiKey: process.env.ANTHROPIC_API_KEY,\n",
        "+      },\n",
        "+      instructions: `You are a MARKET AND INDUSTRY ANALYST in a parallel analysis workflow.\n",
        "+\n",
        "+  CRITICAL: You must provide detailed, substantive market analysis (minimum 300 words). Focus specifically on:\n",
        "+\n",
        "+  \u2022 Industry disruption patterns and transformation timelines\n",
        "+  \u2022 Competitive landscape shifts and market consolidation\n",
        "+  \u2022 Consumer adoption rates and behavior changes\n",
        "+  \u2022 New business models and market opportunities\n",
        "+  \u2022 Geographic market variations and regional impacts\n",
        "+  \u2022 Regulatory environment and policy implications\n",
        "+  \u2022 Market size projections and growth trajectories\n",
        "+  \u2022 Sector-specific impacts (healthcare, finance, manufacturing, etc.)\n",
        "+\n",
        "+  Provide concrete market assessments, competitive analysis, and industry projections. Never give empty responses, brief acknowledgments, or defer to other analysts. This is your market expertise domain.\n",
        "+\n",
        "+  Start your response with \"MARKET ANALYSIS:\" and provide detailed market insights.`,\n",
        "+    },\n",
        "+    {\n",
        "+      agentId: 'summarizer',\n",
        "+      provider: Providers.ANTHROPIC,\n",
        "+      clientOptions: {\n",
        "+        modelName: 'claude-3-5-sonnet-latest',\n",
        "+        apiKey: process.env.ANTHROPIC_API_KEY,\n",
        "+      },\n",
        "+      instructions: `You are the SYNTHESIS AND SUMMARY EXPERT in a multi-agent workflow.\n",
        "+\n",
        "+  Your job is to:\n",
        "+  1. Receive and analyze the outputs from all three specialist analysts\n",
        "+  2. Identify key themes, conflicts, and complementary insights across analyses\n",
        "+  3. Synthesize findings into a coherent, comprehensive executive summary\n",
        "+  4. Highlight critical interdependencies between financial, technical, and market factors\n",
        "+  5. Provide integrated conclusions and strategic recommendations\n",
        "+\n",
        "+  Structure your summary as:\n",
        "+  - EXECUTIVE SUMMARY (key findings)\n",
        "+  - INTEGRATED INSIGHTS (how the three analyses connect)\n",
        "+  - CRITICAL INTERDEPENDENCIES (financial-technical-market intersections)\n",
        "+  - STRATEGIC RECOMMENDATIONS (actionable next steps)\n",
        "+  - RISK ASSESSMENT (combined risk factors from all domains)\n",
        "+\n",
        "+  Minimum 400 words. Create value through integration, not just concatenation of the three analyses.`,\n",
        "+    },\n",
        "+  ];\n",
        "+\n",
        "+  // Define direct edges (fan-out and fan-in)\n",
        "+  const edges: t.GraphEdge[] = [\n",
        "+    {\n",
        "+      from: 'researcher',\n",
        "+      to: ['analyst1', 'analyst2', 'analyst3'], // Fan-out to multiple analysts\n",
        "+      description: 'Distribute research to specialist analysts',\n",
        "+      edgeType: 'direct', // Explicitly set as direct for automatic transition (enables parallel execution)\n",
        "+    },\n",
        "+    {\n",
        "+      from: ['analyst1', 'analyst2', 'analyst3'], // Fan-in from multiple sources\n",
        "+      to: 'summarizer',\n",
        "+      description: 'Aggregate analysis results',\n",
        "+      edgeType: 'direct', // Fan-in is also direct\n",
        "+      // Add prompt when all analysts have provided input\n",
        "+      promptInstructions: (messages) => {\n",
        "+        // Check if we have analysis content from all three analysts\n",
        "+        // Look for the specific headers each analyst uses\n",
        "+        const aiMessages = messages.filter((msg) => msg._getType() === 'ai');\n",
        "+        const messageContent = aiMessages.map((msg) => msg.content).join('\\n');\n",
        "+\n",
        "+        const hasFinancialAnalysis = messageContent.includes(\n",
        "+          'FINANCIAL ANALYSIS:'\n",
        "+        );\n",
        "+        const hasTechnicalAnalysis = messageContent.includes(\n",
        "+          'TECHNICAL ANALYSIS:'\n",
        "+        );\n",
        "+        const hasMarketAnalysis = messageContent.includes('MARKET ANALYSIS:');\n",
        "+\n",
        "+        console.log(\n",
        "+          `Checking for analyses - Financial: ${hasFinancialAnalysis}, Technical: ${hasTechnicalAnalysis}, Market: ${hasMarketAnalysis}`\n",
        "+        );\n",
        "+\n",
        "+        if (hasFinancialAnalysis && hasTechnicalAnalysis && hasMarketAnalysis) {\n",
        "+          return 'Based on the comprehensive analyses from all three specialist teams above, please synthesize their insights into a cohesive executive summary. Focus on the key findings, common themes, and strategic implications across the financial, technical, and market perspectives.';\n",
        "+        }\n",
        "+        return undefined; // No prompt if we haven't received all analyst inputs\n",
        "+      },\n",
        "+    },\n",
        "+  ];\n",
        "+\n",
        "+  // Track which agents are active\n",
        "+  const activeAgents = new Set<string>();\n",
        "+\n",
        "+  // Create custom handlers\n",
        "+  const customHandlers = {\n",
        "+    [GraphEvents.TOOL_END]: new ToolEndHandler(),\n",
        "+    [GraphEvents.CHAT_MODEL_END]: new ModelEndHandler(),\n",
        "+    [GraphEvents.CHAT_MODEL_STREAM]: new ChatModelStreamHandler(),\n",
        "+    [GraphEvents.ON_RUN_STEP_COMPLETED]: {\n",
        "+      handle: (\n",
        "+        event: GraphEvents.ON_RUN_STEP_COMPLETED,\n",
        "+        data: t.StreamEventData\n",
        "+      ): void => {\n",
        "+        console.log('====== ON_RUN_STEP_COMPLETED ======');\n",
        "+        const runStepData = data as any;\n",
        "+        if (runStepData?.name) {\n",
        "+          activeAgents.delete(runStepData.name);\n",
        "+          console.log(`[${runStepData.name}] Completed analysis`);\n",
        "+        }\n",
        "+        aggregateContent({\n",
        "+          event,\n",
        "+          data: data as unknown as { result: t.ToolEndEvent },\n",
        "+        });\n",
        "+      },\n",
        "+    },\n",
        "+    [GraphEvents.ON_RUN_STEP]: {\n",
        "+      handle: (\n",
        "+        event: GraphEvents.ON_RUN_STEP,\n",
        "+        data: t.StreamEventData\n",
        "+      ): void => {\n",
        "+        console.log('====== ON_RUN_STEP ======');\n",
        "+        const runStepData = data as any;\n",
        "+        if (runStepData?.name) {\n",
        "+          activeAgents.add(runStepData.name);\n",
        "+          console.log(`[${runStepData.name}] Starting analysis...`);\n",
        "+        }\n",
        "+        aggregateContent({ event, data: data as t.RunStep });\n",
        "+      },\n",
        "+    },\n",
        "+    [GraphEvents.ON_RUN_STEP_DELTA]: {\n",
        "+      handle: (\n",
        "+        event: GraphEvents.ON_RUN_STEP_DELTA,\n",
        "+        data: t.StreamEventData\n",
        "+      ): void => {\n",
        "+        aggregateContent({ event, data: data as t.RunStepDeltaEvent });\n",
        "+      },\n",
        "+    },\n",
        "+    [GraphEvents.ON_MESSAGE_DELTA]: {\n",
        "+      handle: (\n",
        "+        event: GraphEvents.ON_MESSAGE_DELTA,\n",
        "+        data: t.StreamEventData\n",
        "+      ): void => {\n",
        "+        console.log('====== ON_MESSAGE_DELTA ======');\n",
        "+        console.dir(data, { depth: null });\n",
        "+        aggregateContent({ event, data: data as t.MessageDeltaEvent });\n",
        "+      },\n",
        "+    },\n",
        "+  };\n",
        "+\n",
        "+  // Create multi-agent run configuration\n",
        "+  const runConfig: t.RunConfig = {\n",
        "+    runId: `parallel-multi-agent-${Date.now()}`,\n",
        "+    graphConfig: {\n",
        "+      type: 'multi-agent',\n",
        "+      agents,\n",
        "+      edges,\n",
        "+      // Add compile options to help with parallel execution\n",
        "+      compileOptions: {\n",
        "+        // checkpointer: new MemorySaver(), // Uncomment if needed\n",
        "+      },\n",
        "+    },\n",
        "+    customHandlers,\n",
        "+    returnContent: true,\n",
        "+  };\n",
        "+\n",
        "+  try {\n",
        "+    // Create and execute the run\n",
        "+    const run = await Run.create(runConfig);\n",
        "+\n",
        "+    // Debug: Log the graph structure\n",
        "+    console.log('=== DEBUG: Graph Structure ===');\n",
        "+    const graph = (run as any).Graph;\n",
        "+    console.log('Graph exists:', !!graph);\n",
        "+    if (graph) {\n",
        "+      console.log('Graph type:', graph.constructor.name);\n",
        "+      console.log('AgentContexts exists:', !!graph.agentContexts);\n",
        "+      if (graph.agentContexts) {\n",
        "+        console.log('AgentContexts size:', graph.agentContexts.size);\n",
        "+        for (const [agentId, context] of graph.agentContexts) {\n",
        "+          console.log(`\\nAgent: ${agentId}`);\n",
        "+          console.log(\n",
        "+            `Tools: ${context.tools?.map((t: any) => t.name || 'unnamed').join(', ') || 'none'}`\n",
        "+          );\n",
        "+        }\n",
        "+      }\n",
        "+    }\n",
        "+    console.log('=== END DEBUG ===\\n');\n",
        "+\n",
        "+    const userMessage = `EXECUTE PARALLEL ANALYSIS WORKFLOW:\n",
        "+\n",
        "+Step 1: Researcher - identify 3 key analysis areas for AI economic impact\n",
        "+Step 2: IMMEDIATELY send to ALL THREE analysts simultaneously:\n",
        "+  - analyst1 (financial): Provide detailed economic impact analysis\n",
        "+  - analyst2 (technical): Provide detailed technical implementation analysis  \n",
        "+  - analyst3 (market): Provide detailed market disruption analysis\n",
        "+Step 3: Summarizer - compile all three analyses\n",
        "+\n",
        "+IMPORTANT: Each analyst must produce substantive analysis (200+ words), not just acknowledgments or empty responses. This is a multi-agent workflow test requiring actual parallel execution.`;\n",
        "+    conversationHistory.push(new HumanMessage(userMessage));\n",
        "+\n",
        "+    console.log('Invoking parallel multi-agent graph...\\n');\n",
        "+\n",
        "+    const config = {\n",
        "+      configurable: {\n",
        "+        thread_id: 'parallel-conversation-1',\n",
        "+      },\n",
        "+      streamMode: 'values',\n",
        "+      version: 'v2' as const,\n",
        "+    };\n",
        "+\n",
        "+    // Process with streaming\n",
        "+    const inputs = {\n",
        "+      messages: conversationHistory,\n",
        "+    };\n",
        "+\n",
        "+    const finalContentParts = await run.processStream(inputs, config);\n",
        "+    const finalMessages = run.getRunMessages();\n",
        "+\n",
        "+    if (finalMessages) {\n",
        "+      conversationHistory.push(...finalMessages);\n",
        "+    }\n",
        "+\n",
        "+    console.log('\\n\\nActive agents during execution:', activeAgents.size);\n",
        "+    console.log('Final content parts:', contentParts.length, 'parts');\n",
        "+    console.dir(contentParts, { depth: null });\n",
        "+  } catch (error) {\n",
        "+    console.error('Error in parallel multi-agent test:', error);\n",
        "+  }\n",
        "+}\n",
        "+\n",
        "+// Run the test\n",
        "+testParallelMultiAgent();\n"
      ]
    },
    {
      "path": "src/scripts/multi-agent-sequence.ts",
      "status": "added",
      "additions": 212,
      "deletions": 0,
      "patch": "@@ -0,0 +1,212 @@\n+import { config } from 'dotenv';\n+config();\n+\n+import { HumanMessage, BaseMessage } from '@langchain/core/messages';\n+import { Run } from '@/run';\n+import { Providers, GraphEvents } from '@/common';\n+import { ChatModelStreamHandler, createContentAggregator } from '@/stream';\n+import { ToolEndHandler, ModelEndHandler } from '@/events';\n+import type * as t from '@/types';\n+\n+const conversationHistory: BaseMessage[] = [];\n+\n+/**\n+ * Example of simple sequential multi-agent system\n+ *\n+ * Graph structure:\n+ * START -> agent_a -> agent_b -> agent_c -> END\n+ *\n+ * No conditions, no tools, just automatic sequential flow\n+ */\n+async function testSequentialMultiAgent() {\n+  console.log('Testing Sequential Multi-Agent System (A \u2192 B \u2192 C)...\\n');\n+\n+  // Set up content aggregator\n+  const { contentParts, aggregateContent } = createContentAggregator();\n+\n+  // Define three simple agents\n+  const agents: t.AgentInputs[] = [\n+    {\n+      agentId: 'agent_a',\n+      provider: Providers.ANTHROPIC,\n+      clientOptions: {\n+        modelName: 'claude-3-5-sonnet-latest',\n+        apiKey: process.env.ANTHROPIC_API_KEY,\n+      },\n+      instructions: `You are Agent A, the first agent in a sequential workflow.\n+      Your job is to:\n+      1. Receive the initial user request\n+      2. Process it and add your perspective (keep it brief, 2-3 sentences)\n+      3. Pass it along to Agent B\n+      \n+      Start your response with \"AGENT A:\" and end with \"Passing to Agent B...\"`,\n+      maxContextTokens: 8000,\n+    },\n+    {\n+      agentId: 'agent_b',\n+      provider: Providers.ANTHROPIC,\n+      clientOptions: {\n+        modelName: 'claude-3-5-sonnet-latest',\n+        apiKey: process.env.ANTHROPIC_API_KEY,\n+      },\n+      instructions: `You are Agent B, the second agent in a sequential workflow.\n+      Your job is to:\n+      1. Receive the context from Agent A\n+      2. Add your own analysis or perspective (keep it brief, 2-3 sentences)\n+      3. Pass it along to Agent C\n+      \n+      Start your response with \"AGENT B:\" and end with \"Passing to Agent C...\"`,\n+      maxContextTokens: 8000,\n+    },\n+    {\n+      agentId: 'agent_c',\n+      provider: Providers.ANTHROPIC,\n+      clientOptions: {\n+        modelName: 'claude-3-5-sonnet-latest',\n+        apiKey: process.env.ANTHROPIC_API_KEY,\n+      },\n+      instructions: `You are Agent C, the final agent in a sequential workflow.\n+      Your job is to:\n+      1. Receive the context from Agents A and B\n+      2. Provide a final summary or conclusion based on all previous inputs\n+      3. Complete the workflow\n+      \n+      Start your response with \"AGENT C:\" and end with \"Workflow complete.\"`,\n+      maxContextTokens: 8000,\n+    },\n+  ];\n+\n+  // Define sequential edges using 'direct' type to avoid tool creation\n+  // This creates automatic transitions without requiring agents to call tools\n+  const edges: t.GraphEdge[] = [\n+    {\n+      from: 'agent_a',\n+      to: 'agent_b',\n+      edgeType: 'direct', // This creates direct edges without tools\n+      description: 'Automatic transition from A to B',\n+    },\n+    {\n+      from: 'agent_b',\n+      to: 'agent_c',\n+      edgeType: 'direct', // This creates direct edges without tools\n+      description: 'Automatic transition from B to C',\n+    },\n+  ];\n+\n+  // Track agent progression\n+  let currentAgent = '';\n+\n+  // Create custom handlers\n+  const customHandlers = {\n+    [GraphEvents.TOOL_END]: new ToolEndHandler(),\n+    [GraphEvents.CHAT_MODEL_END]: new ModelEndHandler(),\n+    [GraphEvents.CHAT_MODEL_STREAM]: new ChatModelStreamHandler(),\n+    [GraphEvents.ON_RUN_STEP]: {\n+      handle: (\n+        event: GraphEvents.ON_RUN_STEP,\n+        data: t.StreamEventData\n+      ): void => {\n+        const runStepData = data as any;\n+        if (runStepData?.name) {\n+          currentAgent = runStepData.name;\n+          console.log(`\\n\u2192 ${currentAgent} is processing...`);\n+        }\n+        aggregateContent({ event, data: data as t.RunStep });\n+      },\n+    },\n+    [GraphEvents.ON_RUN_STEP_COMPLETED]: {\n+      handle: (\n+        event: GraphEvents.ON_RUN_STEP_COMPLETED,\n+        data: t.StreamEventData\n+      ): void => {\n+        const runStepData = data as any;\n+        if (runStepData?.name) {\n+          console.log(`\u2713 ${runStepData.name} completed`);\n+        }\n+        aggregateContent({\n+          event,\n+          data: data as unknown as { result: t.ToolEndEvent },\n+        });\n+      },\n+    },\n+    [GraphEvents.ON_RUN_STEP_DELTA]: {\n+      handle: (\n+        event: GraphEvents.ON_RUN_STEP_DELTA,\n+        data: t.StreamEventData\n+      ): void => {\n+        aggregateContent({ event, data: data as t.RunStepDeltaEvent });\n+      },\n+    },\n+    [GraphEvents.ON_MESSAGE_DELTA]: {\n+      handle: (\n+        event: GraphEvents.ON_MESSAGE_DELTA,\n+        data: t.StreamEventData\n+      ): void => {\n+        aggregateContent({ event, data: data as t.MessageDeltaEvent });\n+      },\n+    },\n+  };\n+\n+  // Create multi-agent run configuration\n+  const runConfig: t.RunConfig = {\n+    runId: `sequential-multi-agent-${Date.now()}`,\n+    graphConfig: {\n+      type: 'multi-agent',\n+      agents,\n+      edges,\n+    },\n+    customHandlers,\n+    returnContent: true,\n+  };\n+\n+  try {\n+    // Create and execute the run\n+    const run = await Run.create(runConfig);\n+\n+    // Test with a simple question\n+    const userMessage =\n+      'What are the key considerations for building a recommendation system?';\n+    conversationHistory.push(new HumanMessage(userMessage));\n+\n+    console.log(`User: \"${userMessage}\"\\n`);\n+    console.log('Starting sequential workflow...\\n');\n+\n+    const config = {\n+      configurable: {\n+        thread_id: 'sequential-conversation-1',\n+      },\n+      streamMode: 'values',\n+      version: 'v2' as const,\n+    };\n+\n+    // Process with streaming\n+    const inputs = {\n+      messages: conversationHistory,\n+    };\n+\n+    const finalContentParts = await run.processStream(inputs, config);\n+    const finalMessages = run.getRunMessages();\n+\n+    if (finalMessages) {\n+      conversationHistory.push(...finalMessages);\n+    }\n+\n+    console.log('\\n\\n=== Final Output ===');\n+    console.log('Sequential flow completed successfully!');\n+    console.log(`Total content parts: ${contentParts.length}`);\n+\n+    // Display the sequential responses\n+    const aiMessages = conversationHistory.filter(\n+      (msg) => msg._getType() === 'ai'\n+    );\n+    aiMessages.forEach((msg, index) => {\n+      console.log(`\\n--- Response ${index + 1} ---`);\n+      console.log(msg.content);\n+    });\n+  } catch (error) {\n+    console.error('Error in sequential multi-agent test:', error);\n+  }\n+}\n+\n+// Run the test\n+testSequentialMultiAgent();",
      "patch_lines": [
        "@@ -0,0 +1,212 @@\n",
        "+import { config } from 'dotenv';\n",
        "+config();\n",
        "+\n",
        "+import { HumanMessage, BaseMessage } from '@langchain/core/messages';\n",
        "+import { Run } from '@/run';\n",
        "+import { Providers, GraphEvents } from '@/common';\n",
        "+import { ChatModelStreamHandler, createContentAggregator } from '@/stream';\n",
        "+import { ToolEndHandler, ModelEndHandler } from '@/events';\n",
        "+import type * as t from '@/types';\n",
        "+\n",
        "+const conversationHistory: BaseMessage[] = [];\n",
        "+\n",
        "+/**\n",
        "+ * Example of simple sequential multi-agent system\n",
        "+ *\n",
        "+ * Graph structure:\n",
        "+ * START -> agent_a -> agent_b -> agent_c -> END\n",
        "+ *\n",
        "+ * No conditions, no tools, just automatic sequential flow\n",
        "+ */\n",
        "+async function testSequentialMultiAgent() {\n",
        "+  console.log('Testing Sequential Multi-Agent System (A \u2192 B \u2192 C)...\\n');\n",
        "+\n",
        "+  // Set up content aggregator\n",
        "+  const { contentParts, aggregateContent } = createContentAggregator();\n",
        "+\n",
        "+  // Define three simple agents\n",
        "+  const agents: t.AgentInputs[] = [\n",
        "+    {\n",
        "+      agentId: 'agent_a',\n",
        "+      provider: Providers.ANTHROPIC,\n",
        "+      clientOptions: {\n",
        "+        modelName: 'claude-3-5-sonnet-latest',\n",
        "+        apiKey: process.env.ANTHROPIC_API_KEY,\n",
        "+      },\n",
        "+      instructions: `You are Agent A, the first agent in a sequential workflow.\n",
        "+      Your job is to:\n",
        "+      1. Receive the initial user request\n",
        "+      2. Process it and add your perspective (keep it brief, 2-3 sentences)\n",
        "+      3. Pass it along to Agent B\n",
        "+      \n",
        "+      Start your response with \"AGENT A:\" and end with \"Passing to Agent B...\"`,\n",
        "+      maxContextTokens: 8000,\n",
        "+    },\n",
        "+    {\n",
        "+      agentId: 'agent_b',\n",
        "+      provider: Providers.ANTHROPIC,\n",
        "+      clientOptions: {\n",
        "+        modelName: 'claude-3-5-sonnet-latest',\n",
        "+        apiKey: process.env.ANTHROPIC_API_KEY,\n",
        "+      },\n",
        "+      instructions: `You are Agent B, the second agent in a sequential workflow.\n",
        "+      Your job is to:\n",
        "+      1. Receive the context from Agent A\n",
        "+      2. Add your own analysis or perspective (keep it brief, 2-3 sentences)\n",
        "+      3. Pass it along to Agent C\n",
        "+      \n",
        "+      Start your response with \"AGENT B:\" and end with \"Passing to Agent C...\"`,\n",
        "+      maxContextTokens: 8000,\n",
        "+    },\n",
        "+    {\n",
        "+      agentId: 'agent_c',\n",
        "+      provider: Providers.ANTHROPIC,\n",
        "+      clientOptions: {\n",
        "+        modelName: 'claude-3-5-sonnet-latest',\n",
        "+        apiKey: process.env.ANTHROPIC_API_KEY,\n",
        "+      },\n",
        "+      instructions: `You are Agent C, the final agent in a sequential workflow.\n",
        "+      Your job is to:\n",
        "+      1. Receive the context from Agents A and B\n",
        "+      2. Provide a final summary or conclusion based on all previous inputs\n",
        "+      3. Complete the workflow\n",
        "+      \n",
        "+      Start your response with \"AGENT C:\" and end with \"Workflow complete.\"`,\n",
        "+      maxContextTokens: 8000,\n",
        "+    },\n",
        "+  ];\n",
        "+\n",
        "+  // Define sequential edges using 'direct' type to avoid tool creation\n",
        "+  // This creates automatic transitions without requiring agents to call tools\n",
        "+  const edges: t.GraphEdge[] = [\n",
        "+    {\n",
        "+      from: 'agent_a',\n",
        "+      to: 'agent_b',\n",
        "+      edgeType: 'direct', // This creates direct edges without tools\n",
        "+      description: 'Automatic transition from A to B',\n",
        "+    },\n",
        "+    {\n",
        "+      from: 'agent_b',\n",
        "+      to: 'agent_c',\n",
        "+      edgeType: 'direct', // This creates direct edges without tools\n",
        "+      description: 'Automatic transition from B to C',\n",
        "+    },\n",
        "+  ];\n",
        "+\n",
        "+  // Track agent progression\n",
        "+  let currentAgent = '';\n",
        "+\n",
        "+  // Create custom handlers\n",
        "+  const customHandlers = {\n",
        "+    [GraphEvents.TOOL_END]: new ToolEndHandler(),\n",
        "+    [GraphEvents.CHAT_MODEL_END]: new ModelEndHandler(),\n",
        "+    [GraphEvents.CHAT_MODEL_STREAM]: new ChatModelStreamHandler(),\n",
        "+    [GraphEvents.ON_RUN_STEP]: {\n",
        "+      handle: (\n",
        "+        event: GraphEvents.ON_RUN_STEP,\n",
        "+        data: t.StreamEventData\n",
        "+      ): void => {\n",
        "+        const runStepData = data as any;\n",
        "+        if (runStepData?.name) {\n",
        "+          currentAgent = runStepData.name;\n",
        "+          console.log(`\\n\u2192 ${currentAgent} is processing...`);\n",
        "+        }\n",
        "+        aggregateContent({ event, data: data as t.RunStep });\n",
        "+      },\n",
        "+    },\n",
        "+    [GraphEvents.ON_RUN_STEP_COMPLETED]: {\n",
        "+      handle: (\n",
        "+        event: GraphEvents.ON_RUN_STEP_COMPLETED,\n",
        "+        data: t.StreamEventData\n",
        "+      ): void => {\n",
        "+        const runStepData = data as any;\n",
        "+        if (runStepData?.name) {\n",
        "+          console.log(`\u2713 ${runStepData.name} completed`);\n",
        "+        }\n",
        "+        aggregateContent({\n",
        "+          event,\n",
        "+          data: data as unknown as { result: t.ToolEndEvent },\n",
        "+        });\n",
        "+      },\n",
        "+    },\n",
        "+    [GraphEvents.ON_RUN_STEP_DELTA]: {\n",
        "+      handle: (\n",
        "+        event: GraphEvents.ON_RUN_STEP_DELTA,\n",
        "+        data: t.StreamEventData\n",
        "+      ): void => {\n",
        "+        aggregateContent({ event, data: data as t.RunStepDeltaEvent });\n",
        "+      },\n",
        "+    },\n",
        "+    [GraphEvents.ON_MESSAGE_DELTA]: {\n",
        "+      handle: (\n",
        "+        event: GraphEvents.ON_MESSAGE_DELTA,\n",
        "+        data: t.StreamEventData\n",
        "+      ): void => {\n",
        "+        aggregateContent({ event, data: data as t.MessageDeltaEvent });\n",
        "+      },\n",
        "+    },\n",
        "+  };\n",
        "+\n",
        "+  // Create multi-agent run configuration\n",
        "+  const runConfig: t.RunConfig = {\n",
        "+    runId: `sequential-multi-agent-${Date.now()}`,\n",
        "+    graphConfig: {\n",
        "+      type: 'multi-agent',\n",
        "+      agents,\n",
        "+      edges,\n",
        "+    },\n",
        "+    customHandlers,\n",
        "+    returnContent: true,\n",
        "+  };\n",
        "+\n",
        "+  try {\n",
        "+    // Create and execute the run\n",
        "+    const run = await Run.create(runConfig);\n",
        "+\n",
        "+    // Test with a simple question\n",
        "+    const userMessage =\n",
        "+      'What are the key considerations for building a recommendation system?';\n",
        "+    conversationHistory.push(new HumanMessage(userMessage));\n",
        "+\n",
        "+    console.log(`User: \"${userMessage}\"\\n`);\n",
        "+    console.log('Starting sequential workflow...\\n');\n",
        "+\n",
        "+    const config = {\n",
        "+      configurable: {\n",
        "+        thread_id: 'sequential-conversation-1',\n",
        "+      },\n",
        "+      streamMode: 'values',\n",
        "+      version: 'v2' as const,\n",
        "+    };\n",
        "+\n",
        "+    // Process with streaming\n",
        "+    const inputs = {\n",
        "+      messages: conversationHistory,\n",
        "+    };\n",
        "+\n",
        "+    const finalContentParts = await run.processStream(inputs, config);\n",
        "+    const finalMessages = run.getRunMessages();\n",
        "+\n",
        "+    if (finalMessages) {\n",
        "+      conversationHistory.push(...finalMessages);\n",
        "+    }\n",
        "+\n",
        "+    console.log('\\n\\n=== Final Output ===');\n",
        "+    console.log('Sequential flow completed successfully!');\n",
        "+    console.log(`Total content parts: ${contentParts.length}`);\n",
        "+\n",
        "+    // Display the sequential responses\n",
        "+    const aiMessages = conversationHistory.filter(\n",
        "+      (msg) => msg._getType() === 'ai'\n",
        "+    );\n",
        "+    aiMessages.forEach((msg, index) => {\n",
        "+      console.log(`\\n--- Response ${index + 1} ---`);\n",
        "+      console.log(msg.content);\n",
        "+    });\n",
        "+  } catch (error) {\n",
        "+    console.error('Error in sequential multi-agent test:', error);\n",
        "+  }\n",
        "+}\n",
        "+\n",
        "+// Run the test\n",
        "+testSequentialMultiAgent();\n"
      ]
    },
    {
      "path": "src/scripts/multi-agent-test.ts",
      "status": "added",
      "additions": 186,
      "deletions": 0,
      "patch": "@@ -0,0 +1,186 @@\n+import { config } from 'dotenv';\n+config();\n+\n+import { HumanMessage, BaseMessage } from '@langchain/core/messages';\n+import { Run } from '@/run';\n+import { Providers, GraphEvents } from '@/common';\n+import { ChatModelStreamHandler, createContentAggregator } from '@/stream';\n+import { ToolEndHandler, ModelEndHandler } from '@/events';\n+import type * as t from '@/types';\n+\n+const conversationHistory: BaseMessage[] = [];\n+\n+async function testMultiAgentHandoff() {\n+  console.log('Testing Multi-Agent Handoff System...\\n');\n+\n+  // Set up content aggregator\n+  const { contentParts, aggregateContent } = createContentAggregator();\n+\n+  // Define agent configurations\n+  const agents: t.AgentInputs[] = [\n+    {\n+      agentId: 'flight_assistant',\n+      provider: Providers.ANTHROPIC,\n+      clientOptions: {\n+        modelName: 'claude-3-5-sonnet-latest',\n+        apiKey: process.env.ANTHROPIC_API_KEY,\n+      },\n+      instructions:\n+        'You are a flight booking assistant. Help users book flights between airports.',\n+      maxContextTokens: 28000,\n+    },\n+    {\n+      agentId: 'hotel_assistant',\n+      provider: Providers.ANTHROPIC,\n+      clientOptions: {\n+        modelName: 'claude-3-5-sonnet-latest',\n+        apiKey: process.env.ANTHROPIC_API_KEY,\n+      },\n+      instructions:\n+        'You are a hotel booking assistant. Help users book hotel stays.',\n+      maxContextTokens: 28000,\n+    },\n+  ];\n+\n+  // Define edges (handoff relationships)\n+  // These edges create handoff tools that agents can use to transfer control dynamically\n+  const edges: t.GraphEdge[] = [\n+    {\n+      from: 'flight_assistant',\n+      to: 'hotel_assistant',\n+      description:\n+        'Transfer to hotel booking assistant when user needs hotel assistance',\n+      // edgeType defaults to 'handoff' for single-to-single edges\n+    },\n+    {\n+      from: 'hotel_assistant',\n+      to: 'flight_assistant',\n+      description:\n+        'Transfer to flight booking assistant when user needs flight assistance',\n+      // edgeType defaults to 'handoff' for single-to-single edges\n+    },\n+  ];\n+\n+  // Create custom handlers similar to examples\n+  const customHandlers = {\n+    [GraphEvents.TOOL_END]: new ToolEndHandler(),\n+    [GraphEvents.CHAT_MODEL_END]: new ModelEndHandler(),\n+    [GraphEvents.CHAT_MODEL_STREAM]: new ChatModelStreamHandler(),\n+    [GraphEvents.ON_RUN_STEP_COMPLETED]: {\n+      handle: (\n+        event: GraphEvents.ON_RUN_STEP_COMPLETED,\n+        data: t.StreamEventData\n+      ): void => {\n+        console.log('====== ON_RUN_STEP_COMPLETED ======');\n+        console.dir(data, { depth: null });\n+        aggregateContent({\n+          event,\n+          data: data as unknown as { result: t.ToolEndEvent },\n+        });\n+      },\n+    },\n+    [GraphEvents.ON_RUN_STEP]: {\n+      handle: (\n+        event: GraphEvents.ON_RUN_STEP,\n+        data: t.StreamEventData\n+      ): void => {\n+        console.log('====== ON_RUN_STEP ======');\n+        console.dir(data, { depth: null });\n+        aggregateContent({ event, data: data as t.RunStep });\n+      },\n+    },\n+    [GraphEvents.ON_RUN_STEP_DELTA]: {\n+      handle: (\n+        event: GraphEvents.ON_RUN_STEP_DELTA,\n+        data: t.StreamEventData\n+      ): void => {\n+        console.log('====== ON_RUN_STEP_DELTA ======');\n+        console.dir(data, { depth: null });\n+        aggregateContent({ event, data: data as t.RunStepDeltaEvent });\n+      },\n+    },\n+    [GraphEvents.ON_MESSAGE_DELTA]: {\n+      handle: (\n+        event: GraphEvents.ON_MESSAGE_DELTA,\n+        data: t.StreamEventData\n+      ): void => {\n+        console.log('====== ON_MESSAGE_DELTA ======');\n+        console.dir(data, { depth: null });\n+        aggregateContent({ event, data: data as t.MessageDeltaEvent });\n+      },\n+    },\n+    [GraphEvents.ON_REASONING_DELTA]: {\n+      handle: (\n+        event: GraphEvents.ON_REASONING_DELTA,\n+        data: t.StreamEventData\n+      ): void => {\n+        console.log('====== ON_REASONING_DELTA ======');\n+        console.dir(data, { depth: null });\n+        aggregateContent({ event, data: data as t.ReasoningDeltaEvent });\n+      },\n+    },\n+    [GraphEvents.TOOL_START]: {\n+      handle: (\n+        _event: string,\n+        data: t.StreamEventData,\n+        metadata?: Record<string, unknown>\n+      ): void => {\n+        console.log('====== TOOL_START ======');\n+        console.dir(data, { depth: null });\n+      },\n+    },\n+  };\n+\n+  // Create multi-agent run configuration\n+  const runConfig: t.RunConfig = {\n+    runId: `multi-agent-test-${Date.now()}`,\n+    graphConfig: {\n+      type: 'multi-agent',\n+      agents,\n+      edges,\n+    },\n+    customHandlers,\n+    returnContent: true,\n+  };\n+\n+  try {\n+    // Create and execute the run\n+    const run = await Run.create(runConfig);\n+\n+    const userMessage =\n+      'I need to book a flight from Boston to New York, and also need a hotel near Times Square.';\n+    conversationHistory.push(new HumanMessage(userMessage));\n+\n+    console.log('Invoking multi-agent graph...\\n');\n+\n+    const config = {\n+      configurable: {\n+        thread_id: 'multi-agent-conversation-1',\n+      },\n+      streamMode: 'values',\n+      version: 'v2' as const,\n+    };\n+\n+    // Process with streaming\n+    const inputs = {\n+      messages: conversationHistory,\n+    };\n+\n+    const finalContentParts = await run.processStream(inputs, config);\n+    const finalMessages = run.getRunMessages();\n+\n+    if (finalMessages) {\n+      conversationHistory.push(...finalMessages);\n+      console.log('\\n\\nConversation history:');\n+      console.dir(conversationHistory, { depth: null });\n+    }\n+\n+    console.log('\\n\\nFinal content parts:');\n+    console.dir(contentParts, { depth: null });\n+  } catch (error) {\n+    console.error('Error in multi-agent test:', error);\n+  }\n+}\n+\n+// Run the test\n+testMultiAgentHandoff();",
      "patch_lines": [
        "@@ -0,0 +1,186 @@\n",
        "+import { config } from 'dotenv';\n",
        "+config();\n",
        "+\n",
        "+import { HumanMessage, BaseMessage } from '@langchain/core/messages';\n",
        "+import { Run } from '@/run';\n",
        "+import { Providers, GraphEvents } from '@/common';\n",
        "+import { ChatModelStreamHandler, createContentAggregator } from '@/stream';\n",
        "+import { ToolEndHandler, ModelEndHandler } from '@/events';\n",
        "+import type * as t from '@/types';\n",
        "+\n",
        "+const conversationHistory: BaseMessage[] = [];\n",
        "+\n",
        "+async function testMultiAgentHandoff() {\n",
        "+  console.log('Testing Multi-Agent Handoff System...\\n');\n",
        "+\n",
        "+  // Set up content aggregator\n",
        "+  const { contentParts, aggregateContent } = createContentAggregator();\n",
        "+\n",
        "+  // Define agent configurations\n",
        "+  const agents: t.AgentInputs[] = [\n",
        "+    {\n",
        "+      agentId: 'flight_assistant',\n",
        "+      provider: Providers.ANTHROPIC,\n",
        "+      clientOptions: {\n",
        "+        modelName: 'claude-3-5-sonnet-latest',\n",
        "+        apiKey: process.env.ANTHROPIC_API_KEY,\n",
        "+      },\n",
        "+      instructions:\n",
        "+        'You are a flight booking assistant. Help users book flights between airports.',\n",
        "+      maxContextTokens: 28000,\n",
        "+    },\n",
        "+    {\n",
        "+      agentId: 'hotel_assistant',\n",
        "+      provider: Providers.ANTHROPIC,\n",
        "+      clientOptions: {\n",
        "+        modelName: 'claude-3-5-sonnet-latest',\n",
        "+        apiKey: process.env.ANTHROPIC_API_KEY,\n",
        "+      },\n",
        "+      instructions:\n",
        "+        'You are a hotel booking assistant. Help users book hotel stays.',\n",
        "+      maxContextTokens: 28000,\n",
        "+    },\n",
        "+  ];\n",
        "+\n",
        "+  // Define edges (handoff relationships)\n",
        "+  // These edges create handoff tools that agents can use to transfer control dynamically\n",
        "+  const edges: t.GraphEdge[] = [\n",
        "+    {\n",
        "+      from: 'flight_assistant',\n",
        "+      to: 'hotel_assistant',\n",
        "+      description:\n",
        "+        'Transfer to hotel booking assistant when user needs hotel assistance',\n",
        "+      // edgeType defaults to 'handoff' for single-to-single edges\n",
        "+    },\n",
        "+    {\n",
        "+      from: 'hotel_assistant',\n",
        "+      to: 'flight_assistant',\n",
        "+      description:\n",
        "+        'Transfer to flight booking assistant when user needs flight assistance',\n",
        "+      // edgeType defaults to 'handoff' for single-to-single edges\n",
        "+    },\n",
        "+  ];\n",
        "+\n",
        "+  // Create custom handlers similar to examples\n",
        "+  const customHandlers = {\n",
        "+    [GraphEvents.TOOL_END]: new ToolEndHandler(),\n",
        "+    [GraphEvents.CHAT_MODEL_END]: new ModelEndHandler(),\n",
        "+    [GraphEvents.CHAT_MODEL_STREAM]: new ChatModelStreamHandler(),\n",
        "+    [GraphEvents.ON_RUN_STEP_COMPLETED]: {\n",
        "+      handle: (\n",
        "+        event: GraphEvents.ON_RUN_STEP_COMPLETED,\n",
        "+        data: t.StreamEventData\n",
        "+      ): void => {\n",
        "+        console.log('====== ON_RUN_STEP_COMPLETED ======');\n",
        "+        console.dir(data, { depth: null });\n",
        "+        aggregateContent({\n",
        "+          event,\n",
        "+          data: data as unknown as { result: t.ToolEndEvent },\n",
        "+        });\n",
        "+      },\n",
        "+    },\n",
        "+    [GraphEvents.ON_RUN_STEP]: {\n",
        "+      handle: (\n",
        "+        event: GraphEvents.ON_RUN_STEP,\n",
        "+        data: t.StreamEventData\n",
        "+      ): void => {\n",
        "+        console.log('====== ON_RUN_STEP ======');\n",
        "+        console.dir(data, { depth: null });\n",
        "+        aggregateContent({ event, data: data as t.RunStep });\n",
        "+      },\n",
        "+    },\n",
        "+    [GraphEvents.ON_RUN_STEP_DELTA]: {\n",
        "+      handle: (\n",
        "+        event: GraphEvents.ON_RUN_STEP_DELTA,\n",
        "+        data: t.StreamEventData\n",
        "+      ): void => {\n",
        "+        console.log('====== ON_RUN_STEP_DELTA ======');\n",
        "+        console.dir(data, { depth: null });\n",
        "+        aggregateContent({ event, data: data as t.RunStepDeltaEvent });\n",
        "+      },\n",
        "+    },\n",
        "+    [GraphEvents.ON_MESSAGE_DELTA]: {\n",
        "+      handle: (\n",
        "+        event: GraphEvents.ON_MESSAGE_DELTA,\n",
        "+        data: t.StreamEventData\n",
        "+      ): void => {\n",
        "+        console.log('====== ON_MESSAGE_DELTA ======');\n",
        "+        console.dir(data, { depth: null });\n",
        "+        aggregateContent({ event, data: data as t.MessageDeltaEvent });\n",
        "+      },\n",
        "+    },\n",
        "+    [GraphEvents.ON_REASONING_DELTA]: {\n",
        "+      handle: (\n",
        "+        event: GraphEvents.ON_REASONING_DELTA,\n",
        "+        data: t.StreamEventData\n",
        "+      ): void => {\n",
        "+        console.log('====== ON_REASONING_DELTA ======');\n",
        "+        console.dir(data, { depth: null });\n",
        "+        aggregateContent({ event, data: data as t.ReasoningDeltaEvent });\n",
        "+      },\n",
        "+    },\n",
        "+    [GraphEvents.TOOL_START]: {\n",
        "+      handle: (\n",
        "+        _event: string,\n",
        "+        data: t.StreamEventData,\n",
        "+        metadata?: Record<string, unknown>\n",
        "+      ): void => {\n",
        "+        console.log('====== TOOL_START ======');\n",
        "+        console.dir(data, { depth: null });\n",
        "+      },\n",
        "+    },\n",
        "+  };\n",
        "+\n",
        "+  // Create multi-agent run configuration\n",
        "+  const runConfig: t.RunConfig = {\n",
        "+    runId: `multi-agent-test-${Date.now()}`,\n",
        "+    graphConfig: {\n",
        "+      type: 'multi-agent',\n",
        "+      agents,\n",
        "+      edges,\n",
        "+    },\n",
        "+    customHandlers,\n",
        "+    returnContent: true,\n",
        "+  };\n",
        "+\n",
        "+  try {\n",
        "+    // Create and execute the run\n",
        "+    const run = await Run.create(runConfig);\n",
        "+\n",
        "+    const userMessage =\n",
        "+      'I need to book a flight from Boston to New York, and also need a hotel near Times Square.';\n",
        "+    conversationHistory.push(new HumanMessage(userMessage));\n",
        "+\n",
        "+    console.log('Invoking multi-agent graph...\\n');\n",
        "+\n",
        "+    const config = {\n",
        "+      configurable: {\n",
        "+        thread_id: 'multi-agent-conversation-1',\n",
        "+      },\n",
        "+      streamMode: 'values',\n",
        "+      version: 'v2' as const,\n",
        "+    };\n",
        "+\n",
        "+    // Process with streaming\n",
        "+    const inputs = {\n",
        "+      messages: conversationHistory,\n",
        "+    };\n",
        "+\n",
        "+    const finalContentParts = await run.processStream(inputs, config);\n",
        "+    const finalMessages = run.getRunMessages();\n",
        "+\n",
        "+    if (finalMessages) {\n",
        "+      conversationHistory.push(...finalMessages);\n",
        "+      console.log('\\n\\nConversation history:');\n",
        "+      console.dir(conversationHistory, { depth: null });\n",
        "+    }\n",
        "+\n",
        "+    console.log('\\n\\nFinal content parts:');\n",
        "+    console.dir(contentParts, { depth: null });\n",
        "+  } catch (error) {\n",
        "+    console.error('Error in multi-agent test:', error);\n",
        "+  }\n",
        "+}\n",
        "+\n",
        "+// Run the test\n",
        "+testMultiAgentHandoff();\n"
      ]
    },
    {
      "path": "src/scripts/search.ts",
      "status": "modified",
      "additions": 1,
      "deletions": 9,
      "patch": "@@ -39,8 +39,6 @@ async function testStandardStreaming(): Promise<void> {\n         event: GraphEvents.ON_RUN_STEP,\n         data: t.StreamEventData\n       ): void => {\n-        console.log('====== ON_RUN_STEP ======');\n-        console.dir(data, { depth: null });\n         aggregateContent({ event, data: data as t.RunStep });\n       },\n     },\n@@ -49,8 +47,6 @@ async function testStandardStreaming(): Promise<void> {\n         event: GraphEvents.ON_RUN_STEP_DELTA,\n         data: t.StreamEventData\n       ): void => {\n-        console.log('====== ON_RUN_STEP_DELTA ======');\n-        console.dir(data, { depth: null });\n         aggregateContent({ event, data: data as t.RunStepDeltaEvent });\n       },\n     },\n@@ -59,8 +55,6 @@ async function testStandardStreaming(): Promise<void> {\n         event: GraphEvents.ON_MESSAGE_DELTA,\n         data: t.StreamEventData\n       ): void => {\n-        console.log('====== ON_MESSAGE_DELTA ======');\n-        console.dir(data, { depth: null });\n         aggregateContent({ event, data: data as t.MessageDeltaEvent });\n       },\n     },\n@@ -137,9 +131,7 @@ async function testStandardStreaming(): Promise<void> {\n }\n \n process.on('unhandledRejection', (reason, promise) => {\n-  console.error('Unhandled Rejection at:', promise, 'reason:', reason);\n-  console.log('Conversation history:');\n-  process.exit(1);\n+  console.warn('Unhandled Rejection (non-fatal):', reason);\n });\n \n testStandardStreaming().catch((err) => {",
      "patch_lines": [
        "@@ -39,8 +39,6 @@ async function testStandardStreaming(): Promise<void> {\n",
        "         event: GraphEvents.ON_RUN_STEP,\n",
        "         data: t.StreamEventData\n",
        "       ): void => {\n",
        "-        console.log('====== ON_RUN_STEP ======');\n",
        "-        console.dir(data, { depth: null });\n",
        "         aggregateContent({ event, data: data as t.RunStep });\n",
        "       },\n",
        "     },\n",
        "@@ -49,8 +47,6 @@ async function testStandardStreaming(): Promise<void> {\n",
        "         event: GraphEvents.ON_RUN_STEP_DELTA,\n",
        "         data: t.StreamEventData\n",
        "       ): void => {\n",
        "-        console.log('====== ON_RUN_STEP_DELTA ======');\n",
        "-        console.dir(data, { depth: null });\n",
        "         aggregateContent({ event, data: data as t.RunStepDeltaEvent });\n",
        "       },\n",
        "     },\n",
        "@@ -59,8 +55,6 @@ async function testStandardStreaming(): Promise<void> {\n",
        "         event: GraphEvents.ON_MESSAGE_DELTA,\n",
        "         data: t.StreamEventData\n",
        "       ): void => {\n",
        "-        console.log('====== ON_MESSAGE_DELTA ======');\n",
        "-        console.dir(data, { depth: null });\n",
        "         aggregateContent({ event, data: data as t.MessageDeltaEvent });\n",
        "       },\n",
        "     },\n",
        "@@ -137,9 +131,7 @@ async function testStandardStreaming(): Promise<void> {\n",
        " }\n",
        " \n",
        " process.on('unhandledRejection', (reason, promise) => {\n",
        "-  console.error('Unhandled Rejection at:', promise, 'reason:', reason);\n",
        "-  console.log('Conversation history:');\n",
        "-  process.exit(1);\n",
        "+  console.warn('Unhandled Rejection (non-fatal):', reason);\n",
        " });\n",
        " \n",
        " testStandardStreaming().catch((err) => {\n"
      ]
    },
    {
      "path": "src/scripts/simple.ts",
      "status": "modified",
      "additions": 3,
      "deletions": 2,
      "patch": "@@ -1,5 +1,6 @@\n // src/scripts/cli.ts\n import { config } from 'dotenv';\n+import { v4 as uuidv4 } from 'uuid';\n config();\n import {\n   HumanMessage,\n@@ -43,7 +44,7 @@ async function testStandardStreaming(): Promise<void> {\n         data: t.StreamEventData\n       ): void => {\n         console.log('====== ON_RUN_STEP_COMPLETED ======');\n-        // console.dir(data, { depth: null });\n+        console.dir(data, { depth: null });\n         aggregateContent({\n           event,\n           data: data as unknown as { result: t.ToolEndEvent },\n@@ -130,7 +131,7 @@ async function testStandardStreaming(): Promise<void> {\n   }\n \n   const run = await Run.create<t.IState>({\n-    runId: 'test-simple-script',\n+    runId: uuidv4(),\n     graphConfig: {\n       type: 'standard',\n       llmConfig,",
      "patch_lines": [
        "@@ -1,5 +1,6 @@\n",
        " // src/scripts/cli.ts\n",
        " import { config } from 'dotenv';\n",
        "+import { v4 as uuidv4 } from 'uuid';\n",
        " config();\n",
        " import {\n",
        "   HumanMessage,\n",
        "@@ -43,7 +44,7 @@ async function testStandardStreaming(): Promise<void> {\n",
        "         data: t.StreamEventData\n",
        "       ): void => {\n",
        "         console.log('====== ON_RUN_STEP_COMPLETED ======');\n",
        "-        // console.dir(data, { depth: null });\n",
        "+        console.dir(data, { depth: null });\n",
        "         aggregateContent({\n",
        "           event,\n",
        "           data: data as unknown as { result: t.ToolEndEvent },\n",
        "@@ -130,7 +131,7 @@ async function testStandardStreaming(): Promise<void> {\n",
        "   }\n",
        " \n",
        "   const run = await Run.create<t.IState>({\n",
        "-    runId: 'test-simple-script',\n",
        "+    runId: uuidv4(),\n",
        "     graphConfig: {\n",
        "       type: 'standard',\n",
        "       llmConfig,\n"
      ]
    },
    {
      "path": "src/scripts/tools.ts",
      "status": "modified",
      "additions": 3,
      "deletions": 4,
      "patch": "@@ -29,7 +29,7 @@ async function testStandardStreaming(): Promise<void> {\n         data: t.StreamEventData\n       ): void => {\n         console.log('====== ON_RUN_STEP_COMPLETED ======');\n-        // console.dir(data, { depth: null });\n+        console.dir(data, { depth: null });\n         aggregateContent({\n           event,\n           data: data as unknown as { result: t.ToolEndEvent },\n@@ -83,7 +83,7 @@ async function testStandardStreaming(): Promise<void> {\n         metadata?: Record<string, unknown>\n       ): void => {\n         console.log('====== TOOL_START ======');\n-        // console.dir(data, { depth: null });\n+        console.dir(data, { depth: null });\n       },\n     },\n   };\n@@ -117,7 +117,6 @@ async function testStandardStreaming(): Promise<void> {\n \n   const userMessage = `\n   Make a search for the weather in ${location} today, which is ${currentDate}.\n-  Before making the search, please let me know what you're about to do, then immediately start searching without hesitation.\n   Make sure to always refer to me by name, which is ${userName}.\n   After giving me a thorough summary, tell me a joke about the weather forecast we went over.\n   `;\n@@ -136,7 +135,7 @@ async function testStandardStreaming(): Promise<void> {\n     conversationHistory.push(...finalMessages);\n     console.dir(conversationHistory, { depth: null });\n   }\n-  // console.dir(finalContentParts, { depth: null });\n+  console.dir(finalContentParts, { depth: null });\n   console.log('\\n\\n====================\\n\\n');\n   // console.dir(contentParts, { depth: null });\n }",
      "patch_lines": [
        "@@ -29,7 +29,7 @@ async function testStandardStreaming(): Promise<void> {\n",
        "         data: t.StreamEventData\n",
        "       ): void => {\n",
        "         console.log('====== ON_RUN_STEP_COMPLETED ======');\n",
        "-        // console.dir(data, { depth: null });\n",
        "+        console.dir(data, { depth: null });\n",
        "         aggregateContent({\n",
        "           event,\n",
        "           data: data as unknown as { result: t.ToolEndEvent },\n",
        "@@ -83,7 +83,7 @@ async function testStandardStreaming(): Promise<void> {\n",
        "         metadata?: Record<string, unknown>\n",
        "       ): void => {\n",
        "         console.log('====== TOOL_START ======');\n",
        "-        // console.dir(data, { depth: null });\n",
        "+        console.dir(data, { depth: null });\n",
        "       },\n",
        "     },\n",
        "   };\n",
        "@@ -117,7 +117,6 @@ async function testStandardStreaming(): Promise<void> {\n",
        " \n",
        "   const userMessage = `\n",
        "   Make a search for the weather in ${location} today, which is ${currentDate}.\n",
        "-  Before making the search, please let me know what you're about to do, then immediately start searching without hesitation.\n",
        "   Make sure to always refer to me by name, which is ${userName}.\n",
        "   After giving me a thorough summary, tell me a joke about the weather forecast we went over.\n",
        "   `;\n",
        "@@ -136,7 +135,7 @@ async function testStandardStreaming(): Promise<void> {\n",
        "     conversationHistory.push(...finalMessages);\n",
        "     console.dir(conversationHistory, { depth: null });\n",
        "   }\n",
        "-  // console.dir(finalContentParts, { depth: null });\n",
        "+  console.dir(finalContentParts, { depth: null });\n",
        "   console.log('\\n\\n====================\\n\\n');\n",
        "   // console.dir(contentParts, { depth: null });\n",
        " }\n"
      ]
    },
    {
      "path": "src/specs/anthropic.simple.test.ts",
      "status": "modified",
      "additions": 10,
      "deletions": 7,
      "patch": "@@ -9,7 +9,6 @@ import {\n   BaseMessage,\n   UsageMetadata,\n } from '@langchain/core/messages';\n-import type { StandardGraph } from '@/graphs';\n import type * as t from '@/types';\n import {\n   ToolEndHandler,\n@@ -161,22 +160,22 @@ describe(`${capitalizeFirstLetter(provider)} Streaming Tests`, () => {\n \n     expect(onMessageDeltaSpy).toHaveBeenCalled();\n     expect(onMessageDeltaSpy.mock.calls.length).toBeGreaterThan(1);\n-    expect(\n-      (onMessageDeltaSpy.mock.calls[0][3] as StandardGraph).provider\n-    ).toBeDefined();\n+    expect(onMessageDeltaSpy.mock.calls[0][3]).toBeDefined(); // Graph exists\n \n     expect(onRunStepSpy).toHaveBeenCalled();\n     expect(onRunStepSpy.mock.calls.length).toBeGreaterThan(0);\n-    expect(\n-      (onRunStepSpy.mock.calls[0][3] as StandardGraph).provider\n-    ).toBeDefined();\n+    expect(onRunStepSpy.mock.calls[0][3]).toBeDefined(); // Graph exists\n \n     const { handleLLMEnd, collected } = createMetadataAggregator();\n     const titleResult = await run.generateTitle({\n       provider,\n       inputText: userMessage,\n       titleMethod: TitleMethod.STRUCTURED,\n       contentParts,\n+      clientOptions: {\n+        ...llmConfig,\n+        model: 'claude-3-5-haiku-latest',\n+      },\n       chainOptions: {\n         callbacks: [\n           {\n@@ -229,6 +228,10 @@ describe(`${capitalizeFirstLetter(provider)} Streaming Tests`, () => {\n       inputText: userMessage,\n       titleMethod: TitleMethod.COMPLETION, // Using completion method\n       contentParts,\n+      clientOptions: {\n+        ...llmConfig,\n+        model: 'claude-3-5-haiku-latest',\n+      },\n       chainOptions: {\n         callbacks: [\n           {",
      "patch_lines": [
        "@@ -9,7 +9,6 @@ import {\n",
        "   BaseMessage,\n",
        "   UsageMetadata,\n",
        " } from '@langchain/core/messages';\n",
        "-import type { StandardGraph } from '@/graphs';\n",
        " import type * as t from '@/types';\n",
        " import {\n",
        "   ToolEndHandler,\n",
        "@@ -161,22 +160,22 @@ describe(`${capitalizeFirstLetter(provider)} Streaming Tests`, () => {\n",
        " \n",
        "     expect(onMessageDeltaSpy).toHaveBeenCalled();\n",
        "     expect(onMessageDeltaSpy.mock.calls.length).toBeGreaterThan(1);\n",
        "-    expect(\n",
        "-      (onMessageDeltaSpy.mock.calls[0][3] as StandardGraph).provider\n",
        "-    ).toBeDefined();\n",
        "+    expect(onMessageDeltaSpy.mock.calls[0][3]).toBeDefined(); // Graph exists\n",
        " \n",
        "     expect(onRunStepSpy).toHaveBeenCalled();\n",
        "     expect(onRunStepSpy.mock.calls.length).toBeGreaterThan(0);\n",
        "-    expect(\n",
        "-      (onRunStepSpy.mock.calls[0][3] as StandardGraph).provider\n",
        "-    ).toBeDefined();\n",
        "+    expect(onRunStepSpy.mock.calls[0][3]).toBeDefined(); // Graph exists\n",
        " \n",
        "     const { handleLLMEnd, collected } = createMetadataAggregator();\n",
        "     const titleResult = await run.generateTitle({\n",
        "       provider,\n",
        "       inputText: userMessage,\n",
        "       titleMethod: TitleMethod.STRUCTURED,\n",
        "       contentParts,\n",
        "+      clientOptions: {\n",
        "+        ...llmConfig,\n",
        "+        model: 'claude-3-5-haiku-latest',\n",
        "+      },\n",
        "       chainOptions: {\n",
        "         callbacks: [\n",
        "           {\n",
        "@@ -229,6 +228,10 @@ describe(`${capitalizeFirstLetter(provider)} Streaming Tests`, () => {\n",
        "       inputText: userMessage,\n",
        "       titleMethod: TitleMethod.COMPLETION, // Using completion method\n",
        "       contentParts,\n",
        "+      clientOptions: {\n",
        "+        ...llmConfig,\n",
        "+        model: 'claude-3-5-haiku-latest',\n",
        "+      },\n",
        "       chainOptions: {\n",
        "         callbacks: [\n",
        "           {\n"
      ]
    },
    {
      "path": "src/specs/azure.simple.test.ts",
      "status": "modified",
      "additions": 17,
      "deletions": 8,
      "patch": "@@ -9,7 +9,6 @@ import {\n   BaseMessage,\n   UsageMetadata,\n } from '@langchain/core/messages';\n-import type { StandardGraph } from '@/graphs';\n import type * as t from '@/types';\n import {\n   ToolEndHandler,\n@@ -23,8 +22,20 @@ import { getLLMConfig } from '@/utils/llmConfig';\n import { getArgs } from '@/scripts/args';\n import { Run } from '@/run';\n \n+// Auto-skip this suite if Azure env vars are not present\n+const requiredAzureEnv = [\n+  'AZURE_OPENAI_API_KEY',\n+  'AZURE_OPENAI_API_INSTANCE',\n+  'AZURE_OPENAI_API_DEPLOYMENT',\n+  'AZURE_OPENAI_API_VERSION',\n+];\n+const hasAzure = requiredAzureEnv.every(\n+  (k) => (process.env[k] ?? '').trim() !== ''\n+);\n+const describeIfAzure = hasAzure ? describe : describe.skip;\n+\n const provider = Providers.AZURE;\n-describe(`${capitalizeFirstLetter(provider)} Streaming Tests`, () => {\n+describeIfAzure(`${capitalizeFirstLetter(provider)} Streaming Tests`, () => {\n   jest.setTimeout(30000);\n   let run: Run<t.IState>;\n   let runningHistory: BaseMessage[];\n@@ -161,22 +172,19 @@ describe(`${capitalizeFirstLetter(provider)} Streaming Tests`, () => {\n \n     expect(onMessageDeltaSpy).toHaveBeenCalled();\n     expect(onMessageDeltaSpy.mock.calls.length).toBeGreaterThan(1);\n-    expect(\n-      (onMessageDeltaSpy.mock.calls[0][3] as StandardGraph).provider\n-    ).toBeDefined();\n+    expect(onMessageDeltaSpy.mock.calls[0][3]).toBeDefined(); // Graph exists\n \n     expect(onRunStepSpy).toHaveBeenCalled();\n     expect(onRunStepSpy.mock.calls.length).toBeGreaterThan(0);\n-    expect(\n-      (onRunStepSpy.mock.calls[0][3] as StandardGraph).provider\n-    ).toBeDefined();\n+    expect(onRunStepSpy.mock.calls[0][3]).toBeDefined(); // Graph exists\n \n     const { handleLLMEnd, collected } = createMetadataAggregator();\n     const titleResult = await run.generateTitle({\n       provider,\n       inputText: userMessage,\n       titleMethod: TitleMethod.STRUCTURED,\n       contentParts,\n+      clientOptions: llmConfig,\n       chainOptions: {\n         callbacks: [\n           {\n@@ -228,6 +236,7 @@ describe(`${capitalizeFirstLetter(provider)} Streaming Tests`, () => {\n       inputText: userMessage,\n       titleMethod: TitleMethod.COMPLETION, // Using completion method\n       contentParts,\n+      clientOptions: llmConfig,\n       chainOptions: {\n         callbacks: [\n           {",
      "patch_lines": [
        "@@ -9,7 +9,6 @@ import {\n",
        "   BaseMessage,\n",
        "   UsageMetadata,\n",
        " } from '@langchain/core/messages';\n",
        "-import type { StandardGraph } from '@/graphs';\n",
        " import type * as t from '@/types';\n",
        " import {\n",
        "   ToolEndHandler,\n",
        "@@ -23,8 +22,20 @@ import { getLLMConfig } from '@/utils/llmConfig';\n",
        " import { getArgs } from '@/scripts/args';\n",
        " import { Run } from '@/run';\n",
        " \n",
        "+// Auto-skip this suite if Azure env vars are not present\n",
        "+const requiredAzureEnv = [\n",
        "+  'AZURE_OPENAI_API_KEY',\n",
        "+  'AZURE_OPENAI_API_INSTANCE',\n",
        "+  'AZURE_OPENAI_API_DEPLOYMENT',\n",
        "+  'AZURE_OPENAI_API_VERSION',\n",
        "+];\n",
        "+const hasAzure = requiredAzureEnv.every(\n",
        "+  (k) => (process.env[k] ?? '').trim() !== ''\n",
        "+);\n",
        "+const describeIfAzure = hasAzure ? describe : describe.skip;\n",
        "+\n",
        " const provider = Providers.AZURE;\n",
        "-describe(`${capitalizeFirstLetter(provider)} Streaming Tests`, () => {\n",
        "+describeIfAzure(`${capitalizeFirstLetter(provider)} Streaming Tests`, () => {\n",
        "   jest.setTimeout(30000);\n",
        "   let run: Run<t.IState>;\n",
        "   let runningHistory: BaseMessage[];\n",
        "@@ -161,22 +172,19 @@ describe(`${capitalizeFirstLetter(provider)} Streaming Tests`, () => {\n",
        " \n",
        "     expect(onMessageDeltaSpy).toHaveBeenCalled();\n",
        "     expect(onMessageDeltaSpy.mock.calls.length).toBeGreaterThan(1);\n",
        "-    expect(\n",
        "-      (onMessageDeltaSpy.mock.calls[0][3] as StandardGraph).provider\n",
        "-    ).toBeDefined();\n",
        "+    expect(onMessageDeltaSpy.mock.calls[0][3]).toBeDefined(); // Graph exists\n",
        " \n",
        "     expect(onRunStepSpy).toHaveBeenCalled();\n",
        "     expect(onRunStepSpy.mock.calls.length).toBeGreaterThan(0);\n",
        "-    expect(\n",
        "-      (onRunStepSpy.mock.calls[0][3] as StandardGraph).provider\n",
        "-    ).toBeDefined();\n",
        "+    expect(onRunStepSpy.mock.calls[0][3]).toBeDefined(); // Graph exists\n",
        " \n",
        "     const { handleLLMEnd, collected } = createMetadataAggregator();\n",
        "     const titleResult = await run.generateTitle({\n",
        "       provider,\n",
        "       inputText: userMessage,\n",
        "       titleMethod: TitleMethod.STRUCTURED,\n",
        "       contentParts,\n",
        "+      clientOptions: llmConfig,\n",
        "       chainOptions: {\n",
        "         callbacks: [\n",
        "           {\n",
        "@@ -228,6 +236,7 @@ describe(`${capitalizeFirstLetter(provider)} Streaming Tests`, () => {\n",
        "       inputText: userMessage,\n",
        "       titleMethod: TitleMethod.COMPLETION, // Using completion method\n",
        "       contentParts,\n",
        "+      clientOptions: llmConfig,\n",
        "       chainOptions: {\n",
        "         callbacks: [\n",
        "           {\n"
      ]
    },
    {
      "path": "src/specs/openai.simple.test.ts",
      "status": "modified",
      "additions": 2,
      "deletions": 7,
      "patch": "@@ -9,7 +9,6 @@ import {\n   BaseMessage,\n   UsageMetadata,\n } from '@langchain/core/messages';\n-import type { StandardGraph } from '@/graphs';\n import type * as t from '@/types';\n import {\n   ToolEndHandler,\n@@ -161,15 +160,11 @@ describe(`${capitalizeFirstLetter(provider)} Streaming Tests`, () => {\n \n     expect(onMessageDeltaSpy).toHaveBeenCalled();\n     expect(onMessageDeltaSpy.mock.calls.length).toBeGreaterThan(1);\n-    expect(\n-      (onMessageDeltaSpy.mock.calls[0][3] as StandardGraph).provider\n-    ).toBeDefined();\n+    expect(onMessageDeltaSpy.mock.calls[0][3]).toBeDefined(); // Graph exists\n \n     expect(onRunStepSpy).toHaveBeenCalled();\n     expect(onRunStepSpy.mock.calls.length).toBeGreaterThan(0);\n-    expect(\n-      (onRunStepSpy.mock.calls[0][3] as StandardGraph).provider\n-    ).toBeDefined();\n+    expect(onRunStepSpy.mock.calls[0][3]).toBeDefined(); // Graph exists\n \n     const { handleLLMEnd, collected } = createMetadataAggregator();\n     const titleResult = await run.generateTitle({",
      "patch_lines": [
        "@@ -9,7 +9,6 @@ import {\n",
        "   BaseMessage,\n",
        "   UsageMetadata,\n",
        " } from '@langchain/core/messages';\n",
        "-import type { StandardGraph } from '@/graphs';\n",
        " import type * as t from '@/types';\n",
        " import {\n",
        "   ToolEndHandler,\n",
        "@@ -161,15 +160,11 @@ describe(`${capitalizeFirstLetter(provider)} Streaming Tests`, () => {\n",
        " \n",
        "     expect(onMessageDeltaSpy).toHaveBeenCalled();\n",
        "     expect(onMessageDeltaSpy.mock.calls.length).toBeGreaterThan(1);\n",
        "-    expect(\n",
        "-      (onMessageDeltaSpy.mock.calls[0][3] as StandardGraph).provider\n",
        "-    ).toBeDefined();\n",
        "+    expect(onMessageDeltaSpy.mock.calls[0][3]).toBeDefined(); // Graph exists\n",
        " \n",
        "     expect(onRunStepSpy).toHaveBeenCalled();\n",
        "     expect(onRunStepSpy.mock.calls.length).toBeGreaterThan(0);\n",
        "-    expect(\n",
        "-      (onRunStepSpy.mock.calls[0][3] as StandardGraph).provider\n",
        "-    ).toBeDefined();\n",
        "+    expect(onRunStepSpy.mock.calls[0][3]).toBeDefined(); // Graph exists\n",
        " \n",
        "     const { handleLLMEnd, collected } = createMetadataAggregator();\n",
        "     const titleResult = await run.generateTitle({\n"
      ]
    },
    {
      "path": "src/specs/openrouter.simple.test.ts",
      "status": "added",
      "additions": 107,
      "deletions": 0,
      "patch": "@@ -0,0 +1,107 @@\n+import { config } from 'dotenv';\n+config();\n+import { Calculator } from '@langchain/community/tools/calculator';\n+import {\n+  HumanMessage,\n+  BaseMessage,\n+  UsageMetadata,\n+} from '@langchain/core/messages';\n+import type * as t from '@/types';\n+import { ToolEndHandler, ModelEndHandler } from '@/events';\n+import { ContentTypes, GraphEvents, Providers, TitleMethod } from '@/common';\n+import { ChatModelStreamHandler, createContentAggregator } from '@/stream';\n+import { capitalizeFirstLetter } from './spec.utils';\n+import { getLLMConfig } from '@/utils/llmConfig';\n+import { getArgs } from '@/scripts/args';\n+import { Run } from '@/run';\n+\n+// Auto-skip if OpenRouter env is missing\n+const hasOpenRouter = (process.env.OPENROUTER_API_KEY ?? '').trim() !== '';\n+const describeIf = hasOpenRouter ? describe : describe.skip;\n+\n+const provider = Providers.OPENROUTER;\n+describeIf(`${capitalizeFirstLetter(provider)} Streaming Tests`, () => {\n+  jest.setTimeout(60000);\n+  let run: Run<t.IState>;\n+  let collectedUsage: UsageMetadata[];\n+  let conversationHistory: BaseMessage[];\n+  let contentParts: t.MessageContentComplex[];\n+\n+  const configV2 = {\n+    configurable: { thread_id: 'or-convo-1' },\n+    streamMode: 'values',\n+    version: 'v2' as const,\n+  };\n+\n+  beforeEach(async () => {\n+    conversationHistory = [];\n+    collectedUsage = [];\n+    const { contentParts: cp } = createContentAggregator();\n+    contentParts = cp as t.MessageContentComplex[];\n+  });\n+\n+  const onMessageDeltaSpy = jest.fn();\n+  const onRunStepSpy = jest.fn();\n+\n+  afterAll(() => {\n+    onMessageDeltaSpy.mockReset();\n+    onRunStepSpy.mockReset();\n+  });\n+\n+  const setupCustomHandlers = (): Record<\n+    string | GraphEvents,\n+    t.EventHandler\n+  > => ({\n+    [GraphEvents.TOOL_END]: new ToolEndHandler(),\n+    [GraphEvents.CHAT_MODEL_END]: new ModelEndHandler(collectedUsage),\n+    [GraphEvents.CHAT_MODEL_STREAM]: new ChatModelStreamHandler(),\n+  });\n+\n+  test(`${capitalizeFirstLetter(provider)}: simple stream + title`, async () => {\n+    const { userName, location } = await getArgs();\n+    const llmConfig = getLLMConfig(provider);\n+    const customHandlers = setupCustomHandlers();\n+\n+    run = await Run.create<t.IState>({\n+      runId: 'or-run-1',\n+      graphConfig: {\n+        type: 'standard',\n+        llmConfig,\n+        tools: [new Calculator()],\n+        instructions: 'You are a friendly AI assistant.',\n+        additional_instructions: `The user's name is ${userName} and they are located in ${location}.`,\n+      },\n+      returnContent: true,\n+      customHandlers,\n+    });\n+\n+    const userMessage = 'hi';\n+    conversationHistory.push(new HumanMessage(userMessage));\n+\n+    const finalContentParts = await run.processStream(\n+      { messages: conversationHistory },\n+      configV2\n+    );\n+    expect(finalContentParts).toBeDefined();\n+    const allTextParts = finalContentParts?.every(\n+      (part) => part.type === ContentTypes.TEXT\n+    );\n+    expect(allTextParts).toBe(true);\n+    expect(\n+      (collectedUsage[0]?.input_tokens ?? 0) +\n+        (collectedUsage[0]?.output_tokens ?? 0)\n+    ).toBeGreaterThan(0);\n+\n+    const finalMessages = run.getRunMessages();\n+    expect(finalMessages).toBeDefined();\n+    conversationHistory.push(...(finalMessages ?? []));\n+\n+    const titleRes = await run.generateTitle({\n+      provider,\n+      inputText: userMessage,\n+      titleMethod: TitleMethod.COMPLETION,\n+      contentParts,\n+    });\n+    expect(titleRes.title).toBeDefined();\n+  });\n+});",
      "patch_lines": [
        "@@ -0,0 +1,107 @@\n",
        "+import { config } from 'dotenv';\n",
        "+config();\n",
        "+import { Calculator } from '@langchain/community/tools/calculator';\n",
        "+import {\n",
        "+  HumanMessage,\n",
        "+  BaseMessage,\n",
        "+  UsageMetadata,\n",
        "+} from '@langchain/core/messages';\n",
        "+import type * as t from '@/types';\n",
        "+import { ToolEndHandler, ModelEndHandler } from '@/events';\n",
        "+import { ContentTypes, GraphEvents, Providers, TitleMethod } from '@/common';\n",
        "+import { ChatModelStreamHandler, createContentAggregator } from '@/stream';\n",
        "+import { capitalizeFirstLetter } from './spec.utils';\n",
        "+import { getLLMConfig } from '@/utils/llmConfig';\n",
        "+import { getArgs } from '@/scripts/args';\n",
        "+import { Run } from '@/run';\n",
        "+\n",
        "+// Auto-skip if OpenRouter env is missing\n",
        "+const hasOpenRouter = (process.env.OPENROUTER_API_KEY ?? '').trim() !== '';\n",
        "+const describeIf = hasOpenRouter ? describe : describe.skip;\n",
        "+\n",
        "+const provider = Providers.OPENROUTER;\n",
        "+describeIf(`${capitalizeFirstLetter(provider)} Streaming Tests`, () => {\n",
        "+  jest.setTimeout(60000);\n",
        "+  let run: Run<t.IState>;\n",
        "+  let collectedUsage: UsageMetadata[];\n",
        "+  let conversationHistory: BaseMessage[];\n",
        "+  let contentParts: t.MessageContentComplex[];\n",
        "+\n",
        "+  const configV2 = {\n",
        "+    configurable: { thread_id: 'or-convo-1' },\n",
        "+    streamMode: 'values',\n",
        "+    version: 'v2' as const,\n",
        "+  };\n",
        "+\n",
        "+  beforeEach(async () => {\n",
        "+    conversationHistory = [];\n",
        "+    collectedUsage = [];\n",
        "+    const { contentParts: cp } = createContentAggregator();\n",
        "+    contentParts = cp as t.MessageContentComplex[];\n",
        "+  });\n",
        "+\n",
        "+  const onMessageDeltaSpy = jest.fn();\n",
        "+  const onRunStepSpy = jest.fn();\n",
        "+\n",
        "+  afterAll(() => {\n",
        "+    onMessageDeltaSpy.mockReset();\n",
        "+    onRunStepSpy.mockReset();\n",
        "+  });\n",
        "+\n",
        "+  const setupCustomHandlers = (): Record<\n",
        "+    string | GraphEvents,\n",
        "+    t.EventHandler\n",
        "+  > => ({\n",
        "+    [GraphEvents.TOOL_END]: new ToolEndHandler(),\n",
        "+    [GraphEvents.CHAT_MODEL_END]: new ModelEndHandler(collectedUsage),\n",
        "+    [GraphEvents.CHAT_MODEL_STREAM]: new ChatModelStreamHandler(),\n",
        "+  });\n",
        "+\n",
        "+  test(`${capitalizeFirstLetter(provider)}: simple stream + title`, async () => {\n",
        "+    const { userName, location } = await getArgs();\n",
        "+    const llmConfig = getLLMConfig(provider);\n",
        "+    const customHandlers = setupCustomHandlers();\n",
        "+\n",
        "+    run = await Run.create<t.IState>({\n",
        "+      runId: 'or-run-1',\n",
        "+      graphConfig: {\n",
        "+        type: 'standard',\n",
        "+        llmConfig,\n",
        "+        tools: [new Calculator()],\n",
        "+        instructions: 'You are a friendly AI assistant.',\n",
        "+        additional_instructions: `The user's name is ${userName} and they are located in ${location}.`,\n",
        "+      },\n",
        "+      returnContent: true,\n",
        "+      customHandlers,\n",
        "+    });\n",
        "+\n",
        "+    const userMessage = 'hi';\n",
        "+    conversationHistory.push(new HumanMessage(userMessage));\n",
        "+\n",
        "+    const finalContentParts = await run.processStream(\n",
        "+      { messages: conversationHistory },\n",
        "+      configV2\n",
        "+    );\n",
        "+    expect(finalContentParts).toBeDefined();\n",
        "+    const allTextParts = finalContentParts?.every(\n",
        "+      (part) => part.type === ContentTypes.TEXT\n",
        "+    );\n",
        "+    expect(allTextParts).toBe(true);\n",
        "+    expect(\n",
        "+      (collectedUsage[0]?.input_tokens ?? 0) +\n",
        "+        (collectedUsage[0]?.output_tokens ?? 0)\n",
        "+    ).toBeGreaterThan(0);\n",
        "+\n",
        "+    const finalMessages = run.getRunMessages();\n",
        "+    expect(finalMessages).toBeDefined();\n",
        "+    conversationHistory.push(...(finalMessages ?? []));\n",
        "+\n",
        "+    const titleRes = await run.generateTitle({\n",
        "+      provider,\n",
        "+      inputText: userMessage,\n",
        "+      titleMethod: TitleMethod.COMPLETION,\n",
        "+      contentParts,\n",
        "+    });\n",
        "+    expect(titleRes.title).toBeDefined();\n",
        "+  });\n",
        "+});\n"
      ]
    },
    {
      "path": "src/specs/prune.test.ts",
      "status": "modified",
      "additions": 4,
      "deletions": 9,
      "patch": "@@ -725,19 +725,18 @@ describe('Prune Messages Tests', () => {\n           type: 'standard',\n           llmConfig,\n           instructions: 'You are a helpful assistant.',\n+          maxContextTokens: 1000,\n         },\n         returnContent: true,\n+        tokenCounter,\n+        indexTokenCountMap: {},\n       });\n \n       // Override the model to use a fake LLM\n       run.Graph?.overrideTestModel(['This is a test response'], 1);\n \n       const messages = [new HumanMessage('Hello, how are you?')];\n \n-      const indexTokenCountMap = {\n-        0: tokenCounter(messages[0]),\n-      };\n-\n       const config: Partial<RunnableConfig> & {\n         version: 'v1' | 'v2';\n         streamMode: string;\n@@ -749,11 +748,7 @@ describe('Prune Messages Tests', () => {\n         version: 'v2' as const,\n       };\n \n-      await run.processStream({ messages }, config, {\n-        maxContextTokens: 1000,\n-        indexTokenCountMap,\n-        tokenCounter,\n-      });\n+      await run.processStream({ messages }, config);\n \n       const finalMessages = run.getRunMessages();\n       expect(finalMessages).toBeDefined();",
      "patch_lines": [
        "@@ -725,19 +725,18 @@ describe('Prune Messages Tests', () => {\n",
        "           type: 'standard',\n",
        "           llmConfig,\n",
        "           instructions: 'You are a helpful assistant.',\n",
        "+          maxContextTokens: 1000,\n",
        "         },\n",
        "         returnContent: true,\n",
        "+        tokenCounter,\n",
        "+        indexTokenCountMap: {},\n",
        "       });\n",
        " \n",
        "       // Override the model to use a fake LLM\n",
        "       run.Graph?.overrideTestModel(['This is a test response'], 1);\n",
        " \n",
        "       const messages = [new HumanMessage('Hello, how are you?')];\n",
        " \n",
        "-      const indexTokenCountMap = {\n",
        "-        0: tokenCounter(messages[0]),\n",
        "-      };\n",
        "-\n",
        "       const config: Partial<RunnableConfig> & {\n",
        "         version: 'v1' | 'v2';\n",
        "         streamMode: string;\n",
        "@@ -749,11 +748,7 @@ describe('Prune Messages Tests', () => {\n",
        "         version: 'v2' as const,\n",
        "       };\n",
        " \n",
        "-      await run.processStream({ messages }, config, {\n",
        "-        maxContextTokens: 1000,\n",
        "-        indexTokenCountMap,\n",
        "-        tokenCounter,\n",
        "-      });\n",
        "+      await run.processStream({ messages }, config);\n",
        " \n",
        "       const finalMessages = run.getRunMessages();\n",
        "       expect(finalMessages).toBeDefined();\n"
      ]
    },
    {
      "path": "src/specs/reasoning.test.ts",
      "status": "modified",
      "additions": 80,
      "deletions": 44,
      "patch": "@@ -3,9 +3,12 @@\n // src/scripts/cli.test.ts\n import { config } from 'dotenv';\n config();\n-import { HumanMessage, BaseMessage, MessageContentText } from '@langchain/core/messages';\n+import {\n+  HumanMessage,\n+  BaseMessage,\n+  MessageContentText,\n+} from '@langchain/core/messages';\n import type { RunnableConfig } from '@langchain/core/runnables';\n-import type { StandardGraph } from '@/graphs';\n import type * as t from '@/types';\n import { ChatModelStreamHandler, createContentAggregator } from '@/stream';\n import { capitalizeFirstLetter } from './spec.utils';\n@@ -28,40 +31,49 @@ describe(`${capitalizeFirstLetter(provider)} Streaming Tests`, () => {\n   let aggregateContent: t.ContentAggregator;\n   let runSteps: Set<string>;\n \n-  const config: Partial<RunnableConfig> & { version: 'v1' | 'v2'; run_id?: string; streamMode: string } = {\n+  const config: Partial<RunnableConfig> & {\n+    version: 'v1' | 'v2';\n+    run_id?: string;\n+    streamMode: string;\n+  } = {\n     configurable: {\n       thread_id: 'conversation-num-1',\n     },\n     streamMode: 'values',\n     version: 'v2' as const,\n-    callbacks: [{\n-      async handleCustomEvent(event, data, metadata): Promise<void> {\n-        if (event !== GraphEvents.ON_MESSAGE_DELTA) {\n-          return;\n-        }\n-        const messageDeltaData = data as t.MessageDeltaEvent;\n-\n-        // Wait until we see the run step (with timeout for safety)\n-        const maxAttempts = 50; // 5 seconds total\n-        let attempts = 0;\n-        while (!runSteps.has(messageDeltaData.id) && attempts < maxAttempts) {\n-          await new Promise(resolve => setTimeout(resolve, 100));\n-          attempts++;\n-        }\n-\n-        if (!runSteps.has(messageDeltaData.id)) {\n-          console.warn(`Timeout waiting for run step: ${messageDeltaData.id}`);\n-        }\n-\n-        onMessageDeltaSpy(event, data, metadata, run.Graph);\n-        aggregateContent({ event, data: messageDeltaData });\n+    callbacks: [\n+      {\n+        async handleCustomEvent(event, data, metadata): Promise<void> {\n+          if (event !== GraphEvents.ON_MESSAGE_DELTA) {\n+            return;\n+          }\n+          const messageDeltaData = data as t.MessageDeltaEvent;\n+\n+          // Wait until we see the run step (with timeout for safety)\n+          const maxAttempts = 50; // 5 seconds total\n+          let attempts = 0;\n+          while (!runSteps.has(messageDeltaData.id) && attempts < maxAttempts) {\n+            await new Promise((resolve) => setTimeout(resolve, 100));\n+            attempts++;\n+          }\n+\n+          if (!runSteps.has(messageDeltaData.id)) {\n+            console.warn(\n+              `Timeout waiting for run step: ${messageDeltaData.id}`\n+            );\n+          }\n+\n+          onMessageDeltaSpy(event, data, metadata, run.Graph);\n+          aggregateContent({ event, data: messageDeltaData });\n+        },\n       },\n-    }],\n+    ],\n   };\n \n   beforeEach(async () => {\n     conversationHistory = [];\n-    const { contentParts: parts, aggregateContent: ac } = createContentAggregator();\n+    const { contentParts: parts, aggregateContent: ac } =\n+      createContentAggregator();\n     aggregateContent = ac;\n     runSteps = new Set();\n     contentParts = parts as t.MessageContentComplex[];\n@@ -81,32 +93,54 @@ describe(`${capitalizeFirstLetter(provider)} Streaming Tests`, () => {\n     onRunStepSpy.mockReset();\n   });\n \n-  const setupCustomHandlers = (): Record<string | GraphEvents, t.EventHandler> => ({\n+  const setupCustomHandlers = (): Record<\n+    string | GraphEvents,\n+    t.EventHandler\n+  > => ({\n     [GraphEvents.CHAT_MODEL_STREAM]: new ChatModelStreamHandler(),\n     [GraphEvents.ON_RUN_STEP_COMPLETED]: {\n-      handle: (event: GraphEvents.ON_RUN_STEP_COMPLETED, data: t.StreamEventData): void => {\n-        aggregateContent({ event, data: data as unknown as { result: t.ToolEndEvent; } });\n-      }\n+      handle: (\n+        event: GraphEvents.ON_RUN_STEP_COMPLETED,\n+        data: t.StreamEventData\n+      ): void => {\n+        aggregateContent({\n+          event,\n+          data: data as unknown as { result: t.ToolEndEvent },\n+        });\n+      },\n     },\n     [GraphEvents.ON_RUN_STEP]: {\n-      handle: (event: GraphEvents.ON_RUN_STEP, data: t.StreamEventData, metadata, graph): void => {\n+      handle: (\n+        event: GraphEvents.ON_RUN_STEP,\n+        data: t.StreamEventData,\n+        metadata,\n+        graph\n+      ): void => {\n         const runStepData = data as t.RunStep;\n         runSteps.add(runStepData.id);\n \n         onRunStepSpy(event, runStepData, metadata, graph);\n         aggregateContent({ event, data: runStepData });\n-      }\n+      },\n     },\n     [GraphEvents.ON_RUN_STEP_DELTA]: {\n-      handle: (event: GraphEvents.ON_RUN_STEP_DELTA, data: t.StreamEventData): void => {\n+      handle: (\n+        event: GraphEvents.ON_RUN_STEP_DELTA,\n+        data: t.StreamEventData\n+      ): void => {\n         aggregateContent({ event, data: data as t.RunStepDeltaEvent });\n-      }\n+      },\n     },\n     [GraphEvents.ON_REASONING_DELTA]: {\n-      handle: (event: GraphEvents.ON_REASONING_DELTA, data: t.StreamEventData, metadata, graph): void => {\n+      handle: (\n+        event: GraphEvents.ON_REASONING_DELTA,\n+        data: t.StreamEventData,\n+        metadata,\n+        graph\n+      ): void => {\n         onReasoningDeltaSpy(event, data, metadata, graph);\n         aggregateContent({ event, data: data as t.ReasoningDeltaEvent });\n-      }\n+      },\n     },\n   });\n \n@@ -120,7 +154,8 @@ describe(`${capitalizeFirstLetter(provider)} Streaming Tests`, () => {\n       graphConfig: {\n         type: 'standard',\n         llmConfig,\n-        instructions: 'You are a friendly AI assistant. Always address the user by their name.',\n+        instructions:\n+          'You are a friendly AI assistant. Always address the user by their name.',\n         additional_instructions: `The user's name is ${userName} and they are located in ${location}.`,\n       },\n       returnContent: true,\n@@ -141,25 +176,26 @@ describe(`${capitalizeFirstLetter(provider)} Streaming Tests`, () => {\n     expect(contentParts.length).toBe(2);\n     const reasoningContent = reasoningText.match(/<think>(.*)<\\/think>/s)?.[0];\n     const content = reasoningText.split(/<\\/think>/)[1];\n-    expect((contentParts[0] as t.ReasoningContentText).think).toBe(reasoningContent);\n+    expect((contentParts[0] as t.ReasoningContentText).think).toBe(\n+      reasoningContent\n+    );\n     expect((contentParts[1] as MessageContentText).text).toBe(content);\n \n     const finalMessages = run.getRunMessages();\n     expect(finalMessages).toBeDefined();\n-    conversationHistory.push(...finalMessages ?? []);\n+    conversationHistory.push(...(finalMessages ?? []));\n     expect(conversationHistory.length).toBeGreaterThan(1);\n \n     expect(onMessageDeltaSpy).toHaveBeenCalled();\n     expect(onMessageDeltaSpy.mock.calls.length).toBeGreaterThan(1);\n-    expect((onMessageDeltaSpy.mock.calls[0][3] as StandardGraph).provider).toBeDefined();\n+    expect(onMessageDeltaSpy.mock.calls[0][3]).toBeDefined(); // Graph exists\n \n     expect(onReasoningDeltaSpy).toHaveBeenCalled();\n     expect(onReasoningDeltaSpy.mock.calls.length).toBeGreaterThan(1);\n-    expect((onReasoningDeltaSpy.mock.calls[0][3] as StandardGraph).provider).toBeDefined();\n+    expect(onReasoningDeltaSpy.mock.calls[0][3]).toBeDefined(); // Graph exists\n \n     expect(onRunStepSpy).toHaveBeenCalled();\n     expect(onRunStepSpy.mock.calls.length).toBeGreaterThan(0);\n-    expect((onRunStepSpy.mock.calls[0][3] as StandardGraph).provider).toBeDefined();\n-\n+    expect(onRunStepSpy.mock.calls[0][3]).toBeDefined(); // Graph exists\n   });\n-});\n\\ No newline at end of file\n+});",
      "patch_lines": [
        "@@ -3,9 +3,12 @@\n",
        " // src/scripts/cli.test.ts\n",
        " import { config } from 'dotenv';\n",
        " config();\n",
        "-import { HumanMessage, BaseMessage, MessageContentText } from '@langchain/core/messages';\n",
        "+import {\n",
        "+  HumanMessage,\n",
        "+  BaseMessage,\n",
        "+  MessageContentText,\n",
        "+} from '@langchain/core/messages';\n",
        " import type { RunnableConfig } from '@langchain/core/runnables';\n",
        "-import type { StandardGraph } from '@/graphs';\n",
        " import type * as t from '@/types';\n",
        " import { ChatModelStreamHandler, createContentAggregator } from '@/stream';\n",
        " import { capitalizeFirstLetter } from './spec.utils';\n",
        "@@ -28,40 +31,49 @@ describe(`${capitalizeFirstLetter(provider)} Streaming Tests`, () => {\n",
        "   let aggregateContent: t.ContentAggregator;\n",
        "   let runSteps: Set<string>;\n",
        " \n",
        "-  const config: Partial<RunnableConfig> & { version: 'v1' | 'v2'; run_id?: string; streamMode: string } = {\n",
        "+  const config: Partial<RunnableConfig> & {\n",
        "+    version: 'v1' | 'v2';\n",
        "+    run_id?: string;\n",
        "+    streamMode: string;\n",
        "+  } = {\n",
        "     configurable: {\n",
        "       thread_id: 'conversation-num-1',\n",
        "     },\n",
        "     streamMode: 'values',\n",
        "     version: 'v2' as const,\n",
        "-    callbacks: [{\n",
        "-      async handleCustomEvent(event, data, metadata): Promise<void> {\n",
        "-        if (event !== GraphEvents.ON_MESSAGE_DELTA) {\n",
        "-          return;\n",
        "-        }\n",
        "-        const messageDeltaData = data as t.MessageDeltaEvent;\n",
        "-\n",
        "-        // Wait until we see the run step (with timeout for safety)\n",
        "-        const maxAttempts = 50; // 5 seconds total\n",
        "-        let attempts = 0;\n",
        "-        while (!runSteps.has(messageDeltaData.id) && attempts < maxAttempts) {\n",
        "-          await new Promise(resolve => setTimeout(resolve, 100));\n",
        "-          attempts++;\n",
        "-        }\n",
        "-\n",
        "-        if (!runSteps.has(messageDeltaData.id)) {\n",
        "-          console.warn(`Timeout waiting for run step: ${messageDeltaData.id}`);\n",
        "-        }\n",
        "-\n",
        "-        onMessageDeltaSpy(event, data, metadata, run.Graph);\n",
        "-        aggregateContent({ event, data: messageDeltaData });\n",
        "+    callbacks: [\n",
        "+      {\n",
        "+        async handleCustomEvent(event, data, metadata): Promise<void> {\n",
        "+          if (event !== GraphEvents.ON_MESSAGE_DELTA) {\n",
        "+            return;\n",
        "+          }\n",
        "+          const messageDeltaData = data as t.MessageDeltaEvent;\n",
        "+\n",
        "+          // Wait until we see the run step (with timeout for safety)\n",
        "+          const maxAttempts = 50; // 5 seconds total\n",
        "+          let attempts = 0;\n",
        "+          while (!runSteps.has(messageDeltaData.id) && attempts < maxAttempts) {\n",
        "+            await new Promise((resolve) => setTimeout(resolve, 100));\n",
        "+            attempts++;\n",
        "+          }\n",
        "+\n",
        "+          if (!runSteps.has(messageDeltaData.id)) {\n",
        "+            console.warn(\n",
        "+              `Timeout waiting for run step: ${messageDeltaData.id}`\n",
        "+            );\n",
        "+          }\n",
        "+\n",
        "+          onMessageDeltaSpy(event, data, metadata, run.Graph);\n",
        "+          aggregateContent({ event, data: messageDeltaData });\n",
        "+        },\n",
        "       },\n",
        "-    }],\n",
        "+    ],\n",
        "   };\n",
        " \n",
        "   beforeEach(async () => {\n",
        "     conversationHistory = [];\n",
        "-    const { contentParts: parts, aggregateContent: ac } = createContentAggregator();\n",
        "+    const { contentParts: parts, aggregateContent: ac } =\n",
        "+      createContentAggregator();\n",
        "     aggregateContent = ac;\n",
        "     runSteps = new Set();\n",
        "     contentParts = parts as t.MessageContentComplex[];\n",
        "@@ -81,32 +93,54 @@ describe(`${capitalizeFirstLetter(provider)} Streaming Tests`, () => {\n",
        "     onRunStepSpy.mockReset();\n",
        "   });\n",
        " \n",
        "-  const setupCustomHandlers = (): Record<string | GraphEvents, t.EventHandler> => ({\n",
        "+  const setupCustomHandlers = (): Record<\n",
        "+    string | GraphEvents,\n",
        "+    t.EventHandler\n",
        "+  > => ({\n",
        "     [GraphEvents.CHAT_MODEL_STREAM]: new ChatModelStreamHandler(),\n",
        "     [GraphEvents.ON_RUN_STEP_COMPLETED]: {\n",
        "-      handle: (event: GraphEvents.ON_RUN_STEP_COMPLETED, data: t.StreamEventData): void => {\n",
        "-        aggregateContent({ event, data: data as unknown as { result: t.ToolEndEvent; } });\n",
        "-      }\n",
        "+      handle: (\n",
        "+        event: GraphEvents.ON_RUN_STEP_COMPLETED,\n",
        "+        data: t.StreamEventData\n",
        "+      ): void => {\n",
        "+        aggregateContent({\n",
        "+          event,\n",
        "+          data: data as unknown as { result: t.ToolEndEvent },\n",
        "+        });\n",
        "+      },\n",
        "     },\n",
        "     [GraphEvents.ON_RUN_STEP]: {\n",
        "-      handle: (event: GraphEvents.ON_RUN_STEP, data: t.StreamEventData, metadata, graph): void => {\n",
        "+      handle: (\n",
        "+        event: GraphEvents.ON_RUN_STEP,\n",
        "+        data: t.StreamEventData,\n",
        "+        metadata,\n",
        "+        graph\n",
        "+      ): void => {\n",
        "         const runStepData = data as t.RunStep;\n",
        "         runSteps.add(runStepData.id);\n",
        " \n",
        "         onRunStepSpy(event, runStepData, metadata, graph);\n",
        "         aggregateContent({ event, data: runStepData });\n",
        "-      }\n",
        "+      },\n",
        "     },\n",
        "     [GraphEvents.ON_RUN_STEP_DELTA]: {\n",
        "-      handle: (event: GraphEvents.ON_RUN_STEP_DELTA, data: t.StreamEventData): void => {\n",
        "+      handle: (\n",
        "+        event: GraphEvents.ON_RUN_STEP_DELTA,\n",
        "+        data: t.StreamEventData\n",
        "+      ): void => {\n",
        "         aggregateContent({ event, data: data as t.RunStepDeltaEvent });\n",
        "-      }\n",
        "+      },\n",
        "     },\n",
        "     [GraphEvents.ON_REASONING_DELTA]: {\n",
        "-      handle: (event: GraphEvents.ON_REASONING_DELTA, data: t.StreamEventData, metadata, graph): void => {\n",
        "+      handle: (\n",
        "+        event: GraphEvents.ON_REASONING_DELTA,\n",
        "+        data: t.StreamEventData,\n",
        "+        metadata,\n",
        "+        graph\n",
        "+      ): void => {\n",
        "         onReasoningDeltaSpy(event, data, metadata, graph);\n",
        "         aggregateContent({ event, data: data as t.ReasoningDeltaEvent });\n",
        "-      }\n",
        "+      },\n",
        "     },\n",
        "   });\n",
        " \n",
        "@@ -120,7 +154,8 @@ describe(`${capitalizeFirstLetter(provider)} Streaming Tests`, () => {\n",
        "       graphConfig: {\n",
        "         type: 'standard',\n",
        "         llmConfig,\n",
        "-        instructions: 'You are a friendly AI assistant. Always address the user by their name.',\n",
        "+        instructions:\n",
        "+          'You are a friendly AI assistant. Always address the user by their name.',\n",
        "         additional_instructions: `The user's name is ${userName} and they are located in ${location}.`,\n",
        "       },\n",
        "       returnContent: true,\n",
        "@@ -141,25 +176,26 @@ describe(`${capitalizeFirstLetter(provider)} Streaming Tests`, () => {\n",
        "     expect(contentParts.length).toBe(2);\n",
        "     const reasoningContent = reasoningText.match(/<think>(.*)<\\/think>/s)?.[0];\n",
        "     const content = reasoningText.split(/<\\/think>/)[1];\n",
        "-    expect((contentParts[0] as t.ReasoningContentText).think).toBe(reasoningContent);\n",
        "+    expect((contentParts[0] as t.ReasoningContentText).think).toBe(\n",
        "+      reasoningContent\n",
        "+    );\n",
        "     expect((contentParts[1] as MessageContentText).text).toBe(content);\n",
        " \n",
        "     const finalMessages = run.getRunMessages();\n",
        "     expect(finalMessages).toBeDefined();\n",
        "-    conversationHistory.push(...finalMessages ?? []);\n",
        "+    conversationHistory.push(...(finalMessages ?? []));\n",
        "     expect(conversationHistory.length).toBeGreaterThan(1);\n",
        " \n",
        "     expect(onMessageDeltaSpy).toHaveBeenCalled();\n",
        "     expect(onMessageDeltaSpy.mock.calls.length).toBeGreaterThan(1);\n",
        "-    expect((onMessageDeltaSpy.mock.calls[0][3] as StandardGraph).provider).toBeDefined();\n",
        "+    expect(onMessageDeltaSpy.mock.calls[0][3]).toBeDefined(); // Graph exists\n",
        " \n",
        "     expect(onReasoningDeltaSpy).toHaveBeenCalled();\n",
        "     expect(onReasoningDeltaSpy.mock.calls.length).toBeGreaterThan(1);\n",
        "-    expect((onReasoningDeltaSpy.mock.calls[0][3] as StandardGraph).provider).toBeDefined();\n",
        "+    expect(onReasoningDeltaSpy.mock.calls[0][3]).toBeDefined(); // Graph exists\n",
        " \n",
        "     expect(onRunStepSpy).toHaveBeenCalled();\n",
        "     expect(onRunStepSpy.mock.calls.length).toBeGreaterThan(0);\n",
        "-    expect((onRunStepSpy.mock.calls[0][3] as StandardGraph).provider).toBeDefined();\n",
        "-\n",
        "+    expect(onRunStepSpy.mock.calls[0][3]).toBeDefined(); // Graph exists\n",
        "   });\n",
        "-});\n",
        "\\ No newline at end of file\n",
        "+});\n"
      ]
    },
    {
      "path": "src/specs/token-memoization.test.ts",
      "status": "added",
      "additions": 39,
      "deletions": 0,
      "patch": "@@ -0,0 +1,39 @@\n+import { HumanMessage } from '@langchain/core/messages';\n+import { createTokenCounter } from '@/utils/tokens';\n+\n+describe('Token encoder memoization', () => {\n+  jest.setTimeout(45000);\n+\n+  test('fetches BPE once and reuses encoder across counters', async () => {\n+    const originalFetch = global.fetch;\n+    let fetchCalls = 0;\n+    global.fetch = (async (...args: Parameters<typeof fetch>) => {\n+      fetchCalls += 1;\n+      // Delegate to real fetch\n+      return await originalFetch(...args);\n+    }) as typeof fetch;\n+\n+    try {\n+      const counter1 = await createTokenCounter();\n+      const counter2 = await createTokenCounter();\n+\n+      const m1 = new HumanMessage('hello world');\n+      const m2 = new HumanMessage('another short text');\n+\n+      const c11 = counter1(m1);\n+      const c12 = counter1(m2);\n+      const c21 = counter2(m1);\n+      const c22 = counter2(m2);\n+\n+      expect(c11).toBeGreaterThan(0);\n+      expect(c12).toBeGreaterThan(0);\n+      expect(c21).toBe(c11);\n+      expect(c22).toBe(c12);\n+\n+      // Only one fetch for the shared encoder\n+      expect(fetchCalls).toBe(1);\n+    } finally {\n+      global.fetch = originalFetch;\n+    }\n+  });\n+});",
      "patch_lines": [
        "@@ -0,0 +1,39 @@\n",
        "+import { HumanMessage } from '@langchain/core/messages';\n",
        "+import { createTokenCounter } from '@/utils/tokens';\n",
        "+\n",
        "+describe('Token encoder memoization', () => {\n",
        "+  jest.setTimeout(45000);\n",
        "+\n",
        "+  test('fetches BPE once and reuses encoder across counters', async () => {\n",
        "+    const originalFetch = global.fetch;\n",
        "+    let fetchCalls = 0;\n",
        "+    global.fetch = (async (...args: Parameters<typeof fetch>) => {\n",
        "+      fetchCalls += 1;\n",
        "+      // Delegate to real fetch\n",
        "+      return await originalFetch(...args);\n",
        "+    }) as typeof fetch;\n",
        "+\n",
        "+    try {\n",
        "+      const counter1 = await createTokenCounter();\n",
        "+      const counter2 = await createTokenCounter();\n",
        "+\n",
        "+      const m1 = new HumanMessage('hello world');\n",
        "+      const m2 = new HumanMessage('another short text');\n",
        "+\n",
        "+      const c11 = counter1(m1);\n",
        "+      const c12 = counter1(m2);\n",
        "+      const c21 = counter2(m1);\n",
        "+      const c22 = counter2(m2);\n",
        "+\n",
        "+      expect(c11).toBeGreaterThan(0);\n",
        "+      expect(c12).toBeGreaterThan(0);\n",
        "+      expect(c21).toBe(c11);\n",
        "+      expect(c22).toBe(c12);\n",
        "+\n",
        "+      // Only one fetch for the shared encoder\n",
        "+      expect(fetchCalls).toBe(1);\n",
        "+    } finally {\n",
        "+      global.fetch = originalFetch;\n",
        "+    }\n",
        "+  });\n",
        "+});\n"
      ]
    },
    {
      "path": "src/stream.ts",
      "status": "modified",
      "additions": 56,
      "deletions": 55,
      "patch": "@@ -2,7 +2,8 @@\n import type { ChatOpenAIReasoningSummary } from '@langchain/openai';\n import type { AIMessageChunk } from '@langchain/core/messages';\n import type { ToolCall } from '@langchain/core/messages/tool';\n-import type { Graph } from '@/graphs';\n+import type { AgentContext } from '@/agents/AgentContext';\n+import type { StandardGraph } from '@/graphs';\n import type * as t from '@/types';\n import {\n   ToolCallTypes,\n@@ -113,12 +114,12 @@ export function getChunkContent({\n }\n \n export class ChatModelStreamHandler implements t.EventHandler {\n-  handle(\n+  async handle(\n     event: string,\n     data: t.StreamEventData,\n     metadata?: Record<string, unknown>,\n-    graph?: Graph\n-  ): void {\n+    graph?: StandardGraph\n+  ): Promise<void> {\n     if (!graph) {\n       throw new Error('Graph not found');\n     }\n@@ -130,29 +131,32 @@ export class ChatModelStreamHandler implements t.EventHandler {\n       return;\n     }\n \n+    const agentContext = graph.getAgentContext(metadata);\n+\n     const chunk = data.chunk as Partial<AIMessageChunk>;\n     const content = getChunkContent({\n       chunk,\n-      reasoningKey: graph.reasoningKey,\n-      provider: metadata?.provider as Providers,\n+      reasoningKey: agentContext.reasoningKey,\n+      provider: agentContext.provider,\n     });\n-    const skipHandling = handleServerToolResult({\n+    const skipHandling = await handleServerToolResult({\n+      graph,\n       content,\n       metadata,\n-      graph,\n+      agentContext,\n     });\n     if (skipHandling) {\n       return;\n     }\n-    this.handleReasoning(chunk, graph, metadata?.provider as Providers);\n+    this.handleReasoning(chunk, agentContext);\n     let hasToolCalls = false;\n     if (\n       chunk.tool_calls &&\n       chunk.tool_calls.length > 0 &&\n       chunk.tool_calls.every((tc) => tc.id != null && tc.id !== '')\n     ) {\n       hasToolCalls = true;\n-      handleToolCalls(chunk.tool_calls, metadata, graph);\n+      await handleToolCalls(chunk.tool_calls, metadata, graph);\n     }\n \n     const hasToolCallChunks =\n@@ -161,18 +165,16 @@ export class ChatModelStreamHandler implements t.EventHandler {\n       typeof content === 'undefined' ||\n       !content.length ||\n       (typeof content === 'string' && !content);\n-    const isEmptyChunk = isEmptyContent && !hasToolCallChunks;\n-    const chunkId = chunk.id ?? '';\n-    if (isEmptyChunk && chunkId && chunkId.startsWith('msg')) {\n-      if (graph.messageIdsByStepKey.has(chunkId)) {\n-        return;\n-      } else if (graph.prelimMessageIdsByStepKey.has(chunkId)) {\n-        return;\n-      }\n \n+    /** Set a preliminary message ID if found in empty chunk */\n+    const isEmptyChunk = isEmptyContent && !hasToolCallChunks;\n+    if (\n+      isEmptyChunk &&\n+      (chunk.id ?? '') !== '' &&\n+      !graph.prelimMessageIdsByStepKey.has(chunk.id ?? '')\n+    ) {\n       const stepKey = graph.getStepKey(metadata);\n-      graph.prelimMessageIdsByStepKey.set(stepKey, chunkId);\n-      return;\n+      graph.prelimMessageIdsByStepKey.set(stepKey, chunk.id ?? '');\n     } else if (isEmptyChunk) {\n       return;\n     }\n@@ -185,7 +187,7 @@ export class ChatModelStreamHandler implements t.EventHandler {\n       chunk.tool_call_chunks.length &&\n       typeof chunk.tool_call_chunks[0]?.index === 'number'\n     ) {\n-      handleToolCallChunks({\n+      await handleToolCallChunks({\n         graph,\n         stepKey,\n         toolCallChunks: chunk.tool_call_chunks,\n@@ -198,7 +200,7 @@ export class ChatModelStreamHandler implements t.EventHandler {\n \n     const message_id = getMessageId(stepKey, graph) ?? '';\n     if (message_id) {\n-      graph.dispatchRunStep(stepKey, {\n+      await graph.dispatchRunStep(stepKey, {\n         type: StepTypes.MESSAGE_CREATION,\n         message_creation: {\n           message_id,\n@@ -236,19 +238,19 @@ hasToolCallChunks: ${hasToolCallChunks}\n     ) {\n       return;\n     } else if (typeof content === 'string') {\n-      if (graph.currentTokenType === ContentTypes.TEXT) {\n-        graph.dispatchMessageDelta(stepId, {\n+      if (agentContext.currentTokenType === ContentTypes.TEXT) {\n+        await graph.dispatchMessageDelta(stepId, {\n           content: [\n             {\n               type: ContentTypes.TEXT,\n               text: content,\n             },\n           ],\n         });\n-      } else if (graph.currentTokenType === 'think_and_text') {\n+      } else if (agentContext.currentTokenType === 'think_and_text') {\n         const { text, thinking } = parseThinkingContent(content);\n         if (thinking) {\n-          graph.dispatchReasoningDelta(stepId, {\n+          await graph.dispatchReasoningDelta(stepId, {\n             content: [\n               {\n                 type: ContentTypes.THINK,\n@@ -258,19 +260,19 @@ hasToolCallChunks: ${hasToolCallChunks}\n           });\n         }\n         if (text) {\n-          graph.currentTokenType = ContentTypes.TEXT;\n-          graph.tokenTypeSwitch = 'content';\n+          agentContext.currentTokenType = ContentTypes.TEXT;\n+          agentContext.tokenTypeSwitch = 'content';\n           const newStepKey = graph.getStepKey(metadata);\n           const message_id = getMessageId(newStepKey, graph) ?? '';\n-          graph.dispatchRunStep(newStepKey, {\n+          await graph.dispatchRunStep(newStepKey, {\n             type: StepTypes.MESSAGE_CREATION,\n             message_creation: {\n               message_id,\n             },\n           });\n \n           const newStepId = graph.getStepIdByKey(newStepKey);\n-          graph.dispatchMessageDelta(newStepId, {\n+          await graph.dispatchMessageDelta(newStepId, {\n             content: [\n               {\n                 type: ContentTypes.TEXT,\n@@ -280,7 +282,7 @@ hasToolCallChunks: ${hasToolCallChunks}\n           });\n         }\n       } else {\n-        graph.dispatchReasoningDelta(stepId, {\n+        await graph.dispatchReasoningDelta(stepId, {\n           content: [\n             {\n               type: ContentTypes.THINK,\n@@ -292,7 +294,7 @@ hasToolCallChunks: ${hasToolCallChunks}\n     } else if (\n       content.every((c) => c.type?.startsWith(ContentTypes.TEXT) ?? false)\n     ) {\n-      graph.dispatchMessageDelta(stepId, {\n+      await graph.dispatchMessageDelta(stepId, {\n         content,\n       });\n     } else if (\n@@ -303,7 +305,7 @@ hasToolCallChunks: ${hasToolCallChunks}\n           (c.type?.startsWith(ContentTypes.REASONING_CONTENT) ?? false)\n       )\n     ) {\n-      graph.dispatchReasoningDelta(stepId, {\n+      await graph.dispatchReasoningDelta(stepId, {\n         content: content.map((c) => ({\n           type: ContentTypes.THINK,\n           think:\n@@ -317,13 +319,11 @@ hasToolCallChunks: ${hasToolCallChunks}\n   }\n   handleReasoning(\n     chunk: Partial<AIMessageChunk>,\n-    graph: Graph,\n-    provider?: Providers\n+    agentContext: AgentContext\n   ): void {\n-    let reasoning_content = chunk.additional_kwargs?.[graph.reasoningKey] as\n-      | string\n-      | Partial<ChatOpenAIReasoningSummary>\n-      | undefined;\n+    let reasoning_content = chunk.additional_kwargs?.[\n+      agentContext.reasoningKey\n+    ] as string | Partial<ChatOpenAIReasoningSummary> | undefined;\n     if (\n       Array.isArray(chunk.content) &&\n       (chunk.content[0]?.type === ContentTypes.THINKING ||\n@@ -332,7 +332,8 @@ hasToolCallChunks: ${hasToolCallChunks}\n     ) {\n       reasoning_content = 'valid';\n     } else if (\n-      (provider === Providers.OPENAI || provider === Providers.AZURE) &&\n+      (agentContext.provider === Providers.OPENAI ||\n+        agentContext.provider === Providers.AZURE) &&\n       reasoning_content != null &&\n       typeof reasoning_content !== 'string' &&\n       reasoning_content.summary?.[0]?.text != null &&\n@@ -347,43 +348,43 @@ hasToolCallChunks: ${hasToolCallChunks}\n         chunk.content === '' ||\n         reasoning_content === 'valid')\n     ) {\n-      graph.currentTokenType = ContentTypes.THINK;\n-      graph.tokenTypeSwitch = 'reasoning';\n+      agentContext.currentTokenType = ContentTypes.THINK;\n+      agentContext.tokenTypeSwitch = 'reasoning';\n       return;\n     } else if (\n-      graph.tokenTypeSwitch === 'reasoning' &&\n-      graph.currentTokenType !== ContentTypes.TEXT &&\n+      agentContext.tokenTypeSwitch === 'reasoning' &&\n+      agentContext.currentTokenType !== ContentTypes.TEXT &&\n       ((chunk.content != null && chunk.content !== '') ||\n         (chunk.tool_calls?.length ?? 0) > 0)\n     ) {\n-      graph.currentTokenType = ContentTypes.TEXT;\n-      graph.tokenTypeSwitch = 'content';\n+      agentContext.currentTokenType = ContentTypes.TEXT;\n+      agentContext.tokenTypeSwitch = 'content';\n     } else if (\n       chunk.content != null &&\n       typeof chunk.content === 'string' &&\n       chunk.content.includes('<think>') &&\n       chunk.content.includes('</think>')\n     ) {\n-      graph.currentTokenType = 'think_and_text';\n-      graph.tokenTypeSwitch = 'content';\n+      agentContext.currentTokenType = 'think_and_text';\n+      agentContext.tokenTypeSwitch = 'content';\n     } else if (\n       chunk.content != null &&\n       typeof chunk.content === 'string' &&\n       chunk.content.includes('<think>')\n     ) {\n-      graph.currentTokenType = ContentTypes.THINK;\n-      graph.tokenTypeSwitch = 'content';\n+      agentContext.currentTokenType = ContentTypes.THINK;\n+      agentContext.tokenTypeSwitch = 'content';\n     } else if (\n-      graph.lastToken != null &&\n-      graph.lastToken.includes('</think>')\n+      agentContext.lastToken != null &&\n+      agentContext.lastToken.includes('</think>')\n     ) {\n-      graph.currentTokenType = ContentTypes.TEXT;\n-      graph.tokenTypeSwitch = 'content';\n+      agentContext.currentTokenType = ContentTypes.TEXT;\n+      agentContext.tokenTypeSwitch = 'content';\n     }\n     if (typeof chunk.content !== 'string') {\n       return;\n     }\n-    graph.lastToken = chunk.content;\n+    agentContext.lastToken = chunk.content;\n   }\n }\n ",
      "patch_lines": [
        "@@ -2,7 +2,8 @@\n",
        " import type { ChatOpenAIReasoningSummary } from '@langchain/openai';\n",
        " import type { AIMessageChunk } from '@langchain/core/messages';\n",
        " import type { ToolCall } from '@langchain/core/messages/tool';\n",
        "-import type { Graph } from '@/graphs';\n",
        "+import type { AgentContext } from '@/agents/AgentContext';\n",
        "+import type { StandardGraph } from '@/graphs';\n",
        " import type * as t from '@/types';\n",
        " import {\n",
        "   ToolCallTypes,\n",
        "@@ -113,12 +114,12 @@ export function getChunkContent({\n",
        " }\n",
        " \n",
        " export class ChatModelStreamHandler implements t.EventHandler {\n",
        "-  handle(\n",
        "+  async handle(\n",
        "     event: string,\n",
        "     data: t.StreamEventData,\n",
        "     metadata?: Record<string, unknown>,\n",
        "-    graph?: Graph\n",
        "-  ): void {\n",
        "+    graph?: StandardGraph\n",
        "+  ): Promise<void> {\n",
        "     if (!graph) {\n",
        "       throw new Error('Graph not found');\n",
        "     }\n",
        "@@ -130,29 +131,32 @@ export class ChatModelStreamHandler implements t.EventHandler {\n",
        "       return;\n",
        "     }\n",
        " \n",
        "+    const agentContext = graph.getAgentContext(metadata);\n",
        "+\n",
        "     const chunk = data.chunk as Partial<AIMessageChunk>;\n",
        "     const content = getChunkContent({\n",
        "       chunk,\n",
        "-      reasoningKey: graph.reasoningKey,\n",
        "-      provider: metadata?.provider as Providers,\n",
        "+      reasoningKey: agentContext.reasoningKey,\n",
        "+      provider: agentContext.provider,\n",
        "     });\n",
        "-    const skipHandling = handleServerToolResult({\n",
        "+    const skipHandling = await handleServerToolResult({\n",
        "+      graph,\n",
        "       content,\n",
        "       metadata,\n",
        "-      graph,\n",
        "+      agentContext,\n",
        "     });\n",
        "     if (skipHandling) {\n",
        "       return;\n",
        "     }\n",
        "-    this.handleReasoning(chunk, graph, metadata?.provider as Providers);\n",
        "+    this.handleReasoning(chunk, agentContext);\n",
        "     let hasToolCalls = false;\n",
        "     if (\n",
        "       chunk.tool_calls &&\n",
        "       chunk.tool_calls.length > 0 &&\n",
        "       chunk.tool_calls.every((tc) => tc.id != null && tc.id !== '')\n",
        "     ) {\n",
        "       hasToolCalls = true;\n",
        "-      handleToolCalls(chunk.tool_calls, metadata, graph);\n",
        "+      await handleToolCalls(chunk.tool_calls, metadata, graph);\n",
        "     }\n",
        " \n",
        "     const hasToolCallChunks =\n",
        "@@ -161,18 +165,16 @@ export class ChatModelStreamHandler implements t.EventHandler {\n",
        "       typeof content === 'undefined' ||\n",
        "       !content.length ||\n",
        "       (typeof content === 'string' && !content);\n",
        "-    const isEmptyChunk = isEmptyContent && !hasToolCallChunks;\n",
        "-    const chunkId = chunk.id ?? '';\n",
        "-    if (isEmptyChunk && chunkId && chunkId.startsWith('msg')) {\n",
        "-      if (graph.messageIdsByStepKey.has(chunkId)) {\n",
        "-        return;\n",
        "-      } else if (graph.prelimMessageIdsByStepKey.has(chunkId)) {\n",
        "-        return;\n",
        "-      }\n",
        " \n",
        "+    /** Set a preliminary message ID if found in empty chunk */\n",
        "+    const isEmptyChunk = isEmptyContent && !hasToolCallChunks;\n",
        "+    if (\n",
        "+      isEmptyChunk &&\n",
        "+      (chunk.id ?? '') !== '' &&\n",
        "+      !graph.prelimMessageIdsByStepKey.has(chunk.id ?? '')\n",
        "+    ) {\n",
        "       const stepKey = graph.getStepKey(metadata);\n",
        "-      graph.prelimMessageIdsByStepKey.set(stepKey, chunkId);\n",
        "-      return;\n",
        "+      graph.prelimMessageIdsByStepKey.set(stepKey, chunk.id ?? '');\n",
        "     } else if (isEmptyChunk) {\n",
        "       return;\n",
        "     }\n",
        "@@ -185,7 +187,7 @@ export class ChatModelStreamHandler implements t.EventHandler {\n",
        "       chunk.tool_call_chunks.length &&\n",
        "       typeof chunk.tool_call_chunks[0]?.index === 'number'\n",
        "     ) {\n",
        "-      handleToolCallChunks({\n",
        "+      await handleToolCallChunks({\n",
        "         graph,\n",
        "         stepKey,\n",
        "         toolCallChunks: chunk.tool_call_chunks,\n",
        "@@ -198,7 +200,7 @@ export class ChatModelStreamHandler implements t.EventHandler {\n",
        " \n",
        "     const message_id = getMessageId(stepKey, graph) ?? '';\n",
        "     if (message_id) {\n",
        "-      graph.dispatchRunStep(stepKey, {\n",
        "+      await graph.dispatchRunStep(stepKey, {\n",
        "         type: StepTypes.MESSAGE_CREATION,\n",
        "         message_creation: {\n",
        "           message_id,\n",
        "@@ -236,19 +238,19 @@ hasToolCallChunks: ${hasToolCallChunks}\n",
        "     ) {\n",
        "       return;\n",
        "     } else if (typeof content === 'string') {\n",
        "-      if (graph.currentTokenType === ContentTypes.TEXT) {\n",
        "-        graph.dispatchMessageDelta(stepId, {\n",
        "+      if (agentContext.currentTokenType === ContentTypes.TEXT) {\n",
        "+        await graph.dispatchMessageDelta(stepId, {\n",
        "           content: [\n",
        "             {\n",
        "               type: ContentTypes.TEXT,\n",
        "               text: content,\n",
        "             },\n",
        "           ],\n",
        "         });\n",
        "-      } else if (graph.currentTokenType === 'think_and_text') {\n",
        "+      } else if (agentContext.currentTokenType === 'think_and_text') {\n",
        "         const { text, thinking } = parseThinkingContent(content);\n",
        "         if (thinking) {\n",
        "-          graph.dispatchReasoningDelta(stepId, {\n",
        "+          await graph.dispatchReasoningDelta(stepId, {\n",
        "             content: [\n",
        "               {\n",
        "                 type: ContentTypes.THINK,\n",
        "@@ -258,19 +260,19 @@ hasToolCallChunks: ${hasToolCallChunks}\n",
        "           });\n",
        "         }\n",
        "         if (text) {\n",
        "-          graph.currentTokenType = ContentTypes.TEXT;\n",
        "-          graph.tokenTypeSwitch = 'content';\n",
        "+          agentContext.currentTokenType = ContentTypes.TEXT;\n",
        "+          agentContext.tokenTypeSwitch = 'content';\n",
        "           const newStepKey = graph.getStepKey(metadata);\n",
        "           const message_id = getMessageId(newStepKey, graph) ?? '';\n",
        "-          graph.dispatchRunStep(newStepKey, {\n",
        "+          await graph.dispatchRunStep(newStepKey, {\n",
        "             type: StepTypes.MESSAGE_CREATION,\n",
        "             message_creation: {\n",
        "               message_id,\n",
        "             },\n",
        "           });\n",
        " \n",
        "           const newStepId = graph.getStepIdByKey(newStepKey);\n",
        "-          graph.dispatchMessageDelta(newStepId, {\n",
        "+          await graph.dispatchMessageDelta(newStepId, {\n",
        "             content: [\n",
        "               {\n",
        "                 type: ContentTypes.TEXT,\n",
        "@@ -280,7 +282,7 @@ hasToolCallChunks: ${hasToolCallChunks}\n",
        "           });\n",
        "         }\n",
        "       } else {\n",
        "-        graph.dispatchReasoningDelta(stepId, {\n",
        "+        await graph.dispatchReasoningDelta(stepId, {\n",
        "           content: [\n",
        "             {\n",
        "               type: ContentTypes.THINK,\n",
        "@@ -292,7 +294,7 @@ hasToolCallChunks: ${hasToolCallChunks}\n",
        "     } else if (\n",
        "       content.every((c) => c.type?.startsWith(ContentTypes.TEXT) ?? false)\n",
        "     ) {\n",
        "-      graph.dispatchMessageDelta(stepId, {\n",
        "+      await graph.dispatchMessageDelta(stepId, {\n",
        "         content,\n",
        "       });\n",
        "     } else if (\n",
        "@@ -303,7 +305,7 @@ hasToolCallChunks: ${hasToolCallChunks}\n",
        "           (c.type?.startsWith(ContentTypes.REASONING_CONTENT) ?? false)\n",
        "       )\n",
        "     ) {\n",
        "-      graph.dispatchReasoningDelta(stepId, {\n",
        "+      await graph.dispatchReasoningDelta(stepId, {\n",
        "         content: content.map((c) => ({\n",
        "           type: ContentTypes.THINK,\n",
        "           think:\n",
        "@@ -317,13 +319,11 @@ hasToolCallChunks: ${hasToolCallChunks}\n",
        "   }\n",
        "   handleReasoning(\n",
        "     chunk: Partial<AIMessageChunk>,\n",
        "-    graph: Graph,\n",
        "-    provider?: Providers\n",
        "+    agentContext: AgentContext\n",
        "   ): void {\n",
        "-    let reasoning_content = chunk.additional_kwargs?.[graph.reasoningKey] as\n",
        "-      | string\n",
        "-      | Partial<ChatOpenAIReasoningSummary>\n",
        "-      | undefined;\n",
        "+    let reasoning_content = chunk.additional_kwargs?.[\n",
        "+      agentContext.reasoningKey\n",
        "+    ] as string | Partial<ChatOpenAIReasoningSummary> | undefined;\n",
        "     if (\n",
        "       Array.isArray(chunk.content) &&\n",
        "       (chunk.content[0]?.type === ContentTypes.THINKING ||\n",
        "@@ -332,7 +332,8 @@ hasToolCallChunks: ${hasToolCallChunks}\n",
        "     ) {\n",
        "       reasoning_content = 'valid';\n",
        "     } else if (\n",
        "-      (provider === Providers.OPENAI || provider === Providers.AZURE) &&\n",
        "+      (agentContext.provider === Providers.OPENAI ||\n",
        "+        agentContext.provider === Providers.AZURE) &&\n",
        "       reasoning_content != null &&\n",
        "       typeof reasoning_content !== 'string' &&\n",
        "       reasoning_content.summary?.[0]?.text != null &&\n",
        "@@ -347,43 +348,43 @@ hasToolCallChunks: ${hasToolCallChunks}\n",
        "         chunk.content === '' ||\n",
        "         reasoning_content === 'valid')\n",
        "     ) {\n",
        "-      graph.currentTokenType = ContentTypes.THINK;\n",
        "-      graph.tokenTypeSwitch = 'reasoning';\n",
        "+      agentContext.currentTokenType = ContentTypes.THINK;\n",
        "+      agentContext.tokenTypeSwitch = 'reasoning';\n",
        "       return;\n",
        "     } else if (\n",
        "-      graph.tokenTypeSwitch === 'reasoning' &&\n",
        "-      graph.currentTokenType !== ContentTypes.TEXT &&\n",
        "+      agentContext.tokenTypeSwitch === 'reasoning' &&\n",
        "+      agentContext.currentTokenType !== ContentTypes.TEXT &&\n",
        "       ((chunk.content != null && chunk.content !== '') ||\n",
        "         (chunk.tool_calls?.length ?? 0) > 0)\n",
        "     ) {\n",
        "-      graph.currentTokenType = ContentTypes.TEXT;\n",
        "-      graph.tokenTypeSwitch = 'content';\n",
        "+      agentContext.currentTokenType = ContentTypes.TEXT;\n",
        "+      agentContext.tokenTypeSwitch = 'content';\n",
        "     } else if (\n",
        "       chunk.content != null &&\n",
        "       typeof chunk.content === 'string' &&\n",
        "       chunk.content.includes('<think>') &&\n",
        "       chunk.content.includes('</think>')\n",
        "     ) {\n",
        "-      graph.currentTokenType = 'think_and_text';\n",
        "-      graph.tokenTypeSwitch = 'content';\n",
        "+      agentContext.currentTokenType = 'think_and_text';\n",
        "+      agentContext.tokenTypeSwitch = 'content';\n",
        "     } else if (\n",
        "       chunk.content != null &&\n",
        "       typeof chunk.content === 'string' &&\n",
        "       chunk.content.includes('<think>')\n",
        "     ) {\n",
        "-      graph.currentTokenType = ContentTypes.THINK;\n",
        "-      graph.tokenTypeSwitch = 'content';\n",
        "+      agentContext.currentTokenType = ContentTypes.THINK;\n",
        "+      agentContext.tokenTypeSwitch = 'content';\n",
        "     } else if (\n",
        "-      graph.lastToken != null &&\n",
        "-      graph.lastToken.includes('</think>')\n",
        "+      agentContext.lastToken != null &&\n",
        "+      agentContext.lastToken.includes('</think>')\n",
        "     ) {\n",
        "-      graph.currentTokenType = ContentTypes.TEXT;\n",
        "-      graph.tokenTypeSwitch = 'content';\n",
        "+      agentContext.currentTokenType = ContentTypes.TEXT;\n",
        "+      agentContext.tokenTypeSwitch = 'content';\n",
        "     }\n",
        "     if (typeof chunk.content !== 'string') {\n",
        "       return;\n",
        "     }\n",
        "-    graph.lastToken = chunk.content;\n",
        "+    agentContext.lastToken = chunk.content;\n",
        "   }\n",
        " }\n",
        " \n"
      ]
    },
    {
      "path": "src/tools/ToolNode.ts",
      "status": "modified",
      "additions": 4,
      "deletions": 4,
      "patch": "@@ -13,7 +13,6 @@ import type { BaseMessage, AIMessage } from '@langchain/core/messages';\n import type { StructuredToolInterface } from '@langchain/core/tools';\n import type * as t from '@/types';\n import { RunnableCallable } from '@/utils';\n-import { GraphNodeKeys } from '@/common';\n \n // eslint-disable-next-line @typescript-eslint/no-explicit-any\n export class ToolNode<T = any> extends RunnableCallable<T, T> {\n@@ -150,10 +149,11 @@ function areToolCallsInvoked(\n   );\n }\n \n-export function toolsCondition(\n+export function toolsCondition<T extends string>(\n   state: BaseMessage[] | typeof MessagesAnnotation.State,\n+  toolNode: T,\n   invokedToolIds?: Set<string>\n-): 'tools' | typeof END {\n+): T | typeof END {\n   const message: AIMessage = Array.isArray(state)\n     ? state[state.length - 1]\n     : state.messages[state.messages.length - 1];\n@@ -163,7 +163,7 @@ export function toolsCondition(\n     (message.tool_calls?.length ?? 0) > 0 &&\n     !areToolCallsInvoked(message, invokedToolIds)\n   ) {\n-    return GraphNodeKeys.TOOLS;\n+    return toolNode;\n   } else {\n     return END;\n   }",
      "patch_lines": [
        "@@ -13,7 +13,6 @@ import type { BaseMessage, AIMessage } from '@langchain/core/messages';\n",
        " import type { StructuredToolInterface } from '@langchain/core/tools';\n",
        " import type * as t from '@/types';\n",
        " import { RunnableCallable } from '@/utils';\n",
        "-import { GraphNodeKeys } from '@/common';\n",
        " \n",
        " // eslint-disable-next-line @typescript-eslint/no-explicit-any\n",
        " export class ToolNode<T = any> extends RunnableCallable<T, T> {\n",
        "@@ -150,10 +149,11 @@ function areToolCallsInvoked(\n",
        "   );\n",
        " }\n",
        " \n",
        "-export function toolsCondition(\n",
        "+export function toolsCondition<T extends string>(\n",
        "   state: BaseMessage[] | typeof MessagesAnnotation.State,\n",
        "+  toolNode: T,\n",
        "   invokedToolIds?: Set<string>\n",
        "-): 'tools' | typeof END {\n",
        "+): T | typeof END {\n",
        "   const message: AIMessage = Array.isArray(state)\n",
        "     ? state[state.length - 1]\n",
        "     : state.messages[state.messages.length - 1];\n",
        "@@ -163,7 +163,7 @@ export function toolsCondition(\n",
        "     (message.tool_calls?.length ?? 0) > 0 &&\n",
        "     !areToolCallsInvoked(message, invokedToolIds)\n",
        "   ) {\n",
        "-    return GraphNodeKeys.TOOLS;\n",
        "+    return toolNode;\n",
        "   } else {\n",
        "     return END;\n",
        "   }\n"
      ]
    },
    {
      "path": "src/tools/handlers.ts",
      "status": "modified",
      "additions": 32,
      "deletions": 27,
      "patch": "@@ -4,7 +4,8 @@ import { nanoid } from 'nanoid';\n import { ToolMessage } from '@langchain/core/messages';\n import type { AnthropicWebSearchResultBlockParam } from '@/llm/anthropic/types';\n import type { ToolCall, ToolCallChunk } from '@langchain/core/messages/tool';\n-import type { Graph } from '@/graphs';\n+import type { MultiAgentGraph, StandardGraph } from '@/graphs';\n+import type { AgentContext } from '@/agents/AgentContext';\n import type * as t from '@/types';\n import {\n   ToolCallTypes,\n@@ -21,15 +22,15 @@ import {\n import { formatResultsForLLM } from '@/tools/search/format';\n import { getMessageId } from '@/messages';\n \n-export function handleToolCallChunks({\n+export async function handleToolCallChunks({\n   graph,\n   stepKey,\n   toolCallChunks,\n }: {\n-  graph: Graph;\n+  graph: StandardGraph | MultiAgentGraph;\n   stepKey: string;\n   toolCallChunks: ToolCallChunk[];\n-}): void {\n+}): Promise<void> {\n   let prevStepId: string;\n   let prevRunStep: t.RunStep | undefined;\n   try {\n@@ -38,7 +39,7 @@ export function handleToolCallChunks({\n   } catch {\n     /** Edge Case: If no previous step exists, create a new message creation step */\n     const message_id = getMessageId(stepKey, graph, true) ?? '';\n-    prevStepId = graph.dispatchRunStep(stepKey, {\n+    prevStepId = await graph.dispatchRunStep(stepKey, {\n       type: StepTypes.MESSAGE_CREATION,\n       message_creation: {\n         message_id,\n@@ -81,7 +82,7 @@ export function handleToolCallChunks({\n     prevRunStep?.type === StepTypes.MESSAGE_CREATION &&\n     graph.messageStepHasToolCalls.has(prevStepId);\n   if (!alreadyDispatched && tool_calls?.length === toolCallChunks.length) {\n-    graph.dispatchMessageDelta(prevStepId, {\n+    await graph.dispatchMessageDelta(prevStepId, {\n       content: [\n         {\n           type: ContentTypes.TEXT,\n@@ -91,22 +92,22 @@ export function handleToolCallChunks({\n       ],\n     });\n     graph.messageStepHasToolCalls.set(prevStepId, true);\n-    stepId = graph.dispatchRunStep(stepKey, {\n+    stepId = await graph.dispatchRunStep(stepKey, {\n       type: StepTypes.TOOL_CALLS,\n       tool_calls,\n     });\n   }\n-  graph.dispatchRunStepDelta(stepId, {\n+  await graph.dispatchRunStepDelta(stepId, {\n     type: StepTypes.TOOL_CALLS,\n     tool_calls: toolCallChunks,\n   });\n }\n \n-export const handleToolCalls = (\n+export const handleToolCalls = async (\n   toolCalls?: ToolCall[],\n   metadata?: Record<string, unknown>,\n-  graph?: Graph\n-): void => {\n+  graph?: StandardGraph | MultiAgentGraph\n+): Promise<void> => {\n   if (!graph || !metadata) {\n     console.warn(`Graph or metadata not found in ${event} event`);\n     return;\n@@ -138,8 +139,10 @@ export const handleToolCalls = (\n       // no previous step\n     }\n \n-    const dispatchToolCallIds = (lastMessageStepId: string): void => {\n-      graph.dispatchMessageDelta(lastMessageStepId, {\n+    const dispatchToolCallIds = async (\n+      lastMessageStepId: string\n+    ): Promise<void> => {\n+      await graph.dispatchMessageDelta(lastMessageStepId, {\n         content: [\n           {\n             type: 'text',\n@@ -155,25 +158,25 @@ export const handleToolCalls = (\n       prevRunStep &&\n       prevRunStep.type === StepTypes.MESSAGE_CREATION\n     ) {\n-      dispatchToolCallIds(prevStepId);\n+      await dispatchToolCallIds(prevStepId);\n       graph.messageStepHasToolCalls.set(prevStepId, true);\n       /* If the previous step doesn't exist or is not a message creation */\n     } else if (\n       !prevRunStep ||\n       prevRunStep.type !== StepTypes.MESSAGE_CREATION\n     ) {\n       const messageId = getMessageId(stepKey, graph, true) ?? '';\n-      const stepId = graph.dispatchRunStep(stepKey, {\n+      const stepId = await graph.dispatchRunStep(stepKey, {\n         type: StepTypes.MESSAGE_CREATION,\n         message_creation: {\n           message_id: messageId,\n         },\n       });\n-      dispatchToolCallIds(stepId);\n+      await dispatchToolCallIds(stepId);\n       graph.messageStepHasToolCalls.set(prevStepId, true);\n     }\n \n-    graph.dispatchRunStep(stepKey, {\n+    await graph.dispatchRunStep(stepKey, {\n       type: StepTypes.TOOL_CALLS,\n       tool_calls: [tool_call],\n     });\n@@ -193,17 +196,19 @@ export const toolResultTypes = new Set([\n  * Handles the result of a server tool call; in other words, a provider's built-in tool.\n  * As of 2025-07-06, only Anthropic handles server tool calls with this pattern.\n  */\n-export function handleServerToolResult({\n+export async function handleServerToolResult({\n+  graph,\n   content,\n   metadata,\n-  graph,\n+  agentContext,\n }: {\n+  graph: StandardGraph | MultiAgentGraph;\n   content?: string | t.MessageContentComplex[];\n   metadata?: Record<string, unknown>;\n-  graph: Graph;\n-}): boolean {\n+  agentContext?: AgentContext;\n+}): Promise<boolean> {\n   let skipHandling = false;\n-  if (metadata?.provider !== Providers.ANTHROPIC) {\n+  if (agentContext?.provider !== Providers.ANTHROPIC) {\n     return skipHandling;\n   }\n   if (\n@@ -256,7 +261,7 @@ export function handleServerToolResult({\n       contentPart.type === 'web_search_result' ||\n       contentPart.type === 'web_search_tool_result'\n     ) {\n-      handleAnthropicSearchResults({\n+      await handleAnthropicSearchResults({\n         contentPart: contentPart as t.ToolResultContent,\n         toolCall,\n         metadata,\n@@ -272,7 +277,7 @@ export function handleServerToolResult({\n   return skipHandling;\n }\n \n-function handleAnthropicSearchResults({\n+async function handleAnthropicSearchResults({\n   contentPart,\n   toolCall,\n   metadata,\n@@ -281,8 +286,8 @@ function handleAnthropicSearchResults({\n   contentPart: t.ToolResultContent;\n   toolCall: Partial<ToolCall>;\n   metadata?: Record<string, unknown>;\n-  graph: Graph;\n-}): void {\n+  graph: StandardGraph | MultiAgentGraph;\n+}): Promise<void> {\n   if (!Array.isArray(contentPart.content)) {\n     console.warn(\n       `Expected content to be an array, got ${typeof contentPart.content}`\n@@ -324,7 +329,7 @@ function handleAnthropicSearchResults({\n     input,\n     output,\n   };\n-  graph.handlerRegistry\n+  await graph.handlerRegistry\n     ?.getHandler(GraphEvents.TOOL_END)\n     ?.handle(GraphEvents.TOOL_END, toolEndData, metadata, graph);\n ",
      "patch_lines": [
        "@@ -4,7 +4,8 @@ import { nanoid } from 'nanoid';\n",
        " import { ToolMessage } from '@langchain/core/messages';\n",
        " import type { AnthropicWebSearchResultBlockParam } from '@/llm/anthropic/types';\n",
        " import type { ToolCall, ToolCallChunk } from '@langchain/core/messages/tool';\n",
        "-import type { Graph } from '@/graphs';\n",
        "+import type { MultiAgentGraph, StandardGraph } from '@/graphs';\n",
        "+import type { AgentContext } from '@/agents/AgentContext';\n",
        " import type * as t from '@/types';\n",
        " import {\n",
        "   ToolCallTypes,\n",
        "@@ -21,15 +22,15 @@ import {\n",
        " import { formatResultsForLLM } from '@/tools/search/format';\n",
        " import { getMessageId } from '@/messages';\n",
        " \n",
        "-export function handleToolCallChunks({\n",
        "+export async function handleToolCallChunks({\n",
        "   graph,\n",
        "   stepKey,\n",
        "   toolCallChunks,\n",
        " }: {\n",
        "-  graph: Graph;\n",
        "+  graph: StandardGraph | MultiAgentGraph;\n",
        "   stepKey: string;\n",
        "   toolCallChunks: ToolCallChunk[];\n",
        "-}): void {\n",
        "+}): Promise<void> {\n",
        "   let prevStepId: string;\n",
        "   let prevRunStep: t.RunStep | undefined;\n",
        "   try {\n",
        "@@ -38,7 +39,7 @@ export function handleToolCallChunks({\n",
        "   } catch {\n",
        "     /** Edge Case: If no previous step exists, create a new message creation step */\n",
        "     const message_id = getMessageId(stepKey, graph, true) ?? '';\n",
        "-    prevStepId = graph.dispatchRunStep(stepKey, {\n",
        "+    prevStepId = await graph.dispatchRunStep(stepKey, {\n",
        "       type: StepTypes.MESSAGE_CREATION,\n",
        "       message_creation: {\n",
        "         message_id,\n",
        "@@ -81,7 +82,7 @@ export function handleToolCallChunks({\n",
        "     prevRunStep?.type === StepTypes.MESSAGE_CREATION &&\n",
        "     graph.messageStepHasToolCalls.has(prevStepId);\n",
        "   if (!alreadyDispatched && tool_calls?.length === toolCallChunks.length) {\n",
        "-    graph.dispatchMessageDelta(prevStepId, {\n",
        "+    await graph.dispatchMessageDelta(prevStepId, {\n",
        "       content: [\n",
        "         {\n",
        "           type: ContentTypes.TEXT,\n",
        "@@ -91,22 +92,22 @@ export function handleToolCallChunks({\n",
        "       ],\n",
        "     });\n",
        "     graph.messageStepHasToolCalls.set(prevStepId, true);\n",
        "-    stepId = graph.dispatchRunStep(stepKey, {\n",
        "+    stepId = await graph.dispatchRunStep(stepKey, {\n",
        "       type: StepTypes.TOOL_CALLS,\n",
        "       tool_calls,\n",
        "     });\n",
        "   }\n",
        "-  graph.dispatchRunStepDelta(stepId, {\n",
        "+  await graph.dispatchRunStepDelta(stepId, {\n",
        "     type: StepTypes.TOOL_CALLS,\n",
        "     tool_calls: toolCallChunks,\n",
        "   });\n",
        " }\n",
        " \n",
        "-export const handleToolCalls = (\n",
        "+export const handleToolCalls = async (\n",
        "   toolCalls?: ToolCall[],\n",
        "   metadata?: Record<string, unknown>,\n",
        "-  graph?: Graph\n",
        "-): void => {\n",
        "+  graph?: StandardGraph | MultiAgentGraph\n",
        "+): Promise<void> => {\n",
        "   if (!graph || !metadata) {\n",
        "     console.warn(`Graph or metadata not found in ${event} event`);\n",
        "     return;\n",
        "@@ -138,8 +139,10 @@ export const handleToolCalls = (\n",
        "       // no previous step\n",
        "     }\n",
        " \n",
        "-    const dispatchToolCallIds = (lastMessageStepId: string): void => {\n",
        "-      graph.dispatchMessageDelta(lastMessageStepId, {\n",
        "+    const dispatchToolCallIds = async (\n",
        "+      lastMessageStepId: string\n",
        "+    ): Promise<void> => {\n",
        "+      await graph.dispatchMessageDelta(lastMessageStepId, {\n",
        "         content: [\n",
        "           {\n",
        "             type: 'text',\n",
        "@@ -155,25 +158,25 @@ export const handleToolCalls = (\n",
        "       prevRunStep &&\n",
        "       prevRunStep.type === StepTypes.MESSAGE_CREATION\n",
        "     ) {\n",
        "-      dispatchToolCallIds(prevStepId);\n",
        "+      await dispatchToolCallIds(prevStepId);\n",
        "       graph.messageStepHasToolCalls.set(prevStepId, true);\n",
        "       /* If the previous step doesn't exist or is not a message creation */\n",
        "     } else if (\n",
        "       !prevRunStep ||\n",
        "       prevRunStep.type !== StepTypes.MESSAGE_CREATION\n",
        "     ) {\n",
        "       const messageId = getMessageId(stepKey, graph, true) ?? '';\n",
        "-      const stepId = graph.dispatchRunStep(stepKey, {\n",
        "+      const stepId = await graph.dispatchRunStep(stepKey, {\n",
        "         type: StepTypes.MESSAGE_CREATION,\n",
        "         message_creation: {\n",
        "           message_id: messageId,\n",
        "         },\n",
        "       });\n",
        "-      dispatchToolCallIds(stepId);\n",
        "+      await dispatchToolCallIds(stepId);\n",
        "       graph.messageStepHasToolCalls.set(prevStepId, true);\n",
        "     }\n",
        " \n",
        "-    graph.dispatchRunStep(stepKey, {\n",
        "+    await graph.dispatchRunStep(stepKey, {\n",
        "       type: StepTypes.TOOL_CALLS,\n",
        "       tool_calls: [tool_call],\n",
        "     });\n",
        "@@ -193,17 +196,19 @@ export const toolResultTypes = new Set([\n",
        "  * Handles the result of a server tool call; in other words, a provider's built-in tool.\n",
        "  * As of 2025-07-06, only Anthropic handles server tool calls with this pattern.\n",
        "  */\n",
        "-export function handleServerToolResult({\n",
        "+export async function handleServerToolResult({\n",
        "+  graph,\n",
        "   content,\n",
        "   metadata,\n",
        "-  graph,\n",
        "+  agentContext,\n",
        " }: {\n",
        "+  graph: StandardGraph | MultiAgentGraph;\n",
        "   content?: string | t.MessageContentComplex[];\n",
        "   metadata?: Record<string, unknown>;\n",
        "-  graph: Graph;\n",
        "-}): boolean {\n",
        "+  agentContext?: AgentContext;\n",
        "+}): Promise<boolean> {\n",
        "   let skipHandling = false;\n",
        "-  if (metadata?.provider !== Providers.ANTHROPIC) {\n",
        "+  if (agentContext?.provider !== Providers.ANTHROPIC) {\n",
        "     return skipHandling;\n",
        "   }\n",
        "   if (\n",
        "@@ -256,7 +261,7 @@ export function handleServerToolResult({\n",
        "       contentPart.type === 'web_search_result' ||\n",
        "       contentPart.type === 'web_search_tool_result'\n",
        "     ) {\n",
        "-      handleAnthropicSearchResults({\n",
        "+      await handleAnthropicSearchResults({\n",
        "         contentPart: contentPart as t.ToolResultContent,\n",
        "         toolCall,\n",
        "         metadata,\n",
        "@@ -272,7 +277,7 @@ export function handleServerToolResult({\n",
        "   return skipHandling;\n",
        " }\n",
        " \n",
        "-function handleAnthropicSearchResults({\n",
        "+async function handleAnthropicSearchResults({\n",
        "   contentPart,\n",
        "   toolCall,\n",
        "   metadata,\n",
        "@@ -281,8 +286,8 @@ function handleAnthropicSearchResults({\n",
        "   contentPart: t.ToolResultContent;\n",
        "   toolCall: Partial<ToolCall>;\n",
        "   metadata?: Record<string, unknown>;\n",
        "-  graph: Graph;\n",
        "-}): void {\n",
        "+  graph: StandardGraph | MultiAgentGraph;\n",
        "+}): Promise<void> {\n",
        "   if (!Array.isArray(contentPart.content)) {\n",
        "     console.warn(\n",
        "       `Expected content to be an array, got ${typeof contentPart.content}`\n",
        "@@ -324,7 +329,7 @@ function handleAnthropicSearchResults({\n",
        "     input,\n",
        "     output,\n",
        "   };\n",
        "-  graph.handlerRegistry\n",
        "+  await graph.handlerRegistry\n",
        "     ?.getHandler(GraphEvents.TOOL_END)\n",
        "     ?.handle(GraphEvents.TOOL_END, toolEndData, metadata, graph);\n",
        " \n"
      ]
    },
    {
      "path": "src/types/graph.ts",
      "status": "modified",
      "additions": 159,
      "deletions": 20,
      "patch": "@@ -1,39 +1,87 @@\n // src/types/graph.ts\n import type {\n-  StateGraphArgs,\n+  START,\n+  StateType,\n+  UpdateType,\n   StateGraph,\n+  StateGraphArgs,\n+  StateDefinition,\n   CompiledStateGraph,\n+  BinaryOperatorAggregate,\n } from '@langchain/langgraph';\n import type { BindToolsInput } from '@langchain/core/language_models/chat_models';\n-import type { BaseMessage, AIMessageChunk } from '@langchain/core/messages';\n+import type {\n+  BaseMessage,\n+  AIMessageChunk,\n+  SystemMessage,\n+} from '@langchain/core/messages';\n+import type { RunnableConfig, Runnable } from '@langchain/core/runnables';\n import type { ChatGenerationChunk } from '@langchain/core/outputs';\n import type { GoogleAIToolType } from '@langchain/google-common';\n-import type { RunnableConfig } from '@langchain/core/runnables';\n-import type { ToolMap, GenericTool } from '@/types/tools';\n+import type { ToolMap, ToolEndEvent, GenericTool } from '@/types/tools';\n+import type { Providers, Callback, GraphNodeKeys } from '@/common';\n+import type { StandardGraph, MultiAgentGraph } from '@/graphs';\n import type { ClientOptions } from '@/types/llm';\n-import type { Providers } from '@/common';\n-import type { Graph } from '@/graphs';\n-// import type { RunnableConfig } from '@langchain/core/runnables';\n+import type {\n+  RunStep,\n+  RunStepDeltaEvent,\n+  MessageDeltaEvent,\n+  ReasoningDeltaEvent,\n+} from '@/types/stream';\n+import type { TokenCounter } from '@/types/run';\n+\n+/** Interface for bound model with stream and invoke methods */\n+export interface ChatModel {\n+  stream?: (\n+    messages: BaseMessage[],\n+    config?: RunnableConfig\n+  ) => Promise<AsyncIterable<AIMessageChunk>>;\n+  invoke: (\n+    messages: BaseMessage[],\n+    config?: RunnableConfig\n+  ) => Promise<AIMessageChunk>;\n+}\n+\n+export type GraphNode = GraphNodeKeys | typeof START;\n+export type ClientCallback<T extends unknown[]> = (\n+  graph: StandardGraph,\n+  ...args: T\n+) => void;\n+\n+export type ClientCallbacks = {\n+  [Callback.TOOL_ERROR]?: ClientCallback<[Error, string]>;\n+  [Callback.TOOL_START]?: ClientCallback<unknown[]>;\n+  [Callback.TOOL_END]?: ClientCallback<unknown[]>;\n+};\n+\n+export type SystemCallbacks = {\n+  [K in keyof ClientCallbacks]: ClientCallbacks[K] extends ClientCallback<\n+    infer Args\n+  >\n+    ? (...args: Args) => void\n+    : never;\n+};\n \n export type BaseGraphState = {\n   messages: BaseMessage[];\n-  // [key: string]: unknown;\n };\n \n export type IState = BaseGraphState;\n \n-// export interface IState extends BaseGraphState {\n-//   instructions?: string;\n-//   additional_instructions?: string;\n-// }\n-\n export interface EventHandler {\n   handle(\n     event: string,\n-    data: StreamEventData | ModelEndData,\n+    data:\n+      | StreamEventData\n+      | ModelEndData\n+      | RunStep\n+      | RunStepDeltaEvent\n+      | MessageDeltaEvent\n+      | ReasoningDeltaEvent\n+      | { result: ToolEndEvent },\n     metadata?: Record<string, unknown>,\n-    graph?: Graph\n-  ): void;\n+    graph?: StandardGraph | MultiAgentGraph\n+  ): void | Promise<void>;\n }\n \n export type GraphStateChannels<T extends BaseGraphState> =\n@@ -51,6 +99,65 @@ export type CompiledWorkflow<\n   N extends string = string,\n > = CompiledStateGraph<T, U, N>;\n \n+export type CompiledStateWorkflow = CompiledStateGraph<\n+  StateType<{\n+    messages: BinaryOperatorAggregate<BaseMessage[], BaseMessage[]>;\n+  }>,\n+  UpdateType<{\n+    messages: BinaryOperatorAggregate<BaseMessage[], BaseMessage[]>;\n+  }>,\n+  string,\n+  {\n+    messages: BinaryOperatorAggregate<BaseMessage[], BaseMessage[]>;\n+  },\n+  {\n+    messages: BinaryOperatorAggregate<BaseMessage[], BaseMessage[]>;\n+  },\n+  StateDefinition\n+>;\n+\n+export type CompiledAgentWorfklow = CompiledStateGraph<\n+  {\n+    messages: BaseMessage[];\n+  },\n+  {\n+    messages?: BaseMessage[] | undefined;\n+  },\n+  '__start__' | `agent=${string}` | `tools=${string}`,\n+  {\n+    messages: BinaryOperatorAggregate<BaseMessage[], BaseMessage[]>;\n+  },\n+  {\n+    messages: BinaryOperatorAggregate<BaseMessage[], BaseMessage[]>;\n+  },\n+  StateDefinition,\n+  {\n+    [x: `agent=${string}`]: Partial<BaseGraphState>;\n+    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+    [x: `tools=${string}`]: any;\n+  }\n+>;\n+\n+export type SystemRunnable =\n+  | Runnable<\n+      BaseMessage[],\n+      (BaseMessage | SystemMessage)[],\n+      RunnableConfig<Record<string, unknown>>\n+    >\n+  | undefined;\n+\n+/**\n+ * Optional compile options passed to workflow.compile().\n+ * These are intentionally untyped to avoid coupling to library internals.\n+ */\n+export type CompileOptions = {\n+  // A checkpointer instance (e.g., MemorySaver, SQL saver)\n+  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+  checkpointer?: any;\n+  interruptBefore?: string[];\n+  interruptAfter?: string[];\n+};\n+\n export type EventStreamCallbackHandlerInput =\n   Parameters<CompiledWorkflow['streamEvents']>[2] extends Omit<\n     infer T,\n@@ -100,6 +207,10 @@ export type StreamEventData = {\n    * Custom result from the runnable that generated the event.\n    */\n   result?: unknown;\n+  /**\n+   * Custom field to indicate the event was manually emitted, and may have been handled already\n+   */\n+  emitted?: boolean;\n };\n \n /**\n@@ -172,14 +283,42 @@ export type ModelEndData =\n export type GraphTools = GenericTool[] | BindToolsInput[] | GoogleAIToolType[];\n export type StandardGraphInput = {\n   runId?: string;\n+  signal?: AbortSignal;\n+  agents: AgentInputs[];\n+  tokenCounter?: TokenCounter;\n+  indexTokenCountMap?: Record<string, number>;\n+};\n+\n+export type GraphEdge = {\n+  /** Use a list for multiple sources */\n+  from: string | string[];\n+  /** Use a list for multiple destinations */\n+  to: string | string[];\n+  description?: string;\n+  /** Can return boolean or specific destination(s) */\n+  condition?: (state: BaseGraphState) => boolean | string | string[];\n+  /** 'handoff' creates tools for dynamic routing, 'direct' creates direct edges, which also allow parallel execution */\n+  edgeType?: 'handoff' | 'direct';\n+  /** Optional prompt to add when transitioning through this edge */\n+  promptInstructions?:\n+    | string\n+    | ((messages: BaseMessage[]) => string | undefined);\n+};\n+\n+export type MultiAgentGraphInput = StandardGraphInput & {\n+  edges: GraphEdge[];\n+};\n+\n+export interface AgentInputs {\n+  agentId: string;\n   toolEnd?: boolean;\n   toolMap?: ToolMap;\n+  tools?: GraphTools;\n   provider: Providers;\n-  signal?: AbortSignal;\n   instructions?: string;\n   streamBuffer?: number;\n-  clientOptions: ClientOptions;\n+  maxContextTokens?: number;\n+  clientOptions?: ClientOptions;\n   additional_instructions?: string;\n   reasoningKey?: 'reasoning_content' | 'reasoning';\n-  tools?: GraphTools;\n-};\n+}",
      "patch_lines": [
        "@@ -1,39 +1,87 @@\n",
        " // src/types/graph.ts\n",
        " import type {\n",
        "-  StateGraphArgs,\n",
        "+  START,\n",
        "+  StateType,\n",
        "+  UpdateType,\n",
        "   StateGraph,\n",
        "+  StateGraphArgs,\n",
        "+  StateDefinition,\n",
        "   CompiledStateGraph,\n",
        "+  BinaryOperatorAggregate,\n",
        " } from '@langchain/langgraph';\n",
        " import type { BindToolsInput } from '@langchain/core/language_models/chat_models';\n",
        "-import type { BaseMessage, AIMessageChunk } from '@langchain/core/messages';\n",
        "+import type {\n",
        "+  BaseMessage,\n",
        "+  AIMessageChunk,\n",
        "+  SystemMessage,\n",
        "+} from '@langchain/core/messages';\n",
        "+import type { RunnableConfig, Runnable } from '@langchain/core/runnables';\n",
        " import type { ChatGenerationChunk } from '@langchain/core/outputs';\n",
        " import type { GoogleAIToolType } from '@langchain/google-common';\n",
        "-import type { RunnableConfig } from '@langchain/core/runnables';\n",
        "-import type { ToolMap, GenericTool } from '@/types/tools';\n",
        "+import type { ToolMap, ToolEndEvent, GenericTool } from '@/types/tools';\n",
        "+import type { Providers, Callback, GraphNodeKeys } from '@/common';\n",
        "+import type { StandardGraph, MultiAgentGraph } from '@/graphs';\n",
        " import type { ClientOptions } from '@/types/llm';\n",
        "-import type { Providers } from '@/common';\n",
        "-import type { Graph } from '@/graphs';\n",
        "-// import type { RunnableConfig } from '@langchain/core/runnables';\n",
        "+import type {\n",
        "+  RunStep,\n",
        "+  RunStepDeltaEvent,\n",
        "+  MessageDeltaEvent,\n",
        "+  ReasoningDeltaEvent,\n",
        "+} from '@/types/stream';\n",
        "+import type { TokenCounter } from '@/types/run';\n",
        "+\n",
        "+/** Interface for bound model with stream and invoke methods */\n",
        "+export interface ChatModel {\n",
        "+  stream?: (\n",
        "+    messages: BaseMessage[],\n",
        "+    config?: RunnableConfig\n",
        "+  ) => Promise<AsyncIterable<AIMessageChunk>>;\n",
        "+  invoke: (\n",
        "+    messages: BaseMessage[],\n",
        "+    config?: RunnableConfig\n",
        "+  ) => Promise<AIMessageChunk>;\n",
        "+}\n",
        "+\n",
        "+export type GraphNode = GraphNodeKeys | typeof START;\n",
        "+export type ClientCallback<T extends unknown[]> = (\n",
        "+  graph: StandardGraph,\n",
        "+  ...args: T\n",
        "+) => void;\n",
        "+\n",
        "+export type ClientCallbacks = {\n",
        "+  [Callback.TOOL_ERROR]?: ClientCallback<[Error, string]>;\n",
        "+  [Callback.TOOL_START]?: ClientCallback<unknown[]>;\n",
        "+  [Callback.TOOL_END]?: ClientCallback<unknown[]>;\n",
        "+};\n",
        "+\n",
        "+export type SystemCallbacks = {\n",
        "+  [K in keyof ClientCallbacks]: ClientCallbacks[K] extends ClientCallback<\n",
        "+    infer Args\n",
        "+  >\n",
        "+    ? (...args: Args) => void\n",
        "+    : never;\n",
        "+};\n",
        " \n",
        " export type BaseGraphState = {\n",
        "   messages: BaseMessage[];\n",
        "-  // [key: string]: unknown;\n",
        " };\n",
        " \n",
        " export type IState = BaseGraphState;\n",
        " \n",
        "-// export interface IState extends BaseGraphState {\n",
        "-//   instructions?: string;\n",
        "-//   additional_instructions?: string;\n",
        "-// }\n",
        "-\n",
        " export interface EventHandler {\n",
        "   handle(\n",
        "     event: string,\n",
        "-    data: StreamEventData | ModelEndData,\n",
        "+    data:\n",
        "+      | StreamEventData\n",
        "+      | ModelEndData\n",
        "+      | RunStep\n",
        "+      | RunStepDeltaEvent\n",
        "+      | MessageDeltaEvent\n",
        "+      | ReasoningDeltaEvent\n",
        "+      | { result: ToolEndEvent },\n",
        "     metadata?: Record<string, unknown>,\n",
        "-    graph?: Graph\n",
        "-  ): void;\n",
        "+    graph?: StandardGraph | MultiAgentGraph\n",
        "+  ): void | Promise<void>;\n",
        " }\n",
        " \n",
        " export type GraphStateChannels<T extends BaseGraphState> =\n",
        "@@ -51,6 +99,65 @@ export type CompiledWorkflow<\n",
        "   N extends string = string,\n",
        " > = CompiledStateGraph<T, U, N>;\n",
        " \n",
        "+export type CompiledStateWorkflow = CompiledStateGraph<\n",
        "+  StateType<{\n",
        "+    messages: BinaryOperatorAggregate<BaseMessage[], BaseMessage[]>;\n",
        "+  }>,\n",
        "+  UpdateType<{\n",
        "+    messages: BinaryOperatorAggregate<BaseMessage[], BaseMessage[]>;\n",
        "+  }>,\n",
        "+  string,\n",
        "+  {\n",
        "+    messages: BinaryOperatorAggregate<BaseMessage[], BaseMessage[]>;\n",
        "+  },\n",
        "+  {\n",
        "+    messages: BinaryOperatorAggregate<BaseMessage[], BaseMessage[]>;\n",
        "+  },\n",
        "+  StateDefinition\n",
        "+>;\n",
        "+\n",
        "+export type CompiledAgentWorfklow = CompiledStateGraph<\n",
        "+  {\n",
        "+    messages: BaseMessage[];\n",
        "+  },\n",
        "+  {\n",
        "+    messages?: BaseMessage[] | undefined;\n",
        "+  },\n",
        "+  '__start__' | `agent=${string}` | `tools=${string}`,\n",
        "+  {\n",
        "+    messages: BinaryOperatorAggregate<BaseMessage[], BaseMessage[]>;\n",
        "+  },\n",
        "+  {\n",
        "+    messages: BinaryOperatorAggregate<BaseMessage[], BaseMessage[]>;\n",
        "+  },\n",
        "+  StateDefinition,\n",
        "+  {\n",
        "+    [x: `agent=${string}`]: Partial<BaseGraphState>;\n",
        "+    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n",
        "+    [x: `tools=${string}`]: any;\n",
        "+  }\n",
        "+>;\n",
        "+\n",
        "+export type SystemRunnable =\n",
        "+  | Runnable<\n",
        "+      BaseMessage[],\n",
        "+      (BaseMessage | SystemMessage)[],\n",
        "+      RunnableConfig<Record<string, unknown>>\n",
        "+    >\n",
        "+  | undefined;\n",
        "+\n",
        "+/**\n",
        "+ * Optional compile options passed to workflow.compile().\n",
        "+ * These are intentionally untyped to avoid coupling to library internals.\n",
        "+ */\n",
        "+export type CompileOptions = {\n",
        "+  // A checkpointer instance (e.g., MemorySaver, SQL saver)\n",
        "+  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n",
        "+  checkpointer?: any;\n",
        "+  interruptBefore?: string[];\n",
        "+  interruptAfter?: string[];\n",
        "+};\n",
        "+\n",
        " export type EventStreamCallbackHandlerInput =\n",
        "   Parameters<CompiledWorkflow['streamEvents']>[2] extends Omit<\n",
        "     infer T,\n",
        "@@ -100,6 +207,10 @@ export type StreamEventData = {\n",
        "    * Custom result from the runnable that generated the event.\n",
        "    */\n",
        "   result?: unknown;\n",
        "+  /**\n",
        "+   * Custom field to indicate the event was manually emitted, and may have been handled already\n",
        "+   */\n",
        "+  emitted?: boolean;\n",
        " };\n",
        " \n",
        " /**\n",
        "@@ -172,14 +283,42 @@ export type ModelEndData =\n",
        " export type GraphTools = GenericTool[] | BindToolsInput[] | GoogleAIToolType[];\n",
        " export type StandardGraphInput = {\n",
        "   runId?: string;\n",
        "+  signal?: AbortSignal;\n",
        "+  agents: AgentInputs[];\n",
        "+  tokenCounter?: TokenCounter;\n",
        "+  indexTokenCountMap?: Record<string, number>;\n",
        "+};\n",
        "+\n",
        "+export type GraphEdge = {\n",
        "+  /** Use a list for multiple sources */\n",
        "+  from: string | string[];\n",
        "+  /** Use a list for multiple destinations */\n",
        "+  to: string | string[];\n",
        "+  description?: string;\n",
        "+  /** Can return boolean or specific destination(s) */\n",
        "+  condition?: (state: BaseGraphState) => boolean | string | string[];\n",
        "+  /** 'handoff' creates tools for dynamic routing, 'direct' creates direct edges, which also allow parallel execution */\n",
        "+  edgeType?: 'handoff' | 'direct';\n",
        "+  /** Optional prompt to add when transitioning through this edge */\n",
        "+  promptInstructions?:\n",
        "+    | string\n",
        "+    | ((messages: BaseMessage[]) => string | undefined);\n",
        "+};\n",
        "+\n",
        "+export type MultiAgentGraphInput = StandardGraphInput & {\n",
        "+  edges: GraphEdge[];\n",
        "+};\n",
        "+\n",
        "+export interface AgentInputs {\n",
        "+  agentId: string;\n",
        "   toolEnd?: boolean;\n",
        "   toolMap?: ToolMap;\n",
        "+  tools?: GraphTools;\n",
        "   provider: Providers;\n",
        "-  signal?: AbortSignal;\n",
        "   instructions?: string;\n",
        "   streamBuffer?: number;\n",
        "-  clientOptions: ClientOptions;\n",
        "+  maxContextTokens?: number;\n",
        "+  clientOptions?: ClientOptions;\n",
        "   additional_instructions?: string;\n",
        "   reasoningKey?: 'reasoning_content' | 'reasoning';\n",
        "-  tools?: GraphTools;\n",
        "-};\n",
        "+}\n"
      ]
    },
    {
      "path": "src/types/llm.ts",
      "status": "modified",
      "additions": 7,
      "deletions": 3,
      "patch": "@@ -1,6 +1,5 @@\n // src/types/llm.ts\n import { ChatOllama } from '@langchain/ollama';\n-import { ChatAnthropic } from '@langchain/anthropic';\n import { ChatMistralAI } from '@langchain/mistralai';\n import { ChatBedrockConverse } from '@langchain/aws';\n import { BedrockChat } from '@langchain/community/chat_models/bedrock/web';\n@@ -36,6 +35,7 @@ import {\n   ChatXAI,\n } from '@/llm/openai';\n import { CustomChatGoogleGenerativeAI } from '@/llm/google';\n+import { CustomAnthropic } from '@/llm/anthropic';\n import { ChatOpenRouter } from '@/llm/openrouter';\n import { ChatVertexAI } from '@/llm/vertexai';\n import { Providers } from '@/common';\n@@ -95,7 +95,11 @@ export type SharedLLMConfig = {\n   provider: Providers;\n };\n \n-export type LLMConfig = SharedLLMConfig & ClientOptions;\n+export type LLMConfig = SharedLLMConfig &\n+  ClientOptions & {\n+    /** Optional provider fallbacks in order of attempt */\n+    fallbacks?: Array<{ provider: Providers; clientOptions?: ClientOptions }>;\n+  };\n \n export type ProviderOptionsMap = {\n   [Providers.AZURE]: AzureClientOptions;\n@@ -120,7 +124,7 @@ export type ChatModelMap = {\n   [Providers.AZURE]: AzureChatOpenAI;\n   [Providers.DEEPSEEK]: ChatDeepSeek;\n   [Providers.VERTEXAI]: ChatVertexAI;\n-  [Providers.ANTHROPIC]: ChatAnthropic;\n+  [Providers.ANTHROPIC]: CustomAnthropic;\n   [Providers.MISTRALAI]: ChatMistralAI;\n   [Providers.MISTRAL]: ChatMistralAI;\n   [Providers.OPENROUTER]: ChatOpenRouter;",
      "patch_lines": [
        "@@ -1,6 +1,5 @@\n",
        " // src/types/llm.ts\n",
        " import { ChatOllama } from '@langchain/ollama';\n",
        "-import { ChatAnthropic } from '@langchain/anthropic';\n",
        " import { ChatMistralAI } from '@langchain/mistralai';\n",
        " import { ChatBedrockConverse } from '@langchain/aws';\n",
        " import { BedrockChat } from '@langchain/community/chat_models/bedrock/web';\n",
        "@@ -36,6 +35,7 @@ import {\n",
        "   ChatXAI,\n",
        " } from '@/llm/openai';\n",
        " import { CustomChatGoogleGenerativeAI } from '@/llm/google';\n",
        "+import { CustomAnthropic } from '@/llm/anthropic';\n",
        " import { ChatOpenRouter } from '@/llm/openrouter';\n",
        " import { ChatVertexAI } from '@/llm/vertexai';\n",
        " import { Providers } from '@/common';\n",
        "@@ -95,7 +95,11 @@ export type SharedLLMConfig = {\n",
        "   provider: Providers;\n",
        " };\n",
        " \n",
        "-export type LLMConfig = SharedLLMConfig & ClientOptions;\n",
        "+export type LLMConfig = SharedLLMConfig &\n",
        "+  ClientOptions & {\n",
        "+    /** Optional provider fallbacks in order of attempt */\n",
        "+    fallbacks?: Array<{ provider: Providers; clientOptions?: ClientOptions }>;\n",
        "+  };\n",
        " \n",
        " export type ProviderOptionsMap = {\n",
        "   [Providers.AZURE]: AzureClientOptions;\n",
        "@@ -120,7 +124,7 @@ export type ChatModelMap = {\n",
        "   [Providers.AZURE]: AzureChatOpenAI;\n",
        "   [Providers.DEEPSEEK]: ChatDeepSeek;\n",
        "   [Providers.VERTEXAI]: ChatVertexAI;\n",
        "-  [Providers.ANTHROPIC]: ChatAnthropic;\n",
        "+  [Providers.ANTHROPIC]: CustomAnthropic;\n",
        "   [Providers.MISTRALAI]: ChatMistralAI;\n",
        "   [Providers.MISTRAL]: ChatMistralAI;\n",
        "   [Providers.OPENROUTER]: ChatOpenRouter;\n"
      ]
    },
    {
      "path": "src/types/run.ts",
      "status": "modified",
      "additions": 47,
      "deletions": 13,
      "patch": "@@ -7,7 +7,6 @@ import type {\n   BaseCallbackHandler,\n   CallbackHandlerMethods,\n } from '@langchain/core/callbacks/base';\n-import type * as graph from '@/graphs/Graph';\n import type * as s from '@/types/stream';\n import type * as e from '@/common/enum';\n import type * as g from '@/types/graph';\n@@ -16,13 +15,46 @@ import type * as l from '@/types/llm';\n // eslint-disable-next-line @typescript-eslint/no-explicit-any\n export type ZodObjectAny = z.ZodObject<any, any, any, any>;\n export type BaseGraphConfig = {\n-  type?: 'standard';\n   llmConfig: l.LLMConfig;\n   provider?: e.Providers;\n   clientOptions?: l.ClientOptions;\n+  /** Optional compile options for workflow.compile() */\n+  compileOptions?: g.CompileOptions;\n };\n-export type StandardGraphConfig = BaseGraphConfig &\n-  Omit<g.StandardGraphInput, 'provider' | 'clientOptions'>;\n+export type LegacyGraphConfig = BaseGraphConfig & {\n+  type?: 'standard';\n+} & Omit<g.StandardGraphInput, 'provider' | 'clientOptions' | 'agents'> &\n+  Omit<g.AgentInputs, 'provider' | 'clientOptions' | 'agentId'>;\n+\n+/* Supervised graph (opt-in) */\n+export type SupervisedGraphConfig = BaseGraphConfig & {\n+  type: 'supervised';\n+  /** Enable supervised router; when false, fall back to standard loop */\n+  routerEnabled?: boolean;\n+  /** Table-driven routing policy per stage */\n+  routingPolicies?: Array<{\n+    stage: string;\n+    agents?: string[];\n+    model?: e.Providers;\n+    parallel?: boolean;\n+    /** Optional simple condition on content/tools */\n+    when?:\n+      | 'always'\n+      | 'has_tools'\n+      | 'no_tools'\n+      | { includes?: string[]; excludes?: string[] };\n+  }>;\n+  /** Opt-in feature flags */\n+  featureFlags?: {\n+    multi_model_routing?: boolean;\n+    fan_out?: boolean;\n+    fan_out_retries?: number;\n+    fan_out_backoff_ms?: number;\n+    fan_out_concurrency?: number;\n+  };\n+  /** Optional per-stage model configs */\n+  models?: Record<string, l.LLMConfig>;\n+} & Omit<g.StandardGraphInput, 'provider' | 'clientOptions'>;\n \n export type RunTitleOptions = {\n   inputText: string;\n@@ -64,14 +96,20 @@ export type TaskManagerGraphConfig = {\n   supervisorConfig: { systemPrompt?: string; llmConfig: l.LLMConfig };\n };\n \n+export type MultiAgentGraphConfig = {\n+  type: 'multi-agent';\n+  compileOptions?: g.CompileOptions;\n+  agents: g.AgentInputs[];\n+  edges: g.GraphEdge[];\n+};\n+\n export type RunConfig = {\n   runId: string;\n-  graphConfig:\n-    | StandardGraphConfig\n-    | CollaborativeGraphConfig\n-    | TaskManagerGraphConfig;\n+  graphConfig: LegacyGraphConfig | MultiAgentGraphConfig;\n   customHandlers?: Record<string, g.EventHandler>;\n   returnContent?: boolean;\n+  tokenCounter?: TokenCounter;\n+  indexTokenCountMap?: Record<string, number>;\n };\n \n export type ProvidedCallbacks =\n@@ -80,10 +118,6 @@ export type ProvidedCallbacks =\n \n export type TokenCounter = (message: BaseMessage) => number;\n export type EventStreamOptions = {\n-  callbacks?: graph.ClientCallbacks;\n+  callbacks?: g.ClientCallbacks;\n   keepContent?: boolean;\n-  /* Context Management */\n-  maxContextTokens?: number;\n-  tokenCounter?: TokenCounter;\n-  indexTokenCountMap?: Record<string, number>;\n };",
      "patch_lines": [
        "@@ -7,7 +7,6 @@ import type {\n",
        "   BaseCallbackHandler,\n",
        "   CallbackHandlerMethods,\n",
        " } from '@langchain/core/callbacks/base';\n",
        "-import type * as graph from '@/graphs/Graph';\n",
        " import type * as s from '@/types/stream';\n",
        " import type * as e from '@/common/enum';\n",
        " import type * as g from '@/types/graph';\n",
        "@@ -16,13 +15,46 @@ import type * as l from '@/types/llm';\n",
        " // eslint-disable-next-line @typescript-eslint/no-explicit-any\n",
        " export type ZodObjectAny = z.ZodObject<any, any, any, any>;\n",
        " export type BaseGraphConfig = {\n",
        "-  type?: 'standard';\n",
        "   llmConfig: l.LLMConfig;\n",
        "   provider?: e.Providers;\n",
        "   clientOptions?: l.ClientOptions;\n",
        "+  /** Optional compile options for workflow.compile() */\n",
        "+  compileOptions?: g.CompileOptions;\n",
        " };\n",
        "-export type StandardGraphConfig = BaseGraphConfig &\n",
        "-  Omit<g.StandardGraphInput, 'provider' | 'clientOptions'>;\n",
        "+export type LegacyGraphConfig = BaseGraphConfig & {\n",
        "+  type?: 'standard';\n",
        "+} & Omit<g.StandardGraphInput, 'provider' | 'clientOptions' | 'agents'> &\n",
        "+  Omit<g.AgentInputs, 'provider' | 'clientOptions' | 'agentId'>;\n",
        "+\n",
        "+/* Supervised graph (opt-in) */\n",
        "+export type SupervisedGraphConfig = BaseGraphConfig & {\n",
        "+  type: 'supervised';\n",
        "+  /** Enable supervised router; when false, fall back to standard loop */\n",
        "+  routerEnabled?: boolean;\n",
        "+  /** Table-driven routing policy per stage */\n",
        "+  routingPolicies?: Array<{\n",
        "+    stage: string;\n",
        "+    agents?: string[];\n",
        "+    model?: e.Providers;\n",
        "+    parallel?: boolean;\n",
        "+    /** Optional simple condition on content/tools */\n",
        "+    when?:\n",
        "+      | 'always'\n",
        "+      | 'has_tools'\n",
        "+      | 'no_tools'\n",
        "+      | { includes?: string[]; excludes?: string[] };\n",
        "+  }>;\n",
        "+  /** Opt-in feature flags */\n",
        "+  featureFlags?: {\n",
        "+    multi_model_routing?: boolean;\n",
        "+    fan_out?: boolean;\n",
        "+    fan_out_retries?: number;\n",
        "+    fan_out_backoff_ms?: number;\n",
        "+    fan_out_concurrency?: number;\n",
        "+  };\n",
        "+  /** Optional per-stage model configs */\n",
        "+  models?: Record<string, l.LLMConfig>;\n",
        "+} & Omit<g.StandardGraphInput, 'provider' | 'clientOptions'>;\n",
        " \n",
        " export type RunTitleOptions = {\n",
        "   inputText: string;\n",
        "@@ -64,14 +96,20 @@ export type TaskManagerGraphConfig = {\n",
        "   supervisorConfig: { systemPrompt?: string; llmConfig: l.LLMConfig };\n",
        " };\n",
        " \n",
        "+export type MultiAgentGraphConfig = {\n",
        "+  type: 'multi-agent';\n",
        "+  compileOptions?: g.CompileOptions;\n",
        "+  agents: g.AgentInputs[];\n",
        "+  edges: g.GraphEdge[];\n",
        "+};\n",
        "+\n",
        " export type RunConfig = {\n",
        "   runId: string;\n",
        "-  graphConfig:\n",
        "-    | StandardGraphConfig\n",
        "-    | CollaborativeGraphConfig\n",
        "-    | TaskManagerGraphConfig;\n",
        "+  graphConfig: LegacyGraphConfig | MultiAgentGraphConfig;\n",
        "   customHandlers?: Record<string, g.EventHandler>;\n",
        "   returnContent?: boolean;\n",
        "+  tokenCounter?: TokenCounter;\n",
        "+  indexTokenCountMap?: Record<string, number>;\n",
        " };\n",
        " \n",
        " export type ProvidedCallbacks =\n",
        "@@ -80,10 +118,6 @@ export type ProvidedCallbacks =\n",
        " \n",
        " export type TokenCounter = (message: BaseMessage) => number;\n",
        " export type EventStreamOptions = {\n",
        "-  callbacks?: graph.ClientCallbacks;\n",
        "+  callbacks?: g.ClientCallbacks;\n",
        "   keepContent?: boolean;\n",
        "-  /* Context Management */\n",
        "-  maxContextTokens?: number;\n",
        "-  tokenCounter?: TokenCounter;\n",
        "-  indexTokenCountMap?: Record<string, number>;\n",
        " };\n"
      ]
    },
    {
      "path": "src/types/stream.ts",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "patch": "@@ -9,6 +9,7 @@ import type {\n import type { ToolCall, ToolCallChunk } from '@langchain/core/messages/tool';\n import type { LLMResult, Generation } from '@langchain/core/outputs';\n import type { AnthropicContentBlock } from '@/llm/anthropic/types';\n+import type { Command } from '@langchain/langgraph';\n import type { ToolEndEvent } from '@/types/tools';\n import { StepTypes, ContentTypes, GraphEvents } from '@/common/enum';\n \n@@ -104,7 +105,7 @@ export type MessageCreationDetails = {\n \n export type ToolEndData = {\n   input: string | Record<string, unknown>;\n-  output?: ToolMessage;\n+  output?: ToolMessage | Command;\n };\n export type ToolErrorData = {\n   id: string;",
      "patch_lines": [
        "@@ -9,6 +9,7 @@ import type {\n",
        " import type { ToolCall, ToolCallChunk } from '@langchain/core/messages/tool';\n",
        " import type { LLMResult, Generation } from '@langchain/core/outputs';\n",
        " import type { AnthropicContentBlock } from '@/llm/anthropic/types';\n",
        "+import type { Command } from '@langchain/langgraph';\n",
        " import type { ToolEndEvent } from '@/types/tools';\n",
        " import { StepTypes, ContentTypes, GraphEvents } from '@/common/enum';\n",
        " \n",
        "@@ -104,7 +105,7 @@ export type MessageCreationDetails = {\n",
        " \n",
        " export type ToolEndData = {\n",
        "   input: string | Record<string, unknown>;\n",
        "-  output?: ToolMessage;\n",
        "+  output?: ToolMessage | Command;\n",
        " };\n",
        " export type ToolErrorData = {\n",
        "   id: string;\n"
      ]
    },
    {
      "path": "src/utils/events.ts",
      "status": "added",
      "additions": 32,
      "deletions": 0,
      "patch": "@@ -0,0 +1,32 @@\n+/* eslint-disable no-console */\n+// src/utils/events.ts\n+import { dispatchCustomEvent } from '@langchain/core/callbacks/dispatch';\n+import type { RunnableConfig } from '@langchain/core/runnables';\n+\n+/**\n+ * Safely dispatches a custom event and properly awaits it to avoid\n+ * race conditions where events are dispatched after run cleanup.\n+ */\n+export async function safeDispatchCustomEvent(\n+  event: string,\n+  payload: unknown,\n+  config?: RunnableConfig\n+): Promise<void> {\n+  try {\n+    await dispatchCustomEvent(event, payload, config);\n+  } catch (e) {\n+    // Check if this is the known EventStreamCallbackHandler error\n+    if (\n+      e instanceof Error &&\n+      e.message.includes('handleCustomEvent: Run ID') &&\n+      e.message.includes('not found in run map')\n+    ) {\n+      // Suppress this specific error - it's expected during parallel execution\n+      // when EventStreamCallbackHandler loses track of run IDs\n+      // console.debug('Suppressed error dispatching custom event:', e);\n+      return;\n+    }\n+    // Log other errors\n+    console.error('Error dispatching custom event:', e);\n+  }\n+}",
      "patch_lines": [
        "@@ -0,0 +1,32 @@\n",
        "+/* eslint-disable no-console */\n",
        "+// src/utils/events.ts\n",
        "+import { dispatchCustomEvent } from '@langchain/core/callbacks/dispatch';\n",
        "+import type { RunnableConfig } from '@langchain/core/runnables';\n",
        "+\n",
        "+/**\n",
        "+ * Safely dispatches a custom event and properly awaits it to avoid\n",
        "+ * race conditions where events are dispatched after run cleanup.\n",
        "+ */\n",
        "+export async function safeDispatchCustomEvent(\n",
        "+  event: string,\n",
        "+  payload: unknown,\n",
        "+  config?: RunnableConfig\n",
        "+): Promise<void> {\n",
        "+  try {\n",
        "+    await dispatchCustomEvent(event, payload, config);\n",
        "+  } catch (e) {\n",
        "+    // Check if this is the known EventStreamCallbackHandler error\n",
        "+    if (\n",
        "+      e instanceof Error &&\n",
        "+      e.message.includes('handleCustomEvent: Run ID') &&\n",
        "+      e.message.includes('not found in run map')\n",
        "+    ) {\n",
        "+      // Suppress this specific error - it's expected during parallel execution\n",
        "+      // when EventStreamCallbackHandler loses track of run IDs\n",
        "+      // console.debug('Suppressed error dispatching custom event:', e);\n",
        "+      return;\n",
        "+    }\n",
        "+    // Log other errors\n",
        "+    console.error('Error dispatching custom event:', e);\n",
        "+  }\n",
        "+}\n"
      ]
    },
    {
      "path": "src/utils/llmConfig.ts",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "patch": "@@ -7,7 +7,7 @@ export const llmConfigs: Record<string, t.LLMConfig | undefined> = {\n   [Providers.OPENAI]: {\n     provider: Providers.OPENAI,\n     model: 'gpt-4.1',\n-    temperature: 0.7,\n+    // temperature: 0.7,\n     streaming: true,\n     streamUsage: true,\n     // disableStreaming: true,\n@@ -56,7 +56,7 @@ export const llmConfigs: Record<string, t.LLMConfig | undefined> = {\n     provider: Providers.OPENROUTER,\n     streaming: true,\n     streamUsage: true,\n-    model: 'deepseek/deepseek-r1',\n+    model: 'openai/gpt-4.1',\n     openAIApiKey: process.env.OPENROUTER_API_KEY,\n     configuration: {\n       baseURL: process.env.OPENROUTER_BASE_URL,",
      "patch_lines": [
        "@@ -7,7 +7,7 @@ export const llmConfigs: Record<string, t.LLMConfig | undefined> = {\n",
        "   [Providers.OPENAI]: {\n",
        "     provider: Providers.OPENAI,\n",
        "     model: 'gpt-4.1',\n",
        "-    temperature: 0.7,\n",
        "+    // temperature: 0.7,\n",
        "     streaming: true,\n",
        "     streamUsage: true,\n",
        "     // disableStreaming: true,\n",
        "@@ -56,7 +56,7 @@ export const llmConfigs: Record<string, t.LLMConfig | undefined> = {\n",
        "     provider: Providers.OPENROUTER,\n",
        "     streaming: true,\n",
        "     streamUsage: true,\n",
        "-    model: 'deepseek/deepseek-r1',\n",
        "+    model: 'openai/gpt-4.1',\n",
        "     openAIApiKey: process.env.OPENROUTER_API_KEY,\n",
        "     configuration: {\n",
        "       baseURL: process.env.OPENROUTER_BASE_URL,\n"
      ]
    },
    {
      "path": "src/utils/tokens.ts",
      "status": "modified",
      "additions": 69,
      "deletions": 10,
      "patch": "@@ -2,7 +2,10 @@ import { Tiktoken } from 'js-tiktoken/lite';\n import type { BaseMessage } from '@langchain/core/messages';\n import { ContentTypes } from '@/common/enum';\n \n-export function getTokenCountForMessage(message: BaseMessage, getTokenCount: (text: string) => number): number {\n+export function getTokenCountForMessage(\n+  message: BaseMessage,\n+  getTokenCount: (text: string) => number\n+): number {\n   const tokensPerMessage = 3;\n \n   const processValue = (value: unknown): void => {\n@@ -57,14 +60,70 @@ export function getTokenCountForMessage(message: BaseMessage, getTokenCount: (te\n   return numTokens;\n }\n \n-export const createTokenCounter = async () => {\n-  const res = await fetch('https://tiktoken.pages.dev/js/o200k_base.json');\n-  const o200k_base = await res.json();\n+let encoderPromise: Promise<Tiktoken> | undefined;\n+let tokenCounterPromise: Promise<(message: BaseMessage) => number> | undefined;\n \n-  const countTokens = (text: string): number => {\n-    const enc = new Tiktoken(o200k_base);\n-    return enc.encode(text).length;\n-  };\n+async function getSharedEncoder(): Promise<Tiktoken> {\n+  if (encoderPromise) {\n+    return encoderPromise;\n+  }\n+  encoderPromise = (async (): Promise<Tiktoken> => {\n+    const res = await fetch('https://tiktoken.pages.dev/js/o200k_base.json');\n+    const o200k_base = await res.json();\n+    return new Tiktoken(o200k_base);\n+  })();\n+  return encoderPromise;\n+}\n+\n+/**\n+ * Creates a singleton token counter function that reuses the same encoder instance.\n+ * This avoids creating multiple function closures and prevents potential memory issues.\n+ */\n+export const createTokenCounter = async (): Promise<\n+  (message: BaseMessage) => number\n+> => {\n+  if (tokenCounterPromise) {\n+    return tokenCounterPromise;\n+  }\n+\n+  tokenCounterPromise = (async (): Promise<\n+    (message: BaseMessage) => number\n+  > => {\n+    const enc = await getSharedEncoder();\n+    const countTokens = (text: string): number => enc.encode(text).length;\n+    return (message: BaseMessage): number =>\n+      getTokenCountForMessage(message, countTokens);\n+  })();\n+\n+  return tokenCounterPromise;\n+};\n+\n+/**\n+ * Utility to manage the token encoder lifecycle explicitly.\n+ * Useful for applications that need fine-grained control over resource management.\n+ */\n+export const TokenEncoderManager = {\n+  /**\n+   * Pre-initializes the encoder. This can be called during app startup\n+   * to avoid lazy loading delays later.\n+   */\n+  async initialize(): Promise<void> {\n+    await getSharedEncoder();\n+  },\n+\n+  /**\n+   * Clears the cached encoder and token counter.\n+   * Useful for testing or when you need to force a fresh reload.\n+   */\n+  reset(): void {\n+    encoderPromise = undefined;\n+    tokenCounterPromise = undefined;\n+  },\n \n-  return (message: BaseMessage): number => getTokenCountForMessage(message, countTokens);\n-};\n\\ No newline at end of file\n+  /**\n+   * Checks if the encoder has been initialized.\n+   */\n+  isInitialized(): boolean {\n+    return encoderPromise !== undefined;\n+  },\n+};",
      "patch_lines": [
        "@@ -2,7 +2,10 @@ import { Tiktoken } from 'js-tiktoken/lite';\n",
        " import type { BaseMessage } from '@langchain/core/messages';\n",
        " import { ContentTypes } from '@/common/enum';\n",
        " \n",
        "-export function getTokenCountForMessage(message: BaseMessage, getTokenCount: (text: string) => number): number {\n",
        "+export function getTokenCountForMessage(\n",
        "+  message: BaseMessage,\n",
        "+  getTokenCount: (text: string) => number\n",
        "+): number {\n",
        "   const tokensPerMessage = 3;\n",
        " \n",
        "   const processValue = (value: unknown): void => {\n",
        "@@ -57,14 +60,70 @@ export function getTokenCountForMessage(message: BaseMessage, getTokenCount: (te\n",
        "   return numTokens;\n",
        " }\n",
        " \n",
        "-export const createTokenCounter = async () => {\n",
        "-  const res = await fetch('https://tiktoken.pages.dev/js/o200k_base.json');\n",
        "-  const o200k_base = await res.json();\n",
        "+let encoderPromise: Promise<Tiktoken> | undefined;\n",
        "+let tokenCounterPromise: Promise<(message: BaseMessage) => number> | undefined;\n",
        " \n",
        "-  const countTokens = (text: string): number => {\n",
        "-    const enc = new Tiktoken(o200k_base);\n",
        "-    return enc.encode(text).length;\n",
        "-  };\n",
        "+async function getSharedEncoder(): Promise<Tiktoken> {\n",
        "+  if (encoderPromise) {\n",
        "+    return encoderPromise;\n",
        "+  }\n",
        "+  encoderPromise = (async (): Promise<Tiktoken> => {\n",
        "+    const res = await fetch('https://tiktoken.pages.dev/js/o200k_base.json');\n",
        "+    const o200k_base = await res.json();\n",
        "+    return new Tiktoken(o200k_base);\n",
        "+  })();\n",
        "+  return encoderPromise;\n",
        "+}\n",
        "+\n",
        "+/**\n",
        "+ * Creates a singleton token counter function that reuses the same encoder instance.\n",
        "+ * This avoids creating multiple function closures and prevents potential memory issues.\n",
        "+ */\n",
        "+export const createTokenCounter = async (): Promise<\n",
        "+  (message: BaseMessage) => number\n",
        "+> => {\n",
        "+  if (tokenCounterPromise) {\n",
        "+    return tokenCounterPromise;\n",
        "+  }\n",
        "+\n",
        "+  tokenCounterPromise = (async (): Promise<\n",
        "+    (message: BaseMessage) => number\n",
        "+  > => {\n",
        "+    const enc = await getSharedEncoder();\n",
        "+    const countTokens = (text: string): number => enc.encode(text).length;\n",
        "+    return (message: BaseMessage): number =>\n",
        "+      getTokenCountForMessage(message, countTokens);\n",
        "+  })();\n",
        "+\n",
        "+  return tokenCounterPromise;\n",
        "+};\n",
        "+\n",
        "+/**\n",
        "+ * Utility to manage the token encoder lifecycle explicitly.\n",
        "+ * Useful for applications that need fine-grained control over resource management.\n",
        "+ */\n",
        "+export const TokenEncoderManager = {\n",
        "+  /**\n",
        "+   * Pre-initializes the encoder. This can be called during app startup\n",
        "+   * to avoid lazy loading delays later.\n",
        "+   */\n",
        "+  async initialize(): Promise<void> {\n",
        "+    await getSharedEncoder();\n",
        "+  },\n",
        "+\n",
        "+  /**\n",
        "+   * Clears the cached encoder and token counter.\n",
        "+   * Useful for testing or when you need to force a fresh reload.\n",
        "+   */\n",
        "+  reset(): void {\n",
        "+    encoderPromise = undefined;\n",
        "+    tokenCounterPromise = undefined;\n",
        "+  },\n",
        " \n",
        "-  return (message: BaseMessage): number => getTokenCountForMessage(message, countTokens);\n",
        "-};\n",
        "\\ No newline at end of file\n",
        "+  /**\n",
        "+   * Checks if the encoder has been initialized.\n",
        "+   */\n",
        "+  isInitialized(): boolean {\n",
        "+    return encoderPromise !== undefined;\n",
        "+  },\n",
        "+};\n"
      ]
    },
    {
      "path": "types/package.json",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "patch": "@@ -1,6 +1,6 @@\n {\n   \"name\": \"@librechat/agents-types\",\n-  \"version\": \"2.4.90\",\n+  \"version\": \"3.0.00-rc1\",\n   \"description\": \"Type definitions for @librechat/agents\",\n   \"types\": \"index.d.ts\",\n   \"scripts\": {",
      "patch_lines": [
        "@@ -1,6 +1,6 @@\n",
        " {\n",
        "   \"name\": \"@librechat/agents-types\",\n",
        "-  \"version\": \"2.4.90\",\n",
        "+  \"version\": \"3.0.00-rc1\",\n",
        "   \"description\": \"Type definitions for @librechat/agents\",\n",
        "   \"types\": \"index.d.ts\",\n",
        "   \"scripts\": {\n"
      ]
    }
  ]
}
{
  "project": "Research Data/export-langsmith-data",
  "repo": "stride-nyc/export-langsmith-data",
  "prior_commit": "570dea898391b06a8510012a66d2d9266aa86524",
  "researched_commit": "ffe23613dfa5a90e3b03cff809114aa21df74599",
  "compare_url": "https://github.com/stride-nyc/export-langsmith-data/compare/570dea898391b06a8510012a66d2d9266aa86524...ffe23613dfa5a90e3b03cff809114aa21df74599",
  "ahead_by": 1,
  "behind_by": 0,
  "changed_files": [
    {
      "path": ".bandit",
      "status": "modified",
      "additions": 4,
      "deletions": 1,
      "patch": "@@ -5,7 +5,10 @@\n # Exclude directories\n exclude_dirs = ['.venv', '.pytest_cache', '__pycache__']\n \n-# Skip B101 (assert_used) check for test files\n+# Exclude test files from scanning\n+exclude = ['/test_*.py', '*/test_*.py']\n+\n+# Skip B101 (assert_used) check for test files (if not excluded)\n # Asserts are acceptable and expected in test files\n skips = B101\n ",
      "patch_lines": [
        "@@ -5,7 +5,10 @@\n",
        " # Exclude directories\n",
        " exclude_dirs = ['.venv', '.pytest_cache', '__pycache__']\n",
        " \n",
        "-# Skip B101 (assert_used) check for test files\n",
        "+# Exclude test files from scanning\n",
        "+exclude = ['/test_*.py', '*/test_*.py']\n",
        "+\n",
        "+# Skip B101 (assert_used) check for test files (if not excluded)\n",
        " # Asserts are acceptable and expected in test files\n",
        " skips = B101\n",
        " \n"
      ]
    },
    {
      "path": "README.md",
      "status": "modified",
      "additions": 205,
      "deletions": 8,
      "patch": "@@ -4,11 +4,13 @@ Export and analyze workflow trace data from LangSmith projects for performance i\n \n ## Overview\n \n-This toolkit provides two main capabilities:\n+This toolkit provides comprehensive capabilities for LangSmith trace analysis:\n 1. **Data Export** (`export_langsmith_traces.py`) - Export trace data from LangSmith using the SDK API\n-2. **Performance Analysis** (`analyze_traces.py`) - Analyze exported traces for latency, bottlenecks, and parallel execution\n+2. **Performance Analysis** (`analyze_traces.py`) - Analyze exported traces for latency, bottlenecks, and parallel execution (Phase 3A)\n+3. **Cost Analysis** (`analyze_cost.py`) - Calculate workflow costs with configurable pricing models (Phase 3B)\n+4. **Failure Pattern Analysis** (`analyze_failures.py`) - Detect failures, retry sequences, and error patterns (Phase 3C)\n \n-Designed for users on Individual Developer plans without bulk export features, with robust error handling, rate limiting, and comprehensive analysis capabilities.\n+Designed for users on Individual Developer plans without bulk export features, with robust error handling, rate limiting, and comprehensive analysis capabilities. All modules follow strict TDD methodology with 99+ tests and full type safety.\n \n ## Features\n \n@@ -241,17 +243,29 @@ Once you have exported trace data, use the analysis tools to gain performance in\n After generating analysis results, use the verification tool to ensure accuracy:\n \n ```bash\n-# Basic verification (regenerates all statistics)\n+# Basic verification - Phase 3A only (default)\n python verify_analysis_report.py traces_export.json\n \n+# Verify all phases (3A + 3B + 3C)\n+python verify_analysis_report.py traces_export.json --phases all\n+\n+# Verify specific phases\n+python verify_analysis_report.py traces_export.json --phases 3b\n+python verify_analysis_report.py traces_export.json --phases 3c\n+python verify_analysis_report.py traces_export.json --phases \"3a,3b\"\n+\n # Verify against expected values\n python verify_analysis_report.py traces_export.json --expected-values expected.json\n+\n+# Use custom pricing model for cost analysis\n+python verify_analysis_report.py traces_export.json --phases 3b --pricing-model gemini_1.5_pro\n ```\n \n The verification tool:\n - Regenerates all calculations from raw data\n - Provides deterministic verification of findings\n - Optionally compares against expected values (PASS/FAIL indicators)\n+- Supports selective phase verification (3a, 3b, 3c, or all)\n - Useful for auditing and validating reports\n \n **Example expected values JSON:**\n@@ -270,6 +284,130 @@ The verification tool:\n }\n ```\n \n+### Cost Analysis (Phase 3B)\n+\n+Analyze workflow costs based on token usage with configurable pricing models:\n+\n+```python\n+from analyze_cost import (\n+    analyze_costs,\n+    PricingConfig,\n+    EXAMPLE_PRICING_CONFIGS,\n+)\n+from analyze_traces import load_from_json\n+\n+# Load exported trace data\n+dataset = load_from_json(\"traces_export.json\")\n+\n+# Option 1: Use example pricing config\n+pricing = EXAMPLE_PRICING_CONFIGS[\"gemini_1.5_pro\"]\n+\n+# Option 2: Create custom pricing config\n+pricing = PricingConfig(\n+    model_name=\"Custom Model\",\n+    input_tokens_per_1k=0.001,      # $1.00 per 1M input tokens\n+    output_tokens_per_1k=0.003,     # $3.00 per 1M output tokens\n+    cache_read_per_1k=0.0001,       # $0.10 per 1M cache read tokens (optional)\n+)\n+\n+# Run cost analysis\n+results = analyze_costs(\n+    workflows=dataset.workflows,\n+    pricing_config=pricing,\n+    scaling_factors=[1, 10, 100, 1000],  # Optional, defaults to [1, 10, 100, 1000]\n+    monthly_workflow_estimate=10000,     # Optional, for monthly cost projections\n+)\n+\n+# Access results\n+print(f\"Average cost per workflow: ${results.avg_cost_per_workflow:.4f}\")\n+print(f\"Median cost: ${results.median_cost_per_workflow:.4f}\")\n+print(f\"Top cost driver: {results.top_cost_driver}\")\n+\n+# View node-level breakdown\n+for node in results.node_summaries[:3]:  # Top 3 nodes\n+    print(f\"  {node.node_name}:\")\n+    print(f\"    Total cost: ${node.total_cost:.4f}\")\n+    print(f\"    Executions: {node.execution_count}\")\n+    print(f\"    Avg per execution: ${node.avg_cost_per_execution:.6f}\")\n+    print(f\"    % of total: {node.percent_of_total_cost:.1f}%\")\n+\n+# View scaling projections\n+for scale_label, projection in results.scaling_projections.items():\n+    print(f\"{scale_label}: ${projection.total_cost:.2f} for {projection.workflow_count} workflows\")\n+    if projection.cost_per_month_30days:\n+        print(f\"  Monthly estimate: ${projection.cost_per_month_30days:.2f}/month\")\n+```\n+\n+**Cost Analysis Features:**\n+- Configurable pricing for any LLM provider (not hard-coded)\n+- Token usage extraction (input/output/cache tokens)\n+- Workflow-level cost aggregation\n+- Node-level cost breakdown with percentages\n+- Scaling projections at 1x, 10x, 100x, 1000x volume\n+- Optional monthly cost estimates\n+- Data quality reporting for missing token data\n+\n+### Failure Pattern Analysis (Phase 3C)\n+\n+Detect and analyze failure patterns, retry sequences, and error distributions:\n+\n+```python\n+from analyze_failures import (\n+    analyze_failures,\n+    FAILURE_STATUSES,\n+    ERROR_PATTERNS,\n+)\n+from analyze_traces import load_from_json\n+\n+# Load exported trace data\n+dataset = load_from_json(\"traces_export.json\")\n+\n+# Run failure analysis\n+results = analyze_failures(workflows=dataset.workflows)\n+\n+# Overall metrics\n+print(f\"Total workflows: {results.total_workflows}\")\n+print(f\"Success rate: {results.overall_success_rate_percent:.1f}%\")\n+print(f\"Failed workflows: {results.failed_workflows}\")\n+\n+# Node failure breakdown\n+print(\"\\nTop 5 nodes by failure rate:\")\n+for node in results.node_failure_stats[:5]:\n+    print(f\"  {node.node_name}:\")\n+    print(f\"    Failure rate: {node.failure_rate_percent:.1f}%\")\n+    print(f\"    Failures: {node.failure_count}/{node.total_executions}\")\n+    print(f\"    Retry sequences: {node.retry_sequences_detected}\")\n+    print(f\"    Common errors: {node.common_error_types}\")\n+\n+# Error distribution\n+print(\"\\nError type distribution:\")\n+for error_type, count in results.error_type_distribution.items():\n+    print(f\"  {error_type}: {count}\")\n+\n+# Retry analysis\n+print(f\"\\nTotal retry sequences detected: {results.total_retry_sequences}\")\n+if results.retry_success_rate_percent:\n+    print(f\"Retry success rate: {results.retry_success_rate_percent:.1f}%\")\n+\n+# Example retry sequence details\n+for retry_seq in results.retry_sequences[:3]:  # First 3 retry sequences\n+    print(f\"\\nRetry sequence in {retry_seq.node_name}:\")\n+    print(f\"  Attempts: {retry_seq.attempt_count}\")\n+    print(f\"  Final status: {retry_seq.final_status}\")\n+    print(f\"  Total duration: {retry_seq.total_duration_seconds:.1f}s\")\n+```\n+\n+**Failure Analysis Features:**\n+- Status-based failure detection (error, failed, cancelled)\n+- Regex-based error classification (validation, timeout, import, LLM errors)\n+- Heuristic retry sequence detection:\n+  - Multiple executions of same node within 5-minute window\n+  - Ordered by start time\n+- Node-level failure statistics\n+- Retry success rate calculation\n+- Error distribution across workflows\n+- Quality risk identification (placeholder for future enhancement)\n+\n ### Using Python API Directly\n \n You can also use the analysis functions programmatically:\n@@ -420,9 +558,41 @@ pytest test_analyze_traces.py::TestCSVExport -v\n pytest --cov=analyze_traces test_analyze_traces.py\n ```\n \n+**Cost analysis module tests (20 tests):**\n+```bash\n+# Run all cost analysis tests\n+pytest test_analyze_cost.py -v\n+\n+# Run specific test classes\n+pytest test_analyze_cost.py::TestPricingConfig -v\n+pytest test_analyze_cost.py::TestTokenExtraction -v\n+pytest test_analyze_cost.py::TestCostCalculation -v\n+\n+# Run with coverage\n+pytest --cov=analyze_cost test_analyze_cost.py\n+```\n+\n+**Failure analysis module tests (15 tests):**\n+```bash\n+# Run all failure analysis tests\n+pytest test_analyze_failures.py -v\n+\n+# Run specific test classes\n+pytest test_analyze_failures.py::TestFailureDetection -v\n+pytest test_analyze_failures.py::TestRetryDetection -v\n+pytest test_analyze_failures.py::TestNodeFailureAnalysis -v\n+\n+# Run with coverage\n+pytest --cov=analyze_failures test_analyze_failures.py\n+```\n+\n **Run all tests:**\n ```bash\n+# Run all 99 tests (33 export + 31 analysis + 20 cost + 15 failure)\n pytest -v\n+\n+# Run with coverage\n+pytest --cov=. -v\n ```\n \n ### Project Structure\n@@ -435,11 +605,16 @@ export-langsmith-data/\n \u251c\u2500\u2500 PLAN.md                          # PDCA implementation plan\n \u251c\u2500\u2500 export-langsmith-requirements.md # Export requirements specification\n \u251c\u2500\u2500 export_langsmith_traces.py       # Data export script\n-\u251c\u2500\u2500 test_export_langsmith_traces.py  # Export test suite (42 tests)\n+\u251c\u2500\u2500 test_export_langsmith_traces.py  # Export test suite (33 tests)\n \u251c\u2500\u2500 validate_export.py               # Export validation utility\n \u251c\u2500\u2500 test_validate_export.py          # Validation test suite (7 tests)\n-\u251c\u2500\u2500 analyze_traces.py                # Performance analysis module\n+\u251c\u2500\u2500 analyze_traces.py                # Performance analysis module (Phase 3A)\n \u251c\u2500\u2500 test_analyze_traces.py           # Analysis test suite (31 tests)\n+\u251c\u2500\u2500 analyze_cost.py                  # Cost analysis module (Phase 3B)\n+\u251c\u2500\u2500 test_analyze_cost.py             # Cost analysis test suite (20 tests)\n+\u251c\u2500\u2500 analyze_failures.py              # Failure pattern analysis module (Phase 3C)\n+\u251c\u2500\u2500 test_analyze_failures.py         # Failure analysis test suite (15 tests)\n+\u251c\u2500\u2500 verify_analysis_report.py        # Verification tool for all phases\n \u251c\u2500\u2500 notebooks/\n \u2502   \u2514\u2500\u2500 langsmith_trace_performance_analysis.ipynb  # Interactive analysis notebook\n \u251c\u2500\u2500 output/                          # Generated CSV analysis results\n@@ -490,11 +665,33 @@ This project follows the **PDCA (Plan-Do-Check-Act) framework** with strict Test\n - \u2705 Code quality: Black, Ruff, mypy checks passing\n - \u2705 TDD methodology: Strict RED-GREEN-REFACTOR cycles across all 5 phases\n \n+### \u2705 Complete - Production Ready (Continued)\n+\n+**Cost Analysis Module (Phase 3B):**\n+- \u2705 Configurable pricing models for any LLM provider\n+- \u2705 Token usage extraction from trace metadata\n+- \u2705 Cost calculation with input/output/cache token pricing\n+- \u2705 Workflow-level cost aggregation\n+- \u2705 Node-level cost breakdown with percentages\n+- \u2705 Scaling projections (1x, 10x, 100x, 1000x)\n+- \u2705 Test suite: 20 tests, full coverage\n+- \u2705 Code quality: Black, Ruff, mypy, Bandit checks passing\n+\n+**Failure Pattern Analysis Module (Phase 3C):**\n+- \u2705 Status-based failure detection\n+- \u2705 Regex-based error classification (5 patterns + unknown)\n+- \u2705 Heuristic retry sequence detection\n+- \u2705 Node-level failure statistics\n+- \u2705 Retry success rate calculation\n+- \u2705 Error distribution tracking\n+- \u2705 Test suite: 15 tests, full coverage\n+- \u2705 Code quality: Black, Ruff, mypy, Bandit checks passing\n+\n ### Optional Features Not Implemented\n \n - \u23f8\ufe0f Progress indication (tqdm) - Skipped in favor of simple console output\n-- \u23f8\ufe0f Cost analysis (Phase 3B) - Future enhancement for token usage tracking\n-- \u23f8\ufe0f Failure analysis (Phase 3C) - Future enhancement for error pattern detection\n+- \u23f8\ufe0f Validator effectiveness analysis - Placeholder in Phase 3C for future enhancement\n+- \u23f8\ufe0f Cache effectiveness analysis - Placeholder in Phase 3B for future enhancement\n \n ## Troubleshooting\n ",
      "patch_lines": [
        "@@ -4,11 +4,13 @@ Export and analyze workflow trace data from LangSmith projects for performance i\n",
        " \n",
        " ## Overview\n",
        " \n",
        "-This toolkit provides two main capabilities:\n",
        "+This toolkit provides comprehensive capabilities for LangSmith trace analysis:\n",
        " 1. **Data Export** (`export_langsmith_traces.py`) - Export trace data from LangSmith using the SDK API\n",
        "-2. **Performance Analysis** (`analyze_traces.py`) - Analyze exported traces for latency, bottlenecks, and parallel execution\n",
        "+2. **Performance Analysis** (`analyze_traces.py`) - Analyze exported traces for latency, bottlenecks, and parallel execution (Phase 3A)\n",
        "+3. **Cost Analysis** (`analyze_cost.py`) - Calculate workflow costs with configurable pricing models (Phase 3B)\n",
        "+4. **Failure Pattern Analysis** (`analyze_failures.py`) - Detect failures, retry sequences, and error patterns (Phase 3C)\n",
        " \n",
        "-Designed for users on Individual Developer plans without bulk export features, with robust error handling, rate limiting, and comprehensive analysis capabilities.\n",
        "+Designed for users on Individual Developer plans without bulk export features, with robust error handling, rate limiting, and comprehensive analysis capabilities. All modules follow strict TDD methodology with 99+ tests and full type safety.\n",
        " \n",
        " ## Features\n",
        " \n",
        "@@ -241,17 +243,29 @@ Once you have exported trace data, use the analysis tools to gain performance in\n",
        " After generating analysis results, use the verification tool to ensure accuracy:\n",
        " \n",
        " ```bash\n",
        "-# Basic verification (regenerates all statistics)\n",
        "+# Basic verification - Phase 3A only (default)\n",
        " python verify_analysis_report.py traces_export.json\n",
        " \n",
        "+# Verify all phases (3A + 3B + 3C)\n",
        "+python verify_analysis_report.py traces_export.json --phases all\n",
        "+\n",
        "+# Verify specific phases\n",
        "+python verify_analysis_report.py traces_export.json --phases 3b\n",
        "+python verify_analysis_report.py traces_export.json --phases 3c\n",
        "+python verify_analysis_report.py traces_export.json --phases \"3a,3b\"\n",
        "+\n",
        " # Verify against expected values\n",
        " python verify_analysis_report.py traces_export.json --expected-values expected.json\n",
        "+\n",
        "+# Use custom pricing model for cost analysis\n",
        "+python verify_analysis_report.py traces_export.json --phases 3b --pricing-model gemini_1.5_pro\n",
        " ```\n",
        " \n",
        " The verification tool:\n",
        " - Regenerates all calculations from raw data\n",
        " - Provides deterministic verification of findings\n",
        " - Optionally compares against expected values (PASS/FAIL indicators)\n",
        "+- Supports selective phase verification (3a, 3b, 3c, or all)\n",
        " - Useful for auditing and validating reports\n",
        " \n",
        " **Example expected values JSON:**\n",
        "@@ -270,6 +284,130 @@ The verification tool:\n",
        " }\n",
        " ```\n",
        " \n",
        "+### Cost Analysis (Phase 3B)\n",
        "+\n",
        "+Analyze workflow costs based on token usage with configurable pricing models:\n",
        "+\n",
        "+```python\n",
        "+from analyze_cost import (\n",
        "+    analyze_costs,\n",
        "+    PricingConfig,\n",
        "+    EXAMPLE_PRICING_CONFIGS,\n",
        "+)\n",
        "+from analyze_traces import load_from_json\n",
        "+\n",
        "+# Load exported trace data\n",
        "+dataset = load_from_json(\"traces_export.json\")\n",
        "+\n",
        "+# Option 1: Use example pricing config\n",
        "+pricing = EXAMPLE_PRICING_CONFIGS[\"gemini_1.5_pro\"]\n",
        "+\n",
        "+# Option 2: Create custom pricing config\n",
        "+pricing = PricingConfig(\n",
        "+    model_name=\"Custom Model\",\n",
        "+    input_tokens_per_1k=0.001,      # $1.00 per 1M input tokens\n",
        "+    output_tokens_per_1k=0.003,     # $3.00 per 1M output tokens\n",
        "+    cache_read_per_1k=0.0001,       # $0.10 per 1M cache read tokens (optional)\n",
        "+)\n",
        "+\n",
        "+# Run cost analysis\n",
        "+results = analyze_costs(\n",
        "+    workflows=dataset.workflows,\n",
        "+    pricing_config=pricing,\n",
        "+    scaling_factors=[1, 10, 100, 1000],  # Optional, defaults to [1, 10, 100, 1000]\n",
        "+    monthly_workflow_estimate=10000,     # Optional, for monthly cost projections\n",
        "+)\n",
        "+\n",
        "+# Access results\n",
        "+print(f\"Average cost per workflow: ${results.avg_cost_per_workflow:.4f}\")\n",
        "+print(f\"Median cost: ${results.median_cost_per_workflow:.4f}\")\n",
        "+print(f\"Top cost driver: {results.top_cost_driver}\")\n",
        "+\n",
        "+# View node-level breakdown\n",
        "+for node in results.node_summaries[:3]:  # Top 3 nodes\n",
        "+    print(f\"  {node.node_name}:\")\n",
        "+    print(f\"    Total cost: ${node.total_cost:.4f}\")\n",
        "+    print(f\"    Executions: {node.execution_count}\")\n",
        "+    print(f\"    Avg per execution: ${node.avg_cost_per_execution:.6f}\")\n",
        "+    print(f\"    % of total: {node.percent_of_total_cost:.1f}%\")\n",
        "+\n",
        "+# View scaling projections\n",
        "+for scale_label, projection in results.scaling_projections.items():\n",
        "+    print(f\"{scale_label}: ${projection.total_cost:.2f} for {projection.workflow_count} workflows\")\n",
        "+    if projection.cost_per_month_30days:\n",
        "+        print(f\"  Monthly estimate: ${projection.cost_per_month_30days:.2f}/month\")\n",
        "+```\n",
        "+\n",
        "+**Cost Analysis Features:**\n",
        "+- Configurable pricing for any LLM provider (not hard-coded)\n",
        "+- Token usage extraction (input/output/cache tokens)\n",
        "+- Workflow-level cost aggregation\n",
        "+- Node-level cost breakdown with percentages\n",
        "+- Scaling projections at 1x, 10x, 100x, 1000x volume\n",
        "+- Optional monthly cost estimates\n",
        "+- Data quality reporting for missing token data\n",
        "+\n",
        "+### Failure Pattern Analysis (Phase 3C)\n",
        "+\n",
        "+Detect and analyze failure patterns, retry sequences, and error distributions:\n",
        "+\n",
        "+```python\n",
        "+from analyze_failures import (\n",
        "+    analyze_failures,\n",
        "+    FAILURE_STATUSES,\n",
        "+    ERROR_PATTERNS,\n",
        "+)\n",
        "+from analyze_traces import load_from_json\n",
        "+\n",
        "+# Load exported trace data\n",
        "+dataset = load_from_json(\"traces_export.json\")\n",
        "+\n",
        "+# Run failure analysis\n",
        "+results = analyze_failures(workflows=dataset.workflows)\n",
        "+\n",
        "+# Overall metrics\n",
        "+print(f\"Total workflows: {results.total_workflows}\")\n",
        "+print(f\"Success rate: {results.overall_success_rate_percent:.1f}%\")\n",
        "+print(f\"Failed workflows: {results.failed_workflows}\")\n",
        "+\n",
        "+# Node failure breakdown\n",
        "+print(\"\\nTop 5 nodes by failure rate:\")\n",
        "+for node in results.node_failure_stats[:5]:\n",
        "+    print(f\"  {node.node_name}:\")\n",
        "+    print(f\"    Failure rate: {node.failure_rate_percent:.1f}%\")\n",
        "+    print(f\"    Failures: {node.failure_count}/{node.total_executions}\")\n",
        "+    print(f\"    Retry sequences: {node.retry_sequences_detected}\")\n",
        "+    print(f\"    Common errors: {node.common_error_types}\")\n",
        "+\n",
        "+# Error distribution\n",
        "+print(\"\\nError type distribution:\")\n",
        "+for error_type, count in results.error_type_distribution.items():\n",
        "+    print(f\"  {error_type}: {count}\")\n",
        "+\n",
        "+# Retry analysis\n",
        "+print(f\"\\nTotal retry sequences detected: {results.total_retry_sequences}\")\n",
        "+if results.retry_success_rate_percent:\n",
        "+    print(f\"Retry success rate: {results.retry_success_rate_percent:.1f}%\")\n",
        "+\n",
        "+# Example retry sequence details\n",
        "+for retry_seq in results.retry_sequences[:3]:  # First 3 retry sequences\n",
        "+    print(f\"\\nRetry sequence in {retry_seq.node_name}:\")\n",
        "+    print(f\"  Attempts: {retry_seq.attempt_count}\")\n",
        "+    print(f\"  Final status: {retry_seq.final_status}\")\n",
        "+    print(f\"  Total duration: {retry_seq.total_duration_seconds:.1f}s\")\n",
        "+```\n",
        "+\n",
        "+**Failure Analysis Features:**\n",
        "+- Status-based failure detection (error, failed, cancelled)\n",
        "+- Regex-based error classification (validation, timeout, import, LLM errors)\n",
        "+- Heuristic retry sequence detection:\n",
        "+  - Multiple executions of same node within 5-minute window\n",
        "+  - Ordered by start time\n",
        "+- Node-level failure statistics\n",
        "+- Retry success rate calculation\n",
        "+- Error distribution across workflows\n",
        "+- Quality risk identification (placeholder for future enhancement)\n",
        "+\n",
        " ### Using Python API Directly\n",
        " \n",
        " You can also use the analysis functions programmatically:\n",
        "@@ -420,9 +558,41 @@ pytest test_analyze_traces.py::TestCSVExport -v\n",
        " pytest --cov=analyze_traces test_analyze_traces.py\n",
        " ```\n",
        " \n",
        "+**Cost analysis module tests (20 tests):**\n",
        "+```bash\n",
        "+# Run all cost analysis tests\n",
        "+pytest test_analyze_cost.py -v\n",
        "+\n",
        "+# Run specific test classes\n",
        "+pytest test_analyze_cost.py::TestPricingConfig -v\n",
        "+pytest test_analyze_cost.py::TestTokenExtraction -v\n",
        "+pytest test_analyze_cost.py::TestCostCalculation -v\n",
        "+\n",
        "+# Run with coverage\n",
        "+pytest --cov=analyze_cost test_analyze_cost.py\n",
        "+```\n",
        "+\n",
        "+**Failure analysis module tests (15 tests):**\n",
        "+```bash\n",
        "+# Run all failure analysis tests\n",
        "+pytest test_analyze_failures.py -v\n",
        "+\n",
        "+# Run specific test classes\n",
        "+pytest test_analyze_failures.py::TestFailureDetection -v\n",
        "+pytest test_analyze_failures.py::TestRetryDetection -v\n",
        "+pytest test_analyze_failures.py::TestNodeFailureAnalysis -v\n",
        "+\n",
        "+# Run with coverage\n",
        "+pytest --cov=analyze_failures test_analyze_failures.py\n",
        "+```\n",
        "+\n",
        " **Run all tests:**\n",
        " ```bash\n",
        "+# Run all 99 tests (33 export + 31 analysis + 20 cost + 15 failure)\n",
        " pytest -v\n",
        "+\n",
        "+# Run with coverage\n",
        "+pytest --cov=. -v\n",
        " ```\n",
        " \n",
        " ### Project Structure\n",
        "@@ -435,11 +605,16 @@ export-langsmith-data/\n",
        " \u251c\u2500\u2500 PLAN.md                          # PDCA implementation plan\n",
        " \u251c\u2500\u2500 export-langsmith-requirements.md # Export requirements specification\n",
        " \u251c\u2500\u2500 export_langsmith_traces.py       # Data export script\n",
        "-\u251c\u2500\u2500 test_export_langsmith_traces.py  # Export test suite (42 tests)\n",
        "+\u251c\u2500\u2500 test_export_langsmith_traces.py  # Export test suite (33 tests)\n",
        " \u251c\u2500\u2500 validate_export.py               # Export validation utility\n",
        " \u251c\u2500\u2500 test_validate_export.py          # Validation test suite (7 tests)\n",
        "-\u251c\u2500\u2500 analyze_traces.py                # Performance analysis module\n",
        "+\u251c\u2500\u2500 analyze_traces.py                # Performance analysis module (Phase 3A)\n",
        " \u251c\u2500\u2500 test_analyze_traces.py           # Analysis test suite (31 tests)\n",
        "+\u251c\u2500\u2500 analyze_cost.py                  # Cost analysis module (Phase 3B)\n",
        "+\u251c\u2500\u2500 test_analyze_cost.py             # Cost analysis test suite (20 tests)\n",
        "+\u251c\u2500\u2500 analyze_failures.py              # Failure pattern analysis module (Phase 3C)\n",
        "+\u251c\u2500\u2500 test_analyze_failures.py         # Failure analysis test suite (15 tests)\n",
        "+\u251c\u2500\u2500 verify_analysis_report.py        # Verification tool for all phases\n",
        " \u251c\u2500\u2500 notebooks/\n",
        " \u2502   \u2514\u2500\u2500 langsmith_trace_performance_analysis.ipynb  # Interactive analysis notebook\n",
        " \u251c\u2500\u2500 output/                          # Generated CSV analysis results\n",
        "@@ -490,11 +665,33 @@ This project follows the **PDCA (Plan-Do-Check-Act) framework** with strict Test\n",
        " - \u2705 Code quality: Black, Ruff, mypy checks passing\n",
        " - \u2705 TDD methodology: Strict RED-GREEN-REFACTOR cycles across all 5 phases\n",
        " \n",
        "+### \u2705 Complete - Production Ready (Continued)\n",
        "+\n",
        "+**Cost Analysis Module (Phase 3B):**\n",
        "+- \u2705 Configurable pricing models for any LLM provider\n",
        "+- \u2705 Token usage extraction from trace metadata\n",
        "+- \u2705 Cost calculation with input/output/cache token pricing\n",
        "+- \u2705 Workflow-level cost aggregation\n",
        "+- \u2705 Node-level cost breakdown with percentages\n",
        "+- \u2705 Scaling projections (1x, 10x, 100x, 1000x)\n",
        "+- \u2705 Test suite: 20 tests, full coverage\n",
        "+- \u2705 Code quality: Black, Ruff, mypy, Bandit checks passing\n",
        "+\n",
        "+**Failure Pattern Analysis Module (Phase 3C):**\n",
        "+- \u2705 Status-based failure detection\n",
        "+- \u2705 Regex-based error classification (5 patterns + unknown)\n",
        "+- \u2705 Heuristic retry sequence detection\n",
        "+- \u2705 Node-level failure statistics\n",
        "+- \u2705 Retry success rate calculation\n",
        "+- \u2705 Error distribution tracking\n",
        "+- \u2705 Test suite: 15 tests, full coverage\n",
        "+- \u2705 Code quality: Black, Ruff, mypy, Bandit checks passing\n",
        "+\n",
        " ### Optional Features Not Implemented\n",
        " \n",
        " - \u23f8\ufe0f Progress indication (tqdm) - Skipped in favor of simple console output\n",
        "-- \u23f8\ufe0f Cost analysis (Phase 3B) - Future enhancement for token usage tracking\n",
        "-- \u23f8\ufe0f Failure analysis (Phase 3C) - Future enhancement for error pattern detection\n",
        "+- \u23f8\ufe0f Validator effectiveness analysis - Placeholder in Phase 3C for future enhancement\n",
        "+- \u23f8\ufe0f Cache effectiveness analysis - Placeholder in Phase 3B for future enhancement\n",
        " \n",
        " ## Troubleshooting\n",
        " \n"
      ]
    },
    {
      "path": "analyze_cost.py",
      "status": "added",
      "additions": 718,
      "deletions": 0,
      "patch": "@@ -0,0 +1,718 @@\n+\"\"\"\n+LangSmith Trace Cost Analysis Tool - Phase 3B\n+\n+This module provides cost analysis capabilities for LangSmith trace exports.\n+Calculates costs based on token usage with configurable pricing models.\n+\n+Following PDCA (Plan-Do-Check-Act) methodology with TDD approach.\n+\n+Author: Generated with Claude Code (PDCA Framework)\n+Date: 2025-12-09\n+\"\"\"\n+\n+from dataclasses import dataclass\n+from typing import Any, Dict, List, Optional\n+from analyze_traces import Trace, Workflow\n+\n+\n+# ============================================================================\n+# Pricing Configuration\n+# ============================================================================\n+\n+\n+@dataclass\n+class PricingConfig:\n+    \"\"\"Configurable pricing model for any LLM provider.\"\"\"\n+\n+    model_name: str\n+    input_tokens_per_1k: float  # Cost per 1K input tokens\n+    output_tokens_per_1k: float  # Cost per 1K output tokens\n+    cache_read_per_1k: Optional[float] = (\n+        None  # Cost per 1K cache read tokens (if applicable)\n+    )\n+\n+    def __post_init__(self) -> None:\n+        \"\"\"Validate pricing configuration.\"\"\"\n+        if self.input_tokens_per_1k < 0 or self.output_tokens_per_1k < 0:\n+            raise ValueError(\"Token prices must be non-negative\")\n+        if self.cache_read_per_1k is not None and self.cache_read_per_1k < 0:\n+            raise ValueError(\"Cache read price must be non-negative\")\n+\n+\n+# Example pricing configs for reference (NOT hard-coded defaults)\n+# Users should create their own PricingConfig instances\n+EXAMPLE_PRICING_CONFIGS = {\n+    \"gemini_1.5_pro\": PricingConfig(\n+        model_name=\"Gemini 1.5 Pro\",\n+        input_tokens_per_1k=0.00125,  # $1.25 per 1M input tokens\n+        output_tokens_per_1k=0.005,  # $5.00 per 1M output tokens\n+        cache_read_per_1k=0.0003125,  # $0.3125 per 1M cache read tokens\n+    ),\n+}\n+\n+SCALING_FACTORS = [1, 10, 100, 1000]  # Current, 10x, 100x, 1000x\n+\n+\n+# ============================================================================\n+# Core Data Structures\n+# ============================================================================\n+\n+\n+@dataclass\n+class TokenUsage:\n+    \"\"\"Token usage for a single trace.\"\"\"\n+\n+    input_tokens: int\n+    output_tokens: int\n+    total_tokens: int\n+    cached_tokens: Optional[int] = None  # From input_token_details.cache_read\n+\n+    def has_cache_data(self) -> bool:\n+        \"\"\"Check if cache token data is available.\"\"\"\n+        return self.cached_tokens is not None\n+\n+\n+@dataclass\n+class CostBreakdown:\n+    \"\"\"Cost breakdown for a single trace.\"\"\"\n+\n+    trace_id: str\n+    trace_name: str\n+    input_cost: float\n+    output_cost: float\n+    cache_cost: float\n+    total_cost: float\n+    token_usage: TokenUsage\n+\n+    def to_dict(self) -> Dict[str, Any]:\n+        \"\"\"Export to dictionary for CSV/reporting.\"\"\"\n+        return {\n+            \"trace_id\": self.trace_id,\n+            \"trace_name\": self.trace_name,\n+            \"input_tokens\": self.token_usage.input_tokens,\n+            \"output_tokens\": self.token_usage.output_tokens,\n+            \"total_tokens\": self.token_usage.total_tokens,\n+            \"input_cost\": self.input_cost,\n+            \"output_cost\": self.output_cost,\n+            \"cache_cost\": self.cache_cost,\n+            \"total_cost\": self.total_cost,\n+        }\n+\n+\n+@dataclass\n+class WorkflowCostAnalysis:\n+    \"\"\"Cost analysis for a single workflow.\"\"\"\n+\n+    workflow_id: str\n+    total_cost: float\n+    node_costs: List[CostBreakdown]\n+    total_tokens: int\n+    cache_effectiveness_percent: Optional[float] = None  # If cache data available\n+\n+\n+@dataclass\n+class ScalingProjection:\n+    \"\"\"Cost projection at a specific scale factor.\"\"\"\n+\n+    scale_factor: int\n+    workflow_count: int\n+    total_cost: float\n+    cost_per_month_30days: Optional[float] = None  # If monthly estimate provided\n+\n+\n+@dataclass\n+class NodeCostSummary:\n+    \"\"\"Cost summary for a node type across workflows.\"\"\"\n+\n+    node_name: str\n+    execution_count: int\n+    total_cost: float\n+    avg_cost_per_execution: float\n+    percent_of_total_cost: float\n+\n+\n+@dataclass\n+class CacheCostComparison:\n+    \"\"\"Comparison of costs with cache vs without cache.\"\"\"\n+\n+    cost_with_cache: float  # Actual cost using cache\n+    cost_without_cache: float  # Hypothetical cost if no cache used\n+    total_savings: float  # Dollar savings from using cache\n+    savings_percent: float  # Percentage savings (0-100)\n+    traces_analyzed: int  # Total number of traces analyzed\n+    traces_with_cache: int  # Number of traces that used cache\n+\n+\n+@dataclass\n+class CostAnalysisResults:\n+    \"\"\"Complete cost analysis results.\"\"\"\n+\n+    # Per-workflow metrics\n+    avg_cost_per_workflow: float\n+    median_cost_per_workflow: float\n+    min_cost: float\n+    max_cost: float\n+\n+    # Node-level breakdown\n+    node_summaries: List[NodeCostSummary]  # Sorted by cost descending\n+    top_cost_driver: Optional[str]\n+\n+    # Cache effectiveness\n+    cache_effectiveness_percent: Optional[float]\n+    cache_savings_dollars: Optional[float]\n+\n+    # Scaling projections\n+    scaling_projections: Dict[str, ScalingProjection]  # \"10x\", \"100x\", etc.\n+\n+    # Metadata\n+    total_workflows_analyzed: int\n+    data_quality_notes: List[str]\n+\n+\n+# ============================================================================\n+# Token Extraction Functions\n+# ============================================================================\n+\n+\n+def extract_token_usage(trace: Trace) -> Optional[TokenUsage]:\n+    \"\"\"\n+    Extract token usage from trace.\n+\n+    Checks multiple possible locations:\n+    1. Top-level trace fields (total_tokens, prompt_tokens, completion_tokens)\n+    2. trace.outputs[\"usage_metadata\"]\n+    3. trace.inputs[\"usage_metadata\"] (fallback)\n+    4. Extracts cache_read from input_token_details if available\n+\n+    Args:\n+        trace: Trace object to extract from\n+\n+    Returns:\n+        TokenUsage if data found, None otherwise\n+    \"\"\"\n+    # Try top-level token fields first (from updated export)\n+    if hasattr(trace, \"total_tokens\") and trace.total_tokens is not None:\n+        # LangSmith Run objects have: total_tokens, prompt_tokens, completion_tokens\n+        input_tokens = getattr(trace, \"prompt_tokens\", 0) or 0\n+        output_tokens = getattr(trace, \"completion_tokens\", 0) or 0\n+        total_tokens = trace.total_tokens\n+\n+        # Extract cache tokens from LangChain message format before returning\n+        cached_tokens = None\n+        if trace.outputs is not None and isinstance(trace.outputs, dict):\n+            if \"generations\" in trace.outputs:\n+                generations = trace.outputs.get(\"generations\", [[]])\n+                if generations and len(generations) > 0 and len(generations[0]) > 0:\n+                    message = generations[0][0]\n+                    if isinstance(message, dict):\n+                        message_obj = message.get(\"message\", {})\n+                        if isinstance(message_obj, dict):\n+                            kwargs = message_obj.get(\"kwargs\", {})\n+                            if isinstance(kwargs, dict):\n+                                usage_metadata = kwargs.get(\"usage_metadata\", {})\n+                                if isinstance(usage_metadata, dict):\n+                                    input_token_details = usage_metadata.get(\n+                                        \"input_token_details\", {}\n+                                    )\n+                                    if isinstance(input_token_details, dict):\n+                                        cached_tokens = input_token_details.get(\n+                                            \"cache_read\"\n+                                        )\n+\n+        return TokenUsage(\n+            input_tokens=input_tokens,\n+            output_tokens=output_tokens,\n+            total_tokens=total_tokens,\n+            cached_tokens=cached_tokens,\n+        )\n+\n+    # Try outputs (handle None outputs)\n+    usage_data = None\n+    cached_tokens = None\n+\n+    if trace.outputs is not None:\n+        # First try LangChain message format (where cache data lives)\n+        # Path: outputs.generations[0][0].message.kwargs.usage_metadata\n+        if \"generations\" in trace.outputs:\n+            generations = trace.outputs.get(\"generations\", [[]])\n+            if generations and len(generations) > 0 and len(generations[0]) > 0:\n+                message = generations[0][0]\n+                if isinstance(message, dict):\n+                    message_obj = message.get(\"message\", {})\n+                    if isinstance(message_obj, dict):\n+                        kwargs = message_obj.get(\"kwargs\", {})\n+                        if isinstance(kwargs, dict):\n+                            usage_metadata = kwargs.get(\"usage_metadata\", {})\n+                            if isinstance(usage_metadata, dict) and usage_metadata:\n+                                usage_data = usage_metadata\n+\n+        # Fallback to direct usage_metadata\n+        if not usage_data:\n+            usage_data = trace.outputs.get(\"usage_metadata\")\n+\n+    # Fallback to inputs (handle None inputs)\n+    if not usage_data and trace.inputs is not None:\n+        usage_data = trace.inputs.get(\"usage_metadata\")\n+\n+    if not usage_data:\n+        return None\n+\n+    # Safely extract with defaults\n+    input_tokens = usage_data.get(\"input_tokens\", 0)\n+    output_tokens = usage_data.get(\"output_tokens\", 0)\n+    total_tokens = usage_data.get(\"total_tokens\", input_tokens + output_tokens)\n+\n+    # Extract cache tokens if available\n+    if \"input_token_details\" in usage_data:\n+        token_details = usage_data[\"input_token_details\"]\n+        if isinstance(token_details, dict):\n+            cached_tokens = token_details.get(\"cache_read\")\n+\n+    return TokenUsage(\n+        input_tokens=input_tokens,\n+        output_tokens=output_tokens,\n+        total_tokens=total_tokens,\n+        cached_tokens=cached_tokens,\n+    )\n+\n+\n+# ============================================================================\n+# Cost Calculation Functions\n+# ============================================================================\n+\n+\n+def calculate_trace_cost(\n+    token_usage: TokenUsage,\n+    pricing_config: PricingConfig,\n+    trace_id: str = \"\",\n+    trace_name: str = \"\",\n+) -> CostBreakdown:\n+    \"\"\"\n+    Calculate cost for single trace using pricing model.\n+\n+    Args:\n+        token_usage: Token usage data\n+        pricing_config: Pricing configuration\n+        trace_id: Optional trace ID for breakdown\n+        trace_name: Optional trace name for breakdown\n+\n+    Returns:\n+        CostBreakdown with detailed cost information\n+    \"\"\"\n+    # Calculate input cost: (tokens / 1000) * price_per_1k\n+    input_cost = (\n+        token_usage.input_tokens / 1000.0\n+    ) * pricing_config.input_tokens_per_1k\n+\n+    # Calculate output cost\n+    output_cost = (\n+        token_usage.output_tokens / 1000.0\n+    ) * pricing_config.output_tokens_per_1k\n+\n+    # Calculate cache cost if applicable\n+    cache_cost = 0.0\n+    if (\n+        token_usage.cached_tokens is not None\n+        and pricing_config.cache_read_per_1k is not None\n+    ):\n+        cache_cost = (\n+            token_usage.cached_tokens / 1000.0\n+        ) * pricing_config.cache_read_per_1k\n+\n+    total_cost = input_cost + output_cost + cache_cost\n+\n+    return CostBreakdown(\n+        trace_id=trace_id,\n+        trace_name=trace_name,\n+        input_cost=input_cost,\n+        output_cost=output_cost,\n+        cache_cost=cache_cost,\n+        total_cost=total_cost,\n+        token_usage=token_usage,\n+    )\n+\n+\n+def calculate_workflow_cost(\n+    workflow: Workflow,\n+    pricing_config: PricingConfig,\n+) -> WorkflowCostAnalysis:\n+    \"\"\"\n+    Calculate total cost and breakdown by node for a workflow.\n+\n+    Args:\n+        workflow: Workflow to analyze\n+        pricing_config: Pricing configuration\n+\n+    Returns:\n+        WorkflowCostAnalysis with cost breakdown\n+    \"\"\"\n+    node_costs = []\n+    total_cost = 0.0\n+    total_tokens = 0\n+\n+    # Process all traces in workflow\n+    for trace in workflow.all_traces:\n+        token_usage = extract_token_usage(trace)\n+        if token_usage:\n+            cost_breakdown = calculate_trace_cost(\n+                token_usage,\n+                pricing_config,\n+                trace_id=trace.id,\n+                trace_name=trace.name,\n+            )\n+            node_costs.append(cost_breakdown)\n+            total_cost += cost_breakdown.total_cost\n+            total_tokens += token_usage.total_tokens\n+\n+    return WorkflowCostAnalysis(\n+        workflow_id=workflow.root_trace.id,\n+        total_cost=total_cost,\n+        node_costs=node_costs,\n+        total_tokens=total_tokens,\n+    )\n+\n+\n+# ============================================================================\n+# Scaling Projection Functions\n+# ============================================================================\n+\n+\n+def project_scaling_costs(\n+    avg_cost_per_workflow: float,\n+    current_workflow_count: int,\n+    scaling_factors: List[int],\n+    monthly_workflow_estimate: Optional[int] = None,\n+) -> Dict[str, ScalingProjection]:\n+    \"\"\"\n+    Project costs at different scale factors (1x, 10x, 100x, 1000x).\n+\n+    Args:\n+        avg_cost_per_workflow: Average cost per workflow in dollars\n+        current_workflow_count: Current number of workflows in dataset\n+        scaling_factors: List of scale factors (e.g., [1, 10, 100, 1000])\n+        monthly_workflow_estimate: Optional monthly workflow estimate for monthly cost\n+\n+    Returns:\n+        Dict mapping scale labels (\"1x\", \"10x\", etc.) to ScalingProjection objects\n+    \"\"\"\n+    projections = {}\n+\n+    for factor in scaling_factors:\n+        scaled_workflow_count = current_workflow_count * factor\n+        total_cost = avg_cost_per_workflow * scaled_workflow_count\n+\n+        # Calculate monthly cost if estimate provided\n+        cost_per_month = None\n+        if monthly_workflow_estimate is not None:\n+            monthly_workflows_at_scale = monthly_workflow_estimate * factor\n+            cost_per_month = avg_cost_per_workflow * monthly_workflows_at_scale\n+\n+        projection = ScalingProjection(\n+            scale_factor=factor,\n+            workflow_count=scaled_workflow_count,\n+            total_cost=total_cost,\n+            cost_per_month_30days=cost_per_month,\n+        )\n+\n+        # Create label (1x, 10x, 100x, etc.)\n+        label = f\"{factor}x\"\n+        projections[label] = projection\n+\n+    return projections\n+\n+\n+# ============================================================================\n+# Aggregation and Analysis Functions\n+# ============================================================================\n+\n+\n+def aggregate_node_costs(\n+    workflow_analyses: List[WorkflowCostAnalysis],\n+) -> List[NodeCostSummary]:\n+    \"\"\"\n+    Aggregate costs by node type across all workflows.\n+\n+    Args:\n+        workflow_analyses: List of WorkflowCostAnalysis objects\n+\n+    Returns:\n+        List of NodeCostSummary objects sorted by total_cost descending\n+    \"\"\"\n+    if not workflow_analyses:\n+        return []\n+\n+    # Aggregate by node name\n+    node_data: Dict[str, Dict[str, Any]] = {}\n+    total_overall_cost = 0.0\n+\n+    for workflow_analysis in workflow_analyses:\n+        for cost_breakdown in workflow_analysis.node_costs:\n+            node_name = cost_breakdown.trace_name\n+            if node_name not in node_data:\n+                node_data[node_name] = {\n+                    \"execution_count\": 0,\n+                    \"total_cost\": 0.0,\n+                }\n+            node_data[node_name][\"execution_count\"] += 1\n+            node_data[node_name][\"total_cost\"] += cost_breakdown.total_cost\n+            total_overall_cost += cost_breakdown.total_cost\n+\n+    # Create summaries with percentages\n+    summaries = []\n+    for node_name, data in node_data.items():\n+        execution_count = data[\"execution_count\"]\n+        total_cost = data[\"total_cost\"]\n+        avg_cost = total_cost / execution_count if execution_count > 0 else 0.0\n+        percent_of_total = (\n+            (total_cost / total_overall_cost * 100.0) if total_overall_cost > 0 else 0.0\n+        )\n+\n+        summary = NodeCostSummary(\n+            node_name=node_name,\n+            execution_count=execution_count,\n+            total_cost=total_cost,\n+            avg_cost_per_execution=avg_cost,\n+            percent_of_total_cost=percent_of_total,\n+        )\n+        summaries.append(summary)\n+\n+    # Sort by total cost descending\n+    summaries.sort(key=lambda s: s.total_cost, reverse=True)\n+\n+    return summaries\n+\n+\n+# ============================================================================\n+# Cache Effectiveness Functions\n+# ============================================================================\n+\n+\n+def calculate_cache_hit_rate(workflow_analyses: List[WorkflowCostAnalysis]) -> float:\n+    \"\"\"\n+    Calculate percentage of traces that used cache.\n+\n+    Args:\n+        workflow_analyses: List of WorkflowCostAnalysis objects\n+\n+    Returns:\n+        Percentage of traces with cache data (0-100)\n+    \"\"\"\n+    if not workflow_analyses:\n+        return 0.0\n+\n+    total_traces = 0\n+    cached_traces = 0\n+\n+    for workflow_analysis in workflow_analyses:\n+        for cost_breakdown in workflow_analysis.node_costs:\n+            total_traces += 1\n+            if cost_breakdown.token_usage.has_cache_data():\n+                cached_traces += 1\n+\n+    if total_traces == 0:\n+        return 0.0\n+\n+    return (cached_traces / total_traces) * 100.0\n+\n+\n+def calculate_cache_savings(\n+    workflow_analyses: List[WorkflowCostAnalysis],\n+    pricing_config: PricingConfig,\n+) -> float:\n+    \"\"\"\n+    Calculate total cost savings from cache usage.\n+\n+    Compares actual cache costs to what costs would have been if cache tokens\n+    were charged at input token rate.\n+\n+    Args:\n+        workflow_analyses: List of WorkflowCostAnalysis objects\n+        pricing_config: Pricing configuration\n+\n+    Returns:\n+        Total savings in dollars from using cache\n+    \"\"\"\n+    if not workflow_analyses:\n+        return 0.0\n+\n+    if pricing_config.cache_read_per_1k is None:\n+        return 0.0  # Cannot calculate savings without cache pricing\n+\n+    total_savings = 0.0\n+\n+    for workflow_analysis in workflow_analyses:\n+        for cost_breakdown in workflow_analysis.node_costs:\n+            if cost_breakdown.token_usage.has_cache_data():\n+                cached_tokens = cost_breakdown.token_usage.cached_tokens\n+                if cached_tokens is None:\n+                    continue\n+\n+                # Cost if these tokens were charged at input rate\n+                cost_without_cache = (\n+                    cached_tokens / 1000.0\n+                ) * pricing_config.input_tokens_per_1k\n+\n+                # Actual cost at cache rate\n+                cost_with_cache = (\n+                    cached_tokens / 1000.0\n+                ) * pricing_config.cache_read_per_1k\n+\n+                # Savings is the difference\n+                savings = cost_without_cache - cost_with_cache\n+                total_savings += savings\n+\n+    return total_savings\n+\n+\n+def compare_cached_vs_fresh_costs(\n+    workflow_analyses: List[WorkflowCostAnalysis],\n+    pricing_config: PricingConfig,\n+) -> CacheCostComparison:\n+    \"\"\"\n+    Compare total costs with cache vs hypothetical costs without cache.\n+\n+    Provides detailed breakdown showing actual cost using cache vs what cost\n+    would have been if cache tokens were charged at input token rate.\n+\n+    Args:\n+        workflow_analyses: List of WorkflowCostAnalysis objects\n+        pricing_config: Pricing configuration\n+\n+    Returns:\n+        CacheCostComparison with detailed cost comparison\n+    \"\"\"\n+    cost_with_cache = 0.0\n+    cost_without_cache = 0.0\n+    traces_analyzed = 0\n+    traces_with_cache = 0\n+\n+    for workflow_analysis in workflow_analyses:\n+        for cost_breakdown in workflow_analysis.node_costs:\n+            traces_analyzed += 1\n+\n+            # Add actual cost (with cache)\n+            cost_with_cache += cost_breakdown.total_cost\n+\n+            # Calculate hypothetical cost without cache\n+            if cost_breakdown.token_usage.has_cache_data():\n+                traces_with_cache += 1\n+\n+                # If cache pricing available, calculate what cost would have been\n+                if pricing_config.cache_read_per_1k is not None:\n+                    cached_tokens = cost_breakdown.token_usage.cached_tokens\n+                    if cached_tokens is None:\n+                        cost_without_cache += cost_breakdown.total_cost\n+                        continue\n+\n+                    # Remove actual cache cost\n+                    cost_without_this_cache = (\n+                        cost_breakdown.total_cost - cost_breakdown.cache_cost\n+                    )\n+\n+                    # Add what it would cost at input token rate\n+                    hypothetical_input_cost = (\n+                        cached_tokens / 1000.0\n+                    ) * pricing_config.input_tokens_per_1k\n+\n+                    cost_without_cache += (\n+                        cost_without_this_cache + hypothetical_input_cost\n+                    )\n+                else:\n+                    # No cache pricing, so cost would be same\n+                    cost_without_cache += cost_breakdown.total_cost\n+            else:\n+                # No cache data, cost is same with or without cache\n+                cost_without_cache += cost_breakdown.total_cost\n+\n+    # Calculate savings\n+    total_savings = cost_without_cache - cost_with_cache\n+\n+    # Calculate savings percentage\n+    if cost_without_cache > 0:\n+        savings_percent = (total_savings / cost_without_cache) * 100.0\n+    else:\n+        savings_percent = 0.0\n+\n+    return CacheCostComparison(\n+        cost_with_cache=cost_with_cache,\n+        cost_without_cache=cost_without_cache,\n+        total_savings=total_savings,\n+        savings_percent=savings_percent,\n+        traces_analyzed=traces_analyzed,\n+        traces_with_cache=traces_with_cache,\n+    )\n+\n+\n+def analyze_costs(\n+    workflows: List[Workflow],\n+    pricing_config: PricingConfig,\n+    scaling_factors: Optional[List[int]] = None,\n+    monthly_workflow_estimate: Optional[int] = None,\n+) -> CostAnalysisResults:\n+    \"\"\"\n+    Perform complete cost analysis on workflows.\n+    Main entry point for Phase 3B.\n+\n+    Args:\n+        workflows: List of Workflow objects to analyze\n+        pricing_config: Pricing configuration for cost calculation\n+        scaling_factors: Optional list of scale factors (default: [1, 10, 100, 1000])\n+        monthly_workflow_estimate: Optional monthly workflow estimate for projections\n+\n+    Returns:\n+        CostAnalysisResults with complete analysis\n+    \"\"\"\n+    if scaling_factors is None:\n+        scaling_factors = SCALING_FACTORS\n+\n+    data_quality_notes = []\n+\n+    # Calculate cost for each workflow\n+    workflow_analyses = []\n+    workflow_costs = []\n+\n+    for workflow in workflows:\n+        analysis = calculate_workflow_cost(workflow, pricing_config)\n+        workflow_analyses.append(analysis)\n+        workflow_costs.append(analysis.total_cost)\n+\n+    # Calculate per-workflow statistics\n+    if workflow_costs:\n+        avg_cost = sum(workflow_costs) / len(workflow_costs)\n+        sorted_costs = sorted(workflow_costs)\n+        median_cost = sorted_costs[len(sorted_costs) // 2]\n+        min_cost = min(workflow_costs)\n+        max_cost = max(workflow_costs)\n+    else:\n+        avg_cost = median_cost = min_cost = max_cost = 0.0\n+        data_quality_notes.append(\"No workflows with cost data found\")\n+\n+    # Aggregate node costs\n+    node_summaries = aggregate_node_costs(workflow_analyses)\n+    top_cost_driver = node_summaries[0].node_name if node_summaries else None\n+\n+    # Calculate scaling projections\n+    scaling_projections = project_scaling_costs(\n+        avg_cost_per_workflow=avg_cost,\n+        current_workflow_count=len(workflows),\n+        scaling_factors=scaling_factors,\n+        monthly_workflow_estimate=monthly_workflow_estimate,\n+    )\n+\n+    # Cache effectiveness analysis\n+    cache_effectiveness_percent = calculate_cache_hit_rate(workflow_analyses)\n+    cache_savings_dollars = calculate_cache_savings(workflow_analyses, pricing_config)\n+\n+    return CostAnalysisResults(\n+        avg_cost_per_workflow=avg_cost,\n+        median_cost_per_workflow=median_cost,\n+        min_cost=min_cost,\n+        max_cost=max_cost,\n+        node_summaries=node_summaries,\n+        top_cost_driver=top_cost_driver,\n+        cache_effectiveness_percent=cache_effectiveness_percent,\n+        cache_savings_dollars=cache_savings_dollars,\n+        scaling_projections=scaling_projections,\n+        total_workflows_analyzed=len(workflows),\n+        data_quality_notes=data_quality_notes,\n+    )",
      "patch_lines": [
        "@@ -0,0 +1,718 @@\n",
        "+\"\"\"\n",
        "+LangSmith Trace Cost Analysis Tool - Phase 3B\n",
        "+\n",
        "+This module provides cost analysis capabilities for LangSmith trace exports.\n",
        "+Calculates costs based on token usage with configurable pricing models.\n",
        "+\n",
        "+Following PDCA (Plan-Do-Check-Act) methodology with TDD approach.\n",
        "+\n",
        "+Author: Generated with Claude Code (PDCA Framework)\n",
        "+Date: 2025-12-09\n",
        "+\"\"\"\n",
        "+\n",
        "+from dataclasses import dataclass\n",
        "+from typing import Any, Dict, List, Optional\n",
        "+from analyze_traces import Trace, Workflow\n",
        "+\n",
        "+\n",
        "+# ============================================================================\n",
        "+# Pricing Configuration\n",
        "+# ============================================================================\n",
        "+\n",
        "+\n",
        "+@dataclass\n",
        "+class PricingConfig:\n",
        "+    \"\"\"Configurable pricing model for any LLM provider.\"\"\"\n",
        "+\n",
        "+    model_name: str\n",
        "+    input_tokens_per_1k: float  # Cost per 1K input tokens\n",
        "+    output_tokens_per_1k: float  # Cost per 1K output tokens\n",
        "+    cache_read_per_1k: Optional[float] = (\n",
        "+        None  # Cost per 1K cache read tokens (if applicable)\n",
        "+    )\n",
        "+\n",
        "+    def __post_init__(self) -> None:\n",
        "+        \"\"\"Validate pricing configuration.\"\"\"\n",
        "+        if self.input_tokens_per_1k < 0 or self.output_tokens_per_1k < 0:\n",
        "+            raise ValueError(\"Token prices must be non-negative\")\n",
        "+        if self.cache_read_per_1k is not None and self.cache_read_per_1k < 0:\n",
        "+            raise ValueError(\"Cache read price must be non-negative\")\n",
        "+\n",
        "+\n",
        "+# Example pricing configs for reference (NOT hard-coded defaults)\n",
        "+# Users should create their own PricingConfig instances\n",
        "+EXAMPLE_PRICING_CONFIGS = {\n",
        "+    \"gemini_1.5_pro\": PricingConfig(\n",
        "+        model_name=\"Gemini 1.5 Pro\",\n",
        "+        input_tokens_per_1k=0.00125,  # $1.25 per 1M input tokens\n",
        "+        output_tokens_per_1k=0.005,  # $5.00 per 1M output tokens\n",
        "+        cache_read_per_1k=0.0003125,  # $0.3125 per 1M cache read tokens\n",
        "+    ),\n",
        "+}\n",
        "+\n",
        "+SCALING_FACTORS = [1, 10, 100, 1000]  # Current, 10x, 100x, 1000x\n",
        "+\n",
        "+\n",
        "+# ============================================================================\n",
        "+# Core Data Structures\n",
        "+# ============================================================================\n",
        "+\n",
        "+\n",
        "+@dataclass\n",
        "+class TokenUsage:\n",
        "+    \"\"\"Token usage for a single trace.\"\"\"\n",
        "+\n",
        "+    input_tokens: int\n",
        "+    output_tokens: int\n",
        "+    total_tokens: int\n",
        "+    cached_tokens: Optional[int] = None  # From input_token_details.cache_read\n",
        "+\n",
        "+    def has_cache_data(self) -> bool:\n",
        "+        \"\"\"Check if cache token data is available.\"\"\"\n",
        "+        return self.cached_tokens is not None\n",
        "+\n",
        "+\n",
        "+@dataclass\n",
        "+class CostBreakdown:\n",
        "+    \"\"\"Cost breakdown for a single trace.\"\"\"\n",
        "+\n",
        "+    trace_id: str\n",
        "+    trace_name: str\n",
        "+    input_cost: float\n",
        "+    output_cost: float\n",
        "+    cache_cost: float\n",
        "+    total_cost: float\n",
        "+    token_usage: TokenUsage\n",
        "+\n",
        "+    def to_dict(self) -> Dict[str, Any]:\n",
        "+        \"\"\"Export to dictionary for CSV/reporting.\"\"\"\n",
        "+        return {\n",
        "+            \"trace_id\": self.trace_id,\n",
        "+            \"trace_name\": self.trace_name,\n",
        "+            \"input_tokens\": self.token_usage.input_tokens,\n",
        "+            \"output_tokens\": self.token_usage.output_tokens,\n",
        "+            \"total_tokens\": self.token_usage.total_tokens,\n",
        "+            \"input_cost\": self.input_cost,\n",
        "+            \"output_cost\": self.output_cost,\n",
        "+            \"cache_cost\": self.cache_cost,\n",
        "+            \"total_cost\": self.total_cost,\n",
        "+        }\n",
        "+\n",
        "+\n",
        "+@dataclass\n",
        "+class WorkflowCostAnalysis:\n",
        "+    \"\"\"Cost analysis for a single workflow.\"\"\"\n",
        "+\n",
        "+    workflow_id: str\n",
        "+    total_cost: float\n",
        "+    node_costs: List[CostBreakdown]\n",
        "+    total_tokens: int\n",
        "+    cache_effectiveness_percent: Optional[float] = None  # If cache data available\n",
        "+\n",
        "+\n",
        "+@dataclass\n",
        "+class ScalingProjection:\n",
        "+    \"\"\"Cost projection at a specific scale factor.\"\"\"\n",
        "+\n",
        "+    scale_factor: int\n",
        "+    workflow_count: int\n",
        "+    total_cost: float\n",
        "+    cost_per_month_30days: Optional[float] = None  # If monthly estimate provided\n",
        "+\n",
        "+\n",
        "+@dataclass\n",
        "+class NodeCostSummary:\n",
        "+    \"\"\"Cost summary for a node type across workflows.\"\"\"\n",
        "+\n",
        "+    node_name: str\n",
        "+    execution_count: int\n",
        "+    total_cost: float\n",
        "+    avg_cost_per_execution: float\n",
        "+    percent_of_total_cost: float\n",
        "+\n",
        "+\n",
        "+@dataclass\n",
        "+class CacheCostComparison:\n",
        "+    \"\"\"Comparison of costs with cache vs without cache.\"\"\"\n",
        "+\n",
        "+    cost_with_cache: float  # Actual cost using cache\n",
        "+    cost_without_cache: float  # Hypothetical cost if no cache used\n",
        "+    total_savings: float  # Dollar savings from using cache\n",
        "+    savings_percent: float  # Percentage savings (0-100)\n",
        "+    traces_analyzed: int  # Total number of traces analyzed\n",
        "+    traces_with_cache: int  # Number of traces that used cache\n",
        "+\n",
        "+\n",
        "+@dataclass\n",
        "+class CostAnalysisResults:\n",
        "+    \"\"\"Complete cost analysis results.\"\"\"\n",
        "+\n",
        "+    # Per-workflow metrics\n",
        "+    avg_cost_per_workflow: float\n",
        "+    median_cost_per_workflow: float\n",
        "+    min_cost: float\n",
        "+    max_cost: float\n",
        "+\n",
        "+    # Node-level breakdown\n",
        "+    node_summaries: List[NodeCostSummary]  # Sorted by cost descending\n",
        "+    top_cost_driver: Optional[str]\n",
        "+\n",
        "+    # Cache effectiveness\n",
        "+    cache_effectiveness_percent: Optional[float]\n",
        "+    cache_savings_dollars: Optional[float]\n",
        "+\n",
        "+    # Scaling projections\n",
        "+    scaling_projections: Dict[str, ScalingProjection]  # \"10x\", \"100x\", etc.\n",
        "+\n",
        "+    # Metadata\n",
        "+    total_workflows_analyzed: int\n",
        "+    data_quality_notes: List[str]\n",
        "+\n",
        "+\n",
        "+# ============================================================================\n",
        "+# Token Extraction Functions\n",
        "+# ============================================================================\n",
        "+\n",
        "+\n",
        "+def extract_token_usage(trace: Trace) -> Optional[TokenUsage]:\n",
        "+    \"\"\"\n",
        "+    Extract token usage from trace.\n",
        "+\n",
        "+    Checks multiple possible locations:\n",
        "+    1. Top-level trace fields (total_tokens, prompt_tokens, completion_tokens)\n",
        "+    2. trace.outputs[\"usage_metadata\"]\n",
        "+    3. trace.inputs[\"usage_metadata\"] (fallback)\n",
        "+    4. Extracts cache_read from input_token_details if available\n",
        "+\n",
        "+    Args:\n",
        "+        trace: Trace object to extract from\n",
        "+\n",
        "+    Returns:\n",
        "+        TokenUsage if data found, None otherwise\n",
        "+    \"\"\"\n",
        "+    # Try top-level token fields first (from updated export)\n",
        "+    if hasattr(trace, \"total_tokens\") and trace.total_tokens is not None:\n",
        "+        # LangSmith Run objects have: total_tokens, prompt_tokens, completion_tokens\n",
        "+        input_tokens = getattr(trace, \"prompt_tokens\", 0) or 0\n",
        "+        output_tokens = getattr(trace, \"completion_tokens\", 0) or 0\n",
        "+        total_tokens = trace.total_tokens\n",
        "+\n",
        "+        # Extract cache tokens from LangChain message format before returning\n",
        "+        cached_tokens = None\n",
        "+        if trace.outputs is not None and isinstance(trace.outputs, dict):\n",
        "+            if \"generations\" in trace.outputs:\n",
        "+                generations = trace.outputs.get(\"generations\", [[]])\n",
        "+                if generations and len(generations) > 0 and len(generations[0]) > 0:\n",
        "+                    message = generations[0][0]\n",
        "+                    if isinstance(message, dict):\n",
        "+                        message_obj = message.get(\"message\", {})\n",
        "+                        if isinstance(message_obj, dict):\n",
        "+                            kwargs = message_obj.get(\"kwargs\", {})\n",
        "+                            if isinstance(kwargs, dict):\n",
        "+                                usage_metadata = kwargs.get(\"usage_metadata\", {})\n",
        "+                                if isinstance(usage_metadata, dict):\n",
        "+                                    input_token_details = usage_metadata.get(\n",
        "+                                        \"input_token_details\", {}\n",
        "+                                    )\n",
        "+                                    if isinstance(input_token_details, dict):\n",
        "+                                        cached_tokens = input_token_details.get(\n",
        "+                                            \"cache_read\"\n",
        "+                                        )\n",
        "+\n",
        "+        return TokenUsage(\n",
        "+            input_tokens=input_tokens,\n",
        "+            output_tokens=output_tokens,\n",
        "+            total_tokens=total_tokens,\n",
        "+            cached_tokens=cached_tokens,\n",
        "+        )\n",
        "+\n",
        "+    # Try outputs (handle None outputs)\n",
        "+    usage_data = None\n",
        "+    cached_tokens = None\n",
        "+\n",
        "+    if trace.outputs is not None:\n",
        "+        # First try LangChain message format (where cache data lives)\n",
        "+        # Path: outputs.generations[0][0].message.kwargs.usage_metadata\n",
        "+        if \"generations\" in trace.outputs:\n",
        "+            generations = trace.outputs.get(\"generations\", [[]])\n",
        "+            if generations and len(generations) > 0 and len(generations[0]) > 0:\n",
        "+                message = generations[0][0]\n",
        "+                if isinstance(message, dict):\n",
        "+                    message_obj = message.get(\"message\", {})\n",
        "+                    if isinstance(message_obj, dict):\n",
        "+                        kwargs = message_obj.get(\"kwargs\", {})\n",
        "+                        if isinstance(kwargs, dict):\n",
        "+                            usage_metadata = kwargs.get(\"usage_metadata\", {})\n",
        "+                            if isinstance(usage_metadata, dict) and usage_metadata:\n",
        "+                                usage_data = usage_metadata\n",
        "+\n",
        "+        # Fallback to direct usage_metadata\n",
        "+        if not usage_data:\n",
        "+            usage_data = trace.outputs.get(\"usage_metadata\")\n",
        "+\n",
        "+    # Fallback to inputs (handle None inputs)\n",
        "+    if not usage_data and trace.inputs is not None:\n",
        "+        usage_data = trace.inputs.get(\"usage_metadata\")\n",
        "+\n",
        "+    if not usage_data:\n",
        "+        return None\n",
        "+\n",
        "+    # Safely extract with defaults\n",
        "+    input_tokens = usage_data.get(\"input_tokens\", 0)\n",
        "+    output_tokens = usage_data.get(\"output_tokens\", 0)\n",
        "+    total_tokens = usage_data.get(\"total_tokens\", input_tokens + output_tokens)\n",
        "+\n",
        "+    # Extract cache tokens if available\n",
        "+    if \"input_token_details\" in usage_data:\n",
        "+        token_details = usage_data[\"input_token_details\"]\n",
        "+        if isinstance(token_details, dict):\n",
        "+            cached_tokens = token_details.get(\"cache_read\")\n",
        "+\n",
        "+    return TokenUsage(\n",
        "+        input_tokens=input_tokens,\n",
        "+        output_tokens=output_tokens,\n",
        "+        total_tokens=total_tokens,\n",
        "+        cached_tokens=cached_tokens,\n",
        "+    )\n",
        "+\n",
        "+\n",
        "+# ============================================================================\n",
        "+# Cost Calculation Functions\n",
        "+# ============================================================================\n",
        "+\n",
        "+\n",
        "+def calculate_trace_cost(\n",
        "+    token_usage: TokenUsage,\n",
        "+    pricing_config: PricingConfig,\n",
        "+    trace_id: str = \"\",\n",
        "+    trace_name: str = \"\",\n",
        "+) -> CostBreakdown:\n",
        "+    \"\"\"\n",
        "+    Calculate cost for single trace using pricing model.\n",
        "+\n",
        "+    Args:\n",
        "+        token_usage: Token usage data\n",
        "+        pricing_config: Pricing configuration\n",
        "+        trace_id: Optional trace ID for breakdown\n",
        "+        trace_name: Optional trace name for breakdown\n",
        "+\n",
        "+    Returns:\n",
        "+        CostBreakdown with detailed cost information\n",
        "+    \"\"\"\n",
        "+    # Calculate input cost: (tokens / 1000) * price_per_1k\n",
        "+    input_cost = (\n",
        "+        token_usage.input_tokens / 1000.0\n",
        "+    ) * pricing_config.input_tokens_per_1k\n",
        "+\n",
        "+    # Calculate output cost\n",
        "+    output_cost = (\n",
        "+        token_usage.output_tokens / 1000.0\n",
        "+    ) * pricing_config.output_tokens_per_1k\n",
        "+\n",
        "+    # Calculate cache cost if applicable\n",
        "+    cache_cost = 0.0\n",
        "+    if (\n",
        "+        token_usage.cached_tokens is not None\n",
        "+        and pricing_config.cache_read_per_1k is not None\n",
        "+    ):\n",
        "+        cache_cost = (\n",
        "+            token_usage.cached_tokens / 1000.0\n",
        "+        ) * pricing_config.cache_read_per_1k\n",
        "+\n",
        "+    total_cost = input_cost + output_cost + cache_cost\n",
        "+\n",
        "+    return CostBreakdown(\n",
        "+        trace_id=trace_id,\n",
        "+        trace_name=trace_name,\n",
        "+        input_cost=input_cost,\n",
        "+        output_cost=output_cost,\n",
        "+        cache_cost=cache_cost,\n",
        "+        total_cost=total_cost,\n",
        "+        token_usage=token_usage,\n",
        "+    )\n",
        "+\n",
        "+\n",
        "+def calculate_workflow_cost(\n",
        "+    workflow: Workflow,\n",
        "+    pricing_config: PricingConfig,\n",
        "+) -> WorkflowCostAnalysis:\n",
        "+    \"\"\"\n",
        "+    Calculate total cost and breakdown by node for a workflow.\n",
        "+\n",
        "+    Args:\n",
        "+        workflow: Workflow to analyze\n",
        "+        pricing_config: Pricing configuration\n",
        "+\n",
        "+    Returns:\n",
        "+        WorkflowCostAnalysis with cost breakdown\n",
        "+    \"\"\"\n",
        "+    node_costs = []\n",
        "+    total_cost = 0.0\n",
        "+    total_tokens = 0\n",
        "+\n",
        "+    # Process all traces in workflow\n",
        "+    for trace in workflow.all_traces:\n",
        "+        token_usage = extract_token_usage(trace)\n",
        "+        if token_usage:\n",
        "+            cost_breakdown = calculate_trace_cost(\n",
        "+                token_usage,\n",
        "+                pricing_config,\n",
        "+                trace_id=trace.id,\n",
        "+                trace_name=trace.name,\n",
        "+            )\n",
        "+            node_costs.append(cost_breakdown)\n",
        "+            total_cost += cost_breakdown.total_cost\n",
        "+            total_tokens += token_usage.total_tokens\n",
        "+\n",
        "+    return WorkflowCostAnalysis(\n",
        "+        workflow_id=workflow.root_trace.id,\n",
        "+        total_cost=total_cost,\n",
        "+        node_costs=node_costs,\n",
        "+        total_tokens=total_tokens,\n",
        "+    )\n",
        "+\n",
        "+\n",
        "+# ============================================================================\n",
        "+# Scaling Projection Functions\n",
        "+# ============================================================================\n",
        "+\n",
        "+\n",
        "+def project_scaling_costs(\n",
        "+    avg_cost_per_workflow: float,\n",
        "+    current_workflow_count: int,\n",
        "+    scaling_factors: List[int],\n",
        "+    monthly_workflow_estimate: Optional[int] = None,\n",
        "+) -> Dict[str, ScalingProjection]:\n",
        "+    \"\"\"\n",
        "+    Project costs at different scale factors (1x, 10x, 100x, 1000x).\n",
        "+\n",
        "+    Args:\n",
        "+        avg_cost_per_workflow: Average cost per workflow in dollars\n",
        "+        current_workflow_count: Current number of workflows in dataset\n",
        "+        scaling_factors: List of scale factors (e.g., [1, 10, 100, 1000])\n",
        "+        monthly_workflow_estimate: Optional monthly workflow estimate for monthly cost\n",
        "+\n",
        "+    Returns:\n",
        "+        Dict mapping scale labels (\"1x\", \"10x\", etc.) to ScalingProjection objects\n",
        "+    \"\"\"\n",
        "+    projections = {}\n",
        "+\n",
        "+    for factor in scaling_factors:\n",
        "+        scaled_workflow_count = current_workflow_count * factor\n",
        "+        total_cost = avg_cost_per_workflow * scaled_workflow_count\n",
        "+\n",
        "+        # Calculate monthly cost if estimate provided\n",
        "+        cost_per_month = None\n",
        "+        if monthly_workflow_estimate is not None:\n",
        "+            monthly_workflows_at_scale = monthly_workflow_estimate * factor\n",
        "+            cost_per_month = avg_cost_per_workflow * monthly_workflows_at_scale\n",
        "+\n",
        "+        projection = ScalingProjection(\n",
        "+            scale_factor=factor,\n",
        "+            workflow_count=scaled_workflow_count,\n",
        "+            total_cost=total_cost,\n",
        "+            cost_per_month_30days=cost_per_month,\n",
        "+        )\n",
        "+\n",
        "+        # Create label (1x, 10x, 100x, etc.)\n",
        "+        label = f\"{factor}x\"\n",
        "+        projections[label] = projection\n",
        "+\n",
        "+    return projections\n",
        "+\n",
        "+\n",
        "+# ============================================================================\n",
        "+# Aggregation and Analysis Functions\n",
        "+# ============================================================================\n",
        "+\n",
        "+\n",
        "+def aggregate_node_costs(\n",
        "+    workflow_analyses: List[WorkflowCostAnalysis],\n",
        "+) -> List[NodeCostSummary]:\n",
        "+    \"\"\"\n",
        "+    Aggregate costs by node type across all workflows.\n",
        "+\n",
        "+    Args:\n",
        "+        workflow_analyses: List of WorkflowCostAnalysis objects\n",
        "+\n",
        "+    Returns:\n",
        "+        List of NodeCostSummary objects sorted by total_cost descending\n",
        "+    \"\"\"\n",
        "+    if not workflow_analyses:\n",
        "+        return []\n",
        "+\n",
        "+    # Aggregate by node name\n",
        "+    node_data: Dict[str, Dict[str, Any]] = {}\n",
        "+    total_overall_cost = 0.0\n",
        "+\n",
        "+    for workflow_analysis in workflow_analyses:\n",
        "+        for cost_breakdown in workflow_analysis.node_costs:\n",
        "+            node_name = cost_breakdown.trace_name\n",
        "+            if node_name not in node_data:\n",
        "+                node_data[node_name] = {\n",
        "+                    \"execution_count\": 0,\n",
        "+                    \"total_cost\": 0.0,\n",
        "+                }\n",
        "+            node_data[node_name][\"execution_count\"] += 1\n",
        "+            node_data[node_name][\"total_cost\"] += cost_breakdown.total_cost\n",
        "+            total_overall_cost += cost_breakdown.total_cost\n",
        "+\n",
        "+    # Create summaries with percentages\n",
        "+    summaries = []\n",
        "+    for node_name, data in node_data.items():\n",
        "+        execution_count = data[\"execution_count\"]\n",
        "+        total_cost = data[\"total_cost\"]\n",
        "+        avg_cost = total_cost / execution_count if execution_count > 0 else 0.0\n",
        "+        percent_of_total = (\n",
        "+            (total_cost / total_overall_cost * 100.0) if total_overall_cost > 0 else 0.0\n",
        "+        )\n",
        "+\n",
        "+        summary = NodeCostSummary(\n",
        "+            node_name=node_name,\n",
        "+            execution_count=execution_count,\n",
        "+            total_cost=total_cost,\n",
        "+            avg_cost_per_execution=avg_cost,\n",
        "+            percent_of_total_cost=percent_of_total,\n",
        "+        )\n",
        "+        summaries.append(summary)\n",
        "+\n",
        "+    # Sort by total cost descending\n",
        "+    summaries.sort(key=lambda s: s.total_cost, reverse=True)\n",
        "+\n",
        "+    return summaries\n",
        "+\n",
        "+\n",
        "+# ============================================================================\n",
        "+# Cache Effectiveness Functions\n",
        "+# ============================================================================\n",
        "+\n",
        "+\n",
        "+def calculate_cache_hit_rate(workflow_analyses: List[WorkflowCostAnalysis]) -> float:\n",
        "+    \"\"\"\n",
        "+    Calculate percentage of traces that used cache.\n",
        "+\n",
        "+    Args:\n",
        "+        workflow_analyses: List of WorkflowCostAnalysis objects\n",
        "+\n",
        "+    Returns:\n",
        "+        Percentage of traces with cache data (0-100)\n",
        "+    \"\"\"\n",
        "+    if not workflow_analyses:\n",
        "+        return 0.0\n",
        "+\n",
        "+    total_traces = 0\n",
        "+    cached_traces = 0\n",
        "+\n",
        "+    for workflow_analysis in workflow_analyses:\n",
        "+        for cost_breakdown in workflow_analysis.node_costs:\n",
        "+            total_traces += 1\n",
        "+            if cost_breakdown.token_usage.has_cache_data():\n",
        "+                cached_traces += 1\n",
        "+\n",
        "+    if total_traces == 0:\n",
        "+        return 0.0\n",
        "+\n",
        "+    return (cached_traces / total_traces) * 100.0\n",
        "+\n",
        "+\n",
        "+def calculate_cache_savings(\n",
        "+    workflow_analyses: List[WorkflowCostAnalysis],\n",
        "+    pricing_config: PricingConfig,\n",
        "+) -> float:\n",
        "+    \"\"\"\n",
        "+    Calculate total cost savings from cache usage.\n",
        "+\n",
        "+    Compares actual cache costs to what costs would have been if cache tokens\n",
        "+    were charged at input token rate.\n",
        "+\n",
        "+    Args:\n",
        "+        workflow_analyses: List of WorkflowCostAnalysis objects\n",
        "+        pricing_config: Pricing configuration\n",
        "+\n",
        "+    Returns:\n",
        "+        Total savings in dollars from using cache\n",
        "+    \"\"\"\n",
        "+    if not workflow_analyses:\n",
        "+        return 0.0\n",
        "+\n",
        "+    if pricing_config.cache_read_per_1k is None:\n",
        "+        return 0.0  # Cannot calculate savings without cache pricing\n",
        "+\n",
        "+    total_savings = 0.0\n",
        "+\n",
        "+    for workflow_analysis in workflow_analyses:\n",
        "+        for cost_breakdown in workflow_analysis.node_costs:\n",
        "+            if cost_breakdown.token_usage.has_cache_data():\n",
        "+                cached_tokens = cost_breakdown.token_usage.cached_tokens\n",
        "+                if cached_tokens is None:\n",
        "+                    continue\n",
        "+\n",
        "+                # Cost if these tokens were charged at input rate\n",
        "+                cost_without_cache = (\n",
        "+                    cached_tokens / 1000.0\n",
        "+                ) * pricing_config.input_tokens_per_1k\n",
        "+\n",
        "+                # Actual cost at cache rate\n",
        "+                cost_with_cache = (\n",
        "+                    cached_tokens / 1000.0\n",
        "+                ) * pricing_config.cache_read_per_1k\n",
        "+\n",
        "+                # Savings is the difference\n",
        "+                savings = cost_without_cache - cost_with_cache\n",
        "+                total_savings += savings\n",
        "+\n",
        "+    return total_savings\n",
        "+\n",
        "+\n",
        "+def compare_cached_vs_fresh_costs(\n",
        "+    workflow_analyses: List[WorkflowCostAnalysis],\n",
        "+    pricing_config: PricingConfig,\n",
        "+) -> CacheCostComparison:\n",
        "+    \"\"\"\n",
        "+    Compare total costs with cache vs hypothetical costs without cache.\n",
        "+\n",
        "+    Provides detailed breakdown showing actual cost using cache vs what cost\n",
        "+    would have been if cache tokens were charged at input token rate.\n",
        "+\n",
        "+    Args:\n",
        "+        workflow_analyses: List of WorkflowCostAnalysis objects\n",
        "+        pricing_config: Pricing configuration\n",
        "+\n",
        "+    Returns:\n",
        "+        CacheCostComparison with detailed cost comparison\n",
        "+    \"\"\"\n",
        "+    cost_with_cache = 0.0\n",
        "+    cost_without_cache = 0.0\n",
        "+    traces_analyzed = 0\n",
        "+    traces_with_cache = 0\n",
        "+\n",
        "+    for workflow_analysis in workflow_analyses:\n",
        "+        for cost_breakdown in workflow_analysis.node_costs:\n",
        "+            traces_analyzed += 1\n",
        "+\n",
        "+            # Add actual cost (with cache)\n",
        "+            cost_with_cache += cost_breakdown.total_cost\n",
        "+\n",
        "+            # Calculate hypothetical cost without cache\n",
        "+            if cost_breakdown.token_usage.has_cache_data():\n",
        "+                traces_with_cache += 1\n",
        "+\n",
        "+                # If cache pricing available, calculate what cost would have been\n",
        "+                if pricing_config.cache_read_per_1k is not None:\n",
        "+                    cached_tokens = cost_breakdown.token_usage.cached_tokens\n",
        "+                    if cached_tokens is None:\n",
        "+                        cost_without_cache += cost_breakdown.total_cost\n",
        "+                        continue\n",
        "+\n",
        "+                    # Remove actual cache cost\n",
        "+                    cost_without_this_cache = (\n",
        "+                        cost_breakdown.total_cost - cost_breakdown.cache_cost\n",
        "+                    )\n",
        "+\n",
        "+                    # Add what it would cost at input token rate\n",
        "+                    hypothetical_input_cost = (\n",
        "+                        cached_tokens / 1000.0\n",
        "+                    ) * pricing_config.input_tokens_per_1k\n",
        "+\n",
        "+                    cost_without_cache += (\n",
        "+                        cost_without_this_cache + hypothetical_input_cost\n",
        "+                    )\n",
        "+                else:\n",
        "+                    # No cache pricing, so cost would be same\n",
        "+                    cost_without_cache += cost_breakdown.total_cost\n",
        "+            else:\n",
        "+                # No cache data, cost is same with or without cache\n",
        "+                cost_without_cache += cost_breakdown.total_cost\n",
        "+\n",
        "+    # Calculate savings\n",
        "+    total_savings = cost_without_cache - cost_with_cache\n",
        "+\n",
        "+    # Calculate savings percentage\n",
        "+    if cost_without_cache > 0:\n",
        "+        savings_percent = (total_savings / cost_without_cache) * 100.0\n",
        "+    else:\n",
        "+        savings_percent = 0.0\n",
        "+\n",
        "+    return CacheCostComparison(\n",
        "+        cost_with_cache=cost_with_cache,\n",
        "+        cost_without_cache=cost_without_cache,\n",
        "+        total_savings=total_savings,\n",
        "+        savings_percent=savings_percent,\n",
        "+        traces_analyzed=traces_analyzed,\n",
        "+        traces_with_cache=traces_with_cache,\n",
        "+    )\n",
        "+\n",
        "+\n",
        "+def analyze_costs(\n",
        "+    workflows: List[Workflow],\n",
        "+    pricing_config: PricingConfig,\n",
        "+    scaling_factors: Optional[List[int]] = None,\n",
        "+    monthly_workflow_estimate: Optional[int] = None,\n",
        "+) -> CostAnalysisResults:\n",
        "+    \"\"\"\n",
        "+    Perform complete cost analysis on workflows.\n",
        "+    Main entry point for Phase 3B.\n",
        "+\n",
        "+    Args:\n",
        "+        workflows: List of Workflow objects to analyze\n",
        "+        pricing_config: Pricing configuration for cost calculation\n",
        "+        scaling_factors: Optional list of scale factors (default: [1, 10, 100, 1000])\n",
        "+        monthly_workflow_estimate: Optional monthly workflow estimate for projections\n",
        "+\n",
        "+    Returns:\n",
        "+        CostAnalysisResults with complete analysis\n",
        "+    \"\"\"\n",
        "+    if scaling_factors is None:\n",
        "+        scaling_factors = SCALING_FACTORS\n",
        "+\n",
        "+    data_quality_notes = []\n",
        "+\n",
        "+    # Calculate cost for each workflow\n",
        "+    workflow_analyses = []\n",
        "+    workflow_costs = []\n",
        "+\n",
        "+    for workflow in workflows:\n",
        "+        analysis = calculate_workflow_cost(workflow, pricing_config)\n",
        "+        workflow_analyses.append(analysis)\n",
        "+        workflow_costs.append(analysis.total_cost)\n",
        "+\n",
        "+    # Calculate per-workflow statistics\n",
        "+    if workflow_costs:\n",
        "+        avg_cost = sum(workflow_costs) / len(workflow_costs)\n",
        "+        sorted_costs = sorted(workflow_costs)\n",
        "+        median_cost = sorted_costs[len(sorted_costs) // 2]\n",
        "+        min_cost = min(workflow_costs)\n",
        "+        max_cost = max(workflow_costs)\n",
        "+    else:\n",
        "+        avg_cost = median_cost = min_cost = max_cost = 0.0\n",
        "+        data_quality_notes.append(\"No workflows with cost data found\")\n",
        "+\n",
        "+    # Aggregate node costs\n",
        "+    node_summaries = aggregate_node_costs(workflow_analyses)\n",
        "+    top_cost_driver = node_summaries[0].node_name if node_summaries else None\n",
        "+\n",
        "+    # Calculate scaling projections\n",
        "+    scaling_projections = project_scaling_costs(\n",
        "+        avg_cost_per_workflow=avg_cost,\n",
        "+        current_workflow_count=len(workflows),\n",
        "+        scaling_factors=scaling_factors,\n",
        "+        monthly_workflow_estimate=monthly_workflow_estimate,\n",
        "+    )\n",
        "+\n",
        "+    # Cache effectiveness analysis\n",
        "+    cache_effectiveness_percent = calculate_cache_hit_rate(workflow_analyses)\n",
        "+    cache_savings_dollars = calculate_cache_savings(workflow_analyses, pricing_config)\n",
        "+\n",
        "+    return CostAnalysisResults(\n",
        "+        avg_cost_per_workflow=avg_cost,\n",
        "+        median_cost_per_workflow=median_cost,\n",
        "+        min_cost=min_cost,\n",
        "+        max_cost=max_cost,\n",
        "+        node_summaries=node_summaries,\n",
        "+        top_cost_driver=top_cost_driver,\n",
        "+        cache_effectiveness_percent=cache_effectiveness_percent,\n",
        "+        cache_savings_dollars=cache_savings_dollars,\n",
        "+        scaling_projections=scaling_projections,\n",
        "+        total_workflows_analyzed=len(workflows),\n",
        "+        data_quality_notes=data_quality_notes,\n",
        "+    )\n"
      ]
    },
    {
      "path": "analyze_failures.py",
      "status": "added",
      "additions": 455,
      "deletions": 0,
      "patch": "@@ -0,0 +1,455 @@\n+\"\"\"\n+LangSmith Trace Failure Pattern Analysis Tool - Phase 3C\n+\n+This module provides failure pattern analysis capabilities for LangSmith trace exports.\n+Detects failures, analyzes retry sequences, and assesses quality risks.\n+\n+Following PDCA (Plan-Do-Check-Act) methodology with TDD approach.\n+\n+Author: Generated with Claude Code (PDCA Framework)\n+Date: 2025-12-09\n+\"\"\"\n+\n+from dataclasses import dataclass\n+from datetime import datetime\n+from typing import Any, Dict, List, Optional\n+import re\n+from analyze_traces import Trace, Workflow\n+\n+\n+# ============================================================================\n+# Configuration Constants\n+# ============================================================================\n+\n+# Status values indicating failure\n+FAILURE_STATUSES = {\"error\", \"failed\", \"cancelled\"}\n+SUCCESS_STATUSES = {\"success\"}\n+\n+# Retry detection heuristics\n+RETRY_DETECTION_CONFIG = {\n+    \"max_time_window_seconds\": 300,  # 5 min window for retry detection\n+    \"same_node_threshold\": 2,  # 2+ executions = potential retry\n+}\n+\n+# Error classification patterns (regex)\n+ERROR_PATTERNS = {\n+    \"validation_failure\": r\"validation.*fail|invalid.*spec\",\n+    \"api_timeout\": r\"timeout|timed out\",\n+    \"import_error\": r\"import.*fail|import.*error\",\n+    \"llm_error\": r\"model.*error|generation.*fail|token.*limit\",\n+    \"unknown\": r\".*\",  # Catch-all\n+}\n+\n+\n+# ============================================================================\n+# Core Data Structures\n+# ============================================================================\n+\n+\n+@dataclass\n+class FailureInstance:\n+    \"\"\"Single failure occurrence.\"\"\"\n+\n+    trace_id: str\n+    trace_name: str\n+    workflow_id: str\n+    error_message: Optional[str]\n+    error_type: str  # Classified from ERROR_PATTERNS\n+    timestamp: Optional[datetime]\n+\n+\n+@dataclass\n+class RetrySequence:\n+    \"\"\"Detected retry sequence.\"\"\"\n+\n+    node_name: str\n+    workflow_id: str\n+    attempt_count: int\n+    attempts: List[Trace]  # Ordered by start_time\n+    final_status: str  # 'success' or 'failed'\n+    total_duration_seconds: float\n+    total_cost_estimate: Optional[float] = None\n+\n+\n+@dataclass\n+class NodeFailureStats:\n+    \"\"\"Failure statistics for a node type.\"\"\"\n+\n+    node_name: str\n+    total_executions: int\n+    failure_count: int\n+    success_count: int\n+    failure_rate_percent: float\n+    retry_sequences_detected: int\n+    avg_retries_when_failing: float\n+    common_error_types: Dict[str, int]  # error_type -> count\n+\n+\n+@dataclass\n+class ValidatorEffectivenessAnalysis:\n+    \"\"\"Validator effectiveness assessment.\"\"\"\n+\n+    validator_name: str\n+    total_executions: int\n+    caught_issues_count: int  # Failures detected\n+    pass_rate_percent: float\n+    is_necessary: bool  # Based on redundancy analysis\n+\n+\n+@dataclass\n+class FailureAnalysisResults:\n+    \"\"\"Complete failure pattern analysis results.\"\"\"\n+\n+    # Overall metrics\n+    total_workflows: int\n+    successful_workflows: int\n+    failed_workflows: int\n+    overall_success_rate_percent: float\n+\n+    # Node-level breakdown\n+    node_failure_stats: List[NodeFailureStats]  # Sorted by failure_rate\n+    highest_failure_node: Optional[str]\n+\n+    # Error distribution\n+    error_type_distribution: Dict[str, int]\n+    most_common_error_type: Optional[str]\n+\n+    # Retry analysis\n+    total_retry_sequences: int\n+    retry_sequences: List[RetrySequence]\n+    retry_success_rate_percent: Optional[float]\n+    avg_cost_of_retries: Optional[float]\n+\n+    # Validator analysis\n+    validator_analyses: List[ValidatorEffectivenessAnalysis]\n+    redundant_validators: List[str]\n+\n+    # Quality risks\n+    quality_risks_at_scale: List[str]\n+\n+\n+# ============================================================================\n+# Failure Detection Functions\n+# ============================================================================\n+\n+\n+def detect_failures(workflow: Workflow) -> List[FailureInstance]:\n+    \"\"\"\n+    Detect all failures in workflow using trace.status and trace.error.\n+\n+    Args:\n+        workflow: Workflow to analyze\n+\n+    Returns:\n+        List of FailureInstance objects\n+    \"\"\"\n+    failures = []\n+\n+    for trace in workflow.all_traces:\n+        if trace.status in FAILURE_STATUSES:\n+            error_type = classify_error(trace.error)\n+            failure = FailureInstance(\n+                trace_id=trace.id,\n+                trace_name=trace.name,\n+                workflow_id=workflow.root_trace.id,\n+                error_message=trace.error,\n+                error_type=error_type,\n+                timestamp=trace.start_time,\n+            )\n+            failures.append(failure)\n+\n+    return failures\n+\n+\n+def classify_error(error_message: Optional[str]) -> str:\n+    \"\"\"\n+    Classify error into type using regex patterns.\n+\n+    Args:\n+        error_message: Error message to classify\n+\n+    Returns:\n+        Error type string\n+    \"\"\"\n+    if not error_message:\n+        return \"unknown\"\n+\n+    error_lower = error_message.lower()\n+\n+    # Try each pattern (order matters - more specific first)\n+    for error_type, pattern in ERROR_PATTERNS.items():\n+        if error_type == \"unknown\":\n+            continue  # Skip catch-all for now\n+        if re.search(pattern, error_lower):\n+            return error_type\n+\n+    return \"unknown\"\n+\n+\n+# ============================================================================\n+# Retry Detection Functions\n+# ============================================================================\n+\n+\n+def detect_retry_sequences(workflow: Workflow) -> List[RetrySequence]:\n+    \"\"\"\n+    Detect retry sequences using heuristics:\n+    - Multiple executions of same node within time window\n+    - Ordered by start_time\n+\n+    Args:\n+        workflow: Workflow to analyze\n+\n+    Returns:\n+        List of RetrySequence objects\n+    \"\"\"\n+    # Group traces by node name\n+    node_traces: Dict[str, List[Trace]] = {}\n+    for trace in workflow.all_traces:\n+        if trace.name not in node_traces:\n+            node_traces[trace.name] = []\n+        node_traces[trace.name].append(trace)\n+\n+    retry_sequences = []\n+\n+    for node_name, traces in node_traces.items():\n+        if len(traces) < RETRY_DETECTION_CONFIG[\"same_node_threshold\"]:\n+            continue\n+\n+        # Filter out traces with None start_time and sort by start_time\n+        valid_traces = [t for t in traces if t.start_time is not None]\n+        if len(valid_traces) < RETRY_DETECTION_CONFIG[\"same_node_threshold\"]:\n+            continue\n+\n+        sorted_traces = sorted(valid_traces, key=lambda t: t.start_time)  # type: ignore[arg-type, return-value]\n+\n+        # Check if traces are within time window\n+        first_start = sorted_traces[0].start_time\n+        last_start = sorted_traces[-1].start_time\n+        if first_start is None or last_start is None:\n+            continue\n+        time_diff = (last_start - first_start).total_seconds()\n+\n+        if time_diff <= RETRY_DETECTION_CONFIG[\"max_time_window_seconds\"]:\n+            # This looks like a retry sequence\n+            final_status = sorted_traces[-1].status\n+            total_duration = sum(t.duration_seconds for t in sorted_traces)\n+\n+            retry_seq = RetrySequence(\n+                node_name=node_name,\n+                workflow_id=workflow.root_trace.id,\n+                attempt_count=len(sorted_traces),\n+                attempts=sorted_traces,\n+                final_status=final_status,\n+                total_duration_seconds=total_duration,\n+            )\n+            retry_sequences.append(retry_seq)\n+\n+    return retry_sequences\n+\n+\n+def calculate_retry_success_rate(\n+    retry_sequences: List[RetrySequence],\n+) -> Optional[float]:\n+    \"\"\"\n+    Calculate % of retries that eventually succeed.\n+\n+    Args:\n+        retry_sequences: List of RetrySequence objects\n+\n+    Returns:\n+        Success rate as percentage, or None if no retries\n+    \"\"\"\n+    if not retry_sequences:\n+        return None\n+\n+    successful_retries = sum(\n+        1 for seq in retry_sequences if seq.final_status in SUCCESS_STATUSES\n+    )\n+\n+    return (successful_retries / len(retry_sequences)) * 100.0\n+\n+\n+# ============================================================================\n+# Node Failure Analysis Functions\n+# ============================================================================\n+\n+\n+def analyze_node_failures(workflows: List[Workflow]) -> List[NodeFailureStats]:\n+    \"\"\"\n+    Analyze failure patterns by node type across workflows.\n+\n+    Args:\n+        workflows: List of Workflow objects\n+\n+    Returns:\n+        List of NodeFailureStats sorted by failure_rate descending\n+    \"\"\"\n+    # Aggregate by node name\n+    node_data: Dict[str, Dict[str, Any]] = {}\n+\n+    for workflow in workflows:\n+        # Detect all retry sequences for this workflow\n+        retry_sequences = detect_retry_sequences(workflow)\n+\n+        for trace in workflow.all_traces:\n+            node_name = trace.name\n+            if node_name not in node_data:\n+                node_data[node_name] = {\n+                    \"total_executions\": 0,\n+                    \"failure_count\": 0,\n+                    \"success_count\": 0,\n+                    \"retry_sequences\": 0,\n+                    \"error_types\": {},\n+                }\n+\n+            node_data[node_name][\"total_executions\"] += 1\n+\n+            if trace.status in FAILURE_STATUSES:\n+                node_data[node_name][\"failure_count\"] += 1\n+                # Track error type\n+                error_type = classify_error(trace.error)\n+                if error_type not in node_data[node_name][\"error_types\"]:\n+                    node_data[node_name][\"error_types\"][error_type] = 0\n+                node_data[node_name][\"error_types\"][error_type] += 1\n+            elif trace.status in SUCCESS_STATUSES:\n+                node_data[node_name][\"success_count\"] += 1\n+\n+        # Count retry sequences per node\n+        for retry_seq in retry_sequences:\n+            if retry_seq.node_name in node_data:\n+                node_data[retry_seq.node_name][\"retry_sequences\"] += 1\n+\n+    # Create NodeFailureStats objects\n+    stats_list = []\n+    for node_name, data in node_data.items():\n+        total_exec = data[\"total_executions\"]\n+        failure_count = data[\"failure_count\"]\n+        failure_rate = (failure_count / total_exec * 100.0) if total_exec > 0 else 0.0\n+\n+        # Calculate avg retries when failing\n+        retry_sequences = data[\"retry_sequences\"]\n+        avg_retries = (retry_sequences / failure_count) if failure_count > 0 else 0.0\n+\n+        stats = NodeFailureStats(\n+            node_name=node_name,\n+            total_executions=total_exec,\n+            failure_count=failure_count,\n+            success_count=data[\"success_count\"],\n+            failure_rate_percent=failure_rate,\n+            retry_sequences_detected=retry_sequences,\n+            avg_retries_when_failing=avg_retries,\n+            common_error_types=data[\"error_types\"],\n+        )\n+        stats_list.append(stats)\n+\n+    # Sort by failure rate descending\n+    stats_list.sort(key=lambda s: s.failure_rate_percent, reverse=True)\n+\n+    return stats_list\n+\n+\n+# ============================================================================\n+# Main Analysis Function\n+# ============================================================================\n+\n+\n+def analyze_failures(workflows: List[Workflow]) -> FailureAnalysisResults:\n+    \"\"\"\n+    Perform complete failure pattern analysis.\n+    Main entry point for Phase 3C.\n+\n+    Args:\n+        workflows: List of Workflow objects to analyze\n+\n+    Returns:\n+        FailureAnalysisResults with complete analysis\n+    \"\"\"\n+    if not workflows:\n+        return FailureAnalysisResults(\n+            total_workflows=0,\n+            successful_workflows=0,\n+            failed_workflows=0,\n+            overall_success_rate_percent=0.0,\n+            node_failure_stats=[],\n+            highest_failure_node=None,\n+            error_type_distribution={},\n+            most_common_error_type=None,\n+            total_retry_sequences=0,\n+            retry_sequences=[],\n+            retry_success_rate_percent=None,\n+            avg_cost_of_retries=None,\n+            validator_analyses=[],\n+            redundant_validators=[],\n+            quality_risks_at_scale=[],\n+        )\n+\n+    # Detect all failures\n+    all_failures = []\n+    for workflow in workflows:\n+        failures = detect_failures(workflow)\n+        all_failures.extend(failures)\n+\n+    # Detect all retry sequences\n+    all_retry_sequences = []\n+    for workflow in workflows:\n+        retries = detect_retry_sequences(workflow)\n+        all_retry_sequences.extend(retries)\n+\n+    # Calculate overall success rate\n+    total_workflows = len(workflows)\n+    failed_workflows = sum(\n+        1 for workflow in workflows if workflow.root_trace.status in FAILURE_STATUSES\n+    )\n+    successful_workflows = total_workflows - failed_workflows\n+    overall_success_rate = (\n+        (successful_workflows / total_workflows * 100.0) if total_workflows > 0 else 0.0\n+    )\n+\n+    # Analyze node failures\n+    node_failure_stats = analyze_node_failures(workflows)\n+    highest_failure_node = (\n+        node_failure_stats[0].node_name if node_failure_stats else None\n+    )\n+\n+    # Aggregate error types\n+    error_type_distribution: Dict[str, int] = {}\n+    for failure in all_failures:\n+        error_type = failure.error_type\n+        if error_type not in error_type_distribution:\n+            error_type_distribution[error_type] = 0\n+        error_type_distribution[error_type] += 1\n+\n+    most_common_error = (\n+        max(error_type_distribution, key=lambda k: error_type_distribution[k])\n+        if error_type_distribution\n+        else None\n+    )\n+\n+    # Calculate retry success rate\n+    retry_success_rate = calculate_retry_success_rate(all_retry_sequences)\n+\n+    # Placeholder for validator analysis (not yet implemented)\n+    validator_analyses: List[ValidatorEffectivenessAnalysis] = []\n+    redundant_validators: List[str] = []\n+\n+    # Placeholder for quality risks (not yet implemented)\n+    quality_risks_at_scale: List[str] = []\n+\n+    return FailureAnalysisResults(\n+        total_workflows=total_workflows,\n+        successful_workflows=successful_workflows,\n+        failed_workflows=failed_workflows,\n+        overall_success_rate_percent=overall_success_rate,\n+        node_failure_stats=node_failure_stats,\n+        highest_failure_node=highest_failure_node,\n+        error_type_distribution=error_type_distribution,\n+        most_common_error_type=most_common_error,\n+        total_retry_sequences=len(all_retry_sequences),\n+        retry_sequences=all_retry_sequences,\n+        retry_success_rate_percent=retry_success_rate,\n+        avg_cost_of_retries=None,  # Not yet implemented\n+        validator_analyses=validator_analyses,\n+        redundant_validators=redundant_validators,\n+        quality_risks_at_scale=quality_risks_at_scale,\n+    )",
      "patch_lines": [
        "@@ -0,0 +1,455 @@\n",
        "+\"\"\"\n",
        "+LangSmith Trace Failure Pattern Analysis Tool - Phase 3C\n",
        "+\n",
        "+This module provides failure pattern analysis capabilities for LangSmith trace exports.\n",
        "+Detects failures, analyzes retry sequences, and assesses quality risks.\n",
        "+\n",
        "+Following PDCA (Plan-Do-Check-Act) methodology with TDD approach.\n",
        "+\n",
        "+Author: Generated with Claude Code (PDCA Framework)\n",
        "+Date: 2025-12-09\n",
        "+\"\"\"\n",
        "+\n",
        "+from dataclasses import dataclass\n",
        "+from datetime import datetime\n",
        "+from typing import Any, Dict, List, Optional\n",
        "+import re\n",
        "+from analyze_traces import Trace, Workflow\n",
        "+\n",
        "+\n",
        "+# ============================================================================\n",
        "+# Configuration Constants\n",
        "+# ============================================================================\n",
        "+\n",
        "+# Status values indicating failure\n",
        "+FAILURE_STATUSES = {\"error\", \"failed\", \"cancelled\"}\n",
        "+SUCCESS_STATUSES = {\"success\"}\n",
        "+\n",
        "+# Retry detection heuristics\n",
        "+RETRY_DETECTION_CONFIG = {\n",
        "+    \"max_time_window_seconds\": 300,  # 5 min window for retry detection\n",
        "+    \"same_node_threshold\": 2,  # 2+ executions = potential retry\n",
        "+}\n",
        "+\n",
        "+# Error classification patterns (regex)\n",
        "+ERROR_PATTERNS = {\n",
        "+    \"validation_failure\": r\"validation.*fail|invalid.*spec\",\n",
        "+    \"api_timeout\": r\"timeout|timed out\",\n",
        "+    \"import_error\": r\"import.*fail|import.*error\",\n",
        "+    \"llm_error\": r\"model.*error|generation.*fail|token.*limit\",\n",
        "+    \"unknown\": r\".*\",  # Catch-all\n",
        "+}\n",
        "+\n",
        "+\n",
        "+# ============================================================================\n",
        "+# Core Data Structures\n",
        "+# ============================================================================\n",
        "+\n",
        "+\n",
        "+@dataclass\n",
        "+class FailureInstance:\n",
        "+    \"\"\"Single failure occurrence.\"\"\"\n",
        "+\n",
        "+    trace_id: str\n",
        "+    trace_name: str\n",
        "+    workflow_id: str\n",
        "+    error_message: Optional[str]\n",
        "+    error_type: str  # Classified from ERROR_PATTERNS\n",
        "+    timestamp: Optional[datetime]\n",
        "+\n",
        "+\n",
        "+@dataclass\n",
        "+class RetrySequence:\n",
        "+    \"\"\"Detected retry sequence.\"\"\"\n",
        "+\n",
        "+    node_name: str\n",
        "+    workflow_id: str\n",
        "+    attempt_count: int\n",
        "+    attempts: List[Trace]  # Ordered by start_time\n",
        "+    final_status: str  # 'success' or 'failed'\n",
        "+    total_duration_seconds: float\n",
        "+    total_cost_estimate: Optional[float] = None\n",
        "+\n",
        "+\n",
        "+@dataclass\n",
        "+class NodeFailureStats:\n",
        "+    \"\"\"Failure statistics for a node type.\"\"\"\n",
        "+\n",
        "+    node_name: str\n",
        "+    total_executions: int\n",
        "+    failure_count: int\n",
        "+    success_count: int\n",
        "+    failure_rate_percent: float\n",
        "+    retry_sequences_detected: int\n",
        "+    avg_retries_when_failing: float\n",
        "+    common_error_types: Dict[str, int]  # error_type -> count\n",
        "+\n",
        "+\n",
        "+@dataclass\n",
        "+class ValidatorEffectivenessAnalysis:\n",
        "+    \"\"\"Validator effectiveness assessment.\"\"\"\n",
        "+\n",
        "+    validator_name: str\n",
        "+    total_executions: int\n",
        "+    caught_issues_count: int  # Failures detected\n",
        "+    pass_rate_percent: float\n",
        "+    is_necessary: bool  # Based on redundancy analysis\n",
        "+\n",
        "+\n",
        "+@dataclass\n",
        "+class FailureAnalysisResults:\n",
        "+    \"\"\"Complete failure pattern analysis results.\"\"\"\n",
        "+\n",
        "+    # Overall metrics\n",
        "+    total_workflows: int\n",
        "+    successful_workflows: int\n",
        "+    failed_workflows: int\n",
        "+    overall_success_rate_percent: float\n",
        "+\n",
        "+    # Node-level breakdown\n",
        "+    node_failure_stats: List[NodeFailureStats]  # Sorted by failure_rate\n",
        "+    highest_failure_node: Optional[str]\n",
        "+\n",
        "+    # Error distribution\n",
        "+    error_type_distribution: Dict[str, int]\n",
        "+    most_common_error_type: Optional[str]\n",
        "+\n",
        "+    # Retry analysis\n",
        "+    total_retry_sequences: int\n",
        "+    retry_sequences: List[RetrySequence]\n",
        "+    retry_success_rate_percent: Optional[float]\n",
        "+    avg_cost_of_retries: Optional[float]\n",
        "+\n",
        "+    # Validator analysis\n",
        "+    validator_analyses: List[ValidatorEffectivenessAnalysis]\n",
        "+    redundant_validators: List[str]\n",
        "+\n",
        "+    # Quality risks\n",
        "+    quality_risks_at_scale: List[str]\n",
        "+\n",
        "+\n",
        "+# ============================================================================\n",
        "+# Failure Detection Functions\n",
        "+# ============================================================================\n",
        "+\n",
        "+\n",
        "+def detect_failures(workflow: Workflow) -> List[FailureInstance]:\n",
        "+    \"\"\"\n",
        "+    Detect all failures in workflow using trace.status and trace.error.\n",
        "+\n",
        "+    Args:\n",
        "+        workflow: Workflow to analyze\n",
        "+\n",
        "+    Returns:\n",
        "+        List of FailureInstance objects\n",
        "+    \"\"\"\n",
        "+    failures = []\n",
        "+\n",
        "+    for trace in workflow.all_traces:\n",
        "+        if trace.status in FAILURE_STATUSES:\n",
        "+            error_type = classify_error(trace.error)\n",
        "+            failure = FailureInstance(\n",
        "+                trace_id=trace.id,\n",
        "+                trace_name=trace.name,\n",
        "+                workflow_id=workflow.root_trace.id,\n",
        "+                error_message=trace.error,\n",
        "+                error_type=error_type,\n",
        "+                timestamp=trace.start_time,\n",
        "+            )\n",
        "+            failures.append(failure)\n",
        "+\n",
        "+    return failures\n",
        "+\n",
        "+\n",
        "+def classify_error(error_message: Optional[str]) -> str:\n",
        "+    \"\"\"\n",
        "+    Classify error into type using regex patterns.\n",
        "+\n",
        "+    Args:\n",
        "+        error_message: Error message to classify\n",
        "+\n",
        "+    Returns:\n",
        "+        Error type string\n",
        "+    \"\"\"\n",
        "+    if not error_message:\n",
        "+        return \"unknown\"\n",
        "+\n",
        "+    error_lower = error_message.lower()\n",
        "+\n",
        "+    # Try each pattern (order matters - more specific first)\n",
        "+    for error_type, pattern in ERROR_PATTERNS.items():\n",
        "+        if error_type == \"unknown\":\n",
        "+            continue  # Skip catch-all for now\n",
        "+        if re.search(pattern, error_lower):\n",
        "+            return error_type\n",
        "+\n",
        "+    return \"unknown\"\n",
        "+\n",
        "+\n",
        "+# ============================================================================\n",
        "+# Retry Detection Functions\n",
        "+# ============================================================================\n",
        "+\n",
        "+\n",
        "+def detect_retry_sequences(workflow: Workflow) -> List[RetrySequence]:\n",
        "+    \"\"\"\n",
        "+    Detect retry sequences using heuristics:\n",
        "+    - Multiple executions of same node within time window\n",
        "+    - Ordered by start_time\n",
        "+\n",
        "+    Args:\n",
        "+        workflow: Workflow to analyze\n",
        "+\n",
        "+    Returns:\n",
        "+        List of RetrySequence objects\n",
        "+    \"\"\"\n",
        "+    # Group traces by node name\n",
        "+    node_traces: Dict[str, List[Trace]] = {}\n",
        "+    for trace in workflow.all_traces:\n",
        "+        if trace.name not in node_traces:\n",
        "+            node_traces[trace.name] = []\n",
        "+        node_traces[trace.name].append(trace)\n",
        "+\n",
        "+    retry_sequences = []\n",
        "+\n",
        "+    for node_name, traces in node_traces.items():\n",
        "+        if len(traces) < RETRY_DETECTION_CONFIG[\"same_node_threshold\"]:\n",
        "+            continue\n",
        "+\n",
        "+        # Filter out traces with None start_time and sort by start_time\n",
        "+        valid_traces = [t for t in traces if t.start_time is not None]\n",
        "+        if len(valid_traces) < RETRY_DETECTION_CONFIG[\"same_node_threshold\"]:\n",
        "+            continue\n",
        "+\n",
        "+        sorted_traces = sorted(valid_traces, key=lambda t: t.start_time)  # type: ignore[arg-type, return-value]\n",
        "+\n",
        "+        # Check if traces are within time window\n",
        "+        first_start = sorted_traces[0].start_time\n",
        "+        last_start = sorted_traces[-1].start_time\n",
        "+        if first_start is None or last_start is None:\n",
        "+            continue\n",
        "+        time_diff = (last_start - first_start).total_seconds()\n",
        "+\n",
        "+        if time_diff <= RETRY_DETECTION_CONFIG[\"max_time_window_seconds\"]:\n",
        "+            # This looks like a retry sequence\n",
        "+            final_status = sorted_traces[-1].status\n",
        "+            total_duration = sum(t.duration_seconds for t in sorted_traces)\n",
        "+\n",
        "+            retry_seq = RetrySequence(\n",
        "+                node_name=node_name,\n",
        "+                workflow_id=workflow.root_trace.id,\n",
        "+                attempt_count=len(sorted_traces),\n",
        "+                attempts=sorted_traces,\n",
        "+                final_status=final_status,\n",
        "+                total_duration_seconds=total_duration,\n",
        "+            )\n",
        "+            retry_sequences.append(retry_seq)\n",
        "+\n",
        "+    return retry_sequences\n",
        "+\n",
        "+\n",
        "+def calculate_retry_success_rate(\n",
        "+    retry_sequences: List[RetrySequence],\n",
        "+) -> Optional[float]:\n",
        "+    \"\"\"\n",
        "+    Calculate % of retries that eventually succeed.\n",
        "+\n",
        "+    Args:\n",
        "+        retry_sequences: List of RetrySequence objects\n",
        "+\n",
        "+    Returns:\n",
        "+        Success rate as percentage, or None if no retries\n",
        "+    \"\"\"\n",
        "+    if not retry_sequences:\n",
        "+        return None\n",
        "+\n",
        "+    successful_retries = sum(\n",
        "+        1 for seq in retry_sequences if seq.final_status in SUCCESS_STATUSES\n",
        "+    )\n",
        "+\n",
        "+    return (successful_retries / len(retry_sequences)) * 100.0\n",
        "+\n",
        "+\n",
        "+# ============================================================================\n",
        "+# Node Failure Analysis Functions\n",
        "+# ============================================================================\n",
        "+\n",
        "+\n",
        "+def analyze_node_failures(workflows: List[Workflow]) -> List[NodeFailureStats]:\n",
        "+    \"\"\"\n",
        "+    Analyze failure patterns by node type across workflows.\n",
        "+\n",
        "+    Args:\n",
        "+        workflows: List of Workflow objects\n",
        "+\n",
        "+    Returns:\n",
        "+        List of NodeFailureStats sorted by failure_rate descending\n",
        "+    \"\"\"\n",
        "+    # Aggregate by node name\n",
        "+    node_data: Dict[str, Dict[str, Any]] = {}\n",
        "+\n",
        "+    for workflow in workflows:\n",
        "+        # Detect all retry sequences for this workflow\n",
        "+        retry_sequences = detect_retry_sequences(workflow)\n",
        "+\n",
        "+        for trace in workflow.all_traces:\n",
        "+            node_name = trace.name\n",
        "+            if node_name not in node_data:\n",
        "+                node_data[node_name] = {\n",
        "+                    \"total_executions\": 0,\n",
        "+                    \"failure_count\": 0,\n",
        "+                    \"success_count\": 0,\n",
        "+                    \"retry_sequences\": 0,\n",
        "+                    \"error_types\": {},\n",
        "+                }\n",
        "+\n",
        "+            node_data[node_name][\"total_executions\"] += 1\n",
        "+\n",
        "+            if trace.status in FAILURE_STATUSES:\n",
        "+                node_data[node_name][\"failure_count\"] += 1\n",
        "+                # Track error type\n",
        "+                error_type = classify_error(trace.error)\n",
        "+                if error_type not in node_data[node_name][\"error_types\"]:\n",
        "+                    node_data[node_name][\"error_types\"][error_type] = 0\n",
        "+                node_data[node_name][\"error_types\"][error_type] += 1\n",
        "+            elif trace.status in SUCCESS_STATUSES:\n",
        "+                node_data[node_name][\"success_count\"] += 1\n",
        "+\n",
        "+        # Count retry sequences per node\n",
        "+        for retry_seq in retry_sequences:\n",
        "+            if retry_seq.node_name in node_data:\n",
        "+                node_data[retry_seq.node_name][\"retry_sequences\"] += 1\n",
        "+\n",
        "+    # Create NodeFailureStats objects\n",
        "+    stats_list = []\n",
        "+    for node_name, data in node_data.items():\n",
        "+        total_exec = data[\"total_executions\"]\n",
        "+        failure_count = data[\"failure_count\"]\n",
        "+        failure_rate = (failure_count / total_exec * 100.0) if total_exec > 0 else 0.0\n",
        "+\n",
        "+        # Calculate avg retries when failing\n",
        "+        retry_sequences = data[\"retry_sequences\"]\n",
        "+        avg_retries = (retry_sequences / failure_count) if failure_count > 0 else 0.0\n",
        "+\n",
        "+        stats = NodeFailureStats(\n",
        "+            node_name=node_name,\n",
        "+            total_executions=total_exec,\n",
        "+            failure_count=failure_count,\n",
        "+            success_count=data[\"success_count\"],\n",
        "+            failure_rate_percent=failure_rate,\n",
        "+            retry_sequences_detected=retry_sequences,\n",
        "+            avg_retries_when_failing=avg_retries,\n",
        "+            common_error_types=data[\"error_types\"],\n",
        "+        )\n",
        "+        stats_list.append(stats)\n",
        "+\n",
        "+    # Sort by failure rate descending\n",
        "+    stats_list.sort(key=lambda s: s.failure_rate_percent, reverse=True)\n",
        "+\n",
        "+    return stats_list\n",
        "+\n",
        "+\n",
        "+# ============================================================================\n",
        "+# Main Analysis Function\n",
        "+# ============================================================================\n",
        "+\n",
        "+\n",
        "+def analyze_failures(workflows: List[Workflow]) -> FailureAnalysisResults:\n",
        "+    \"\"\"\n",
        "+    Perform complete failure pattern analysis.\n",
        "+    Main entry point for Phase 3C.\n",
        "+\n",
        "+    Args:\n",
        "+        workflows: List of Workflow objects to analyze\n",
        "+\n",
        "+    Returns:\n",
        "+        FailureAnalysisResults with complete analysis\n",
        "+    \"\"\"\n",
        "+    if not workflows:\n",
        "+        return FailureAnalysisResults(\n",
        "+            total_workflows=0,\n",
        "+            successful_workflows=0,\n",
        "+            failed_workflows=0,\n",
        "+            overall_success_rate_percent=0.0,\n",
        "+            node_failure_stats=[],\n",
        "+            highest_failure_node=None,\n",
        "+            error_type_distribution={},\n",
        "+            most_common_error_type=None,\n",
        "+            total_retry_sequences=0,\n",
        "+            retry_sequences=[],\n",
        "+            retry_success_rate_percent=None,\n",
        "+            avg_cost_of_retries=None,\n",
        "+            validator_analyses=[],\n",
        "+            redundant_validators=[],\n",
        "+            quality_risks_at_scale=[],\n",
        "+        )\n",
        "+\n",
        "+    # Detect all failures\n",
        "+    all_failures = []\n",
        "+    for workflow in workflows:\n",
        "+        failures = detect_failures(workflow)\n",
        "+        all_failures.extend(failures)\n",
        "+\n",
        "+    # Detect all retry sequences\n",
        "+    all_retry_sequences = []\n",
        "+    for workflow in workflows:\n",
        "+        retries = detect_retry_sequences(workflow)\n",
        "+        all_retry_sequences.extend(retries)\n",
        "+\n",
        "+    # Calculate overall success rate\n",
        "+    total_workflows = len(workflows)\n",
        "+    failed_workflows = sum(\n",
        "+        1 for workflow in workflows if workflow.root_trace.status in FAILURE_STATUSES\n",
        "+    )\n",
        "+    successful_workflows = total_workflows - failed_workflows\n",
        "+    overall_success_rate = (\n",
        "+        (successful_workflows / total_workflows * 100.0) if total_workflows > 0 else 0.0\n",
        "+    )\n",
        "+\n",
        "+    # Analyze node failures\n",
        "+    node_failure_stats = analyze_node_failures(workflows)\n",
        "+    highest_failure_node = (\n",
        "+        node_failure_stats[0].node_name if node_failure_stats else None\n",
        "+    )\n",
        "+\n",
        "+    # Aggregate error types\n",
        "+    error_type_distribution: Dict[str, int] = {}\n",
        "+    for failure in all_failures:\n",
        "+        error_type = failure.error_type\n",
        "+        if error_type not in error_type_distribution:\n",
        "+            error_type_distribution[error_type] = 0\n",
        "+        error_type_distribution[error_type] += 1\n",
        "+\n",
        "+    most_common_error = (\n",
        "+        max(error_type_distribution, key=lambda k: error_type_distribution[k])\n",
        "+        if error_type_distribution\n",
        "+        else None\n",
        "+    )\n",
        "+\n",
        "+    # Calculate retry success rate\n",
        "+    retry_success_rate = calculate_retry_success_rate(all_retry_sequences)\n",
        "+\n",
        "+    # Placeholder for validator analysis (not yet implemented)\n",
        "+    validator_analyses: List[ValidatorEffectivenessAnalysis] = []\n",
        "+    redundant_validators: List[str] = []\n",
        "+\n",
        "+    # Placeholder for quality risks (not yet implemented)\n",
        "+    quality_risks_at_scale: List[str] = []\n",
        "+\n",
        "+    return FailureAnalysisResults(\n",
        "+        total_workflows=total_workflows,\n",
        "+        successful_workflows=successful_workflows,\n",
        "+        failed_workflows=failed_workflows,\n",
        "+        overall_success_rate_percent=overall_success_rate,\n",
        "+        node_failure_stats=node_failure_stats,\n",
        "+        highest_failure_node=highest_failure_node,\n",
        "+        error_type_distribution=error_type_distribution,\n",
        "+        most_common_error_type=most_common_error,\n",
        "+        total_retry_sequences=len(all_retry_sequences),\n",
        "+        retry_sequences=all_retry_sequences,\n",
        "+        retry_success_rate_percent=retry_success_rate,\n",
        "+        avg_cost_of_retries=None,  # Not yet implemented\n",
        "+        validator_analyses=validator_analyses,\n",
        "+        redundant_validators=redundant_validators,\n",
        "+        quality_risks_at_scale=quality_risks_at_scale,\n",
        "+    )\n"
      ]
    },
    {
      "path": "analyze_traces.py",
      "status": "modified",
      "additions": 13,
      "deletions": 4,
      "patch": "@@ -26,17 +26,20 @@ class Trace:\n \n     Attributes:\n         id: Unique identifier for the trace\n-        name: Name of the trace (e.g., 'LangGraph', 'generate_spec')\n+        name: Name of the trace (e.g., 'LangGraph', 'process_data')\n         start_time: When the trace started execution\n         end_time: When the trace completed\n         duration_seconds: Total execution time in seconds\n-        status: Execution status ('success', 'error', etc.)\n+        status: Execution status ('success', 'error', etc.')\n         run_type: Type of run ('chain', 'llm', 'tool')\n         parent_id: ID of parent trace (None for root traces)\n         child_ids: List of child trace IDs\n         inputs: Input parameters to the trace\n         outputs: Output results from the trace\n         error: Error message if execution failed\n+        total_tokens: Total tokens used (None for non-LLM traces)\n+        prompt_tokens: Input/prompt tokens (None for non-LLM traces)\n+        completion_tokens: Output/completion tokens (None for non-LLM traces)\n     \"\"\"\n \n     id: str\n@@ -51,6 +54,9 @@ class Trace:\n     inputs: Dict[str, Any]\n     outputs: Dict[str, Any]\n     error: Optional[str]\n+    total_tokens: Optional[int] = None\n+    prompt_tokens: Optional[int] = None\n+    completion_tokens: Optional[int] = None\n \n \n @dataclass\n@@ -59,7 +65,7 @@ class Workflow:\n     Represents a complete workflow execution with hierarchical structure.\n \n     A workflow typically represents a LangGraph execution with multiple\n-    child nodes (e.g., generate_spec, validators, xml_transformation).\n+    child nodes (e.g., process_data, validators, transform_output).\n \n     Attributes:\n         root_trace: The root/parent trace (usually LangGraph)\n@@ -140,6 +146,9 @@ def _build_trace_from_dict(\n         inputs=trace_dict.get(\"inputs\", {}),\n         outputs=trace_dict.get(\"outputs\", {}),\n         error=trace_dict.get(\"error\"),\n+        total_tokens=trace_dict.get(\"total_tokens\"),\n+        prompt_tokens=trace_dict.get(\"prompt_tokens\"),\n+        completion_tokens=trace_dict.get(\"completion_tokens\"),\n     )\n \n     return trace\n@@ -395,7 +404,7 @@ class NodePerformance:\n     Performance metrics for a single node type across workflows.\n \n     Attributes:\n-        node_name: Name of the node (e.g., 'generate_spec', 'xml_transformation')\n+        node_name: Name of the node (e.g., 'process_data', 'transform_output')\n         execution_count: Number of times this node executed across all workflows\n         avg_duration_seconds: Average execution time in seconds\n         median_duration_seconds: Median execution time in seconds",
      "patch_lines": [
        "@@ -26,17 +26,20 @@ class Trace:\n",
        " \n",
        "     Attributes:\n",
        "         id: Unique identifier for the trace\n",
        "-        name: Name of the trace (e.g., 'LangGraph', 'generate_spec')\n",
        "+        name: Name of the trace (e.g., 'LangGraph', 'process_data')\n",
        "         start_time: When the trace started execution\n",
        "         end_time: When the trace completed\n",
        "         duration_seconds: Total execution time in seconds\n",
        "-        status: Execution status ('success', 'error', etc.)\n",
        "+        status: Execution status ('success', 'error', etc.')\n",
        "         run_type: Type of run ('chain', 'llm', 'tool')\n",
        "         parent_id: ID of parent trace (None for root traces)\n",
        "         child_ids: List of child trace IDs\n",
        "         inputs: Input parameters to the trace\n",
        "         outputs: Output results from the trace\n",
        "         error: Error message if execution failed\n",
        "+        total_tokens: Total tokens used (None for non-LLM traces)\n",
        "+        prompt_tokens: Input/prompt tokens (None for non-LLM traces)\n",
        "+        completion_tokens: Output/completion tokens (None for non-LLM traces)\n",
        "     \"\"\"\n",
        " \n",
        "     id: str\n",
        "@@ -51,6 +54,9 @@ class Trace:\n",
        "     inputs: Dict[str, Any]\n",
        "     outputs: Dict[str, Any]\n",
        "     error: Optional[str]\n",
        "+    total_tokens: Optional[int] = None\n",
        "+    prompt_tokens: Optional[int] = None\n",
        "+    completion_tokens: Optional[int] = None\n",
        " \n",
        " \n",
        " @dataclass\n",
        "@@ -59,7 +65,7 @@ class Workflow:\n",
        "     Represents a complete workflow execution with hierarchical structure.\n",
        " \n",
        "     A workflow typically represents a LangGraph execution with multiple\n",
        "-    child nodes (e.g., generate_spec, validators, xml_transformation).\n",
        "+    child nodes (e.g., process_data, validators, transform_output).\n",
        " \n",
        "     Attributes:\n",
        "         root_trace: The root/parent trace (usually LangGraph)\n",
        "@@ -140,6 +146,9 @@ def _build_trace_from_dict(\n",
        "         inputs=trace_dict.get(\"inputs\", {}),\n",
        "         outputs=trace_dict.get(\"outputs\", {}),\n",
        "         error=trace_dict.get(\"error\"),\n",
        "+        total_tokens=trace_dict.get(\"total_tokens\"),\n",
        "+        prompt_tokens=trace_dict.get(\"prompt_tokens\"),\n",
        "+        completion_tokens=trace_dict.get(\"completion_tokens\"),\n",
        "     )\n",
        " \n",
        "     return trace\n",
        "@@ -395,7 +404,7 @@ class NodePerformance:\n",
        "     Performance metrics for a single node type across workflows.\n",
        " \n",
        "     Attributes:\n",
        "-        node_name: Name of the node (e.g., 'generate_spec', 'xml_transformation')\n",
        "+        node_name: Name of the node (e.g., 'process_data', 'transform_output')\n",
        "         execution_count: Number of times this node executed across all workflows\n",
        "         avg_duration_seconds: Average execution time in seconds\n",
        "         median_duration_seconds: Median execution time in seconds\n"
      ]
    },
    {
      "path": "export_langsmith_traces.py",
      "status": "modified",
      "additions": 86,
      "deletions": 0,
      "patch": "@@ -390,6 +390,87 @@ def _format_single_run(self, run: Any) -> Dict[str, Any]:\n             for child in child_runs:\n                 formatted_children.append(self._format_single_run(child))\n \n+        # Extract cache token data with fallback logic\n+        # Try multiple locations: top-level fields, then nested in outputs/inputs\n+        cache_read_tokens = getattr(run, \"cache_read_tokens\", None)\n+        cache_creation_tokens = getattr(run, \"cache_creation_tokens\", None)\n+\n+        # Fallback 1: Check LangChain message format (primary location in exports)\n+        # outputs[\"generations\"][0][0][\"message\"][\"kwargs\"][\"usage_metadata\"][\"input_token_details\"]\n+        if cache_read_tokens is None or cache_creation_tokens is None:\n+            outputs = getattr(run, \"outputs\", {})\n+            if isinstance(outputs, dict):\n+                generations = outputs.get(\"generations\", [[]])\n+                if generations and len(generations) > 0 and len(generations[0]) > 0:\n+                    message = generations[0][0]\n+                    if isinstance(message, dict):\n+                        message_obj = message.get(\"message\", {})\n+                        if isinstance(message_obj, dict):\n+                            kwargs = message_obj.get(\"kwargs\", {})\n+                            if isinstance(kwargs, dict):\n+                                usage_metadata = kwargs.get(\"usage_metadata\", {})\n+                                if isinstance(usage_metadata, dict):\n+                                    input_token_details = usage_metadata.get(\n+                                        \"input_token_details\", {}\n+                                    )\n+                                    if isinstance(input_token_details, dict):\n+                                        if cache_read_tokens is None:\n+                                            cache_read_tokens = input_token_details.get(\n+                                                \"cache_read\"\n+                                            )\n+                                        if cache_creation_tokens is None:\n+                                            cache_creation_tokens = (\n+                                                input_token_details.get(\n+                                                    \"cache_creation\"\n+                                                )\n+                                            )\n+                                            if cache_creation_tokens is None:\n+                                                cache_creation_tokens = (\n+                                                    input_token_details.get(\n+                                                        \"cache_creation_input_tokens\"\n+                                                    )\n+                                                )\n+\n+        # Fallback 2: Check outputs[\"usage_metadata\"][\"input_token_details\"]\n+        if cache_read_tokens is None or cache_creation_tokens is None:\n+            outputs = getattr(run, \"outputs\", {})\n+            if isinstance(outputs, dict):\n+                usage_metadata = outputs.get(\"usage_metadata\", {})\n+                if isinstance(usage_metadata, dict):\n+                    input_token_details = usage_metadata.get(\"input_token_details\", {})\n+                    if isinstance(input_token_details, dict):\n+                        if cache_read_tokens is None:\n+                            cache_read_tokens = input_token_details.get(\"cache_read\")\n+                        if cache_creation_tokens is None:\n+                            # Try both possible field names (use explicit None check to preserve 0 values)\n+                            cache_creation_tokens = input_token_details.get(\n+                                \"cache_creation\"\n+                            )\n+                            if cache_creation_tokens is None:\n+                                cache_creation_tokens = input_token_details.get(\n+                                    \"cache_creation_input_tokens\"\n+                                )\n+\n+        # Fallback 3: Check inputs[\"usage_metadata\"][\"input_token_details\"] (less common)\n+        if cache_read_tokens is None or cache_creation_tokens is None:\n+            inputs = getattr(run, \"inputs\", {})\n+            if isinstance(inputs, dict):\n+                usage_metadata = inputs.get(\"usage_metadata\", {})\n+                if isinstance(usage_metadata, dict):\n+                    input_token_details = usage_metadata.get(\"input_token_details\", {})\n+                    if isinstance(input_token_details, dict):\n+                        if cache_read_tokens is None:\n+                            cache_read_tokens = input_token_details.get(\"cache_read\")\n+                        if cache_creation_tokens is None:\n+                            # Try both possible field names (use explicit None check to preserve 0 values)\n+                            cache_creation_tokens = input_token_details.get(\n+                                \"cache_creation\"\n+                            )\n+                            if cache_creation_tokens is None:\n+                                cache_creation_tokens = input_token_details.get(\n+                                    \"cache_creation_input_tokens\"\n+                                )\n+\n         trace = {\n             \"id\": str(getattr(run, \"id\", None)) if hasattr(run, \"id\") else None,\n             \"name\": getattr(run, \"name\", None),\n@@ -410,6 +491,11 @@ def _format_single_run(self, run: Any) -> Dict[str, Any]:\n             \"error\": getattr(run, \"error\", None),\n             \"run_type\": getattr(run, \"run_type\", None),\n             \"child_runs\": formatted_children,\n+            \"total_tokens\": getattr(run, \"total_tokens\", None),\n+            \"prompt_tokens\": getattr(run, \"prompt_tokens\", None),\n+            \"completion_tokens\": getattr(run, \"completion_tokens\", None),\n+            \"cache_read_tokens\": cache_read_tokens,\n+            \"cache_creation_tokens\": cache_creation_tokens,\n         }\n         return trace\n ",
      "patch_lines": [
        "@@ -390,6 +390,87 @@ def _format_single_run(self, run: Any) -> Dict[str, Any]:\n",
        "             for child in child_runs:\n",
        "                 formatted_children.append(self._format_single_run(child))\n",
        " \n",
        "+        # Extract cache token data with fallback logic\n",
        "+        # Try multiple locations: top-level fields, then nested in outputs/inputs\n",
        "+        cache_read_tokens = getattr(run, \"cache_read_tokens\", None)\n",
        "+        cache_creation_tokens = getattr(run, \"cache_creation_tokens\", None)\n",
        "+\n",
        "+        # Fallback 1: Check LangChain message format (primary location in exports)\n",
        "+        # outputs[\"generations\"][0][0][\"message\"][\"kwargs\"][\"usage_metadata\"][\"input_token_details\"]\n",
        "+        if cache_read_tokens is None or cache_creation_tokens is None:\n",
        "+            outputs = getattr(run, \"outputs\", {})\n",
        "+            if isinstance(outputs, dict):\n",
        "+                generations = outputs.get(\"generations\", [[]])\n",
        "+                if generations and len(generations) > 0 and len(generations[0]) > 0:\n",
        "+                    message = generations[0][0]\n",
        "+                    if isinstance(message, dict):\n",
        "+                        message_obj = message.get(\"message\", {})\n",
        "+                        if isinstance(message_obj, dict):\n",
        "+                            kwargs = message_obj.get(\"kwargs\", {})\n",
        "+                            if isinstance(kwargs, dict):\n",
        "+                                usage_metadata = kwargs.get(\"usage_metadata\", {})\n",
        "+                                if isinstance(usage_metadata, dict):\n",
        "+                                    input_token_details = usage_metadata.get(\n",
        "+                                        \"input_token_details\", {}\n",
        "+                                    )\n",
        "+                                    if isinstance(input_token_details, dict):\n",
        "+                                        if cache_read_tokens is None:\n",
        "+                                            cache_read_tokens = input_token_details.get(\n",
        "+                                                \"cache_read\"\n",
        "+                                            )\n",
        "+                                        if cache_creation_tokens is None:\n",
        "+                                            cache_creation_tokens = (\n",
        "+                                                input_token_details.get(\n",
        "+                                                    \"cache_creation\"\n",
        "+                                                )\n",
        "+                                            )\n",
        "+                                            if cache_creation_tokens is None:\n",
        "+                                                cache_creation_tokens = (\n",
        "+                                                    input_token_details.get(\n",
        "+                                                        \"cache_creation_input_tokens\"\n",
        "+                                                    )\n",
        "+                                                )\n",
        "+\n",
        "+        # Fallback 2: Check outputs[\"usage_metadata\"][\"input_token_details\"]\n",
        "+        if cache_read_tokens is None or cache_creation_tokens is None:\n",
        "+            outputs = getattr(run, \"outputs\", {})\n",
        "+            if isinstance(outputs, dict):\n",
        "+                usage_metadata = outputs.get(\"usage_metadata\", {})\n",
        "+                if isinstance(usage_metadata, dict):\n",
        "+                    input_token_details = usage_metadata.get(\"input_token_details\", {})\n",
        "+                    if isinstance(input_token_details, dict):\n",
        "+                        if cache_read_tokens is None:\n",
        "+                            cache_read_tokens = input_token_details.get(\"cache_read\")\n",
        "+                        if cache_creation_tokens is None:\n",
        "+                            # Try both possible field names (use explicit None check to preserve 0 values)\n",
        "+                            cache_creation_tokens = input_token_details.get(\n",
        "+                                \"cache_creation\"\n",
        "+                            )\n",
        "+                            if cache_creation_tokens is None:\n",
        "+                                cache_creation_tokens = input_token_details.get(\n",
        "+                                    \"cache_creation_input_tokens\"\n",
        "+                                )\n",
        "+\n",
        "+        # Fallback 3: Check inputs[\"usage_metadata\"][\"input_token_details\"] (less common)\n",
        "+        if cache_read_tokens is None or cache_creation_tokens is None:\n",
        "+            inputs = getattr(run, \"inputs\", {})\n",
        "+            if isinstance(inputs, dict):\n",
        "+                usage_metadata = inputs.get(\"usage_metadata\", {})\n",
        "+                if isinstance(usage_metadata, dict):\n",
        "+                    input_token_details = usage_metadata.get(\"input_token_details\", {})\n",
        "+                    if isinstance(input_token_details, dict):\n",
        "+                        if cache_read_tokens is None:\n",
        "+                            cache_read_tokens = input_token_details.get(\"cache_read\")\n",
        "+                        if cache_creation_tokens is None:\n",
        "+                            # Try both possible field names (use explicit None check to preserve 0 values)\n",
        "+                            cache_creation_tokens = input_token_details.get(\n",
        "+                                \"cache_creation\"\n",
        "+                            )\n",
        "+                            if cache_creation_tokens is None:\n",
        "+                                cache_creation_tokens = input_token_details.get(\n",
        "+                                    \"cache_creation_input_tokens\"\n",
        "+                                )\n",
        "+\n",
        "         trace = {\n",
        "             \"id\": str(getattr(run, \"id\", None)) if hasattr(run, \"id\") else None,\n",
        "             \"name\": getattr(run, \"name\", None),\n",
        "@@ -410,6 +491,11 @@ def _format_single_run(self, run: Any) -> Dict[str, Any]:\n",
        "             \"error\": getattr(run, \"error\", None),\n",
        "             \"run_type\": getattr(run, \"run_type\", None),\n",
        "             \"child_runs\": formatted_children,\n",
        "+            \"total_tokens\": getattr(run, \"total_tokens\", None),\n",
        "+            \"prompt_tokens\": getattr(run, \"prompt_tokens\", None),\n",
        "+            \"completion_tokens\": getattr(run, \"completion_tokens\", None),\n",
        "+            \"cache_read_tokens\": cache_read_tokens,\n",
        "+            \"cache_creation_tokens\": cache_creation_tokens,\n",
        "         }\n",
        "         return trace\n",
        " \n"
      ]
    },
    {
      "path": "plans/phase3bc_cost_failure_analysis_plan.md",
      "status": "added",
      "additions": 846,
      "deletions": 0,
      "patch": "@@ -0,0 +1,846 @@\n+# Implementation Plan: Phase 3B & 3C Data Analysis\n+\n+## Executive Summary\n+\n+Implement test-driven Python analysis tools to calculate Phase 3B (Cost Analysis) and Phase 3C (Failure Pattern Analysis) metrics from LangSmith trace data. This extends the existing export-langsmith-data repository with two new analysis modules following established TDD patterns from Phase 3A.\n+\n+**Key Architectural Decision**: Create separate modules (`analyze_cost.py` and `analyze_failures.py`) to maintain clean separation of concerns while reusing shared data structures from Phase 3A.\n+\n+**Estimated Effort**: 18-26 hours (8-12 hours Phase 3B, 10-14 hours Phase 3C)\n+\n+---\n+\n+## Context\n+\n+### Requirements from phase-3-data-analysis-outline.md\n+\n+**Phase 3B: Cost Order-of-Magnitude Analysis**\n+- Calculate cost per workflow using token usage \u00d7 Gemini 1.5 Pro pricing\n+- Breakdown costs by node type to identify expensive components\n+- Assess cache effectiveness (cost savings percentage)\n+- Project scaling costs at 10x, 100x, 1000x volume\n+- Determine economic viability at scale\n+\n+**Phase 3C: Failure Pattern Analysis**\n+- Calculate overall success rate (no external wrapper data - infer from traces)\n+- Identify failure patterns by node and error type\n+- Detect and analyze retry sequences using heuristics\n+- Assess validator effectiveness and redundancy\n+- Quantify retry costs and success rates\n+- Identify quality risks at scale\n+\n+### Data Availability Confirmed\n+\n+\u2705 **Token Usage Data**: Available in traces as `usage_metadata` with fields:\n+- `input_tokens`, `output_tokens`, `total_tokens`\n+- `input_token_details.cache_read` for cached token counts\n+\n+\u2705 **Trace Status**: Available for failure detection\n+- `status` field: \"success\", \"error\", etc.\n+- `error` field: Error messages for classification\n+\n+\u274c **No External Wrapper Data**: Must infer failures and retries from trace patterns\n+\n+### Repository Architecture (Existing)\n+\n+```\n+export-langsmith-data/\n+\u251c\u2500\u2500 analyze_traces.py              # Phase 3A: 727 lines, performance analysis\n+\u251c\u2500\u2500 test_analyze_traces.py         # 1,272 lines, 64 passing tests\n+\u251c\u2500\u2500 export_langsmith_traces.py     # 697 lines, LangSmith API export\n+\u251c\u2500\u2500 verify_analysis_report.py      # 366 lines, deterministic verification\n+\u2514\u2500\u2500 (test files)                   # ~3,500 lines total test coverage\n+```\n+\n+**Shared Data Structures** (from analyze_traces.py):\n+- `Trace`: Single run with id, name, status, duration, inputs, outputs, error\n+- `Workflow`: Complete execution with root_trace and hierarchical nodes\n+- `TraceDataset`: Container with workflows, metadata, hierarchical flag\n+- `load_from_json()`: Data loading function\n+\n+---\n+\n+## Implementation Strategy\n+\n+### 1. Module Organization\n+\n+**NEW Files to Create**:\n+```\n+export-langsmith-data/\n+\u251c\u2500\u2500 analyze_cost.py                # NEW - Phase 3B Cost Analysis (~700 lines)\n+\u251c\u2500\u2500 test_analyze_cost.py           # NEW - Phase 3B tests (~900 lines, ~45 tests)\n+\u251c\u2500\u2500 analyze_failures.py            # NEW - Phase 3C Failure Analysis (~800 lines)\n+\u2514\u2500\u2500 test_analyze_failures.py       # NEW - Phase 3C tests (~1000 lines, ~56 tests)\n+```\n+\n+**MODIFIED Files**:\n+```\n+\u2514\u2500\u2500 verify_analysis_report.py      # Extend with 3B/3C verification functions\n+```\n+\n+**Rationale**: Separate modules maintain clean boundaries, enable independent evolution, and keep test suites focused. Each phase has distinct concerns (economics vs quality) that warrant separate files.\n+\n+---\n+\n+## Phase 3B: Cost Analysis Implementation\n+\n+### Configuration Constants\n+\n+```python\n+# analyze_cost.py - Top of file\n+\n+from dataclasses import dataclass\n+\n+@dataclass\n+class PricingConfig:\n+    \"\"\"Configurable pricing model for any LLM provider.\"\"\"\n+    model_name: str\n+    input_tokens_per_1k: float           # Cost per 1K input tokens\n+    output_tokens_per_1k: float          # Cost per 1K output tokens\n+    cache_read_per_1k: Optional[float] = None   # Cost per 1K cache read tokens (if applicable)\n+\n+    def __post_init__(self):\n+        \"\"\"Validate pricing configuration.\"\"\"\n+        if self.input_tokens_per_1k < 0 or self.output_tokens_per_1k < 0:\n+            raise ValueError(\"Token prices must be non-negative\")\n+        if self.cache_read_per_1k is not None and self.cache_read_per_1k < 0:\n+            raise ValueError(\"Cache read price must be non-negative\")\n+\n+\n+# Example pricing configs for reference (NOT hard-coded defaults)\n+# Users should create their own PricingConfig instances\n+EXAMPLE_PRICING_CONFIGS = {\n+    \"gemini_1.5_pro\": PricingConfig(\n+        model_name=\"Gemini 1.5 Pro\",\n+        input_tokens_per_1k=0.00125,      # $1.25 per 1M input tokens\n+        output_tokens_per_1k=0.005,       # $5.00 per 1M output tokens\n+        cache_read_per_1k=0.0003125,      # $0.3125 per 1M cache read tokens\n+    ),\n+    # Add other providers as examples\n+}\n+\n+SCALING_FACTORS = [1, 10, 100, 1000]  # Current, 10x, 100x, 1000x\n+```\n+\n+### Core Data Structures\n+\n+```python\n+@dataclass\n+class TokenUsage:\n+    \"\"\"Token usage for a single trace.\"\"\"\n+    input_tokens: int\n+    output_tokens: int\n+    total_tokens: int\n+    cached_tokens: Optional[int] = None      # From input_token_details.cache_read\n+\n+@dataclass\n+class CostBreakdown:\n+    \"\"\"Cost breakdown for a single trace.\"\"\"\n+    trace_id: str\n+    trace_name: str\n+    input_cost: float\n+    output_cost: float\n+    cache_cost: float\n+    total_cost: float\n+    token_usage: TokenUsage\n+\n+@dataclass\n+class NodeCostSummary:\n+    \"\"\"Cost summary for a node type across workflows.\"\"\"\n+    node_name: str\n+    execution_count: int\n+    total_cost: float\n+    avg_cost_per_execution: float\n+    percent_of_total_cost: float\n+\n+@dataclass\n+class CostAnalysisResults:\n+    \"\"\"Complete cost analysis results.\"\"\"\n+    # Per-workflow metrics\n+    avg_cost_per_workflow: float\n+    median_cost_per_workflow: float\n+    min_cost: float\n+    max_cost: float\n+\n+    # Node-level breakdown\n+    node_summaries: List[NodeCostSummary]    # Sorted by cost descending\n+    top_cost_driver: Optional[str]\n+\n+    # Cache effectiveness\n+    cache_effectiveness_percent: Optional[float]\n+    cache_savings_dollars: Optional[float]\n+\n+    # Scaling projections\n+    scaling_projections: Dict[str, ScalingProjection]  # \"10x\", \"100x\", etc.\n+\n+    # Metadata\n+    total_workflows_analyzed: int\n+    data_quality_notes: List[str]\n+\n+    def to_csv(self) -> str:\n+        \"\"\"Export to CSV format for reporting.\"\"\"\n+```\n+\n+### Key Functions\n+\n+**Token Extraction**:\n+```python\n+def extract_token_usage(trace: Trace) -> Optional[TokenUsage]:\n+    \"\"\"\n+    Extract token usage from trace outputs/inputs.\n+\n+    Checks:\n+    1. trace.outputs[\"usage_metadata\"]\n+    2. trace.inputs[\"usage_metadata\"] (fallback)\n+    3. Extracts cache_read from input_token_details if available\n+    \"\"\"\n+\n+def extract_workflow_tokens(workflow: Workflow) -> Dict[str, TokenUsage]:\n+    \"\"\"Extract token usage for all traces in workflow.\"\"\"\n+```\n+\n+**Cost Calculation**:\n+```python\n+def calculate_trace_cost(\n+    token_usage: TokenUsage,\n+    pricing_config: PricingConfig\n+) -> CostBreakdown:\n+    \"\"\"Calculate cost for single trace using pricing model.\"\"\"\n+\n+def calculate_workflow_cost(\n+    workflow: Workflow,\n+    pricing_config: PricingConfig\n+) -> Optional[WorkflowCostAnalysis]:\n+    \"\"\"Calculate total cost and breakdown by node.\"\"\"\n+```\n+\n+**Aggregation & Analysis**:\n+```python\n+def aggregate_node_costs(\n+    workflow_analyses: List[WorkflowCostAnalysis]\n+) -> List[NodeCostSummary]:\n+    \"\"\"Aggregate costs by node type, sorted by total_cost descending.\"\"\"\n+\n+def calculate_cache_effectiveness(\n+    workflow_analyses: List[WorkflowCostAnalysis]\n+) -> Optional[Tuple[float, float, float]]:\n+    \"\"\"\n+    Calculate cache effectiveness if cache data available.\n+    Returns (effectiveness_percent, cost_without_cache, savings_dollars).\n+    \"\"\"\n+\n+def project_scaling_costs(\n+    avg_cost_per_workflow: float,\n+    current_workflow_count: int,\n+    scaling_factors: List[int]\n+) -> Dict[str, ScalingProjection]:\n+    \"\"\"Project costs at 10x, 100x, 1000x scale.\"\"\"\n+```\n+\n+**Main Entry Point**:\n+```python\n+def analyze_costs(\n+    workflows: List[Workflow],\n+    pricing_config: PricingConfig\n+) -> CostAnalysisResults:\n+    \"\"\"\n+    Perform complete cost analysis on workflows.\n+\n+    Args:\n+        workflows: List of Workflow objects to analyze\n+        pricing_config: PricingConfig with provider-specific rates\n+\n+    Main entry point for Phase 3B.\n+\n+    Example:\n+        # Create custom pricing config\n+        pricing = PricingConfig(\n+            model_name=\"Gemini 1.5 Pro\",\n+            input_tokens_per_1k=0.00125,\n+            output_tokens_per_1k=0.005,\n+            cache_read_per_1k=0.0003125\n+        )\n+\n+        results = analyze_costs(workflows, pricing)\n+    \"\"\"\n+```\n+\n+### Testing Strategy (Phase 3B)\n+\n+**Test File**: `test_analyze_cost.py` (~45 tests)\n+\n+**Test Classes**:\n+1. `TestTokenExtraction` - Extract tokens from various trace structures\n+2. `TestCostCalculation` - Basic and edge case cost calculations\n+3. `TestNodeCostAggregation` - Multi-workflow aggregation\n+4. `TestCacheEffectiveness` - Cache savings calculations\n+5. `TestScalingProjections` - Scaling math and viability thresholds\n+6. `TestCostAnalysisIntegration` - End-to-end with mock workflows\n+7. `TestCSVExport` - CSV formatting\n+\n+**Key Test Cases**:\n+- Token extraction from `outputs[\"usage_metadata\"]`\n+- Token extraction with cache_read tokens\n+- Missing token data returns None gracefully\n+- Zero-cost traces handled correctly\n+- Cost calculation accuracy (verify pricing math)\n+- Node aggregation sorted correctly\n+- Scaling projections calculate monthly costs\n+- CSV format matches expected structure\n+\n+**TDD Workflow**: Write test \u2192 See it fail \u2192 Implement minimal code \u2192 Pass \u2192 Refactor\n+\n+### CSV Exports\n+\n+```python\n+def export_node_costs_csv(node_summaries: List[NodeCostSummary]) -> str:\n+    \"\"\"Export node cost breakdown to CSV.\"\"\"\n+\n+def export_scaling_projections_csv(projections: Dict[str, ScalingProjection]) -> str:\n+    \"\"\"Export scaling projections to CSV.\"\"\"\n+```\n+\n+---\n+\n+## Phase 3C: Failure Pattern Analysis Implementation\n+\n+### Configuration Constants\n+\n+```python\n+# analyze_failures.py - Top of file\n+\n+# Status values indicating failure\n+FAILURE_STATUSES = {\"error\", \"failed\", \"cancelled\"}\n+SUCCESS_STATUSES = {\"success\"}\n+\n+# Retry detection heuristics\n+RETRY_DETECTION_CONFIG = {\n+    \"max_time_window_seconds\": 300,  # 5 min window for retry detection\n+    \"same_node_threshold\": 2,        # 2+ executions = potential retry\n+}\n+\n+# Validator node names (from Phase 3A)\n+VALIDATOR_NODES = {\n+    \"meta_evaluation_and_validation\",\n+    \"normative_validation\",\n+    \"simulated_testing\",\n+}\n+\n+# Error classification patterns (regex)\n+ERROR_PATTERNS = {\n+    \"validation_failure\": r\"validation.*fail|invalid.*spec\",\n+    \"api_timeout\": r\"timeout|timed out\",\n+    \"import_error\": r\"import.*fail|upload.*fail\",\n+    \"llm_error\": r\"model.*error|generation.*fail|token.*limit\",\n+    \"unknown\": r\".*\",  # Catch-all\n+}\n+```\n+\n+### Core Data Structures\n+\n+```python\n+@dataclass\n+class FailureInstance:\n+    \"\"\"Single failure occurrence.\"\"\"\n+    trace_id: str\n+    trace_name: str\n+    workflow_id: str\n+    error_message: Optional[str]\n+    error_type: str                  # Classified from ERROR_PATTERNS\n+    timestamp: Optional[datetime]\n+\n+@dataclass\n+class RetrySequence:\n+    \"\"\"Detected retry sequence.\"\"\"\n+    node_name: str\n+    workflow_id: str\n+    attempt_count: int\n+    attempts: List[Trace]            # Ordered by start_time\n+    final_status: str                # 'success' or 'failed'\n+    total_duration_seconds: float\n+    total_cost_estimate: Optional[float] = None\n+\n+@dataclass\n+class NodeFailureStats:\n+    \"\"\"Failure statistics for a node type.\"\"\"\n+    node_name: str\n+    total_executions: int\n+    failure_count: int\n+    success_count: int\n+    failure_rate_percent: float\n+    retry_sequences_detected: int\n+    avg_retries_when_failing: float\n+    common_error_types: Dict[str, int]  # error_type -> count\n+\n+@dataclass\n+class ValidatorEffectivenessAnalysis:\n+    \"\"\"Validator effectiveness assessment.\"\"\"\n+    validator_name: str\n+    total_executions: int\n+    caught_issues_count: int         # Failures detected\n+    pass_rate_percent: float\n+    is_necessary: bool               # Based on redundancy analysis\n+\n+@dataclass\n+class FailureAnalysisResults:\n+    \"\"\"Complete failure pattern analysis results.\"\"\"\n+    # Overall metrics\n+    total_workflows: int\n+    successful_workflows: int\n+    failed_workflows: int\n+    overall_success_rate_percent: float\n+\n+    # Node-level breakdown\n+    node_failure_stats: List[NodeFailureStats]  # Sorted by failure_rate\n+    highest_failure_node: Optional[str]\n+\n+    # Error distribution\n+    error_type_distribution: Dict[str, int]\n+    most_common_error_type: Optional[str]\n+\n+    # Retry analysis\n+    total_retry_sequences: int\n+    retry_sequences: List[RetrySequence]\n+    retry_success_rate_percent: Optional[float]\n+    avg_cost_of_retries: Optional[float]\n+\n+    # Validator analysis\n+    validator_analyses: List[ValidatorEffectivenessAnalysis]\n+    redundant_validators: List[str]\n+\n+    # Quality risks\n+    quality_risks_at_scale: List[str]\n+\n+    def to_csv(self) -> str:\n+        \"\"\"Export to CSV format for reporting.\"\"\"\n+```\n+\n+### Key Functions\n+\n+**Failure Detection**:\n+```python\n+def detect_failures(workflow: Workflow) -> List[FailureInstance]:\n+    \"\"\"Detect all failures in workflow using trace.status and trace.error.\"\"\"\n+\n+def classify_error(error_message: Optional[str]) -> str:\n+    \"\"\"Classify error into type using regex patterns.\"\"\"\n+```\n+\n+**Retry Detection**:\n+```python\n+def detect_retry_sequences(workflow: Workflow) -> List[RetrySequence]:\n+    \"\"\"\n+    Detect retry sequences using heuristics:\n+    - Multiple executions of same node within time window\n+    - Ordered by start_time\n+    \"\"\"\n+\n+def calculate_retry_success_rate(\n+    retry_sequences: List[RetrySequence]\n+) -> Optional[float]:\n+    \"\"\"Calculate % of retries that eventually succeed.\"\"\"\n+```\n+\n+**Node & Validator Analysis**:\n+```python\n+def analyze_node_failures(\n+    workflows: List[Workflow]\n+) -> List[NodeFailureStats]:\n+    \"\"\"Analyze failure patterns by node type.\"\"\"\n+\n+def analyze_validator_effectiveness(\n+    workflows: List[Workflow]\n+) -> List[ValidatorEffectivenessAnalysis]:\n+    \"\"\"Assess whether all 3 validators are necessary.\"\"\"\n+\n+def detect_validator_redundancy(\n+    validator_analyses: List[ValidatorEffectivenessAnalysis]\n+) -> List[str]:\n+    \"\"\"Identify validators with >90% overlap in caught issues.\"\"\"\n+```\n+\n+**Root Cause & Risk**:\n+```python\n+def identify_root_causes(\n+    failure_instances: List[FailureInstance],\n+    node_stats: List[NodeFailureStats]\n+) -> Dict[str, List[str]]:\n+    \"\"\"\n+    Categorize root causes:\n+    - Prompt issues (validation failures)\n+    - Logic bugs (repeated errors)\n+    - External failures (API timeouts, import errors)\n+    \"\"\"\n+\n+def assess_quality_risks_at_scale(\n+    results: FailureAnalysisResults,\n+    current_volume: int,\n+    projected_volume: int\n+) -> List[str]:\n+    \"\"\"Generate risk warnings for scaling.\"\"\"\n+```\n+\n+**Main Entry Point**:\n+```python\n+def analyze_failures(workflows: List[Workflow]) -> FailureAnalysisResults:\n+    \"\"\"\n+    Perform complete failure pattern analysis.\n+    Main entry point for Phase 3C.\n+    \"\"\"\n+```\n+\n+### Testing Strategy (Phase 3C)\n+\n+**Test File**: `test_analyze_failures.py` (~56 tests)\n+\n+**Test Classes**:\n+1. `TestFailureDetection` - Detect failures from status/error fields\n+2. `TestRetryDetection` - Heuristic-based retry sequence detection\n+3. `TestNodeFailureAnalysis` - Node-level statistics\n+4. `TestValidatorEffectiveness` - Validator redundancy analysis\n+5. `TestRootCauseAnalysis` - Error categorization\n+6. `TestQualityRiskAssessment` - Risk identification\n+7. `TestFailureAnalysisIntegration` - End-to-end tests\n+8. `TestCSVExport` - CSV formatting\n+\n+**Key Test Cases**:\n+- Detect single and multiple failures in workflow\n+- Classify errors using regex patterns\n+- Detect retry sequences with 2, 3, 5+ attempts\n+- Respect time window for retry detection\n+- Calculate retry success rates correctly\n+- Identify redundant validators (overlap detection)\n+- Generate appropriate risk warnings\n+- Handle workflows with 0% and 100% success rates\n+\n+### CSV Exports\n+\n+```python\n+def export_node_failures_csv(node_stats: List[NodeFailureStats]) -> str:\n+    \"\"\"Export node failure statistics to CSV.\"\"\"\n+\n+def export_validator_effectiveness_csv(\n+    analyses: List[ValidatorEffectivenessAnalysis]\n+) -> str:\n+    \"\"\"Export validator effectiveness to CSV.\"\"\"\n+\n+def export_retry_analysis_csv(retry_sequences: List[RetrySequence]) -> str:\n+    \"\"\"Export retry sequence analysis to CSV.\"\"\"\n+```\n+\n+---\n+\n+## Integration with Verification Tool\n+\n+### Extend verify_analysis_report.py\n+\n+Add two new verification functions:\n+\n+```python\n+def verify_cost_analysis(\n+    dataset: TraceDataset,\n+    expected: Optional[Dict[str, Any]] = None\n+) -> CostAnalysisResults:\n+    \"\"\"\n+    Verify Phase 3B cost calculations.\n+\n+    Displays:\n+    - Cost per workflow (avg, median, range)\n+    - Top 3 cost drivers\n+    - Scaling projections (10x, 100x, 1000x)\n+    - Cache effectiveness if available\n+    \"\"\"\n+\n+def verify_failure_analysis(\n+    dataset: TraceDataset,\n+    expected: Optional[Dict[str, Any]] = None\n+) -> FailureAnalysisResults:\n+    \"\"\"\n+    Verify Phase 3C failure calculations.\n+\n+    Displays:\n+    - Overall success rate\n+    - Top 5 nodes by failure rate\n+    - Retry analysis (sequences detected, success rate)\n+    - Validator effectiveness\n+    \"\"\"\n+```\n+\n+Update `main()` to accept `--phases` argument:\n+```python\n+parser.add_argument(\"--phases\", type=str, default=\"all\",\n+                   help=\"Phases to verify: 3a, 3b, 3c, or all\")\n+```\n+\n+---\n+\n+## Implementation Sequence (TDD Workflow)\n+\n+### Phase 3B: Cost Analysis (8-12 hours)\n+\n+**Step 1: Setup** (30 min)\n+- Create `analyze_cost.py` with pricing constants and imports\n+- Create `test_analyze_cost.py` with test structure\n+- Add initial docstrings\n+\n+**Step 2: Token Extraction** (2-3 hours)\n+1. Write test: `test_extract_token_usage_from_outputs()`\n+2. Implement `extract_token_usage()` to pass\n+3. Write test: `test_extract_token_usage_with_cache_data()`\n+4. Extend implementation for cache tokens\n+5. Write test: `test_extract_token_usage_missing_data()`\n+6. Handle None return gracefully\n+7. **CRITICAL**: Test with REAL trace data first to confirm field locations\n+\n+**Step 3: Cost Calculation** (2-3 hours)\n+1. Write test: `test_calculate_trace_cost_basic()`\n+2. Implement basic calculation (input + output tokens)\n+3. Write test: `test_calculate_trace_cost_with_cache()`\n+4. Add cache cost logic\n+5. Write test: `test_calculate_workflow_cost()`\n+6. Implement workflow-level aggregation\n+\n+**Step 4: Aggregation & Analysis** (2-3 hours)\n+1. Write tests for `aggregate_node_costs()`\n+2. Implement node-level aggregation with sorting\n+3. Write tests for `calculate_cache_effectiveness()`\n+4. Implement cache savings calculation\n+5. Write tests for `project_scaling_costs()`\n+6. Implement scaling projections\n+\n+**Step 5: Main Analysis & Export** (1-2 hours)\n+1. Write integration test for `analyze_costs()`\n+2. Implement main orchestration function\n+3. Add data quality checks and warnings\n+4. Implement CSV export functions\n+5. Test CSV format\n+\n+**Step 6: Verification Integration** (1 hour)\n+1. Add `verify_cost_analysis()` to verify_analysis_report.py\n+2. Test verification with sample data\n+3. Update main() with --phases argument\n+\n+### Phase 3C: Failure Analysis (10-14 hours)\n+\n+**Step 1: Setup** (30 min)\n+- Create `analyze_failures.py` with error patterns and constants\n+- Create `test_analyze_failures.py` with test structure\n+\n+**Step 2: Failure Detection** (2-3 hours)\n+1. Write tests for `detect_failures()`\n+2. Implement status-based detection\n+3. Write tests for `classify_error()`\n+4. Implement regex-based classification\n+\n+**Step 3: Retry Detection** (3-4 hours)\n+1. Write tests for simple retry detection (2 attempts)\n+2. Implement basic retry detection\n+3. Write tests for complex scenarios (5+ attempts, time windows)\n+4. Refine detection heuristics\n+5. Write tests for `calculate_retry_success_rate()`\n+6. Implement success rate calculation\n+\n+**Step 4: Node Failure Analysis** (2-3 hours)\n+1. Write tests for `analyze_node_failures()`\n+2. Implement aggregation and statistics\n+3. Test sorting by failure_rate\n+4. Test error type aggregation\n+\n+**Step 5: Validator Analysis** (2-3 hours)\n+1. Write tests for `analyze_validator_effectiveness()`\n+2. Implement effectiveness metrics\n+3. Write tests for `detect_validator_redundancy()`\n+4. Implement overlap detection logic\n+\n+**Step 6: Root Cause & Integration** (2-3 hours)\n+1. Write tests for `identify_root_causes()`\n+2. Implement categorization logic\n+3. Write tests for `assess_quality_risks_at_scale()`\n+4. Implement risk warning generation\n+5. Write integration test for `analyze_failures()`\n+6. Implement main function and CSV exports\n+\n+**Step 7: Verification Integration** (1 hour)\n+1. Add `verify_failure_analysis()` to verify_analysis_report.py\n+2. Test verification with sample data\n+\n+---\n+\n+## Data Quality Handling\n+\n+### Edge Cases to Handle\n+\n+**Phase 3B**:\n+- Missing token data \u2192 Skip trace, add to data_quality_notes\n+- Zero-cost traces \u2192 Include in analysis (valid for non-LLM nodes)\n+- Missing cache data \u2192 Skip cache analysis, report \"N/A\"\n+- Incomplete workflows \u2192 Partial cost calculated, note in quality report\n+\n+**Phase 3C**:\n+- No failures \u2192 Report 100% success rate (valid result)\n+- Missing error messages \u2192 Classify as \"unknown\"\n+- Ambiguous retries \u2192 Use time window + node name heuristic\n+- Missing validators \u2192 Note in analysis, skip redundancy check\n+\n+### Data Quality Reporting\n+\n+Both modules include `data_quality_notes` field in results:\n+\n+```python\n+data_quality_notes: List[str] = [\n+    \"Token data available for 95/100 workflows\",\n+    \"Cache data not available - cache analysis skipped\",\n+    \"Retry detection based on heuristics (no definitive markers)\",\n+]\n+```\n+\n+---\n+\n+## Critical Pre-Implementation Steps\n+\n+### BEFORE starting implementation, MUST complete:\n+\n+1. **Verify Token Data Structure** (30 min)\n+   ```python\n+   # Inspect actual trace structure\n+   import json\n+   with open(\"trace_export_1000.json\") as f:\n+       data = json.load(f)\n+\n+   # Find LLM trace and print structure\n+   trace = [t for t in data[\"traces\"] if t[\"name\"] == \"ChatGoogleGenerativeAI\"][0]\n+   print(json.dumps(trace[\"outputs\"], indent=2))\n+   ```\n+\n+2. **Create Client-Specific Pricing Script** (15 min)\n+   - Research current Gemini 1.5 Pro pricing (Dec 2025)\n+   - Create `scripts/analyze_with_gemini_pricing.py` in client repo\n+   - Script instantiates PricingConfig with Gemini rates\n+   - Calls analyze_costs() with custom pricing\n+\n+3. **Document Field Locations** (15 min)\n+   - Record exact path to usage_metadata\n+   - Record exact path to cache_read tokens\n+   - Create example trace structure in docstrings\n+\n+---\n+\n+## Success Criteria\n+\n+### Phase 3B Complete When:\n+- \u2705 All 45+ tests passing\n+- \u2705 Can answer: \"What does a workflow cost?\" \u2192 $__\n+- \u2705 Can answer: \"What's the biggest cost driver?\" \u2192 [node name]\n+- \u2705 Can answer: \"Can it scale to 100x volume?\" \u2192 YES/NO with projection\n+- \u2705 Cache effectiveness calculated (or \"N/A\" if unavailable)\n+- \u2705 CSV exports generated for all metrics\n+- \u2705 Verification tool confirms calculations\n+\n+### Phase 3C Complete When:\n+- \u2705 All 56+ tests passing\n+- \u2705 Can answer: \"What's the success rate?\" \u2192 ___%\n+- \u2705 Can answer: \"Where do failures happen most?\" \u2192 [node name + rate]\n+- \u2705 Can answer: \"Are retries effective?\" \u2192 ___% success rate\n+- \u2705 Can answer: \"Are all 3 validators necessary?\" \u2192 YES/NO with data\n+- \u2705 CSV exports generated for failures, retries, validators\n+- \u2705 Quality risks identified for scaling\n+- \u2705 Verification tool confirms calculations\n+\n+---\n+\n+## Deliverables\n+\n+### Code Deliverables\n+1. `analyze_cost.py` (~700 lines) with full documentation\n+2. `test_analyze_cost.py` (~900 lines, 45+ tests passing)\n+3. `analyze_failures.py` (~800 lines) with full documentation\n+4. `test_analyze_failures.py` (~1000 lines, 56+ tests passing)\n+5. Extended `verify_analysis_report.py` with 3B/3C verification\n+6. Updated README.md with Phase 3B/3C usage examples\n+\n+### Analysis Outputs (for client Assessment)\n+1. Cost analysis CSV exports:\n+   - Node cost breakdown\n+   - Scaling projections\n+   - Cache effectiveness (if available)\n+2. Failure analysis CSV exports:\n+   - Node failure statistics\n+   - Retry analysis\n+   - Validator effectiveness\n+3. Verification reports confirming all calculations\n+\n+---\n+\n+## Repository Separation Strategy\n+\n+**Generalized Tools** (export-langsmith-data repo):\n+- `analyze_cost.py` - Generic cost analysis for any LangSmith traces\n+- `analyze_failures.py` - Generic failure analysis for any traces\n+- All test files - Reusable test patterns\n+- `verify_analysis_report.py` - Verification framework\n+\n+**Client-Specific Analysis** (client-project/Assessment/data):\n+- Actual trace data files (.json)\n+- Generated CSV reports\n+- Client-specific interpretation and findings\n+- Phase 3B/3C markdown reports referencing data\n+- **Custom pricing script** (`scripts/analyze_with_gemini_pricing.py`):\n+  - Imports `PricingConfig` from generalized tool\n+  - Creates config with Gemini-specific rates\n+  - Runs analysis with client's trace data\n+\n+This separation allows:\n+- Reuse of analysis tools across projects\n+- Client confidentiality (data stays in Assessment repo)\n+- Test-first development in open repo\n+- Easy updates to pricing models and error patterns\n+\n+---\n+\n+## Risk Mitigation\n+\n+| Risk | Mitigation |\n+|------|------------|\n+| Token data not in expected location | Inspect real data FIRST, implement flexible extraction |\n+| Cache data unavailable | Make cache analysis optional, graceful degradation |\n+| Retry detection false positives | Tune time window (5 min), manual spot-checking |\n+| Pricing model outdated | Externalize constants, document update procedure |\n+| Large datasets cause memory issues | Process in batches if needed (unlikely with n=10-1000) |\n+| Test coverage gaps | Strict TDD, aim for >90% coverage |\n+\n+---\n+\n+## Critical Files\n+\n+**To Read** (for reference patterns):\n+1. `analyze_traces.py`\n+2. `test_analyze_traces.py`\n+3. `verify_analysis_report.py`\n+\n+**To Create**:\n+1. `analyze_cost.py`\n+2. `test_analyze_cost.py`\n+3. `analyze_failures.py`\n+4. `test_analyze_failures.py`\n+\n+**To Modify**:\n+1. `verify_analysis_report.py`\n+\n+**Data Files** (for inspection):\n+1. `trace_export_1000.json` (newly exported)\n+2. `../client-project/Assessment/data/trace_export_1000.json`\n+3. `../client-project/Assessment/data/trace_export_complete_workflows.json`\n+\n+---\n+\n+## Next Steps After Approval\n+\n+1. **Data Investigation** (30 min) - Inspect token field locations in real traces\n+2. **Pricing Research** (15 min) - Confirm Gemini 1.5 Pro current pricing\n+3. **Setup** (30 min) - Create files, initial structure, constants\n+4. **Phase 3B Implementation** (8-12 hours) - TDD workflow\n+5. **Phase 3C Implementation** (10-14 hours) - TDD workflow\n+6. **Verification & Documentation** (2-3 hours) - Final testing, README updates\n+7. **Generate Reports** (1-2 hours) - Run on actual data, export CSVs for Assessment\n+\n+**Total Estimated Time**: 18-26 hours for complete implementation",
      "patch_lines": [
        "@@ -0,0 +1,846 @@\n",
        "+# Implementation Plan: Phase 3B & 3C Data Analysis\n",
        "+\n",
        "+## Executive Summary\n",
        "+\n",
        "+Implement test-driven Python analysis tools to calculate Phase 3B (Cost Analysis) and Phase 3C (Failure Pattern Analysis) metrics from LangSmith trace data. This extends the existing export-langsmith-data repository with two new analysis modules following established TDD patterns from Phase 3A.\n",
        "+\n",
        "+**Key Architectural Decision**: Create separate modules (`analyze_cost.py` and `analyze_failures.py`) to maintain clean separation of concerns while reusing shared data structures from Phase 3A.\n",
        "+\n",
        "+**Estimated Effort**: 18-26 hours (8-12 hours Phase 3B, 10-14 hours Phase 3C)\n",
        "+\n",
        "+---\n",
        "+\n",
        "+## Context\n",
        "+\n",
        "+### Requirements from phase-3-data-analysis-outline.md\n",
        "+\n",
        "+**Phase 3B: Cost Order-of-Magnitude Analysis**\n",
        "+- Calculate cost per workflow using token usage \u00d7 Gemini 1.5 Pro pricing\n",
        "+- Breakdown costs by node type to identify expensive components\n",
        "+- Assess cache effectiveness (cost savings percentage)\n",
        "+- Project scaling costs at 10x, 100x, 1000x volume\n",
        "+- Determine economic viability at scale\n",
        "+\n",
        "+**Phase 3C: Failure Pattern Analysis**\n",
        "+- Calculate overall success rate (no external wrapper data - infer from traces)\n",
        "+- Identify failure patterns by node and error type\n",
        "+- Detect and analyze retry sequences using heuristics\n",
        "+- Assess validator effectiveness and redundancy\n",
        "+- Quantify retry costs and success rates\n",
        "+- Identify quality risks at scale\n",
        "+\n",
        "+### Data Availability Confirmed\n",
        "+\n",
        "+\u2705 **Token Usage Data**: Available in traces as `usage_metadata` with fields:\n",
        "+- `input_tokens`, `output_tokens`, `total_tokens`\n",
        "+- `input_token_details.cache_read` for cached token counts\n",
        "+\n",
        "+\u2705 **Trace Status**: Available for failure detection\n",
        "+- `status` field: \"success\", \"error\", etc.\n",
        "+- `error` field: Error messages for classification\n",
        "+\n",
        "+\u274c **No External Wrapper Data**: Must infer failures and retries from trace patterns\n",
        "+\n",
        "+### Repository Architecture (Existing)\n",
        "+\n",
        "+```\n",
        "+export-langsmith-data/\n",
        "+\u251c\u2500\u2500 analyze_traces.py              # Phase 3A: 727 lines, performance analysis\n",
        "+\u251c\u2500\u2500 test_analyze_traces.py         # 1,272 lines, 64 passing tests\n",
        "+\u251c\u2500\u2500 export_langsmith_traces.py     # 697 lines, LangSmith API export\n",
        "+\u251c\u2500\u2500 verify_analysis_report.py      # 366 lines, deterministic verification\n",
        "+\u2514\u2500\u2500 (test files)                   # ~3,500 lines total test coverage\n",
        "+```\n",
        "+\n",
        "+**Shared Data Structures** (from analyze_traces.py):\n",
        "+- `Trace`: Single run with id, name, status, duration, inputs, outputs, error\n",
        "+- `Workflow`: Complete execution with root_trace and hierarchical nodes\n",
        "+- `TraceDataset`: Container with workflows, metadata, hierarchical flag\n",
        "+- `load_from_json()`: Data loading function\n",
        "+\n",
        "+---\n",
        "+\n",
        "+## Implementation Strategy\n",
        "+\n",
        "+### 1. Module Organization\n",
        "+\n",
        "+**NEW Files to Create**:\n",
        "+```\n",
        "+export-langsmith-data/\n",
        "+\u251c\u2500\u2500 analyze_cost.py                # NEW - Phase 3B Cost Analysis (~700 lines)\n",
        "+\u251c\u2500\u2500 test_analyze_cost.py           # NEW - Phase 3B tests (~900 lines, ~45 tests)\n",
        "+\u251c\u2500\u2500 analyze_failures.py            # NEW - Phase 3C Failure Analysis (~800 lines)\n",
        "+\u2514\u2500\u2500 test_analyze_failures.py       # NEW - Phase 3C tests (~1000 lines, ~56 tests)\n",
        "+```\n",
        "+\n",
        "+**MODIFIED Files**:\n",
        "+```\n",
        "+\u2514\u2500\u2500 verify_analysis_report.py      # Extend with 3B/3C verification functions\n",
        "+```\n",
        "+\n",
        "+**Rationale**: Separate modules maintain clean boundaries, enable independent evolution, and keep test suites focused. Each phase has distinct concerns (economics vs quality) that warrant separate files.\n",
        "+\n",
        "+---\n",
        "+\n",
        "+## Phase 3B: Cost Analysis Implementation\n",
        "+\n",
        "+### Configuration Constants\n",
        "+\n",
        "+```python\n",
        "+# analyze_cost.py - Top of file\n",
        "+\n",
        "+from dataclasses import dataclass\n",
        "+\n",
        "+@dataclass\n",
        "+class PricingConfig:\n",
        "+    \"\"\"Configurable pricing model for any LLM provider.\"\"\"\n",
        "+    model_name: str\n",
        "+    input_tokens_per_1k: float           # Cost per 1K input tokens\n",
        "+    output_tokens_per_1k: float          # Cost per 1K output tokens\n",
        "+    cache_read_per_1k: Optional[float] = None   # Cost per 1K cache read tokens (if applicable)\n",
        "+\n",
        "+    def __post_init__(self):\n",
        "+        \"\"\"Validate pricing configuration.\"\"\"\n",
        "+        if self.input_tokens_per_1k < 0 or self.output_tokens_per_1k < 0:\n",
        "+            raise ValueError(\"Token prices must be non-negative\")\n",
        "+        if self.cache_read_per_1k is not None and self.cache_read_per_1k < 0:\n",
        "+            raise ValueError(\"Cache read price must be non-negative\")\n",
        "+\n",
        "+\n",
        "+# Example pricing configs for reference (NOT hard-coded defaults)\n",
        "+# Users should create their own PricingConfig instances\n",
        "+EXAMPLE_PRICING_CONFIGS = {\n",
        "+    \"gemini_1.5_pro\": PricingConfig(\n",
        "+        model_name=\"Gemini 1.5 Pro\",\n",
        "+        input_tokens_per_1k=0.00125,      # $1.25 per 1M input tokens\n",
        "+        output_tokens_per_1k=0.005,       # $5.00 per 1M output tokens\n",
        "+        cache_read_per_1k=0.0003125,      # $0.3125 per 1M cache read tokens\n",
        "+    ),\n",
        "+    # Add other providers as examples\n",
        "+}\n",
        "+\n",
        "+SCALING_FACTORS = [1, 10, 100, 1000]  # Current, 10x, 100x, 1000x\n",
        "+```\n",
        "+\n",
        "+### Core Data Structures\n",
        "+\n",
        "+```python\n",
        "+@dataclass\n",
        "+class TokenUsage:\n",
        "+    \"\"\"Token usage for a single trace.\"\"\"\n",
        "+    input_tokens: int\n",
        "+    output_tokens: int\n",
        "+    total_tokens: int\n",
        "+    cached_tokens: Optional[int] = None      # From input_token_details.cache_read\n",
        "+\n",
        "+@dataclass\n",
        "+class CostBreakdown:\n",
        "+    \"\"\"Cost breakdown for a single trace.\"\"\"\n",
        "+    trace_id: str\n",
        "+    trace_name: str\n",
        "+    input_cost: float\n",
        "+    output_cost: float\n",
        "+    cache_cost: float\n",
        "+    total_cost: float\n",
        "+    token_usage: TokenUsage\n",
        "+\n",
        "+@dataclass\n",
        "+class NodeCostSummary:\n",
        "+    \"\"\"Cost summary for a node type across workflows.\"\"\"\n",
        "+    node_name: str\n",
        "+    execution_count: int\n",
        "+    total_cost: float\n",
        "+    avg_cost_per_execution: float\n",
        "+    percent_of_total_cost: float\n",
        "+\n",
        "+@dataclass\n",
        "+class CostAnalysisResults:\n",
        "+    \"\"\"Complete cost analysis results.\"\"\"\n",
        "+    # Per-workflow metrics\n",
        "+    avg_cost_per_workflow: float\n",
        "+    median_cost_per_workflow: float\n",
        "+    min_cost: float\n",
        "+    max_cost: float\n",
        "+\n",
        "+    # Node-level breakdown\n",
        "+    node_summaries: List[NodeCostSummary]    # Sorted by cost descending\n",
        "+    top_cost_driver: Optional[str]\n",
        "+\n",
        "+    # Cache effectiveness\n",
        "+    cache_effectiveness_percent: Optional[float]\n",
        "+    cache_savings_dollars: Optional[float]\n",
        "+\n",
        "+    # Scaling projections\n",
        "+    scaling_projections: Dict[str, ScalingProjection]  # \"10x\", \"100x\", etc.\n",
        "+\n",
        "+    # Metadata\n",
        "+    total_workflows_analyzed: int\n",
        "+    data_quality_notes: List[str]\n",
        "+\n",
        "+    def to_csv(self) -> str:\n",
        "+        \"\"\"Export to CSV format for reporting.\"\"\"\n",
        "+```\n",
        "+\n",
        "+### Key Functions\n",
        "+\n",
        "+**Token Extraction**:\n",
        "+```python\n",
        "+def extract_token_usage(trace: Trace) -> Optional[TokenUsage]:\n",
        "+    \"\"\"\n",
        "+    Extract token usage from trace outputs/inputs.\n",
        "+\n",
        "+    Checks:\n",
        "+    1. trace.outputs[\"usage_metadata\"]\n",
        "+    2. trace.inputs[\"usage_metadata\"] (fallback)\n",
        "+    3. Extracts cache_read from input_token_details if available\n",
        "+    \"\"\"\n",
        "+\n",
        "+def extract_workflow_tokens(workflow: Workflow) -> Dict[str, TokenUsage]:\n",
        "+    \"\"\"Extract token usage for all traces in workflow.\"\"\"\n",
        "+```\n",
        "+\n",
        "+**Cost Calculation**:\n",
        "+```python\n",
        "+def calculate_trace_cost(\n",
        "+    token_usage: TokenUsage,\n",
        "+    pricing_config: PricingConfig\n",
        "+) -> CostBreakdown:\n",
        "+    \"\"\"Calculate cost for single trace using pricing model.\"\"\"\n",
        "+\n",
        "+def calculate_workflow_cost(\n",
        "+    workflow: Workflow,\n",
        "+    pricing_config: PricingConfig\n",
        "+) -> Optional[WorkflowCostAnalysis]:\n",
        "+    \"\"\"Calculate total cost and breakdown by node.\"\"\"\n",
        "+```\n",
        "+\n",
        "+**Aggregation & Analysis**:\n",
        "+```python\n",
        "+def aggregate_node_costs(\n",
        "+    workflow_analyses: List[WorkflowCostAnalysis]\n",
        "+) -> List[NodeCostSummary]:\n",
        "+    \"\"\"Aggregate costs by node type, sorted by total_cost descending.\"\"\"\n",
        "+\n",
        "+def calculate_cache_effectiveness(\n",
        "+    workflow_analyses: List[WorkflowCostAnalysis]\n",
        "+) -> Optional[Tuple[float, float, float]]:\n",
        "+    \"\"\"\n",
        "+    Calculate cache effectiveness if cache data available.\n",
        "+    Returns (effectiveness_percent, cost_without_cache, savings_dollars).\n",
        "+    \"\"\"\n",
        "+\n",
        "+def project_scaling_costs(\n",
        "+    avg_cost_per_workflow: float,\n",
        "+    current_workflow_count: int,\n",
        "+    scaling_factors: List[int]\n",
        "+) -> Dict[str, ScalingProjection]:\n",
        "+    \"\"\"Project costs at 10x, 100x, 1000x scale.\"\"\"\n",
        "+```\n",
        "+\n",
        "+**Main Entry Point**:\n",
        "+```python\n",
        "+def analyze_costs(\n",
        "+    workflows: List[Workflow],\n",
        "+    pricing_config: PricingConfig\n",
        "+) -> CostAnalysisResults:\n",
        "+    \"\"\"\n",
        "+    Perform complete cost analysis on workflows.\n",
        "+\n",
        "+    Args:\n",
        "+        workflows: List of Workflow objects to analyze\n",
        "+        pricing_config: PricingConfig with provider-specific rates\n",
        "+\n",
        "+    Main entry point for Phase 3B.\n",
        "+\n",
        "+    Example:\n",
        "+        # Create custom pricing config\n",
        "+        pricing = PricingConfig(\n",
        "+            model_name=\"Gemini 1.5 Pro\",\n",
        "+            input_tokens_per_1k=0.00125,\n",
        "+            output_tokens_per_1k=0.005,\n",
        "+            cache_read_per_1k=0.0003125\n",
        "+        )\n",
        "+\n",
        "+        results = analyze_costs(workflows, pricing)\n",
        "+    \"\"\"\n",
        "+```\n",
        "+\n",
        "+### Testing Strategy (Phase 3B)\n",
        "+\n",
        "+**Test File**: `test_analyze_cost.py` (~45 tests)\n",
        "+\n",
        "+**Test Classes**:\n",
        "+1. `TestTokenExtraction` - Extract tokens from various trace structures\n",
        "+2. `TestCostCalculation` - Basic and edge case cost calculations\n",
        "+3. `TestNodeCostAggregation` - Multi-workflow aggregation\n",
        "+4. `TestCacheEffectiveness` - Cache savings calculations\n",
        "+5. `TestScalingProjections` - Scaling math and viability thresholds\n",
        "+6. `TestCostAnalysisIntegration` - End-to-end with mock workflows\n",
        "+7. `TestCSVExport` - CSV formatting\n",
        "+\n",
        "+**Key Test Cases**:\n",
        "+- Token extraction from `outputs[\"usage_metadata\"]`\n",
        "+- Token extraction with cache_read tokens\n",
        "+- Missing token data returns None gracefully\n",
        "+- Zero-cost traces handled correctly\n",
        "+- Cost calculation accuracy (verify pricing math)\n",
        "+- Node aggregation sorted correctly\n",
        "+- Scaling projections calculate monthly costs\n",
        "+- CSV format matches expected structure\n",
        "+\n",
        "+**TDD Workflow**: Write test \u2192 See it fail \u2192 Implement minimal code \u2192 Pass \u2192 Refactor\n",
        "+\n",
        "+### CSV Exports\n",
        "+\n",
        "+```python\n",
        "+def export_node_costs_csv(node_summaries: List[NodeCostSummary]) -> str:\n",
        "+    \"\"\"Export node cost breakdown to CSV.\"\"\"\n",
        "+\n",
        "+def export_scaling_projections_csv(projections: Dict[str, ScalingProjection]) -> str:\n",
        "+    \"\"\"Export scaling projections to CSV.\"\"\"\n",
        "+```\n",
        "+\n",
        "+---\n",
        "+\n",
        "+## Phase 3C: Failure Pattern Analysis Implementation\n",
        "+\n",
        "+### Configuration Constants\n",
        "+\n",
        "+```python\n",
        "+# analyze_failures.py - Top of file\n",
        "+\n",
        "+# Status values indicating failure\n",
        "+FAILURE_STATUSES = {\"error\", \"failed\", \"cancelled\"}\n",
        "+SUCCESS_STATUSES = {\"success\"}\n",
        "+\n",
        "+# Retry detection heuristics\n",
        "+RETRY_DETECTION_CONFIG = {\n",
        "+    \"max_time_window_seconds\": 300,  # 5 min window for retry detection\n",
        "+    \"same_node_threshold\": 2,        # 2+ executions = potential retry\n",
        "+}\n",
        "+\n",
        "+# Validator node names (from Phase 3A)\n",
        "+VALIDATOR_NODES = {\n",
        "+    \"meta_evaluation_and_validation\",\n",
        "+    \"normative_validation\",\n",
        "+    \"simulated_testing\",\n",
        "+}\n",
        "+\n",
        "+# Error classification patterns (regex)\n",
        "+ERROR_PATTERNS = {\n",
        "+    \"validation_failure\": r\"validation.*fail|invalid.*spec\",\n",
        "+    \"api_timeout\": r\"timeout|timed out\",\n",
        "+    \"import_error\": r\"import.*fail|upload.*fail\",\n",
        "+    \"llm_error\": r\"model.*error|generation.*fail|token.*limit\",\n",
        "+    \"unknown\": r\".*\",  # Catch-all\n",
        "+}\n",
        "+```\n",
        "+\n",
        "+### Core Data Structures\n",
        "+\n",
        "+```python\n",
        "+@dataclass\n",
        "+class FailureInstance:\n",
        "+    \"\"\"Single failure occurrence.\"\"\"\n",
        "+    trace_id: str\n",
        "+    trace_name: str\n",
        "+    workflow_id: str\n",
        "+    error_message: Optional[str]\n",
        "+    error_type: str                  # Classified from ERROR_PATTERNS\n",
        "+    timestamp: Optional[datetime]\n",
        "+\n",
        "+@dataclass\n",
        "+class RetrySequence:\n",
        "+    \"\"\"Detected retry sequence.\"\"\"\n",
        "+    node_name: str\n",
        "+    workflow_id: str\n",
        "+    attempt_count: int\n",
        "+    attempts: List[Trace]            # Ordered by start_time\n",
        "+    final_status: str                # 'success' or 'failed'\n",
        "+    total_duration_seconds: float\n",
        "+    total_cost_estimate: Optional[float] = None\n",
        "+\n",
        "+@dataclass\n",
        "+class NodeFailureStats:\n",
        "+    \"\"\"Failure statistics for a node type.\"\"\"\n",
        "+    node_name: str\n",
        "+    total_executions: int\n",
        "+    failure_count: int\n",
        "+    success_count: int\n",
        "+    failure_rate_percent: float\n",
        "+    retry_sequences_detected: int\n",
        "+    avg_retries_when_failing: float\n",
        "+    common_error_types: Dict[str, int]  # error_type -> count\n",
        "+\n",
        "+@dataclass\n",
        "+class ValidatorEffectivenessAnalysis:\n",
        "+    \"\"\"Validator effectiveness assessment.\"\"\"\n",
        "+    validator_name: str\n",
        "+    total_executions: int\n",
        "+    caught_issues_count: int         # Failures detected\n",
        "+    pass_rate_percent: float\n",
        "+    is_necessary: bool               # Based on redundancy analysis\n",
        "+\n",
        "+@dataclass\n",
        "+class FailureAnalysisResults:\n",
        "+    \"\"\"Complete failure pattern analysis results.\"\"\"\n",
        "+    # Overall metrics\n",
        "+    total_workflows: int\n",
        "+    successful_workflows: int\n",
        "+    failed_workflows: int\n",
        "+    overall_success_rate_percent: float\n",
        "+\n",
        "+    # Node-level breakdown\n",
        "+    node_failure_stats: List[NodeFailureStats]  # Sorted by failure_rate\n",
        "+    highest_failure_node: Optional[str]\n",
        "+\n",
        "+    # Error distribution\n",
        "+    error_type_distribution: Dict[str, int]\n",
        "+    most_common_error_type: Optional[str]\n",
        "+\n",
        "+    # Retry analysis\n",
        "+    total_retry_sequences: int\n",
        "+    retry_sequences: List[RetrySequence]\n",
        "+    retry_success_rate_percent: Optional[float]\n",
        "+    avg_cost_of_retries: Optional[float]\n",
        "+\n",
        "+    # Validator analysis\n",
        "+    validator_analyses: List[ValidatorEffectivenessAnalysis]\n",
        "+    redundant_validators: List[str]\n",
        "+\n",
        "+    # Quality risks\n",
        "+    quality_risks_at_scale: List[str]\n",
        "+\n",
        "+    def to_csv(self) -> str:\n",
        "+        \"\"\"Export to CSV format for reporting.\"\"\"\n",
        "+```\n",
        "+\n",
        "+### Key Functions\n",
        "+\n",
        "+**Failure Detection**:\n",
        "+```python\n",
        "+def detect_failures(workflow: Workflow) -> List[FailureInstance]:\n",
        "+    \"\"\"Detect all failures in workflow using trace.status and trace.error.\"\"\"\n",
        "+\n",
        "+def classify_error(error_message: Optional[str]) -> str:\n",
        "+    \"\"\"Classify error into type using regex patterns.\"\"\"\n",
        "+```\n",
        "+\n",
        "+**Retry Detection**:\n",
        "+```python\n",
        "+def detect_retry_sequences(workflow: Workflow) -> List[RetrySequence]:\n",
        "+    \"\"\"\n",
        "+    Detect retry sequences using heuristics:\n",
        "+    - Multiple executions of same node within time window\n",
        "+    - Ordered by start_time\n",
        "+    \"\"\"\n",
        "+\n",
        "+def calculate_retry_success_rate(\n",
        "+    retry_sequences: List[RetrySequence]\n",
        "+) -> Optional[float]:\n",
        "+    \"\"\"Calculate % of retries that eventually succeed.\"\"\"\n",
        "+```\n",
        "+\n",
        "+**Node & Validator Analysis**:\n",
        "+```python\n",
        "+def analyze_node_failures(\n",
        "+    workflows: List[Workflow]\n",
        "+) -> List[NodeFailureStats]:\n",
        "+    \"\"\"Analyze failure patterns by node type.\"\"\"\n",
        "+\n",
        "+def analyze_validator_effectiveness(\n",
        "+    workflows: List[Workflow]\n",
        "+) -> List[ValidatorEffectivenessAnalysis]:\n",
        "+    \"\"\"Assess whether all 3 validators are necessary.\"\"\"\n",
        "+\n",
        "+def detect_validator_redundancy(\n",
        "+    validator_analyses: List[ValidatorEffectivenessAnalysis]\n",
        "+) -> List[str]:\n",
        "+    \"\"\"Identify validators with >90% overlap in caught issues.\"\"\"\n",
        "+```\n",
        "+\n",
        "+**Root Cause & Risk**:\n",
        "+```python\n",
        "+def identify_root_causes(\n",
        "+    failure_instances: List[FailureInstance],\n",
        "+    node_stats: List[NodeFailureStats]\n",
        "+) -> Dict[str, List[str]]:\n",
        "+    \"\"\"\n",
        "+    Categorize root causes:\n",
        "+    - Prompt issues (validation failures)\n",
        "+    - Logic bugs (repeated errors)\n",
        "+    - External failures (API timeouts, import errors)\n",
        "+    \"\"\"\n",
        "+\n",
        "+def assess_quality_risks_at_scale(\n",
        "+    results: FailureAnalysisResults,\n",
        "+    current_volume: int,\n",
        "+    projected_volume: int\n",
        "+) -> List[str]:\n",
        "+    \"\"\"Generate risk warnings for scaling.\"\"\"\n",
        "+```\n",
        "+\n",
        "+**Main Entry Point**:\n",
        "+```python\n",
        "+def analyze_failures(workflows: List[Workflow]) -> FailureAnalysisResults:\n",
        "+    \"\"\"\n",
        "+    Perform complete failure pattern analysis.\n",
        "+    Main entry point for Phase 3C.\n",
        "+    \"\"\"\n",
        "+```\n",
        "+\n",
        "+### Testing Strategy (Phase 3C)\n",
        "+\n",
        "+**Test File**: `test_analyze_failures.py` (~56 tests)\n",
        "+\n",
        "+**Test Classes**:\n",
        "+1. `TestFailureDetection` - Detect failures from status/error fields\n",
        "+2. `TestRetryDetection` - Heuristic-based retry sequence detection\n",
        "+3. `TestNodeFailureAnalysis` - Node-level statistics\n",
        "+4. `TestValidatorEffectiveness` - Validator redundancy analysis\n",
        "+5. `TestRootCauseAnalysis` - Error categorization\n",
        "+6. `TestQualityRiskAssessment` - Risk identification\n",
        "+7. `TestFailureAnalysisIntegration` - End-to-end tests\n",
        "+8. `TestCSVExport` - CSV formatting\n",
        "+\n",
        "+**Key Test Cases**:\n",
        "+- Detect single and multiple failures in workflow\n",
        "+- Classify errors using regex patterns\n",
        "+- Detect retry sequences with 2, 3, 5+ attempts\n",
        "+- Respect time window for retry detection\n",
        "+- Calculate retry success rates correctly\n",
        "+- Identify redundant validators (overlap detection)\n",
        "+- Generate appropriate risk warnings\n",
        "+- Handle workflows with 0% and 100% success rates\n",
        "+\n",
        "+### CSV Exports\n",
        "+\n",
        "+```python\n",
        "+def export_node_failures_csv(node_stats: List[NodeFailureStats]) -> str:\n",
        "+    \"\"\"Export node failure statistics to CSV.\"\"\"\n",
        "+\n",
        "+def export_validator_effectiveness_csv(\n",
        "+    analyses: List[ValidatorEffectivenessAnalysis]\n",
        "+) -> str:\n",
        "+    \"\"\"Export validator effectiveness to CSV.\"\"\"\n",
        "+\n",
        "+def export_retry_analysis_csv(retry_sequences: List[RetrySequence]) -> str:\n",
        "+    \"\"\"Export retry sequence analysis to CSV.\"\"\"\n",
        "+```\n",
        "+\n",
        "+---\n",
        "+\n",
        "+## Integration with Verification Tool\n",
        "+\n",
        "+### Extend verify_analysis_report.py\n",
        "+\n",
        "+Add two new verification functions:\n",
        "+\n",
        "+```python\n",
        "+def verify_cost_analysis(\n",
        "+    dataset: TraceDataset,\n",
        "+    expected: Optional[Dict[str, Any]] = None\n",
        "+) -> CostAnalysisResults:\n",
        "+    \"\"\"\n",
        "+    Verify Phase 3B cost calculations.\n",
        "+\n",
        "+    Displays:\n",
        "+    - Cost per workflow (avg, median, range)\n",
        "+    - Top 3 cost drivers\n",
        "+    - Scaling projections (10x, 100x, 1000x)\n",
        "+    - Cache effectiveness if available\n",
        "+    \"\"\"\n",
        "+\n",
        "+def verify_failure_analysis(\n",
        "+    dataset: TraceDataset,\n",
        "+    expected: Optional[Dict[str, Any]] = None\n",
        "+) -> FailureAnalysisResults:\n",
        "+    \"\"\"\n",
        "+    Verify Phase 3C failure calculations.\n",
        "+\n",
        "+    Displays:\n",
        "+    - Overall success rate\n",
        "+    - Top 5 nodes by failure rate\n",
        "+    - Retry analysis (sequences detected, success rate)\n",
        "+    - Validator effectiveness\n",
        "+    \"\"\"\n",
        "+```\n",
        "+\n",
        "+Update `main()` to accept `--phases` argument:\n",
        "+```python\n",
        "+parser.add_argument(\"--phases\", type=str, default=\"all\",\n",
        "+                   help=\"Phases to verify: 3a, 3b, 3c, or all\")\n",
        "+```\n",
        "+\n",
        "+---\n",
        "+\n",
        "+## Implementation Sequence (TDD Workflow)\n",
        "+\n",
        "+### Phase 3B: Cost Analysis (8-12 hours)\n",
        "+\n",
        "+**Step 1: Setup** (30 min)\n",
        "+- Create `analyze_cost.py` with pricing constants and imports\n",
        "+- Create `test_analyze_cost.py` with test structure\n",
        "+- Add initial docstrings\n",
        "+\n",
        "+**Step 2: Token Extraction** (2-3 hours)\n",
        "+1. Write test: `test_extract_token_usage_from_outputs()`\n",
        "+2. Implement `extract_token_usage()` to pass\n",
        "+3. Write test: `test_extract_token_usage_with_cache_data()`\n",
        "+4. Extend implementation for cache tokens\n",
        "+5. Write test: `test_extract_token_usage_missing_data()`\n",
        "+6. Handle None return gracefully\n",
        "+7. **CRITICAL**: Test with REAL trace data first to confirm field locations\n",
        "+\n",
        "+**Step 3: Cost Calculation** (2-3 hours)\n",
        "+1. Write test: `test_calculate_trace_cost_basic()`\n",
        "+2. Implement basic calculation (input + output tokens)\n",
        "+3. Write test: `test_calculate_trace_cost_with_cache()`\n",
        "+4. Add cache cost logic\n",
        "+5. Write test: `test_calculate_workflow_cost()`\n",
        "+6. Implement workflow-level aggregation\n",
        "+\n",
        "+**Step 4: Aggregation & Analysis** (2-3 hours)\n",
        "+1. Write tests for `aggregate_node_costs()`\n",
        "+2. Implement node-level aggregation with sorting\n",
        "+3. Write tests for `calculate_cache_effectiveness()`\n",
        "+4. Implement cache savings calculation\n",
        "+5. Write tests for `project_scaling_costs()`\n",
        "+6. Implement scaling projections\n",
        "+\n",
        "+**Step 5: Main Analysis & Export** (1-2 hours)\n",
        "+1. Write integration test for `analyze_costs()`\n",
        "+2. Implement main orchestration function\n",
        "+3. Add data quality checks and warnings\n",
        "+4. Implement CSV export functions\n",
        "+5. Test CSV format\n",
        "+\n",
        "+**Step 6: Verification Integration** (1 hour)\n",
        "+1. Add `verify_cost_analysis()` to verify_analysis_report.py\n",
        "+2. Test verification with sample data\n",
        "+3. Update main() with --phases argument\n",
        "+\n",
        "+### Phase 3C: Failure Analysis (10-14 hours)\n",
        "+\n",
        "+**Step 1: Setup** (30 min)\n",
        "+- Create `analyze_failures.py` with error patterns and constants\n",
        "+- Create `test_analyze_failures.py` with test structure\n",
        "+\n",
        "+**Step 2: Failure Detection** (2-3 hours)\n",
        "+1. Write tests for `detect_failures()`\n",
        "+2. Implement status-based detection\n",
        "+3. Write tests for `classify_error()`\n",
        "+4. Implement regex-based classification\n",
        "+\n",
        "+**Step 3: Retry Detection** (3-4 hours)\n",
        "+1. Write tests for simple retry detection (2 attempts)\n",
        "+2. Implement basic retry detection\n",
        "+3. Write tests for complex scenarios (5+ attempts, time windows)\n",
        "+4. Refine detection heuristics\n",
        "+5. Write tests for `calculate_retry_success_rate()`\n",
        "+6. Implement success rate calculation\n",
        "+\n",
        "+**Step 4: Node Failure Analysis** (2-3 hours)\n",
        "+1. Write tests for `analyze_node_failures()`\n",
        "+2. Implement aggregation and statistics\n",
        "+3. Test sorting by failure_rate\n",
        "+4. Test error type aggregation\n",
        "+\n",
        "+**Step 5: Validator Analysis** (2-3 hours)\n",
        "+1. Write tests for `analyze_validator_effectiveness()`\n",
        "+2. Implement effectiveness metrics\n",
        "+3. Write tests for `detect_validator_redundancy()`\n",
        "+4. Implement overlap detection logic\n",
        "+\n",
        "+**Step 6: Root Cause & Integration** (2-3 hours)\n",
        "+1. Write tests for `identify_root_causes()`\n",
        "+2. Implement categorization logic\n",
        "+3. Write tests for `assess_quality_risks_at_scale()`\n",
        "+4. Implement risk warning generation\n",
        "+5. Write integration test for `analyze_failures()`\n",
        "+6. Implement main function and CSV exports\n",
        "+\n",
        "+**Step 7: Verification Integration** (1 hour)\n",
        "+1. Add `verify_failure_analysis()` to verify_analysis_report.py\n",
        "+2. Test verification with sample data\n",
        "+\n",
        "+---\n",
        "+\n",
        "+## Data Quality Handling\n",
        "+\n",
        "+### Edge Cases to Handle\n",
        "+\n",
        "+**Phase 3B**:\n",
        "+- Missing token data \u2192 Skip trace, add to data_quality_notes\n",
        "+- Zero-cost traces \u2192 Include in analysis (valid for non-LLM nodes)\n",
        "+- Missing cache data \u2192 Skip cache analysis, report \"N/A\"\n",
        "+- Incomplete workflows \u2192 Partial cost calculated, note in quality report\n",
        "+\n",
        "+**Phase 3C**:\n",
        "+- No failures \u2192 Report 100% success rate (valid result)\n",
        "+- Missing error messages \u2192 Classify as \"unknown\"\n",
        "+- Ambiguous retries \u2192 Use time window + node name heuristic\n",
        "+- Missing validators \u2192 Note in analysis, skip redundancy check\n",
        "+\n",
        "+### Data Quality Reporting\n",
        "+\n",
        "+Both modules include `data_quality_notes` field in results:\n",
        "+\n",
        "+```python\n",
        "+data_quality_notes: List[str] = [\n",
        "+    \"Token data available for 95/100 workflows\",\n",
        "+    \"Cache data not available - cache analysis skipped\",\n",
        "+    \"Retry detection based on heuristics (no definitive markers)\",\n",
        "+]\n",
        "+```\n",
        "+\n",
        "+---\n",
        "+\n",
        "+## Critical Pre-Implementation Steps\n",
        "+\n",
        "+### BEFORE starting implementation, MUST complete:\n",
        "+\n",
        "+1. **Verify Token Data Structure** (30 min)\n",
        "+   ```python\n",
        "+   # Inspect actual trace structure\n",
        "+   import json\n",
        "+   with open(\"trace_export_1000.json\") as f:\n",
        "+       data = json.load(f)\n",
        "+\n",
        "+   # Find LLM trace and print structure\n",
        "+   trace = [t for t in data[\"traces\"] if t[\"name\"] == \"ChatGoogleGenerativeAI\"][0]\n",
        "+   print(json.dumps(trace[\"outputs\"], indent=2))\n",
        "+   ```\n",
        "+\n",
        "+2. **Create Client-Specific Pricing Script** (15 min)\n",
        "+   - Research current Gemini 1.5 Pro pricing (Dec 2025)\n",
        "+   - Create `scripts/analyze_with_gemini_pricing.py` in client repo\n",
        "+   - Script instantiates PricingConfig with Gemini rates\n",
        "+   - Calls analyze_costs() with custom pricing\n",
        "+\n",
        "+3. **Document Field Locations** (15 min)\n",
        "+   - Record exact path to usage_metadata\n",
        "+   - Record exact path to cache_read tokens\n",
        "+   - Create example trace structure in docstrings\n",
        "+\n",
        "+---\n",
        "+\n",
        "+## Success Criteria\n",
        "+\n",
        "+### Phase 3B Complete When:\n",
        "+- \u2705 All 45+ tests passing\n",
        "+- \u2705 Can answer: \"What does a workflow cost?\" \u2192 $__\n",
        "+- \u2705 Can answer: \"What's the biggest cost driver?\" \u2192 [node name]\n",
        "+- \u2705 Can answer: \"Can it scale to 100x volume?\" \u2192 YES/NO with projection\n",
        "+- \u2705 Cache effectiveness calculated (or \"N/A\" if unavailable)\n",
        "+- \u2705 CSV exports generated for all metrics\n",
        "+- \u2705 Verification tool confirms calculations\n",
        "+\n",
        "+### Phase 3C Complete When:\n",
        "+- \u2705 All 56+ tests passing\n",
        "+- \u2705 Can answer: \"What's the success rate?\" \u2192 ___%\n",
        "+- \u2705 Can answer: \"Where do failures happen most?\" \u2192 [node name + rate]\n",
        "+- \u2705 Can answer: \"Are retries effective?\" \u2192 ___% success rate\n",
        "+- \u2705 Can answer: \"Are all 3 validators necessary?\" \u2192 YES/NO with data\n",
        "+- \u2705 CSV exports generated for failures, retries, validators\n",
        "+- \u2705 Quality risks identified for scaling\n",
        "+- \u2705 Verification tool confirms calculations\n",
        "+\n",
        "+---\n",
        "+\n",
        "+## Deliverables\n",
        "+\n",
        "+### Code Deliverables\n",
        "+1. `analyze_cost.py` (~700 lines) with full documentation\n",
        "+2. `test_analyze_cost.py` (~900 lines, 45+ tests passing)\n",
        "+3. `analyze_failures.py` (~800 lines) with full documentation\n",
        "+4. `test_analyze_failures.py` (~1000 lines, 56+ tests passing)\n",
        "+5. Extended `verify_analysis_report.py` with 3B/3C verification\n",
        "+6. Updated README.md with Phase 3B/3C usage examples\n",
        "+\n",
        "+### Analysis Outputs (for client Assessment)\n",
        "+1. Cost analysis CSV exports:\n",
        "+   - Node cost breakdown\n",
        "+   - Scaling projections\n",
        "+   - Cache effectiveness (if available)\n",
        "+2. Failure analysis CSV exports:\n",
        "+   - Node failure statistics\n",
        "+   - Retry analysis\n",
        "+   - Validator effectiveness\n",
        "+3. Verification reports confirming all calculations\n",
        "+\n",
        "+---\n",
        "+\n",
        "+## Repository Separation Strategy\n",
        "+\n",
        "+**Generalized Tools** (export-langsmith-data repo):\n",
        "+- `analyze_cost.py` - Generic cost analysis for any LangSmith traces\n",
        "+- `analyze_failures.py` - Generic failure analysis for any traces\n",
        "+- All test files - Reusable test patterns\n",
        "+- `verify_analysis_report.py` - Verification framework\n",
        "+\n",
        "+**Client-Specific Analysis** (client-project/Assessment/data):\n",
        "+- Actual trace data files (.json)\n",
        "+- Generated CSV reports\n",
        "+- Client-specific interpretation and findings\n",
        "+- Phase 3B/3C markdown reports referencing data\n",
        "+- **Custom pricing script** (`scripts/analyze_with_gemini_pricing.py`):\n",
        "+  - Imports `PricingConfig` from generalized tool\n",
        "+  - Creates config with Gemini-specific rates\n",
        "+  - Runs analysis with client's trace data\n",
        "+\n",
        "+This separation allows:\n",
        "+- Reuse of analysis tools across projects\n",
        "+- Client confidentiality (data stays in Assessment repo)\n",
        "+- Test-first development in open repo\n",
        "+- Easy updates to pricing models and error patterns\n",
        "+\n",
        "+---\n",
        "+\n",
        "+## Risk Mitigation\n",
        "+\n",
        "+| Risk | Mitigation |\n",
        "+|------|------------|\n",
        "+| Token data not in expected location | Inspect real data FIRST, implement flexible extraction |\n",
        "+| Cache data unavailable | Make cache analysis optional, graceful degradation |\n",
        "+| Retry detection false positives | Tune time window (5 min), manual spot-checking |\n",
        "+| Pricing model outdated | Externalize constants, document update procedure |\n",
        "+| Large datasets cause memory issues | Process in batches if needed (unlikely with n=10-1000) |\n",
        "+| Test coverage gaps | Strict TDD, aim for >90% coverage |\n",
        "+\n",
        "+---\n",
        "+\n",
        "+## Critical Files\n",
        "+\n",
        "+**To Read** (for reference patterns):\n",
        "+1. `analyze_traces.py`\n",
        "+2. `test_analyze_traces.py`\n",
        "+3. `verify_analysis_report.py`\n",
        "+\n",
        "+**To Create**:\n",
        "+1. `analyze_cost.py`\n",
        "+2. `test_analyze_cost.py`\n",
        "+3. `analyze_failures.py`\n",
        "+4. `test_analyze_failures.py`\n",
        "+\n",
        "+**To Modify**:\n",
        "+1. `verify_analysis_report.py`\n",
        "+\n",
        "+**Data Files** (for inspection):\n",
        "+1. `trace_export_1000.json` (newly exported)\n",
        "+2. `../client-project/Assessment/data/trace_export_1000.json`\n",
        "+3. `../client-project/Assessment/data/trace_export_complete_workflows.json`\n",
        "+\n",
        "+---\n",
        "+\n",
        "+## Next Steps After Approval\n",
        "+\n",
        "+1. **Data Investigation** (30 min) - Inspect token field locations in real traces\n",
        "+2. **Pricing Research** (15 min) - Confirm Gemini 1.5 Pro current pricing\n",
        "+3. **Setup** (30 min) - Create files, initial structure, constants\n",
        "+4. **Phase 3B Implementation** (8-12 hours) - TDD workflow\n",
        "+5. **Phase 3C Implementation** (10-14 hours) - TDD workflow\n",
        "+6. **Verification & Documentation** (2-3 hours) - Final testing, README updates\n",
        "+7. **Generate Reports** (1-2 hours) - Run on actual data, export CSVs for Assessment\n",
        "+\n",
        "+**Total Estimated Time**: 18-26 hours for complete implementation\n"
      ]
    },
    {
      "path": "test_analyze_cost.py",
      "status": "added",
      "additions": 1046,
      "deletions": 0,
      "patch": "@@ -0,0 +1,1046 @@\n+\"\"\"\n+Test suite for Phase 3B: Cost Analysis\n+\n+Following TDD methodology - tests written FIRST, then implementation.\n+Tests for cost analysis functionality including token extraction,\n+cost calculation, and scaling projections.\n+\n+Author: Generated with Claude Code (PDCA Framework)\n+Date: 2025-12-09\n+\"\"\"\n+\n+import pytest\n+from datetime import datetime, timezone\n+from analyze_cost import (\n+    PricingConfig,\n+    TokenUsage,\n+    extract_token_usage,\n+    calculate_trace_cost,\n+)\n+from analyze_traces import Trace\n+\n+\n+class TestPricingConfig:\n+    \"\"\"Test PricingConfig dataclass validation.\"\"\"\n+\n+    def test_pricing_config_creation_valid(self):\n+        \"\"\"Test creating valid pricing config.\"\"\"\n+        pricing = PricingConfig(\n+            model_name=\"Test Model\",\n+            input_tokens_per_1k=0.001,\n+            output_tokens_per_1k=0.002,\n+            cache_read_per_1k=0.0001,\n+        )\n+        assert pricing.model_name == \"Test Model\"\n+        assert pricing.input_tokens_per_1k == 0.001\n+        assert pricing.output_tokens_per_1k == 0.002\n+        assert pricing.cache_read_per_1k == 0.0001\n+\n+    def test_pricing_config_without_cache(self):\n+        \"\"\"Test pricing config without cache pricing.\"\"\"\n+        pricing = PricingConfig(\n+            model_name=\"No Cache Model\",\n+            input_tokens_per_1k=0.001,\n+            output_tokens_per_1k=0.002,\n+        )\n+        assert pricing.cache_read_per_1k is None\n+\n+    def test_pricing_config_negative_input_price_raises(self):\n+        \"\"\"Test that negative input price raises ValueError.\"\"\"\n+        with pytest.raises(ValueError, match=\"non-negative\"):\n+            PricingConfig(\n+                model_name=\"Bad Model\",\n+                input_tokens_per_1k=-0.001,\n+                output_tokens_per_1k=0.002,\n+            )\n+\n+    def test_pricing_config_negative_output_price_raises(self):\n+        \"\"\"Test that negative output price raises ValueError.\"\"\"\n+        with pytest.raises(ValueError, match=\"non-negative\"):\n+            PricingConfig(\n+                model_name=\"Bad Model\",\n+                input_tokens_per_1k=0.001,\n+                output_tokens_per_1k=-0.002,\n+            )\n+\n+    def test_pricing_config_negative_cache_price_raises(self):\n+        \"\"\"Test that negative cache price raises ValueError.\"\"\"\n+        with pytest.raises(ValueError, match=\"non-negative\"):\n+            PricingConfig(\n+                model_name=\"Bad Model\",\n+                input_tokens_per_1k=0.001,\n+                output_tokens_per_1k=0.002,\n+                cache_read_per_1k=-0.0001,\n+            )\n+\n+\n+class TestTokenExtraction:\n+    \"\"\"Test token usage extraction from traces.\"\"\"\n+\n+    def test_extract_token_usage_from_outputs(self):\n+        \"\"\"Test extracting tokens from trace.outputs['usage_metadata'].\"\"\"\n+        trace = Trace(\n+            id=\"test-1\",\n+            name=\"ChatGoogleGenerativeAI\",\n+            start_time=datetime(2025, 1, 1, 12, 0, 0, tzinfo=timezone.utc),\n+            end_time=datetime(2025, 1, 1, 12, 1, 0, tzinfo=timezone.utc),\n+            duration_seconds=60.0,\n+            status=\"success\",\n+            run_type=\"llm\",\n+            parent_id=None,\n+            child_ids=[],\n+            inputs={},\n+            outputs={\n+                \"usage_metadata\": {\n+                    \"input_tokens\": 1000,\n+                    \"output_tokens\": 500,\n+                    \"total_tokens\": 1500,\n+                }\n+            },\n+            error=None,\n+        )\n+\n+        result = extract_token_usage(trace)\n+\n+        assert result is not None\n+        assert result.input_tokens == 1000\n+        assert result.output_tokens == 500\n+        assert result.total_tokens == 1500\n+        assert result.cached_tokens is None\n+\n+    def test_extract_token_usage_with_cache_data(self):\n+        \"\"\"Test extracting tokens including cache_read data.\"\"\"\n+        trace = Trace(\n+            id=\"test-2\",\n+            name=\"ChatGoogleGenerativeAI\",\n+            start_time=datetime(2025, 1, 1, 12, 0, 0, tzinfo=timezone.utc),\n+            end_time=datetime(2025, 1, 1, 12, 1, 0, tzinfo=timezone.utc),\n+            duration_seconds=60.0,\n+            status=\"success\",\n+            run_type=\"llm\",\n+            parent_id=None,\n+            child_ids=[],\n+            inputs={},\n+            outputs={\n+                \"usage_metadata\": {\n+                    \"input_tokens\": 1000,\n+                    \"output_tokens\": 500,\n+                    \"total_tokens\": 1500,\n+                    \"input_token_details\": {\n+                        \"cache_read\": 800,\n+                    },\n+                }\n+            },\n+            error=None,\n+        )\n+\n+        result = extract_token_usage(trace)\n+\n+        assert result is not None\n+        assert result.input_tokens == 1000\n+        assert result.output_tokens == 500\n+        assert result.total_tokens == 1500\n+        assert result.cached_tokens == 800\n+\n+    def test_extract_token_usage_missing_data_returns_none(self):\n+        \"\"\"Test that missing token data returns None.\"\"\"\n+        trace = Trace(\n+            id=\"test-3\",\n+            name=\"non_llm_node\",\n+            start_time=datetime(2025, 1, 1, 12, 0, 0, tzinfo=timezone.utc),\n+            end_time=datetime(2025, 1, 1, 12, 1, 0, tzinfo=timezone.utc),\n+            duration_seconds=60.0,\n+            status=\"success\",\n+            run_type=\"chain\",\n+            parent_id=None,\n+            child_ids=[],\n+            inputs={},\n+            outputs={},\n+            error=None,\n+        )\n+\n+        result = extract_token_usage(trace)\n+\n+        assert result is None\n+\n+    def test_extract_token_usage_from_inputs_fallback(self):\n+        \"\"\"Test fallback to extracting from inputs if not in outputs.\"\"\"\n+        trace = Trace(\n+            id=\"test-4\",\n+            name=\"ChatGoogleGenerativeAI\",\n+            start_time=datetime(2025, 1, 1, 12, 0, 0, tzinfo=timezone.utc),\n+            end_time=datetime(2025, 1, 1, 12, 1, 0, tzinfo=timezone.utc),\n+            duration_seconds=60.0,\n+            status=\"success\",\n+            run_type=\"llm\",\n+            parent_id=None,\n+            child_ids=[],\n+            inputs={\n+                \"usage_metadata\": {\n+                    \"input_tokens\": 2000,\n+                    \"output_tokens\": 1000,\n+                    \"total_tokens\": 3000,\n+                }\n+            },\n+            outputs={},\n+            error=None,\n+        )\n+\n+        result = extract_token_usage(trace)\n+\n+        assert result is not None\n+        assert result.input_tokens == 2000\n+        assert result.output_tokens == 1000\n+\n+\n+class TestCostCalculation:\n+    \"\"\"Test cost calculation functions.\"\"\"\n+\n+    def test_calculate_trace_cost_basic(self):\n+        \"\"\"Test basic cost calculation without cache.\"\"\"\n+        token_usage = TokenUsage(\n+            input_tokens=1000,\n+            output_tokens=500,\n+            total_tokens=1500,\n+            cached_tokens=None,\n+        )\n+        pricing = PricingConfig(\n+            model_name=\"Test Model\",\n+            input_tokens_per_1k=0.00125,  # $1.25 per 1M\n+            output_tokens_per_1k=0.005,  # $5.00 per 1M\n+        )\n+\n+        result = calculate_trace_cost(token_usage, pricing)\n+\n+        # Expected: (1000 * 0.00125 / 1000) + (500 * 0.005 / 1000)\n+        # = 0.00125 + 0.0025 = 0.00375\n+        assert result.trace_id is not None\n+        assert result.input_cost == pytest.approx(0.00125, abs=0.00001)\n+        assert result.output_cost == pytest.approx(0.0025, abs=0.00001)\n+        assert result.cache_cost == 0.0\n+        assert result.total_cost == pytest.approx(0.00375, abs=0.00001)\n+\n+    def test_calculate_trace_cost_with_cache(self):\n+        \"\"\"Test cost calculation with cache reads.\"\"\"\n+        token_usage = TokenUsage(\n+            input_tokens=1000,\n+            output_tokens=500,\n+            total_tokens=1500,\n+            cached_tokens=800,\n+        )\n+        pricing = PricingConfig(\n+            model_name=\"Test Model\",\n+            input_tokens_per_1k=0.00125,\n+            output_tokens_per_1k=0.005,\n+            cache_read_per_1k=0.0003125,  # $0.3125 per 1M\n+        )\n+\n+        result = calculate_trace_cost(token_usage, pricing)\n+\n+        # Expected cache cost: 800 * 0.0003125 / 1000 = 0.00025\n+        assert result.cache_cost == pytest.approx(0.00025, abs=0.00001)\n+        # Total: 0.00125 + 0.0025 + 0.00025 = 0.00400\n+        assert result.total_cost == pytest.approx(0.00400, abs=0.00001)\n+\n+    def test_calculate_trace_cost_zero_tokens(self):\n+        \"\"\"Test handling of zero token usage.\"\"\"\n+        token_usage = TokenUsage(\n+            input_tokens=0,\n+            output_tokens=0,\n+            total_tokens=0,\n+            cached_tokens=None,\n+        )\n+        pricing = PricingConfig(\n+            model_name=\"Test Model\",\n+            input_tokens_per_1k=0.00125,\n+            output_tokens_per_1k=0.005,\n+        )\n+\n+        result = calculate_trace_cost(token_usage, pricing)\n+\n+        assert result.input_cost == 0.0\n+        assert result.output_cost == 0.0\n+        assert result.total_cost == 0.0\n+\n+\n+class TestWorkflowCostAnalysis:\n+    \"\"\"Test workflow-level cost analysis.\"\"\"\n+\n+    def test_calculate_workflow_cost_single_trace(self):\n+        \"\"\"Test calculating cost for workflow with one LLM trace.\"\"\"\n+        from analyze_traces import Workflow\n+\n+        # Create root trace (no cost)\n+        root = Trace(\n+            id=\"root-1\",\n+            name=\"LangGraph\",\n+            start_time=datetime(2025, 1, 1, 12, 0, 0, tzinfo=timezone.utc),\n+            end_time=datetime(2025, 1, 1, 12, 5, 0, tzinfo=timezone.utc),\n+            duration_seconds=300.0,\n+            status=\"success\",\n+            run_type=\"chain\",\n+            parent_id=None,\n+            child_ids=[\"child-1\"],\n+            inputs={},\n+            outputs={},\n+            error=None,\n+        )\n+\n+        # Create child LLM trace with cost\n+        child = Trace(\n+            id=\"child-1\",\n+            name=\"ChatGoogleGenerativeAI\",\n+            start_time=datetime(2025, 1, 1, 12, 1, 0, tzinfo=timezone.utc),\n+            end_time=datetime(2025, 1, 1, 12, 2, 0, tzinfo=timezone.utc),\n+            duration_seconds=60.0,\n+            status=\"success\",\n+            run_type=\"llm\",\n+            parent_id=\"root-1\",\n+            child_ids=[],\n+            inputs={},\n+            outputs={\n+                \"usage_metadata\": {\n+                    \"input_tokens\": 1000,\n+                    \"output_tokens\": 500,\n+                    \"total_tokens\": 1500,\n+                }\n+            },\n+            error=None,\n+        )\n+\n+        workflow = Workflow(\n+            root_trace=root,\n+            nodes={\"ChatGoogleGenerativeAI\": [child]},\n+            all_traces=[root, child],\n+        )\n+\n+        pricing = PricingConfig(\n+            model_name=\"Test Model\",\n+            input_tokens_per_1k=0.00125,\n+            output_tokens_per_1k=0.005,\n+        )\n+\n+        from analyze_cost import calculate_workflow_cost\n+\n+        result = calculate_workflow_cost(workflow, pricing)\n+\n+        assert result is not None\n+        assert result.workflow_id == \"root-1\"\n+        assert len(result.node_costs) == 1\n+        assert result.total_cost == pytest.approx(0.00375, abs=0.00001)\n+\n+    def test_calculate_workflow_cost_no_token_data(self):\n+        \"\"\"Test workflow with no token data returns None.\"\"\"\n+        from analyze_traces import Workflow\n+\n+        root = Trace(\n+            id=\"root-2\",\n+            name=\"LangGraph\",\n+            start_time=datetime(2025, 1, 1, 12, 0, 0, tzinfo=timezone.utc),\n+            end_time=datetime(2025, 1, 1, 12, 5, 0, tzinfo=timezone.utc),\n+            duration_seconds=300.0,\n+            status=\"success\",\n+            run_type=\"chain\",\n+            parent_id=None,\n+            child_ids=[],\n+            inputs={},\n+            outputs={},\n+            error=None,\n+        )\n+\n+        workflow = Workflow(\n+            root_trace=root,\n+            nodes={},\n+            all_traces=[root],\n+        )\n+\n+        pricing = PricingConfig(\n+            model_name=\"Test Model\",\n+            input_tokens_per_1k=0.00125,\n+            output_tokens_per_1k=0.005,\n+        )\n+\n+        from analyze_cost import calculate_workflow_cost\n+\n+        result = calculate_workflow_cost(workflow, pricing)\n+\n+        # Should return result with zero cost, not None\n+        assert result is not None\n+        assert result.total_cost == 0.0\n+        assert len(result.node_costs) == 0\n+\n+\n+class TestScalingProjections:\n+    \"\"\"Test scaling cost projections.\"\"\"\n+\n+    def test_project_scaling_costs_basic(self):\n+        \"\"\"Test basic scaling projections at 10x, 100x, 1000x.\"\"\"\n+        from analyze_cost import project_scaling_costs, SCALING_FACTORS\n+\n+        avg_cost_per_workflow = 0.01  # $0.01 per workflow\n+        current_workflow_count = 100\n+\n+        result = project_scaling_costs(\n+            avg_cost_per_workflow=avg_cost_per_workflow,\n+            current_workflow_count=current_workflow_count,\n+            scaling_factors=SCALING_FACTORS,\n+        )\n+\n+        assert result is not None\n+        assert \"1x\" in result\n+        assert \"10x\" in result\n+        assert \"100x\" in result\n+        assert \"1000x\" in result\n+\n+        # Verify 1x (current)\n+        assert result[\"1x\"].scale_factor == 1\n+        assert result[\"1x\"].workflow_count == 100\n+        assert result[\"1x\"].total_cost == pytest.approx(1.0, abs=0.01)  # 100 * $0.01\n+\n+        # Verify 10x\n+        assert result[\"10x\"].scale_factor == 10\n+        assert result[\"10x\"].workflow_count == 1000\n+        assert result[\"10x\"].total_cost == pytest.approx(10.0, abs=0.01)\n+\n+        # Verify 100x\n+        assert result[\"100x\"].scale_factor == 100\n+        assert result[\"100x\"].workflow_count == 10000\n+        assert result[\"100x\"].total_cost == pytest.approx(100.0, abs=0.01)\n+\n+        # Verify 1000x\n+        assert result[\"1000x\"].scale_factor == 1000\n+        assert result[\"1000x\"].workflow_count == 100000\n+        assert result[\"1000x\"].total_cost == pytest.approx(1000.0, abs=0.01)\n+\n+    def test_project_scaling_costs_with_monthly(self):\n+        \"\"\"Test that monthly costs are calculated correctly.\"\"\"\n+        from analyze_cost import project_scaling_costs\n+\n+        avg_cost_per_workflow = 0.05\n+        current_workflow_count = 50\n+        monthly_estimate = 500  # Assume 500 workflows per month\n+\n+        result = project_scaling_costs(\n+            avg_cost_per_workflow=avg_cost_per_workflow,\n+            current_workflow_count=current_workflow_count,\n+            scaling_factors=[1, 10],\n+            monthly_workflow_estimate=monthly_estimate,\n+        )\n+\n+        # At 1x: 500 workflows/month * $0.05 = $25/month\n+        assert result[\"1x\"].cost_per_month_30days is not None\n+        assert result[\"1x\"].cost_per_month_30days == pytest.approx(25.0, abs=0.01)\n+\n+        # At 10x: 5000 workflows/month * $0.05 = $250/month\n+        assert result[\"10x\"].cost_per_month_30days is not None\n+        assert result[\"10x\"].cost_per_month_30days == pytest.approx(250.0, abs=0.01)\n+\n+    def test_project_scaling_costs_zero_cost(self):\n+        \"\"\"Test handling of zero cost per workflow.\"\"\"\n+        from analyze_cost import project_scaling_costs\n+\n+        result = project_scaling_costs(\n+            avg_cost_per_workflow=0.0,\n+            current_workflow_count=100,\n+            scaling_factors=[1, 10],\n+        )\n+\n+        assert result[\"1x\"].total_cost == 0.0\n+        assert result[\"10x\"].total_cost == 0.0\n+\n+\n+class TestNodeCostAggregation:\n+    \"\"\"Test node-level cost aggregation across workflows.\"\"\"\n+\n+    def test_aggregate_node_costs_multiple_workflows(self):\n+        \"\"\"Test aggregating costs by node type across multiple workflows.\"\"\"\n+        from analyze_cost import (\n+            aggregate_node_costs,\n+            WorkflowCostAnalysis,\n+            CostBreakdown,\n+            TokenUsage,\n+        )\n+\n+        # Create mock workflow analyses\n+        workflow1_costs = [\n+            CostBreakdown(\n+                trace_id=\"1\",\n+                trace_name=\"ChatModel\",\n+                input_cost=0.001,\n+                output_cost=0.002,\n+                cache_cost=0.0,\n+                total_cost=0.003,\n+                token_usage=TokenUsage(1000, 500, 1500),\n+            ),\n+            CostBreakdown(\n+                trace_id=\"2\",\n+                trace_name=\"Validator\",\n+                input_cost=0.0005,\n+                output_cost=0.001,\n+                cache_cost=0.0,\n+                total_cost=0.0015,\n+                token_usage=TokenUsage(500, 250, 750),\n+            ),\n+        ]\n+\n+        workflow2_costs = [\n+            CostBreakdown(\n+                trace_id=\"3\",\n+                trace_name=\"ChatModel\",\n+                input_cost=0.002,\n+                output_cost=0.003,\n+                cache_cost=0.0,\n+                total_cost=0.005,\n+                token_usage=TokenUsage(2000, 1000, 3000),\n+            ),\n+        ]\n+\n+        workflows = [\n+            WorkflowCostAnalysis(\"wf1\", 0.0045, workflow1_costs, 2250),\n+            WorkflowCostAnalysis(\"wf2\", 0.005, workflow2_costs, 3000),\n+        ]\n+\n+        result = aggregate_node_costs(workflows)\n+\n+        assert result is not None\n+        assert len(result) == 2  # ChatModel and Validator\n+\n+        # Should be sorted by total cost descending\n+        assert result[0].node_name == \"ChatModel\"\n+        assert result[0].execution_count == 2\n+        assert result[0].total_cost == pytest.approx(0.008, abs=0.0001)\n+        assert result[0].avg_cost_per_execution == pytest.approx(0.004, abs=0.0001)\n+\n+        assert result[1].node_name == \"Validator\"\n+        assert result[1].execution_count == 1\n+        assert result[1].total_cost == pytest.approx(0.0015, abs=0.0001)\n+\n+    def test_aggregate_node_costs_empty_workflows(self):\n+        \"\"\"Test aggregating with no workflows.\"\"\"\n+        from analyze_cost import aggregate_node_costs\n+\n+        result = aggregate_node_costs([])\n+\n+        assert result is not None\n+        assert len(result) == 0\n+\n+\n+class TestMainAnalysisFunction:\n+    \"\"\"Test main analyze_costs() orchestration function.\"\"\"\n+\n+    def test_analyze_costs_integration(self):\n+        \"\"\"Test complete cost analysis workflow.\"\"\"\n+        from analyze_cost import analyze_costs, PricingConfig\n+        from analyze_traces import Workflow, Trace\n+        from datetime import datetime, timezone\n+\n+        # Create minimal workflow for testing\n+        root = Trace(\n+            id=\"root-1\",\n+            name=\"LangGraph\",\n+            start_time=datetime(2025, 1, 1, 12, 0, 0, tzinfo=timezone.utc),\n+            end_time=datetime(2025, 1, 1, 12, 5, 0, tzinfo=timezone.utc),\n+            duration_seconds=300.0,\n+            status=\"success\",\n+            run_type=\"chain\",\n+            parent_id=None,\n+            child_ids=[\"child-1\"],\n+            inputs={},\n+            outputs={},\n+            error=None,\n+        )\n+\n+        child = Trace(\n+            id=\"child-1\",\n+            name=\"ChatModel\",\n+            start_time=datetime(2025, 1, 1, 12, 1, 0, tzinfo=timezone.utc),\n+            end_time=datetime(2025, 1, 1, 12, 2, 0, tzinfo=timezone.utc),\n+            duration_seconds=60.0,\n+            status=\"success\",\n+            run_type=\"llm\",\n+            parent_id=\"root-1\",\n+            child_ids=[],\n+            inputs={},\n+            outputs={\n+                \"usage_metadata\": {\n+                    \"input_tokens\": 1000,\n+                    \"output_tokens\": 500,\n+                    \"total_tokens\": 1500,\n+                }\n+            },\n+            error=None,\n+        )\n+\n+        workflow = Workflow(\n+            root_trace=root,\n+            nodes={\"ChatModel\": [child]},\n+            all_traces=[root, child],\n+        )\n+\n+        pricing = PricingConfig(\n+            model_name=\"Test Model\",\n+            input_tokens_per_1k=0.001,\n+            output_tokens_per_1k=0.002,\n+        )\n+\n+        result = analyze_costs([workflow], pricing)\n+\n+        assert result is not None\n+        assert result.total_workflows_analyzed == 1\n+        assert result.avg_cost_per_workflow > 0\n+        assert len(result.node_summaries) == 1\n+        assert result.node_summaries[0].node_name == \"ChatModel\"\n+        assert len(result.scaling_projections) > 0\n+        assert \"1x\" in result.scaling_projections\n+\n+\n+class TestCacheEffectiveness:\n+    \"\"\"Test cache effectiveness analysis functions.\"\"\"\n+\n+    def test_calculate_cache_hit_rate_all_cached(self):\n+        \"\"\"Test cache hit rate when all traces have cache data.\"\"\"\n+        from analyze_cost import (\n+            calculate_cache_hit_rate,\n+            WorkflowCostAnalysis,\n+            CostBreakdown,\n+            TokenUsage,\n+        )\n+\n+        # Create workflow analysis with all traces having cache data\n+        costs_with_cache = [\n+            CostBreakdown(\n+                trace_id=\"1\",\n+                trace_name=\"ChatModel\",\n+                input_cost=0.001,\n+                output_cost=0.002,\n+                cache_cost=0.0001,\n+                total_cost=0.0031,\n+                token_usage=TokenUsage(1000, 500, 1500, cached_tokens=800),\n+            ),\n+            CostBreakdown(\n+                trace_id=\"2\",\n+                trace_name=\"Validator\",\n+                input_cost=0.001,\n+                output_cost=0.002,\n+                cache_cost=0.0002,\n+                total_cost=0.0032,\n+                token_usage=TokenUsage(1000, 500, 1500, cached_tokens=900),\n+            ),\n+        ]\n+\n+        workflows = [WorkflowCostAnalysis(\"wf1\", 0.0063, costs_with_cache, 3000)]\n+\n+        result = calculate_cache_hit_rate(workflows)\n+\n+        # All 2 traces have cache data = 100%\n+        assert result == pytest.approx(100.0, abs=0.01)\n+\n+    def test_calculate_cache_hit_rate_partial_cached(self):\n+        \"\"\"Test cache hit rate when only some traces have cache data.\"\"\"\n+        from analyze_cost import (\n+            calculate_cache_hit_rate,\n+            WorkflowCostAnalysis,\n+            CostBreakdown,\n+            TokenUsage,\n+        )\n+\n+        # Mix of cached and non-cached traces\n+        costs_mixed = [\n+            CostBreakdown(\n+                trace_id=\"1\",\n+                trace_name=\"ChatModel\",\n+                input_cost=0.001,\n+                output_cost=0.002,\n+                cache_cost=0.0001,\n+                total_cost=0.0031,\n+                token_usage=TokenUsage(1000, 500, 1500, cached_tokens=800),\n+            ),\n+            CostBreakdown(\n+                trace_id=\"2\",\n+                trace_name=\"Validator\",\n+                input_cost=0.001,\n+                output_cost=0.002,\n+                cache_cost=0.0,\n+                total_cost=0.003,\n+                token_usage=TokenUsage(1000, 500, 1500, cached_tokens=None),\n+            ),\n+            CostBreakdown(\n+                trace_id=\"3\",\n+                trace_name=\"Parser\",\n+                input_cost=0.001,\n+                output_cost=0.002,\n+                cache_cost=0.0,\n+                total_cost=0.003,\n+                token_usage=TokenUsage(1000, 500, 1500, cached_tokens=None),\n+            ),\n+        ]\n+\n+        workflows = [WorkflowCostAnalysis(\"wf1\", 0.0091, costs_mixed, 4500)]\n+\n+        result = calculate_cache_hit_rate(workflows)\n+\n+        # 1 out of 3 traces have cache data = 33.33%\n+        assert result == pytest.approx(33.33, abs=0.01)\n+\n+    def test_calculate_cache_hit_rate_no_traces(self):\n+        \"\"\"Test cache hit rate with no traces returns 0.\"\"\"\n+        from analyze_cost import calculate_cache_hit_rate\n+\n+        result = calculate_cache_hit_rate([])\n+\n+        assert result == 0.0\n+\n+    def test_calculate_cache_hit_rate_no_cache_data(self):\n+        \"\"\"Test cache hit rate when no traces have cache data.\"\"\"\n+        from analyze_cost import (\n+            calculate_cache_hit_rate,\n+            WorkflowCostAnalysis,\n+            CostBreakdown,\n+            TokenUsage,\n+        )\n+\n+        costs_no_cache = [\n+            CostBreakdown(\n+                trace_id=\"1\",\n+                trace_name=\"ChatModel\",\n+                input_cost=0.001,\n+                output_cost=0.002,\n+                cache_cost=0.0,\n+                total_cost=0.003,\n+                token_usage=TokenUsage(1000, 500, 1500, cached_tokens=None),\n+            ),\n+        ]\n+\n+        workflows = [WorkflowCostAnalysis(\"wf1\", 0.003, costs_no_cache, 1500)]\n+\n+        result = calculate_cache_hit_rate(workflows)\n+\n+        # 0 out of 1 traces have cache data = 0%\n+        assert result == 0.0\n+\n+    def test_calculate_cache_savings_with_cache(self):\n+        \"\"\"Test calculating cost savings from cache usage.\"\"\"\n+        from analyze_cost import (\n+            calculate_cache_savings,\n+            WorkflowCostAnalysis,\n+            CostBreakdown,\n+            TokenUsage,\n+            PricingConfig,\n+        )\n+\n+        # Create pricing config\n+        pricing = PricingConfig(\n+            model_name=\"Test Model\",\n+            input_tokens_per_1k=0.00125,  # $1.25 per 1M input tokens\n+            output_tokens_per_1k=0.005,  # $5.00 per 1M output tokens\n+            cache_read_per_1k=0.0003125,  # $0.3125 per 1M cache read tokens\n+        )\n+\n+        # Trace with 1000 input tokens, 800 cached\n+        # Without cache: 1000 * 0.00125 / 1000 = $0.00125\n+        # With cache: 800 * 0.0003125 / 1000 = $0.00025\n+        # Savings: $0.001 per trace\n+        costs_with_cache = [\n+            CostBreakdown(\n+                trace_id=\"1\",\n+                trace_name=\"ChatModel\",\n+                input_cost=0.00125,\n+                output_cost=0.0025,\n+                cache_cost=0.00025,\n+                total_cost=0.004,\n+                token_usage=TokenUsage(1000, 500, 1500, cached_tokens=800),\n+            ),\n+        ]\n+\n+        workflows = [WorkflowCostAnalysis(\"wf1\", 0.004, costs_with_cache, 1500)]\n+\n+        result = calculate_cache_savings(workflows, pricing)\n+\n+        # Savings: (800 tokens * input_price) - (800 tokens * cache_price)\n+        # = (800 * 0.00125 / 1000) - (800 * 0.0003125 / 1000)\n+        # = 0.001 - 0.00025 = 0.00075\n+        assert result == pytest.approx(0.00075, abs=0.00001)\n+\n+    def test_calculate_cache_savings_no_cache(self):\n+        \"\"\"Test that no cache usage results in zero savings.\"\"\"\n+        from analyze_cost import (\n+            calculate_cache_savings,\n+            WorkflowCostAnalysis,\n+            CostBreakdown,\n+            TokenUsage,\n+            PricingConfig,\n+        )\n+\n+        pricing = PricingConfig(\n+            model_name=\"Test Model\",\n+            input_tokens_per_1k=0.00125,\n+            output_tokens_per_1k=0.005,\n+        )\n+\n+        costs_no_cache = [\n+            CostBreakdown(\n+                trace_id=\"1\",\n+                trace_name=\"ChatModel\",\n+                input_cost=0.00125,\n+                output_cost=0.0025,\n+                cache_cost=0.0,\n+                total_cost=0.00375,\n+                token_usage=TokenUsage(1000, 500, 1500, cached_tokens=None),\n+            ),\n+        ]\n+\n+        workflows = [WorkflowCostAnalysis(\"wf1\", 0.00375, costs_no_cache, 1500)]\n+\n+        result = calculate_cache_savings(workflows, pricing)\n+\n+        # No cache usage = no savings\n+        assert result == 0.0\n+\n+    def test_calculate_cache_savings_multiple_traces(self):\n+        \"\"\"Test calculating savings across multiple traces.\"\"\"\n+        from analyze_cost import (\n+            calculate_cache_savings,\n+            WorkflowCostAnalysis,\n+            CostBreakdown,\n+            TokenUsage,\n+            PricingConfig,\n+        )\n+\n+        pricing = PricingConfig(\n+            model_name=\"Test Model\",\n+            input_tokens_per_1k=0.00125,\n+            output_tokens_per_1k=0.005,\n+            cache_read_per_1k=0.0003125,\n+        )\n+\n+        # Two traces with cache, one without\n+        costs_mixed = [\n+            CostBreakdown(\n+                trace_id=\"1\",\n+                trace_name=\"ChatModel\",\n+                input_cost=0.00125,\n+                output_cost=0.0025,\n+                cache_cost=0.00025,\n+                total_cost=0.004,\n+                token_usage=TokenUsage(1000, 500, 1500, cached_tokens=800),\n+            ),\n+            CostBreakdown(\n+                trace_id=\"2\",\n+                trace_name=\"Validator\",\n+                input_cost=0.001,\n+                output_cost=0.002,\n+                cache_cost=0.0002,\n+                total_cost=0.0032,\n+                token_usage=TokenUsage(800, 400, 1200, cached_tokens=600),\n+            ),\n+            CostBreakdown(\n+                trace_id=\"3\",\n+                trace_name=\"Parser\",\n+                input_cost=0.001,\n+                output_cost=0.002,\n+                cache_cost=0.0,\n+                total_cost=0.003,\n+                token_usage=TokenUsage(800, 400, 1200, cached_tokens=None),\n+            ),\n+        ]\n+\n+        workflows = [WorkflowCostAnalysis(\"wf1\", 0.0102, costs_mixed, 3900)]\n+\n+        result = calculate_cache_savings(workflows, pricing)\n+\n+        # Trace 1: (800 * 0.00125 / 1000) - (800 * 0.0003125 / 1000) = 0.00075\n+        # Trace 2: (600 * 0.00125 / 1000) - (600 * 0.0003125 / 1000) = 0.0005625\n+        # Trace 3: 0 (no cache)\n+        # Total: 0.00075 + 0.0005625 = 0.0013125\n+        assert result == pytest.approx(0.0013125, abs=0.00001)\n+\n+    def test_calculate_cache_savings_no_cache_pricing(self):\n+        \"\"\"Test that missing cache pricing returns zero savings.\"\"\"\n+        from analyze_cost import (\n+            calculate_cache_savings,\n+            WorkflowCostAnalysis,\n+            CostBreakdown,\n+            TokenUsage,\n+            PricingConfig,\n+        )\n+\n+        # Pricing without cache_read_per_1k\n+        pricing = PricingConfig(\n+            model_name=\"Test Model\",\n+            input_tokens_per_1k=0.00125,\n+            output_tokens_per_1k=0.005,\n+        )\n+\n+        costs_with_cache = [\n+            CostBreakdown(\n+                trace_id=\"1\",\n+                trace_name=\"ChatModel\",\n+                input_cost=0.00125,\n+                output_cost=0.0025,\n+                cache_cost=0.0,\n+                total_cost=0.00375,\n+                token_usage=TokenUsage(1000, 500, 1500, cached_tokens=800),\n+            ),\n+        ]\n+\n+        workflows = [WorkflowCostAnalysis(\"wf1\", 0.00375, costs_with_cache, 1500)]\n+\n+        result = calculate_cache_savings(workflows, pricing)\n+\n+        # No cache pricing configured = can't calculate savings\n+        assert result == 0.0\n+\n+    def test_compare_cached_vs_fresh_costs_with_cache(self):\n+        \"\"\"Test comparing costs with cache vs without cache.\"\"\"\n+        from analyze_cost import (\n+            compare_cached_vs_fresh_costs,\n+            WorkflowCostAnalysis,\n+            CostBreakdown,\n+            TokenUsage,\n+            PricingConfig,\n+        )\n+\n+        pricing = PricingConfig(\n+            model_name=\"Test Model\",\n+            input_tokens_per_1k=0.00125,\n+            output_tokens_per_1k=0.005,\n+            cache_read_per_1k=0.0003125,\n+        )\n+\n+        # Two traces: one with cache, one without\n+        costs = [\n+            CostBreakdown(\n+                trace_id=\"1\",\n+                trace_name=\"ChatModel\",\n+                input_cost=0.00125,\n+                output_cost=0.0025,\n+                cache_cost=0.00025,\n+                total_cost=0.004,\n+                token_usage=TokenUsage(1000, 500, 1500, cached_tokens=800),\n+            ),\n+            CostBreakdown(\n+                trace_id=\"2\",\n+                trace_name=\"Validator\",\n+                input_cost=0.001,\n+                output_cost=0.002,\n+                cache_cost=0.0,\n+                total_cost=0.003,\n+                token_usage=TokenUsage(800, 400, 1200, cached_tokens=None),\n+            ),\n+        ]\n+\n+        workflows = [WorkflowCostAnalysis(\"wf1\", 0.007, costs, 2700)]\n+\n+        result = compare_cached_vs_fresh_costs(workflows, pricing)\n+\n+        # Cost with cache: 0.004 + 0.003 = 0.007\n+        assert result.cost_with_cache == pytest.approx(0.007, abs=0.0001)\n+\n+        # Cost without cache: trace 1 would be 0.00125 + 0.0025 + (800 * 0.00125 / 1000)\n+        # = 0.00125 + 0.0025 + 0.001 = 0.00475\n+        # trace 2 stays same: 0.003\n+        # Total: 0.00775\n+        assert result.cost_without_cache == pytest.approx(0.00775, abs=0.0001)\n+\n+        # Savings: 0.00775 - 0.007 = 0.00075\n+        assert result.total_savings == pytest.approx(0.00075, abs=0.00001)\n+\n+        # Savings percent: (0.00075 / 0.00775) * 100 = 9.68%\n+        assert result.savings_percent == pytest.approx(9.68, abs=0.01)\n+\n+        assert result.traces_analyzed == 2\n+        assert result.traces_with_cache == 1\n+\n+    def test_compare_cached_vs_fresh_costs_no_cache(self):\n+        \"\"\"Test comparison when no traces use cache.\"\"\"\n+        from analyze_cost import (\n+            compare_cached_vs_fresh_costs,\n+            WorkflowCostAnalysis,\n+            CostBreakdown,\n+            TokenUsage,\n+            PricingConfig,\n+        )\n+\n+        pricing = PricingConfig(\n+            model_name=\"Test Model\",\n+            input_tokens_per_1k=0.00125,\n+            output_tokens_per_1k=0.005,\n+        )\n+\n+        costs = [\n+            CostBreakdown(\n+                trace_id=\"1\",\n+                trace_name=\"ChatModel\",\n+                input_cost=0.00125,\n+                output_cost=0.0025,\n+                cache_cost=0.0,\n+                total_cost=0.00375,\n+                token_usage=TokenUsage(1000, 500, 1500, cached_tokens=None),\n+            ),\n+        ]\n+\n+        workflows = [WorkflowCostAnalysis(\"wf1\", 0.00375, costs, 1500)]\n+\n+        result = compare_cached_vs_fresh_costs(workflows, pricing)\n+\n+        # No cache usage, so costs are identical\n+        assert result.cost_with_cache == pytest.approx(0.00375, abs=0.0001)\n+        assert result.cost_without_cache == pytest.approx(0.00375, abs=0.0001)\n+        assert result.total_savings == 0.0\n+        assert result.savings_percent == 0.0\n+        assert result.traces_analyzed == 1\n+        assert result.traces_with_cache == 0\n+\n+    def test_compare_cached_vs_fresh_costs_all_cached(self):\n+        \"\"\"Test comparison when all traces use cache.\"\"\"\n+        from analyze_cost import (\n+            compare_cached_vs_fresh_costs,\n+            WorkflowCostAnalysis,\n+            CostBreakdown,\n+            TokenUsage,\n+            PricingConfig,\n+        )\n+\n+        pricing = PricingConfig(\n+            model_name=\"Test Model\",\n+            input_tokens_per_1k=0.00125,\n+            output_tokens_per_1k=0.005,\n+            cache_read_per_1k=0.0003125,\n+        )\n+\n+        costs = [\n+            CostBreakdown(\n+                trace_id=\"1\",\n+                trace_name=\"ChatModel\",\n+                input_cost=0.00125,\n+                output_cost=0.0025,\n+                cache_cost=0.00025,\n+                total_cost=0.004,\n+                token_usage=TokenUsage(1000, 500, 1500, cached_tokens=800),\n+            ),\n+            CostBreakdown(\n+                trace_id=\"2\",\n+                trace_name=\"Validator\",\n+                input_cost=0.001,\n+                output_cost=0.002,\n+                cache_cost=0.00015,\n+                total_cost=0.00315,\n+                token_usage=TokenUsage(800, 400, 1200, cached_tokens=480),\n+            ),\n+        ]\n+\n+        workflows = [WorkflowCostAnalysis(\"wf1\", 0.00715, costs, 2700)]\n+\n+        result = compare_cached_vs_fresh_costs(workflows, pricing)\n+\n+        # Both traces use cache\n+        assert result.traces_analyzed == 2\n+        assert result.traces_with_cache == 2\n+\n+        # Savings should be positive\n+        assert result.total_savings > 0\n+        assert result.savings_percent > 0\n+        assert result.cost_without_cache > result.cost_with_cache\n+\n+\n+# Run tests with: pytest test_analyze_cost.py -v",
      "patch_lines": [
        "@@ -0,0 +1,1046 @@\n",
        "+\"\"\"\n",
        "+Test suite for Phase 3B: Cost Analysis\n",
        "+\n",
        "+Following TDD methodology - tests written FIRST, then implementation.\n",
        "+Tests for cost analysis functionality including token extraction,\n",
        "+cost calculation, and scaling projections.\n",
        "+\n",
        "+Author: Generated with Claude Code (PDCA Framework)\n",
        "+Date: 2025-12-09\n",
        "+\"\"\"\n",
        "+\n",
        "+import pytest\n",
        "+from datetime import datetime, timezone\n",
        "+from analyze_cost import (\n",
        "+    PricingConfig,\n",
        "+    TokenUsage,\n",
        "+    extract_token_usage,\n",
        "+    calculate_trace_cost,\n",
        "+)\n",
        "+from analyze_traces import Trace\n",
        "+\n",
        "+\n",
        "+class TestPricingConfig:\n",
        "+    \"\"\"Test PricingConfig dataclass validation.\"\"\"\n",
        "+\n",
        "+    def test_pricing_config_creation_valid(self):\n",
        "+        \"\"\"Test creating valid pricing config.\"\"\"\n",
        "+        pricing = PricingConfig(\n",
        "+            model_name=\"Test Model\",\n",
        "+            input_tokens_per_1k=0.001,\n",
        "+            output_tokens_per_1k=0.002,\n",
        "+            cache_read_per_1k=0.0001,\n",
        "+        )\n",
        "+        assert pricing.model_name == \"Test Model\"\n",
        "+        assert pricing.input_tokens_per_1k == 0.001\n",
        "+        assert pricing.output_tokens_per_1k == 0.002\n",
        "+        assert pricing.cache_read_per_1k == 0.0001\n",
        "+\n",
        "+    def test_pricing_config_without_cache(self):\n",
        "+        \"\"\"Test pricing config without cache pricing.\"\"\"\n",
        "+        pricing = PricingConfig(\n",
        "+            model_name=\"No Cache Model\",\n",
        "+            input_tokens_per_1k=0.001,\n",
        "+            output_tokens_per_1k=0.002,\n",
        "+        )\n",
        "+        assert pricing.cache_read_per_1k is None\n",
        "+\n",
        "+    def test_pricing_config_negative_input_price_raises(self):\n",
        "+        \"\"\"Test that negative input price raises ValueError.\"\"\"\n",
        "+        with pytest.raises(ValueError, match=\"non-negative\"):\n",
        "+            PricingConfig(\n",
        "+                model_name=\"Bad Model\",\n",
        "+                input_tokens_per_1k=-0.001,\n",
        "+                output_tokens_per_1k=0.002,\n",
        "+            )\n",
        "+\n",
        "+    def test_pricing_config_negative_output_price_raises(self):\n",
        "+        \"\"\"Test that negative output price raises ValueError.\"\"\"\n",
        "+        with pytest.raises(ValueError, match=\"non-negative\"):\n",
        "+            PricingConfig(\n",
        "+                model_name=\"Bad Model\",\n",
        "+                input_tokens_per_1k=0.001,\n",
        "+                output_tokens_per_1k=-0.002,\n",
        "+            )\n",
        "+\n",
        "+    def test_pricing_config_negative_cache_price_raises(self):\n",
        "+        \"\"\"Test that negative cache price raises ValueError.\"\"\"\n",
        "+        with pytest.raises(ValueError, match=\"non-negative\"):\n",
        "+            PricingConfig(\n",
        "+                model_name=\"Bad Model\",\n",
        "+                input_tokens_per_1k=0.001,\n",
        "+                output_tokens_per_1k=0.002,\n",
        "+                cache_read_per_1k=-0.0001,\n",
        "+            )\n",
        "+\n",
        "+\n",
        "+class TestTokenExtraction:\n",
        "+    \"\"\"Test token usage extraction from traces.\"\"\"\n",
        "+\n",
        "+    def test_extract_token_usage_from_outputs(self):\n",
        "+        \"\"\"Test extracting tokens from trace.outputs['usage_metadata'].\"\"\"\n",
        "+        trace = Trace(\n",
        "+            id=\"test-1\",\n",
        "+            name=\"ChatGoogleGenerativeAI\",\n",
        "+            start_time=datetime(2025, 1, 1, 12, 0, 0, tzinfo=timezone.utc),\n",
        "+            end_time=datetime(2025, 1, 1, 12, 1, 0, tzinfo=timezone.utc),\n",
        "+            duration_seconds=60.0,\n",
        "+            status=\"success\",\n",
        "+            run_type=\"llm\",\n",
        "+            parent_id=None,\n",
        "+            child_ids=[],\n",
        "+            inputs={},\n",
        "+            outputs={\n",
        "+                \"usage_metadata\": {\n",
        "+                    \"input_tokens\": 1000,\n",
        "+                    \"output_tokens\": 500,\n",
        "+                    \"total_tokens\": 1500,\n",
        "+                }\n",
        "+            },\n",
        "+            error=None,\n",
        "+        )\n",
        "+\n",
        "+        result = extract_token_usage(trace)\n",
        "+\n",
        "+        assert result is not None\n",
        "+        assert result.input_tokens == 1000\n",
        "+        assert result.output_tokens == 500\n",
        "+        assert result.total_tokens == 1500\n",
        "+        assert result.cached_tokens is None\n",
        "+\n",
        "+    def test_extract_token_usage_with_cache_data(self):\n",
        "+        \"\"\"Test extracting tokens including cache_read data.\"\"\"\n",
        "+        trace = Trace(\n",
        "+            id=\"test-2\",\n",
        "+            name=\"ChatGoogleGenerativeAI\",\n",
        "+            start_time=datetime(2025, 1, 1, 12, 0, 0, tzinfo=timezone.utc),\n",
        "+            end_time=datetime(2025, 1, 1, 12, 1, 0, tzinfo=timezone.utc),\n",
        "+            duration_seconds=60.0,\n",
        "+            status=\"success\",\n",
        "+            run_type=\"llm\",\n",
        "+            parent_id=None,\n",
        "+            child_ids=[],\n",
        "+            inputs={},\n",
        "+            outputs={\n",
        "+                \"usage_metadata\": {\n",
        "+                    \"input_tokens\": 1000,\n",
        "+                    \"output_tokens\": 500,\n",
        "+                    \"total_tokens\": 1500,\n",
        "+                    \"input_token_details\": {\n",
        "+                        \"cache_read\": 800,\n",
        "+                    },\n",
        "+                }\n",
        "+            },\n",
        "+            error=None,\n",
        "+        )\n",
        "+\n",
        "+        result = extract_token_usage(trace)\n",
        "+\n",
        "+        assert result is not None\n",
        "+        assert result.input_tokens == 1000\n",
        "+        assert result.output_tokens == 500\n",
        "+        assert result.total_tokens == 1500\n",
        "+        assert result.cached_tokens == 800\n",
        "+\n",
        "+    def test_extract_token_usage_missing_data_returns_none(self):\n",
        "+        \"\"\"Test that missing token data returns None.\"\"\"\n",
        "+        trace = Trace(\n",
        "+            id=\"test-3\",\n",
        "+            name=\"non_llm_node\",\n",
        "+            start_time=datetime(2025, 1, 1, 12, 0, 0, tzinfo=timezone.utc),\n",
        "+            end_time=datetime(2025, 1, 1, 12, 1, 0, tzinfo=timezone.utc),\n",
        "+            duration_seconds=60.0,\n",
        "+            status=\"success\",\n",
        "+            run_type=\"chain\",\n",
        "+            parent_id=None,\n",
        "+            child_ids=[],\n",
        "+            inputs={},\n",
        "+            outputs={},\n",
        "+            error=None,\n",
        "+        )\n",
        "+\n",
        "+        result = extract_token_usage(trace)\n",
        "+\n",
        "+        assert result is None\n",
        "+\n",
        "+    def test_extract_token_usage_from_inputs_fallback(self):\n",
        "+        \"\"\"Test fallback to extracting from inputs if not in outputs.\"\"\"\n",
        "+        trace = Trace(\n",
        "+            id=\"test-4\",\n",
        "+            name=\"ChatGoogleGenerativeAI\",\n",
        "+            start_time=datetime(2025, 1, 1, 12, 0, 0, tzinfo=timezone.utc),\n",
        "+            end_time=datetime(2025, 1, 1, 12, 1, 0, tzinfo=timezone.utc),\n",
        "+            duration_seconds=60.0,\n",
        "+            status=\"success\",\n",
        "+            run_type=\"llm\",\n",
        "+            parent_id=None,\n",
        "+            child_ids=[],\n",
        "+            inputs={\n",
        "+                \"usage_metadata\": {\n",
        "+                    \"input_tokens\": 2000,\n",
        "+                    \"output_tokens\": 1000,\n",
        "+                    \"total_tokens\": 3000,\n",
        "+                }\n",
        "+            },\n",
        "+            outputs={},\n",
        "+            error=None,\n",
        "+        )\n",
        "+\n",
        "+        result = extract_token_usage(trace)\n",
        "+\n",
        "+        assert result is not None\n",
        "+        assert result.input_tokens == 2000\n",
        "+        assert result.output_tokens == 1000\n",
        "+\n",
        "+\n",
        "+class TestCostCalculation:\n",
        "+    \"\"\"Test cost calculation functions.\"\"\"\n",
        "+\n",
        "+    def test_calculate_trace_cost_basic(self):\n",
        "+        \"\"\"Test basic cost calculation without cache.\"\"\"\n",
        "+        token_usage = TokenUsage(\n",
        "+            input_tokens=1000,\n",
        "+            output_tokens=500,\n",
        "+            total_tokens=1500,\n",
        "+            cached_tokens=None,\n",
        "+        )\n",
        "+        pricing = PricingConfig(\n",
        "+            model_name=\"Test Model\",\n",
        "+            input_tokens_per_1k=0.00125,  # $1.25 per 1M\n",
        "+            output_tokens_per_1k=0.005,  # $5.00 per 1M\n",
        "+        )\n",
        "+\n",
        "+        result = calculate_trace_cost(token_usage, pricing)\n",
        "+\n",
        "+        # Expected: (1000 * 0.00125 / 1000) + (500 * 0.005 / 1000)\n",
        "+        # = 0.00125 + 0.0025 = 0.00375\n",
        "+        assert result.trace_id is not None\n",
        "+        assert result.input_cost == pytest.approx(0.00125, abs=0.00001)\n",
        "+        assert result.output_cost == pytest.approx(0.0025, abs=0.00001)\n",
        "+        assert result.cache_cost == 0.0\n",
        "+        assert result.total_cost == pytest.approx(0.00375, abs=0.00001)\n",
        "+\n",
        "+    def test_calculate_trace_cost_with_cache(self):\n",
        "+        \"\"\"Test cost calculation with cache reads.\"\"\"\n",
        "+        token_usage = TokenUsage(\n",
        "+            input_tokens=1000,\n",
        "+            output_tokens=500,\n",
        "+            total_tokens=1500,\n",
        "+            cached_tokens=800,\n",
        "+        )\n",
        "+        pricing = PricingConfig(\n",
        "+            model_name=\"Test Model\",\n",
        "+            input_tokens_per_1k=0.00125,\n",
        "+            output_tokens_per_1k=0.005,\n",
        "+            cache_read_per_1k=0.0003125,  # $0.3125 per 1M\n",
        "+        )\n",
        "+\n",
        "+        result = calculate_trace_cost(token_usage, pricing)\n",
        "+\n",
        "+        # Expected cache cost: 800 * 0.0003125 / 1000 = 0.00025\n",
        "+        assert result.cache_cost == pytest.approx(0.00025, abs=0.00001)\n",
        "+        # Total: 0.00125 + 0.0025 + 0.00025 = 0.00400\n",
        "+        assert result.total_cost == pytest.approx(0.00400, abs=0.00001)\n",
        "+\n",
        "+    def test_calculate_trace_cost_zero_tokens(self):\n",
        "+        \"\"\"Test handling of zero token usage.\"\"\"\n",
        "+        token_usage = TokenUsage(\n",
        "+            input_tokens=0,\n",
        "+            output_tokens=0,\n",
        "+            total_tokens=0,\n",
        "+            cached_tokens=None,\n",
        "+        )\n",
        "+        pricing = PricingConfig(\n",
        "+            model_name=\"Test Model\",\n",
        "+            input_tokens_per_1k=0.00125,\n",
        "+            output_tokens_per_1k=0.005,\n",
        "+        )\n",
        "+\n",
        "+        result = calculate_trace_cost(token_usage, pricing)\n",
        "+\n",
        "+        assert result.input_cost == 0.0\n",
        "+        assert result.output_cost == 0.0\n",
        "+        assert result.total_cost == 0.0\n",
        "+\n",
        "+\n",
        "+class TestWorkflowCostAnalysis:\n",
        "+    \"\"\"Test workflow-level cost analysis.\"\"\"\n",
        "+\n",
        "+    def test_calculate_workflow_cost_single_trace(self):\n",
        "+        \"\"\"Test calculating cost for workflow with one LLM trace.\"\"\"\n",
        "+        from analyze_traces import Workflow\n",
        "+\n",
        "+        # Create root trace (no cost)\n",
        "+        root = Trace(\n",
        "+            id=\"root-1\",\n",
        "+            name=\"LangGraph\",\n",
        "+            start_time=datetime(2025, 1, 1, 12, 0, 0, tzinfo=timezone.utc),\n",
        "+            end_time=datetime(2025, 1, 1, 12, 5, 0, tzinfo=timezone.utc),\n",
        "+            duration_seconds=300.0,\n",
        "+            status=\"success\",\n",
        "+            run_type=\"chain\",\n",
        "+            parent_id=None,\n",
        "+            child_ids=[\"child-1\"],\n",
        "+            inputs={},\n",
        "+            outputs={},\n",
        "+            error=None,\n",
        "+        )\n",
        "+\n",
        "+        # Create child LLM trace with cost\n",
        "+        child = Trace(\n",
        "+            id=\"child-1\",\n",
        "+            name=\"ChatGoogleGenerativeAI\",\n",
        "+            start_time=datetime(2025, 1, 1, 12, 1, 0, tzinfo=timezone.utc),\n",
        "+            end_time=datetime(2025, 1, 1, 12, 2, 0, tzinfo=timezone.utc),\n",
        "+            duration_seconds=60.0,\n",
        "+            status=\"success\",\n",
        "+            run_type=\"llm\",\n",
        "+            parent_id=\"root-1\",\n",
        "+            child_ids=[],\n",
        "+            inputs={},\n",
        "+            outputs={\n",
        "+                \"usage_metadata\": {\n",
        "+                    \"input_tokens\": 1000,\n",
        "+                    \"output_tokens\": 500,\n",
        "+                    \"total_tokens\": 1500,\n",
        "+                }\n",
        "+            },\n",
        "+            error=None,\n",
        "+        )\n",
        "+\n",
        "+        workflow = Workflow(\n",
        "+            root_trace=root,\n",
        "+            nodes={\"ChatGoogleGenerativeAI\": [child]},\n",
        "+            all_traces=[root, child],\n",
        "+        )\n",
        "+\n",
        "+        pricing = PricingConfig(\n",
        "+            model_name=\"Test Model\",\n",
        "+            input_tokens_per_1k=0.00125,\n",
        "+            output_tokens_per_1k=0.005,\n",
        "+        )\n",
        "+\n",
        "+        from analyze_cost import calculate_workflow_cost\n",
        "+\n",
        "+        result = calculate_workflow_cost(workflow, pricing)\n",
        "+\n",
        "+        assert result is not None\n",
        "+        assert result.workflow_id == \"root-1\"\n",
        "+        assert len(result.node_costs) == 1\n",
        "+        assert result.total_cost == pytest.approx(0.00375, abs=0.00001)\n",
        "+\n",
        "+    def test_calculate_workflow_cost_no_token_data(self):\n",
        "+        \"\"\"Test workflow with no token data returns None.\"\"\"\n",
        "+        from analyze_traces import Workflow\n",
        "+\n",
        "+        root = Trace(\n",
        "+            id=\"root-2\",\n",
        "+            name=\"LangGraph\",\n",
        "+            start_time=datetime(2025, 1, 1, 12, 0, 0, tzinfo=timezone.utc),\n",
        "+            end_time=datetime(2025, 1, 1, 12, 5, 0, tzinfo=timezone.utc),\n",
        "+            duration_seconds=300.0,\n",
        "+            status=\"success\",\n",
        "+            run_type=\"chain\",\n",
        "+            parent_id=None,\n",
        "+            child_ids=[],\n",
        "+            inputs={},\n",
        "+            outputs={},\n",
        "+            error=None,\n",
        "+        )\n",
        "+\n",
        "+        workflow = Workflow(\n",
        "+            root_trace=root,\n",
        "+            nodes={},\n",
        "+            all_traces=[root],\n",
        "+        )\n",
        "+\n",
        "+        pricing = PricingConfig(\n",
        "+            model_name=\"Test Model\",\n",
        "+            input_tokens_per_1k=0.00125,\n",
        "+            output_tokens_per_1k=0.005,\n",
        "+        )\n",
        "+\n",
        "+        from analyze_cost import calculate_workflow_cost\n",
        "+\n",
        "+        result = calculate_workflow_cost(workflow, pricing)\n",
        "+\n",
        "+        # Should return result with zero cost, not None\n",
        "+        assert result is not None\n",
        "+        assert result.total_cost == 0.0\n",
        "+        assert len(result.node_costs) == 0\n",
        "+\n",
        "+\n",
        "+class TestScalingProjections:\n",
        "+    \"\"\"Test scaling cost projections.\"\"\"\n",
        "+\n",
        "+    def test_project_scaling_costs_basic(self):\n",
        "+        \"\"\"Test basic scaling projections at 10x, 100x, 1000x.\"\"\"\n",
        "+        from analyze_cost import project_scaling_costs, SCALING_FACTORS\n",
        "+\n",
        "+        avg_cost_per_workflow = 0.01  # $0.01 per workflow\n",
        "+        current_workflow_count = 100\n",
        "+\n",
        "+        result = project_scaling_costs(\n",
        "+            avg_cost_per_workflow=avg_cost_per_workflow,\n",
        "+            current_workflow_count=current_workflow_count,\n",
        "+            scaling_factors=SCALING_FACTORS,\n",
        "+        )\n",
        "+\n",
        "+        assert result is not None\n",
        "+        assert \"1x\" in result\n",
        "+        assert \"10x\" in result\n",
        "+        assert \"100x\" in result\n",
        "+        assert \"1000x\" in result\n",
        "+\n",
        "+        # Verify 1x (current)\n",
        "+        assert result[\"1x\"].scale_factor == 1\n",
        "+        assert result[\"1x\"].workflow_count == 100\n",
        "+        assert result[\"1x\"].total_cost == pytest.approx(1.0, abs=0.01)  # 100 * $0.01\n",
        "+\n",
        "+        # Verify 10x\n",
        "+        assert result[\"10x\"].scale_factor == 10\n",
        "+        assert result[\"10x\"].workflow_count == 1000\n",
        "+        assert result[\"10x\"].total_cost == pytest.approx(10.0, abs=0.01)\n",
        "+\n",
        "+        # Verify 100x\n",
        "+        assert result[\"100x\"].scale_factor == 100\n",
        "+        assert result[\"100x\"].workflow_count == 10000\n",
        "+        assert result[\"100x\"].total_cost == pytest.approx(100.0, abs=0.01)\n",
        "+\n",
        "+        # Verify 1000x\n",
        "+        assert result[\"1000x\"].scale_factor == 1000\n",
        "+        assert result[\"1000x\"].workflow_count == 100000\n",
        "+        assert result[\"1000x\"].total_cost == pytest.approx(1000.0, abs=0.01)\n",
        "+\n",
        "+    def test_project_scaling_costs_with_monthly(self):\n",
        "+        \"\"\"Test that monthly costs are calculated correctly.\"\"\"\n",
        "+        from analyze_cost import project_scaling_costs\n",
        "+\n",
        "+        avg_cost_per_workflow = 0.05\n",
        "+        current_workflow_count = 50\n",
        "+        monthly_estimate = 500  # Assume 500 workflows per month\n",
        "+\n",
        "+        result = project_scaling_costs(\n",
        "+            avg_cost_per_workflow=avg_cost_per_workflow,\n",
        "+            current_workflow_count=current_workflow_count,\n",
        "+            scaling_factors=[1, 10],\n",
        "+            monthly_workflow_estimate=monthly_estimate,\n",
        "+        )\n",
        "+\n",
        "+        # At 1x: 500 workflows/month * $0.05 = $25/month\n",
        "+        assert result[\"1x\"].cost_per_month_30days is not None\n",
        "+        assert result[\"1x\"].cost_per_month_30days == pytest.approx(25.0, abs=0.01)\n",
        "+\n",
        "+        # At 10x: 5000 workflows/month * $0.05 = $250/month\n",
        "+        assert result[\"10x\"].cost_per_month_30days is not None\n",
        "+        assert result[\"10x\"].cost_per_month_30days == pytest.approx(250.0, abs=0.01)\n",
        "+\n",
        "+    def test_project_scaling_costs_zero_cost(self):\n",
        "+        \"\"\"Test handling of zero cost per workflow.\"\"\"\n",
        "+        from analyze_cost import project_scaling_costs\n",
        "+\n",
        "+        result = project_scaling_costs(\n",
        "+            avg_cost_per_workflow=0.0,\n",
        "+            current_workflow_count=100,\n",
        "+            scaling_factors=[1, 10],\n",
        "+        )\n",
        "+\n",
        "+        assert result[\"1x\"].total_cost == 0.0\n",
        "+        assert result[\"10x\"].total_cost == 0.0\n",
        "+\n",
        "+\n",
        "+class TestNodeCostAggregation:\n",
        "+    \"\"\"Test node-level cost aggregation across workflows.\"\"\"\n",
        "+\n",
        "+    def test_aggregate_node_costs_multiple_workflows(self):\n",
        "+        \"\"\"Test aggregating costs by node type across multiple workflows.\"\"\"\n",
        "+        from analyze_cost import (\n",
        "+            aggregate_node_costs,\n",
        "+            WorkflowCostAnalysis,\n",
        "+            CostBreakdown,\n",
        "+            TokenUsage,\n",
        "+        )\n",
        "+\n",
        "+        # Create mock workflow analyses\n",
        "+        workflow1_costs = [\n",
        "+            CostBreakdown(\n",
        "+                trace_id=\"1\",\n",
        "+                trace_name=\"ChatModel\",\n",
        "+                input_cost=0.001,\n",
        "+                output_cost=0.002,\n",
        "+                cache_cost=0.0,\n",
        "+                total_cost=0.003,\n",
        "+                token_usage=TokenUsage(1000, 500, 1500),\n",
        "+            ),\n",
        "+            CostBreakdown(\n",
        "+                trace_id=\"2\",\n",
        "+                trace_name=\"Validator\",\n",
        "+                input_cost=0.0005,\n",
        "+                output_cost=0.001,\n",
        "+                cache_cost=0.0,\n",
        "+                total_cost=0.0015,\n",
        "+                token_usage=TokenUsage(500, 250, 750),\n",
        "+            ),\n",
        "+        ]\n",
        "+\n",
        "+        workflow2_costs = [\n",
        "+            CostBreakdown(\n",
        "+                trace_id=\"3\",\n",
        "+                trace_name=\"ChatModel\",\n",
        "+                input_cost=0.002,\n",
        "+                output_cost=0.003,\n",
        "+                cache_cost=0.0,\n",
        "+                total_cost=0.005,\n",
        "+                token_usage=TokenUsage(2000, 1000, 3000),\n",
        "+            ),\n",
        "+        ]\n",
        "+\n",
        "+        workflows = [\n",
        "+            WorkflowCostAnalysis(\"wf1\", 0.0045, workflow1_costs, 2250),\n",
        "+            WorkflowCostAnalysis(\"wf2\", 0.005, workflow2_costs, 3000),\n",
        "+        ]\n",
        "+\n",
        "+        result = aggregate_node_costs(workflows)\n",
        "+\n",
        "+        assert result is not None\n",
        "+        assert len(result) == 2  # ChatModel and Validator\n",
        "+\n",
        "+        # Should be sorted by total cost descending\n",
        "+        assert result[0].node_name == \"ChatModel\"\n",
        "+        assert result[0].execution_count == 2\n",
        "+        assert result[0].total_cost == pytest.approx(0.008, abs=0.0001)\n",
        "+        assert result[0].avg_cost_per_execution == pytest.approx(0.004, abs=0.0001)\n",
        "+\n",
        "+        assert result[1].node_name == \"Validator\"\n",
        "+        assert result[1].execution_count == 1\n",
        "+        assert result[1].total_cost == pytest.approx(0.0015, abs=0.0001)\n",
        "+\n",
        "+    def test_aggregate_node_costs_empty_workflows(self):\n",
        "+        \"\"\"Test aggregating with no workflows.\"\"\"\n",
        "+        from analyze_cost import aggregate_node_costs\n",
        "+\n",
        "+        result = aggregate_node_costs([])\n",
        "+\n",
        "+        assert result is not None\n",
        "+        assert len(result) == 0\n",
        "+\n",
        "+\n",
        "+class TestMainAnalysisFunction:\n",
        "+    \"\"\"Test main analyze_costs() orchestration function.\"\"\"\n",
        "+\n",
        "+    def test_analyze_costs_integration(self):\n",
        "+        \"\"\"Test complete cost analysis workflow.\"\"\"\n",
        "+        from analyze_cost import analyze_costs, PricingConfig\n",
        "+        from analyze_traces import Workflow, Trace\n",
        "+        from datetime import datetime, timezone\n",
        "+\n",
        "+        # Create minimal workflow for testing\n",
        "+        root = Trace(\n",
        "+            id=\"root-1\",\n",
        "+            name=\"LangGraph\",\n",
        "+            start_time=datetime(2025, 1, 1, 12, 0, 0, tzinfo=timezone.utc),\n",
        "+            end_time=datetime(2025, 1, 1, 12, 5, 0, tzinfo=timezone.utc),\n",
        "+            duration_seconds=300.0,\n",
        "+            status=\"success\",\n",
        "+            run_type=\"chain\",\n",
        "+            parent_id=None,\n",
        "+            child_ids=[\"child-1\"],\n",
        "+            inputs={},\n",
        "+            outputs={},\n",
        "+            error=None,\n",
        "+        )\n",
        "+\n",
        "+        child = Trace(\n",
        "+            id=\"child-1\",\n",
        "+            name=\"ChatModel\",\n",
        "+            start_time=datetime(2025, 1, 1, 12, 1, 0, tzinfo=timezone.utc),\n",
        "+            end_time=datetime(2025, 1, 1, 12, 2, 0, tzinfo=timezone.utc),\n",
        "+            duration_seconds=60.0,\n",
        "+            status=\"success\",\n",
        "+            run_type=\"llm\",\n",
        "+            parent_id=\"root-1\",\n",
        "+            child_ids=[],\n",
        "+            inputs={},\n",
        "+            outputs={\n",
        "+                \"usage_metadata\": {\n",
        "+                    \"input_tokens\": 1000,\n",
        "+                    \"output_tokens\": 500,\n",
        "+                    \"total_tokens\": 1500,\n",
        "+                }\n",
        "+            },\n",
        "+            error=None,\n",
        "+        )\n",
        "+\n",
        "+        workflow = Workflow(\n",
        "+            root_trace=root,\n",
        "+            nodes={\"ChatModel\": [child]},\n",
        "+            all_traces=[root, child],\n",
        "+        )\n",
        "+\n",
        "+        pricing = PricingConfig(\n",
        "+            model_name=\"Test Model\",\n",
        "+            input_tokens_per_1k=0.001,\n",
        "+            output_tokens_per_1k=0.002,\n",
        "+        )\n",
        "+\n",
        "+        result = analyze_costs([workflow], pricing)\n",
        "+\n",
        "+        assert result is not None\n",
        "+        assert result.total_workflows_analyzed == 1\n",
        "+        assert result.avg_cost_per_workflow > 0\n",
        "+        assert len(result.node_summaries) == 1\n",
        "+        assert result.node_summaries[0].node_name == \"ChatModel\"\n",
        "+        assert len(result.scaling_projections) > 0\n",
        "+        assert \"1x\" in result.scaling_projections\n",
        "+\n",
        "+\n",
        "+class TestCacheEffectiveness:\n",
        "+    \"\"\"Test cache effectiveness analysis functions.\"\"\"\n",
        "+\n",
        "+    def test_calculate_cache_hit_rate_all_cached(self):\n",
        "+        \"\"\"Test cache hit rate when all traces have cache data.\"\"\"\n",
        "+        from analyze_cost import (\n",
        "+            calculate_cache_hit_rate,\n",
        "+            WorkflowCostAnalysis,\n",
        "+            CostBreakdown,\n",
        "+            TokenUsage,\n",
        "+        )\n",
        "+\n",
        "+        # Create workflow analysis with all traces having cache data\n",
        "+        costs_with_cache = [\n",
        "+            CostBreakdown(\n",
        "+                trace_id=\"1\",\n",
        "+                trace_name=\"ChatModel\",\n",
        "+                input_cost=0.001,\n",
        "+                output_cost=0.002,\n",
        "+                cache_cost=0.0001,\n",
        "+                total_cost=0.0031,\n",
        "+                token_usage=TokenUsage(1000, 500, 1500, cached_tokens=800),\n",
        "+            ),\n",
        "+            CostBreakdown(\n",
        "+                trace_id=\"2\",\n",
        "+                trace_name=\"Validator\",\n",
        "+                input_cost=0.001,\n",
        "+                output_cost=0.002,\n",
        "+                cache_cost=0.0002,\n",
        "+                total_cost=0.0032,\n",
        "+                token_usage=TokenUsage(1000, 500, 1500, cached_tokens=900),\n",
        "+            ),\n",
        "+        ]\n",
        "+\n",
        "+        workflows = [WorkflowCostAnalysis(\"wf1\", 0.0063, costs_with_cache, 3000)]\n",
        "+\n",
        "+        result = calculate_cache_hit_rate(workflows)\n",
        "+\n",
        "+        # All 2 traces have cache data = 100%\n",
        "+        assert result == pytest.approx(100.0, abs=0.01)\n",
        "+\n",
        "+    def test_calculate_cache_hit_rate_partial_cached(self):\n",
        "+        \"\"\"Test cache hit rate when only some traces have cache data.\"\"\"\n",
        "+        from analyze_cost import (\n",
        "+            calculate_cache_hit_rate,\n",
        "+            WorkflowCostAnalysis,\n",
        "+            CostBreakdown,\n",
        "+            TokenUsage,\n",
        "+        )\n",
        "+\n",
        "+        # Mix of cached and non-cached traces\n",
        "+        costs_mixed = [\n",
        "+            CostBreakdown(\n",
        "+                trace_id=\"1\",\n",
        "+                trace_name=\"ChatModel\",\n",
        "+                input_cost=0.001,\n",
        "+                output_cost=0.002,\n",
        "+                cache_cost=0.0001,\n",
        "+                total_cost=0.0031,\n",
        "+                token_usage=TokenUsage(1000, 500, 1500, cached_tokens=800),\n",
        "+            ),\n",
        "+            CostBreakdown(\n",
        "+                trace_id=\"2\",\n",
        "+                trace_name=\"Validator\",\n",
        "+                input_cost=0.001,\n",
        "+                output_cost=0.002,\n",
        "+                cache_cost=0.0,\n",
        "+                total_cost=0.003,\n",
        "+                token_usage=TokenUsage(1000, 500, 1500, cached_tokens=None),\n",
        "+            ),\n",
        "+            CostBreakdown(\n",
        "+                trace_id=\"3\",\n",
        "+                trace_name=\"Parser\",\n",
        "+                input_cost=0.001,\n",
        "+                output_cost=0.002,\n",
        "+                cache_cost=0.0,\n",
        "+                total_cost=0.003,\n",
        "+                token_usage=TokenUsage(1000, 500, 1500, cached_tokens=None),\n",
        "+            ),\n",
        "+        ]\n",
        "+\n",
        "+        workflows = [WorkflowCostAnalysis(\"wf1\", 0.0091, costs_mixed, 4500)]\n",
        "+\n",
        "+        result = calculate_cache_hit_rate(workflows)\n",
        "+\n",
        "+        # 1 out of 3 traces have cache data = 33.33%\n",
        "+        assert result == pytest.approx(33.33, abs=0.01)\n",
        "+\n",
        "+    def test_calculate_cache_hit_rate_no_traces(self):\n",
        "+        \"\"\"Test cache hit rate with no traces returns 0.\"\"\"\n",
        "+        from analyze_cost import calculate_cache_hit_rate\n",
        "+\n",
        "+        result = calculate_cache_hit_rate([])\n",
        "+\n",
        "+        assert result == 0.0\n",
        "+\n",
        "+    def test_calculate_cache_hit_rate_no_cache_data(self):\n",
        "+        \"\"\"Test cache hit rate when no traces have cache data.\"\"\"\n",
        "+        from analyze_cost import (\n",
        "+            calculate_cache_hit_rate,\n",
        "+            WorkflowCostAnalysis,\n",
        "+            CostBreakdown,\n",
        "+            TokenUsage,\n",
        "+        )\n",
        "+\n",
        "+        costs_no_cache = [\n",
        "+            CostBreakdown(\n",
        "+                trace_id=\"1\",\n",
        "+                trace_name=\"ChatModel\",\n",
        "+                input_cost=0.001,\n",
        "+                output_cost=0.002,\n",
        "+                cache_cost=0.0,\n",
        "+                total_cost=0.003,\n",
        "+                token_usage=TokenUsage(1000, 500, 1500, cached_tokens=None),\n",
        "+            ),\n",
        "+        ]\n",
        "+\n",
        "+        workflows = [WorkflowCostAnalysis(\"wf1\", 0.003, costs_no_cache, 1500)]\n",
        "+\n",
        "+        result = calculate_cache_hit_rate(workflows)\n",
        "+\n",
        "+        # 0 out of 1 traces have cache data = 0%\n",
        "+        assert result == 0.0\n",
        "+\n",
        "+    def test_calculate_cache_savings_with_cache(self):\n",
        "+        \"\"\"Test calculating cost savings from cache usage.\"\"\"\n",
        "+        from analyze_cost import (\n",
        "+            calculate_cache_savings,\n",
        "+            WorkflowCostAnalysis,\n",
        "+            CostBreakdown,\n",
        "+            TokenUsage,\n",
        "+            PricingConfig,\n",
        "+        )\n",
        "+\n",
        "+        # Create pricing config\n",
        "+        pricing = PricingConfig(\n",
        "+            model_name=\"Test Model\",\n",
        "+            input_tokens_per_1k=0.00125,  # $1.25 per 1M input tokens\n",
        "+            output_tokens_per_1k=0.005,  # $5.00 per 1M output tokens\n",
        "+            cache_read_per_1k=0.0003125,  # $0.3125 per 1M cache read tokens\n",
        "+        )\n",
        "+\n",
        "+        # Trace with 1000 input tokens, 800 cached\n",
        "+        # Without cache: 1000 * 0.00125 / 1000 = $0.00125\n",
        "+        # With cache: 800 * 0.0003125 / 1000 = $0.00025\n",
        "+        # Savings: $0.001 per trace\n",
        "+        costs_with_cache = [\n",
        "+            CostBreakdown(\n",
        "+                trace_id=\"1\",\n",
        "+                trace_name=\"ChatModel\",\n",
        "+                input_cost=0.00125,\n",
        "+                output_cost=0.0025,\n",
        "+                cache_cost=0.00025,\n",
        "+                total_cost=0.004,\n",
        "+                token_usage=TokenUsage(1000, 500, 1500, cached_tokens=800),\n",
        "+            ),\n",
        "+        ]\n",
        "+\n",
        "+        workflows = [WorkflowCostAnalysis(\"wf1\", 0.004, costs_with_cache, 1500)]\n",
        "+\n",
        "+        result = calculate_cache_savings(workflows, pricing)\n",
        "+\n",
        "+        # Savings: (800 tokens * input_price) - (800 tokens * cache_price)\n",
        "+        # = (800 * 0.00125 / 1000) - (800 * 0.0003125 / 1000)\n",
        "+        # = 0.001 - 0.00025 = 0.00075\n",
        "+        assert result == pytest.approx(0.00075, abs=0.00001)\n",
        "+\n",
        "+    def test_calculate_cache_savings_no_cache(self):\n",
        "+        \"\"\"Test that no cache usage results in zero savings.\"\"\"\n",
        "+        from analyze_cost import (\n",
        "+            calculate_cache_savings,\n",
        "+            WorkflowCostAnalysis,\n",
        "+            CostBreakdown,\n",
        "+            TokenUsage,\n",
        "+            PricingConfig,\n",
        "+        )\n",
        "+\n",
        "+        pricing = PricingConfig(\n",
        "+            model_name=\"Test Model\",\n",
        "+            input_tokens_per_1k=0.00125,\n",
        "+            output_tokens_per_1k=0.005,\n",
        "+        )\n",
        "+\n",
        "+        costs_no_cache = [\n",
        "+            CostBreakdown(\n",
        "+                trace_id=\"1\",\n",
        "+                trace_name=\"ChatModel\",\n",
        "+                input_cost=0.00125,\n",
        "+                output_cost=0.0025,\n",
        "+                cache_cost=0.0,\n",
        "+                total_cost=0.00375,\n",
        "+                token_usage=TokenUsage(1000, 500, 1500, cached_tokens=None),\n",
        "+            ),\n",
        "+        ]\n",
        "+\n",
        "+        workflows = [WorkflowCostAnalysis(\"wf1\", 0.00375, costs_no_cache, 1500)]\n",
        "+\n",
        "+        result = calculate_cache_savings(workflows, pricing)\n",
        "+\n",
        "+        # No cache usage = no savings\n",
        "+        assert result == 0.0\n",
        "+\n",
        "+    def test_calculate_cache_savings_multiple_traces(self):\n",
        "+        \"\"\"Test calculating savings across multiple traces.\"\"\"\n",
        "+        from analyze_cost import (\n",
        "+            calculate_cache_savings,\n",
        "+            WorkflowCostAnalysis,\n",
        "+            CostBreakdown,\n",
        "+            TokenUsage,\n",
        "+            PricingConfig,\n",
        "+        )\n",
        "+\n",
        "+        pricing = PricingConfig(\n",
        "+            model_name=\"Test Model\",\n",
        "+            input_tokens_per_1k=0.00125,\n",
        "+            output_tokens_per_1k=0.005,\n",
        "+            cache_read_per_1k=0.0003125,\n",
        "+        )\n",
        "+\n",
        "+        # Two traces with cache, one without\n",
        "+        costs_mixed = [\n",
        "+            CostBreakdown(\n",
        "+                trace_id=\"1\",\n",
        "+                trace_name=\"ChatModel\",\n",
        "+                input_cost=0.00125,\n",
        "+                output_cost=0.0025,\n",
        "+                cache_cost=0.00025,\n",
        "+                total_cost=0.004,\n",
        "+                token_usage=TokenUsage(1000, 500, 1500, cached_tokens=800),\n",
        "+            ),\n",
        "+            CostBreakdown(\n",
        "+                trace_id=\"2\",\n",
        "+                trace_name=\"Validator\",\n",
        "+                input_cost=0.001,\n",
        "+                output_cost=0.002,\n",
        "+                cache_cost=0.0002,\n",
        "+                total_cost=0.0032,\n",
        "+                token_usage=TokenUsage(800, 400, 1200, cached_tokens=600),\n",
        "+            ),\n",
        "+            CostBreakdown(\n",
        "+                trace_id=\"3\",\n",
        "+                trace_name=\"Parser\",\n",
        "+                input_cost=0.001,\n",
        "+                output_cost=0.002,\n",
        "+                cache_cost=0.0,\n",
        "+                total_cost=0.003,\n",
        "+                token_usage=TokenUsage(800, 400, 1200, cached_tokens=None),\n",
        "+            ),\n",
        "+        ]\n",
        "+\n",
        "+        workflows = [WorkflowCostAnalysis(\"wf1\", 0.0102, costs_mixed, 3900)]\n",
        "+\n",
        "+        result = calculate_cache_savings(workflows, pricing)\n",
        "+\n",
        "+        # Trace 1: (800 * 0.00125 / 1000) - (800 * 0.0003125 / 1000) = 0.00075\n",
        "+        # Trace 2: (600 * 0.00125 / 1000) - (600 * 0.0003125 / 1000) = 0.0005625\n",
        "+        # Trace 3: 0 (no cache)\n",
        "+        # Total: 0.00075 + 0.0005625 = 0.0013125\n",
        "+        assert result == pytest.approx(0.0013125, abs=0.00001)\n",
        "+\n",
        "+    def test_calculate_cache_savings_no_cache_pricing(self):\n",
        "+        \"\"\"Test that missing cache pricing returns zero savings.\"\"\"\n",
        "+        from analyze_cost import (\n",
        "+            calculate_cache_savings,\n",
        "+            WorkflowCostAnalysis,\n",
        "+            CostBreakdown,\n",
        "+            TokenUsage,\n",
        "+            PricingConfig,\n",
        "+        )\n",
        "+\n",
        "+        # Pricing without cache_read_per_1k\n",
        "+        pricing = PricingConfig(\n",
        "+            model_name=\"Test Model\",\n",
        "+            input_tokens_per_1k=0.00125,\n",
        "+            output_tokens_per_1k=0.005,\n",
        "+        )\n",
        "+\n",
        "+        costs_with_cache = [\n",
        "+            CostBreakdown(\n",
        "+                trace_id=\"1\",\n",
        "+                trace_name=\"ChatModel\",\n",
        "+                input_cost=0.00125,\n",
        "+                output_cost=0.0025,\n",
        "+                cache_cost=0.0,\n",
        "+                total_cost=0.00375,\n",
        "+                token_usage=TokenUsage(1000, 500, 1500, cached_tokens=800),\n",
        "+            ),\n",
        "+        ]\n",
        "+\n",
        "+        workflows = [WorkflowCostAnalysis(\"wf1\", 0.00375, costs_with_cache, 1500)]\n",
        "+\n",
        "+        result = calculate_cache_savings(workflows, pricing)\n",
        "+\n",
        "+        # No cache pricing configured = can't calculate savings\n",
        "+        assert result == 0.0\n",
        "+\n",
        "+    def test_compare_cached_vs_fresh_costs_with_cache(self):\n",
        "+        \"\"\"Test comparing costs with cache vs without cache.\"\"\"\n",
        "+        from analyze_cost import (\n",
        "+            compare_cached_vs_fresh_costs,\n",
        "+            WorkflowCostAnalysis,\n",
        "+            CostBreakdown,\n",
        "+            TokenUsage,\n",
        "+            PricingConfig,\n",
        "+        )\n",
        "+\n",
        "+        pricing = PricingConfig(\n",
        "+            model_name=\"Test Model\",\n",
        "+            input_tokens_per_1k=0.00125,\n",
        "+            output_tokens_per_1k=0.005,\n",
        "+            cache_read_per_1k=0.0003125,\n",
        "+        )\n",
        "+\n",
        "+        # Two traces: one with cache, one without\n",
        "+        costs = [\n",
        "+            CostBreakdown(\n",
        "+                trace_id=\"1\",\n",
        "+                trace_name=\"ChatModel\",\n",
        "+                input_cost=0.00125,\n",
        "+                output_cost=0.0025,\n",
        "+                cache_cost=0.00025,\n",
        "+                total_cost=0.004,\n",
        "+                token_usage=TokenUsage(1000, 500, 1500, cached_tokens=800),\n",
        "+            ),\n",
        "+            CostBreakdown(\n",
        "+                trace_id=\"2\",\n",
        "+                trace_name=\"Validator\",\n",
        "+                input_cost=0.001,\n",
        "+                output_cost=0.002,\n",
        "+                cache_cost=0.0,\n",
        "+                total_cost=0.003,\n",
        "+                token_usage=TokenUsage(800, 400, 1200, cached_tokens=None),\n",
        "+            ),\n",
        "+        ]\n",
        "+\n",
        "+        workflows = [WorkflowCostAnalysis(\"wf1\", 0.007, costs, 2700)]\n",
        "+\n",
        "+        result = compare_cached_vs_fresh_costs(workflows, pricing)\n",
        "+\n",
        "+        # Cost with cache: 0.004 + 0.003 = 0.007\n",
        "+        assert result.cost_with_cache == pytest.approx(0.007, abs=0.0001)\n",
        "+\n",
        "+        # Cost without cache: trace 1 would be 0.00125 + 0.0025 + (800 * 0.00125 / 1000)\n",
        "+        # = 0.00125 + 0.0025 + 0.001 = 0.00475\n",
        "+        # trace 2 stays same: 0.003\n",
        "+        # Total: 0.00775\n",
        "+        assert result.cost_without_cache == pytest.approx(0.00775, abs=0.0001)\n",
        "+\n",
        "+        # Savings: 0.00775 - 0.007 = 0.00075\n",
        "+        assert result.total_savings == pytest.approx(0.00075, abs=0.00001)\n",
        "+\n",
        "+        # Savings percent: (0.00075 / 0.00775) * 100 = 9.68%\n",
        "+        assert result.savings_percent == pytest.approx(9.68, abs=0.01)\n",
        "+\n",
        "+        assert result.traces_analyzed == 2\n",
        "+        assert result.traces_with_cache == 1\n",
        "+\n",
        "+    def test_compare_cached_vs_fresh_costs_no_cache(self):\n",
        "+        \"\"\"Test comparison when no traces use cache.\"\"\"\n",
        "+        from analyze_cost import (\n",
        "+            compare_cached_vs_fresh_costs,\n",
        "+            WorkflowCostAnalysis,\n",
        "+            CostBreakdown,\n",
        "+            TokenUsage,\n",
        "+            PricingConfig,\n",
        "+        )\n",
        "+\n",
        "+        pricing = PricingConfig(\n",
        "+            model_name=\"Test Model\",\n",
        "+            input_tokens_per_1k=0.00125,\n",
        "+            output_tokens_per_1k=0.005,\n",
        "+        )\n",
        "+\n",
        "+        costs = [\n",
        "+            CostBreakdown(\n",
        "+                trace_id=\"1\",\n",
        "+                trace_name=\"ChatModel\",\n",
        "+                input_cost=0.00125,\n",
        "+                output_cost=0.0025,\n",
        "+                cache_cost=0.0,\n",
        "+                total_cost=0.00375,\n",
        "+                token_usage=TokenUsage(1000, 500, 1500, cached_tokens=None),\n",
        "+            ),\n",
        "+        ]\n",
        "+\n",
        "+        workflows = [WorkflowCostAnalysis(\"wf1\", 0.00375, costs, 1500)]\n",
        "+\n",
        "+        result = compare_cached_vs_fresh_costs(workflows, pricing)\n",
        "+\n",
        "+        # No cache usage, so costs are identical\n",
        "+        assert result.cost_with_cache == pytest.approx(0.00375, abs=0.0001)\n",
        "+        assert result.cost_without_cache == pytest.approx(0.00375, abs=0.0001)\n",
        "+        assert result.total_savings == 0.0\n",
        "+        assert result.savings_percent == 0.0\n",
        "+        assert result.traces_analyzed == 1\n",
        "+        assert result.traces_with_cache == 0\n",
        "+\n",
        "+    def test_compare_cached_vs_fresh_costs_all_cached(self):\n",
        "+        \"\"\"Test comparison when all traces use cache.\"\"\"\n",
        "+        from analyze_cost import (\n",
        "+            compare_cached_vs_fresh_costs,\n",
        "+            WorkflowCostAnalysis,\n",
        "+            CostBreakdown,\n",
        "+            TokenUsage,\n",
        "+            PricingConfig,\n",
        "+        )\n",
        "+\n",
        "+        pricing = PricingConfig(\n",
        "+            model_name=\"Test Model\",\n",
        "+            input_tokens_per_1k=0.00125,\n",
        "+            output_tokens_per_1k=0.005,\n",
        "+            cache_read_per_1k=0.0003125,\n",
        "+        )\n",
        "+\n",
        "+        costs = [\n",
        "+            CostBreakdown(\n",
        "+                trace_id=\"1\",\n",
        "+                trace_name=\"ChatModel\",\n",
        "+                input_cost=0.00125,\n",
        "+                output_cost=0.0025,\n",
        "+                cache_cost=0.00025,\n",
        "+                total_cost=0.004,\n",
        "+                token_usage=TokenUsage(1000, 500, 1500, cached_tokens=800),\n",
        "+            ),\n",
        "+            CostBreakdown(\n",
        "+                trace_id=\"2\",\n",
        "+                trace_name=\"Validator\",\n",
        "+                input_cost=0.001,\n",
        "+                output_cost=0.002,\n",
        "+                cache_cost=0.00015,\n",
        "+                total_cost=0.00315,\n",
        "+                token_usage=TokenUsage(800, 400, 1200, cached_tokens=480),\n",
        "+            ),\n",
        "+        ]\n",
        "+\n",
        "+        workflows = [WorkflowCostAnalysis(\"wf1\", 0.00715, costs, 2700)]\n",
        "+\n",
        "+        result = compare_cached_vs_fresh_costs(workflows, pricing)\n",
        "+\n",
        "+        # Both traces use cache\n",
        "+        assert result.traces_analyzed == 2\n",
        "+        assert result.traces_with_cache == 2\n",
        "+\n",
        "+        # Savings should be positive\n",
        "+        assert result.total_savings > 0\n",
        "+        assert result.savings_percent > 0\n",
        "+        assert result.cost_without_cache > result.cost_with_cache\n",
        "+\n",
        "+\n",
        "+# Run tests with: pytest test_analyze_cost.py -v\n"
      ]
    },
    {
      "path": "test_analyze_failures.py",
      "status": "added",
      "additions": 426,
      "deletions": 0,
      "patch": "@@ -0,0 +1,426 @@\n+\"\"\"\n+Test suite for Phase 3C: Failure Pattern Analysis\n+\n+Following TDD methodology - tests written FIRST, then implementation.\n+Tests for failure detection, retry analysis, and quality risk assessment.\n+\n+Author: Generated with Claude Code (PDCA Framework)\n+Date: 2025-12-09\n+\"\"\"\n+\n+import pytest\n+from datetime import datetime, timezone\n+from analyze_failures import (\n+    detect_failures,\n+    classify_error,\n+    detect_retry_sequences,\n+    calculate_retry_success_rate,\n+    RetrySequence,\n+)\n+from analyze_traces import Trace, Workflow\n+\n+\n+class TestFailureDetection:\n+    \"\"\"Test failure detection from traces.\"\"\"\n+\n+    def test_detect_failures_single_failure(self):\n+        \"\"\"Test detecting a single failure in workflow.\"\"\"\n+        root = Trace(\n+            id=\"root-1\",\n+            name=\"LangGraph\",\n+            start_time=datetime(2025, 1, 1, 12, 0, 0, tzinfo=timezone.utc),\n+            end_time=datetime(2025, 1, 1, 12, 5, 0, tzinfo=timezone.utc),\n+            duration_seconds=300.0,\n+            status=\"success\",\n+            run_type=\"chain\",\n+            parent_id=None,\n+            child_ids=[\"child-1\"],\n+            inputs={},\n+            outputs={},\n+            error=None,\n+        )\n+\n+        child = Trace(\n+            id=\"child-1\",\n+            name=\"Validator\",\n+            start_time=datetime(2025, 1, 1, 12, 1, 0, tzinfo=timezone.utc),\n+            end_time=datetime(2025, 1, 1, 12, 2, 0, tzinfo=timezone.utc),\n+            duration_seconds=60.0,\n+            status=\"error\",\n+            run_type=\"chain\",\n+            parent_id=\"root-1\",\n+            child_ids=[],\n+            inputs={},\n+            outputs={},\n+            error=\"Validation failed: invalid spec\",\n+        )\n+\n+        workflow = Workflow(\n+            root_trace=root,\n+            nodes={\"Validator\": [child]},\n+            all_traces=[root, child],\n+        )\n+\n+        result = detect_failures(workflow)\n+\n+        assert result is not None\n+        assert len(result) == 1\n+        assert result[0].trace_id == \"child-1\"\n+        assert result[0].trace_name == \"Validator\"\n+        assert result[0].error_type == \"validation_failure\"\n+\n+    def test_detect_failures_no_failures(self):\n+        \"\"\"Test workflow with no failures.\"\"\"\n+        root = Trace(\n+            id=\"root-2\",\n+            name=\"LangGraph\",\n+            start_time=datetime(2025, 1, 1, 12, 0, 0, tzinfo=timezone.utc),\n+            end_time=datetime(2025, 1, 1, 12, 5, 0, tzinfo=timezone.utc),\n+            duration_seconds=300.0,\n+            status=\"success\",\n+            run_type=\"chain\",\n+            parent_id=None,\n+            child_ids=[],\n+            inputs={},\n+            outputs={},\n+            error=None,\n+        )\n+\n+        workflow = Workflow(\n+            root_trace=root,\n+            nodes={},\n+            all_traces=[root],\n+        )\n+\n+        result = detect_failures(workflow)\n+\n+        assert result is not None\n+        assert len(result) == 0\n+\n+    def test_classify_error_validation_failure(self):\n+        \"\"\"Test classifying validation error.\"\"\"\n+        error_msg = \"Validation failed: invalid specification\"\n+        result = classify_error(error_msg)\n+        assert result == \"validation_failure\"\n+\n+    def test_classify_error_api_timeout(self):\n+        \"\"\"Test classifying timeout error.\"\"\"\n+        error_msg = \"Request timed out after 30 seconds\"\n+        result = classify_error(error_msg)\n+        assert result == \"api_timeout\"\n+\n+    def test_classify_error_import_error(self):\n+        \"\"\"Test classifying import error.\"\"\"\n+        error_msg = \"Import failed: module not found\"\n+        result = classify_error(error_msg)\n+        assert result == \"import_error\"\n+\n+    def test_classify_error_llm_error(self):\n+        \"\"\"Test classifying LLM error.\"\"\"\n+        error_msg = \"Model generation failed: token limit exceeded\"\n+        result = classify_error(error_msg)\n+        assert result == \"llm_error\"\n+\n+    def test_classify_error_unknown(self):\n+        \"\"\"Test classifying unknown error.\"\"\"\n+        error_msg = \"Something went wrong\"\n+        result = classify_error(error_msg)\n+        assert result == \"unknown\"\n+\n+    def test_classify_error_none(self):\n+        \"\"\"Test classifying None error.\"\"\"\n+        result = classify_error(None)\n+        assert result == \"unknown\"\n+\n+\n+class TestRetryDetection:\n+    \"\"\"Test retry sequence detection.\"\"\"\n+\n+    def test_detect_retry_sequences_two_attempts(self):\n+        \"\"\"Test detecting retry sequence with 2 attempts.\"\"\"\n+        root = Trace(\n+            id=\"root-1\",\n+            name=\"LangGraph\",\n+            start_time=datetime(2025, 1, 1, 12, 0, 0, tzinfo=timezone.utc),\n+            end_time=datetime(2025, 1, 1, 12, 5, 0, tzinfo=timezone.utc),\n+            duration_seconds=300.0,\n+            status=\"success\",\n+            run_type=\"chain\",\n+            parent_id=None,\n+            child_ids=[\"attempt-1\", \"attempt-2\"],\n+            inputs={},\n+            outputs={},\n+            error=None,\n+        )\n+\n+        attempt1 = Trace(\n+            id=\"attempt-1\",\n+            name=\"ChatModel\",\n+            start_time=datetime(2025, 1, 1, 12, 1, 0, tzinfo=timezone.utc),\n+            end_time=datetime(2025, 1, 1, 12, 1, 30, tzinfo=timezone.utc),\n+            duration_seconds=30.0,\n+            status=\"error\",\n+            run_type=\"llm\",\n+            parent_id=\"root-1\",\n+            child_ids=[],\n+            inputs={},\n+            outputs={},\n+            error=\"Timeout\",\n+        )\n+\n+        attempt2 = Trace(\n+            id=\"attempt-2\",\n+            name=\"ChatModel\",\n+            start_time=datetime(2025, 1, 1, 12, 1, 35, tzinfo=timezone.utc),\n+            end_time=datetime(2025, 1, 1, 12, 2, 5, tzinfo=timezone.utc),\n+            duration_seconds=30.0,\n+            status=\"success\",\n+            run_type=\"llm\",\n+            parent_id=\"root-1\",\n+            child_ids=[],\n+            inputs={},\n+            outputs={},\n+            error=None,\n+        )\n+\n+        workflow = Workflow(\n+            root_trace=root,\n+            nodes={\"ChatModel\": [attempt1, attempt2]},\n+            all_traces=[root, attempt1, attempt2],\n+        )\n+\n+        result = detect_retry_sequences(workflow)\n+\n+        assert result is not None\n+        assert len(result) == 1\n+        assert result[0].node_name == \"ChatModel\"\n+        assert result[0].attempt_count == 2\n+        assert result[0].final_status == \"success\"\n+        assert result[0].total_duration_seconds == 60.0\n+\n+    def test_detect_retry_sequences_no_retries(self):\n+        \"\"\"Test workflow with no retry sequences.\"\"\"\n+        root = Trace(\n+            id=\"root-2\",\n+            name=\"LangGraph\",\n+            start_time=datetime(2025, 1, 1, 12, 0, 0, tzinfo=timezone.utc),\n+            end_time=datetime(2025, 1, 1, 12, 5, 0, tzinfo=timezone.utc),\n+            duration_seconds=300.0,\n+            status=\"success\",\n+            run_type=\"chain\",\n+            parent_id=None,\n+            child_ids=[\"child-1\"],\n+            inputs={},\n+            outputs={},\n+            error=None,\n+        )\n+\n+        child = Trace(\n+            id=\"child-1\",\n+            name=\"ChatModel\",\n+            start_time=datetime(2025, 1, 1, 12, 1, 0, tzinfo=timezone.utc),\n+            end_time=datetime(2025, 1, 1, 12, 2, 0, tzinfo=timezone.utc),\n+            duration_seconds=60.0,\n+            status=\"success\",\n+            run_type=\"llm\",\n+            parent_id=\"root-2\",\n+            child_ids=[],\n+            inputs={},\n+            outputs={},\n+            error=None,\n+        )\n+\n+        workflow = Workflow(\n+            root_trace=root,\n+            nodes={\"ChatModel\": [child]},\n+            all_traces=[root, child],\n+        )\n+\n+        result = detect_retry_sequences(workflow)\n+\n+        assert result is not None\n+        assert len(result) == 0\n+\n+    def test_calculate_retry_success_rate_all_succeed(self):\n+        \"\"\"Test retry success rate when all retries succeed.\"\"\"\n+        retry1 = RetrySequence(\n+            node_name=\"ChatModel\",\n+            workflow_id=\"wf-1\",\n+            attempt_count=2,\n+            attempts=[],\n+            final_status=\"success\",\n+            total_duration_seconds=60.0,\n+        )\n+\n+        retry2 = RetrySequence(\n+            node_name=\"Validator\",\n+            workflow_id=\"wf-2\",\n+            attempt_count=3,\n+            attempts=[],\n+            final_status=\"success\",\n+            total_duration_seconds=90.0,\n+        )\n+\n+        result = calculate_retry_success_rate([retry1, retry2])\n+\n+        assert result is not None\n+        assert result == pytest.approx(100.0, abs=0.01)\n+\n+    def test_calculate_retry_success_rate_partial_success(self):\n+        \"\"\"Test retry success rate with partial success.\"\"\"\n+        retry1 = RetrySequence(\n+            node_name=\"ChatModel\",\n+            workflow_id=\"wf-1\",\n+            attempt_count=2,\n+            attempts=[],\n+            final_status=\"success\",\n+            total_duration_seconds=60.0,\n+        )\n+\n+        retry2 = RetrySequence(\n+            node_name=\"Validator\",\n+            workflow_id=\"wf-2\",\n+            attempt_count=3,\n+            attempts=[],\n+            final_status=\"error\",\n+            total_duration_seconds=90.0,\n+        )\n+\n+        result = calculate_retry_success_rate([retry1, retry2])\n+\n+        assert result is not None\n+        assert result == pytest.approx(50.0, abs=0.01)\n+\n+    def test_calculate_retry_success_rate_empty_list(self):\n+        \"\"\"Test retry success rate with empty list.\"\"\"\n+        result = calculate_retry_success_rate([])\n+        assert result is None\n+\n+\n+class TestNodeFailureAnalysis:\n+    \"\"\"Test node-level failure analysis.\"\"\"\n+\n+    def test_analyze_node_failures_basic(self):\n+        \"\"\"Test basic node failure analysis.\"\"\"\n+        from analyze_failures import analyze_node_failures\n+\n+        # Create workflows with different failure patterns\n+        root1 = Trace(\n+            id=\"root-1\",\n+            name=\"LangGraph\",\n+            start_time=datetime(2025, 1, 1, 12, 0, 0, tzinfo=timezone.utc),\n+            end_time=datetime(2025, 1, 1, 12, 5, 0, tzinfo=timezone.utc),\n+            duration_seconds=300.0,\n+            status=\"success\",\n+            run_type=\"chain\",\n+            parent_id=None,\n+            child_ids=[\"child-1\", \"child-2\"],\n+            inputs={},\n+            outputs={},\n+            error=None,\n+        )\n+\n+        # Validator that fails\n+        child1 = Trace(\n+            id=\"child-1\",\n+            name=\"Validator\",\n+            start_time=datetime(2025, 1, 1, 12, 1, 0, tzinfo=timezone.utc),\n+            end_time=datetime(2025, 1, 1, 12, 2, 0, tzinfo=timezone.utc),\n+            duration_seconds=60.0,\n+            status=\"error\",\n+            run_type=\"chain\",\n+            parent_id=\"root-1\",\n+            child_ids=[],\n+            inputs={},\n+            outputs={},\n+            error=\"Validation failed\",\n+        )\n+\n+        # ChatModel that succeeds\n+        child2 = Trace(\n+            id=\"child-2\",\n+            name=\"ChatModel\",\n+            start_time=datetime(2025, 1, 1, 12, 2, 0, tzinfo=timezone.utc),\n+            end_time=datetime(2025, 1, 1, 12, 3, 0, tzinfo=timezone.utc),\n+            duration_seconds=60.0,\n+            status=\"success\",\n+            run_type=\"llm\",\n+            parent_id=\"root-1\",\n+            child_ids=[],\n+            inputs={},\n+            outputs={},\n+            error=None,\n+        )\n+\n+        workflow1 = Workflow(\n+            root_trace=root1,\n+            nodes={\"Validator\": [child1], \"ChatModel\": [child2]},\n+            all_traces=[root1, child1, child2],\n+        )\n+\n+        result = analyze_node_failures([workflow1])\n+\n+        assert result is not None\n+        assert len(result) >= 1\n+\n+        # Find Validator stats\n+        validator_stats = next((s for s in result if s.node_name == \"Validator\"), None)\n+        assert validator_stats is not None\n+        assert validator_stats.total_executions == 1\n+        assert validator_stats.failure_count == 1\n+        assert validator_stats.failure_rate_percent == pytest.approx(100.0, abs=0.01)\n+\n+\n+class TestMainAnalysisFunction:\n+    \"\"\"Test main analyze_failures() orchestration function.\"\"\"\n+\n+    def test_analyze_failures_integration(self):\n+        \"\"\"Test complete failure analysis workflow.\"\"\"\n+        from analyze_failures import analyze_failures\n+\n+        # Create workflow with mixed success/failure\n+        root = Trace(\n+            id=\"root-1\",\n+            name=\"LangGraph\",\n+            start_time=datetime(2025, 1, 1, 12, 0, 0, tzinfo=timezone.utc),\n+            end_time=datetime(2025, 1, 1, 12, 5, 0, tzinfo=timezone.utc),\n+            duration_seconds=300.0,\n+            status=\"success\",\n+            run_type=\"chain\",\n+            parent_id=None,\n+            child_ids=[\"child-1\"],\n+            inputs={},\n+            outputs={},\n+            error=None,\n+        )\n+\n+        child = Trace(\n+            id=\"child-1\",\n+            name=\"Validator\",\n+            start_time=datetime(2025, 1, 1, 12, 1, 0, tzinfo=timezone.utc),\n+            end_time=datetime(2025, 1, 1, 12, 2, 0, tzinfo=timezone.utc),\n+            duration_seconds=60.0,\n+            status=\"error\",\n+            run_type=\"chain\",\n+            parent_id=\"root-1\",\n+            child_ids=[],\n+            inputs={},\n+            outputs={},\n+            error=\"Validation failed\",\n+        )\n+\n+        workflow = Workflow(\n+            root_trace=root,\n+            nodes={\"Validator\": [child]},\n+            all_traces=[root, child],\n+        )\n+\n+        result = analyze_failures([workflow])\n+\n+        assert result is not None\n+        assert result.total_workflows == 1\n+        assert len(result.node_failure_stats) >= 1\n+        assert result.error_type_distribution is not None\n+\n+\n+# Run tests with: pytest test_analyze_failures.py -v",
      "patch_lines": [
        "@@ -0,0 +1,426 @@\n",
        "+\"\"\"\n",
        "+Test suite for Phase 3C: Failure Pattern Analysis\n",
        "+\n",
        "+Following TDD methodology - tests written FIRST, then implementation.\n",
        "+Tests for failure detection, retry analysis, and quality risk assessment.\n",
        "+\n",
        "+Author: Generated with Claude Code (PDCA Framework)\n",
        "+Date: 2025-12-09\n",
        "+\"\"\"\n",
        "+\n",
        "+import pytest\n",
        "+from datetime import datetime, timezone\n",
        "+from analyze_failures import (\n",
        "+    detect_failures,\n",
        "+    classify_error,\n",
        "+    detect_retry_sequences,\n",
        "+    calculate_retry_success_rate,\n",
        "+    RetrySequence,\n",
        "+)\n",
        "+from analyze_traces import Trace, Workflow\n",
        "+\n",
        "+\n",
        "+class TestFailureDetection:\n",
        "+    \"\"\"Test failure detection from traces.\"\"\"\n",
        "+\n",
        "+    def test_detect_failures_single_failure(self):\n",
        "+        \"\"\"Test detecting a single failure in workflow.\"\"\"\n",
        "+        root = Trace(\n",
        "+            id=\"root-1\",\n",
        "+            name=\"LangGraph\",\n",
        "+            start_time=datetime(2025, 1, 1, 12, 0, 0, tzinfo=timezone.utc),\n",
        "+            end_time=datetime(2025, 1, 1, 12, 5, 0, tzinfo=timezone.utc),\n",
        "+            duration_seconds=300.0,\n",
        "+            status=\"success\",\n",
        "+            run_type=\"chain\",\n",
        "+            parent_id=None,\n",
        "+            child_ids=[\"child-1\"],\n",
        "+            inputs={},\n",
        "+            outputs={},\n",
        "+            error=None,\n",
        "+        )\n",
        "+\n",
        "+        child = Trace(\n",
        "+            id=\"child-1\",\n",
        "+            name=\"Validator\",\n",
        "+            start_time=datetime(2025, 1, 1, 12, 1, 0, tzinfo=timezone.utc),\n",
        "+            end_time=datetime(2025, 1, 1, 12, 2, 0, tzinfo=timezone.utc),\n",
        "+            duration_seconds=60.0,\n",
        "+            status=\"error\",\n",
        "+            run_type=\"chain\",\n",
        "+            parent_id=\"root-1\",\n",
        "+            child_ids=[],\n",
        "+            inputs={},\n",
        "+            outputs={},\n",
        "+            error=\"Validation failed: invalid spec\",\n",
        "+        )\n",
        "+\n",
        "+        workflow = Workflow(\n",
        "+            root_trace=root,\n",
        "+            nodes={\"Validator\": [child]},\n",
        "+            all_traces=[root, child],\n",
        "+        )\n",
        "+\n",
        "+        result = detect_failures(workflow)\n",
        "+\n",
        "+        assert result is not None\n",
        "+        assert len(result) == 1\n",
        "+        assert result[0].trace_id == \"child-1\"\n",
        "+        assert result[0].trace_name == \"Validator\"\n",
        "+        assert result[0].error_type == \"validation_failure\"\n",
        "+\n",
        "+    def test_detect_failures_no_failures(self):\n",
        "+        \"\"\"Test workflow with no failures.\"\"\"\n",
        "+        root = Trace(\n",
        "+            id=\"root-2\",\n",
        "+            name=\"LangGraph\",\n",
        "+            start_time=datetime(2025, 1, 1, 12, 0, 0, tzinfo=timezone.utc),\n",
        "+            end_time=datetime(2025, 1, 1, 12, 5, 0, tzinfo=timezone.utc),\n",
        "+            duration_seconds=300.0,\n",
        "+            status=\"success\",\n",
        "+            run_type=\"chain\",\n",
        "+            parent_id=None,\n",
        "+            child_ids=[],\n",
        "+            inputs={},\n",
        "+            outputs={},\n",
        "+            error=None,\n",
        "+        )\n",
        "+\n",
        "+        workflow = Workflow(\n",
        "+            root_trace=root,\n",
        "+            nodes={},\n",
        "+            all_traces=[root],\n",
        "+        )\n",
        "+\n",
        "+        result = detect_failures(workflow)\n",
        "+\n",
        "+        assert result is not None\n",
        "+        assert len(result) == 0\n",
        "+\n",
        "+    def test_classify_error_validation_failure(self):\n",
        "+        \"\"\"Test classifying validation error.\"\"\"\n",
        "+        error_msg = \"Validation failed: invalid specification\"\n",
        "+        result = classify_error(error_msg)\n",
        "+        assert result == \"validation_failure\"\n",
        "+\n",
        "+    def test_classify_error_api_timeout(self):\n",
        "+        \"\"\"Test classifying timeout error.\"\"\"\n",
        "+        error_msg = \"Request timed out after 30 seconds\"\n",
        "+        result = classify_error(error_msg)\n",
        "+        assert result == \"api_timeout\"\n",
        "+\n",
        "+    def test_classify_error_import_error(self):\n",
        "+        \"\"\"Test classifying import error.\"\"\"\n",
        "+        error_msg = \"Import failed: module not found\"\n",
        "+        result = classify_error(error_msg)\n",
        "+        assert result == \"import_error\"\n",
        "+\n",
        "+    def test_classify_error_llm_error(self):\n",
        "+        \"\"\"Test classifying LLM error.\"\"\"\n",
        "+        error_msg = \"Model generation failed: token limit exceeded\"\n",
        "+        result = classify_error(error_msg)\n",
        "+        assert result == \"llm_error\"\n",
        "+\n",
        "+    def test_classify_error_unknown(self):\n",
        "+        \"\"\"Test classifying unknown error.\"\"\"\n",
        "+        error_msg = \"Something went wrong\"\n",
        "+        result = classify_error(error_msg)\n",
        "+        assert result == \"unknown\"\n",
        "+\n",
        "+    def test_classify_error_none(self):\n",
        "+        \"\"\"Test classifying None error.\"\"\"\n",
        "+        result = classify_error(None)\n",
        "+        assert result == \"unknown\"\n",
        "+\n",
        "+\n",
        "+class TestRetryDetection:\n",
        "+    \"\"\"Test retry sequence detection.\"\"\"\n",
        "+\n",
        "+    def test_detect_retry_sequences_two_attempts(self):\n",
        "+        \"\"\"Test detecting retry sequence with 2 attempts.\"\"\"\n",
        "+        root = Trace(\n",
        "+            id=\"root-1\",\n",
        "+            name=\"LangGraph\",\n",
        "+            start_time=datetime(2025, 1, 1, 12, 0, 0, tzinfo=timezone.utc),\n",
        "+            end_time=datetime(2025, 1, 1, 12, 5, 0, tzinfo=timezone.utc),\n",
        "+            duration_seconds=300.0,\n",
        "+            status=\"success\",\n",
        "+            run_type=\"chain\",\n",
        "+            parent_id=None,\n",
        "+            child_ids=[\"attempt-1\", \"attempt-2\"],\n",
        "+            inputs={},\n",
        "+            outputs={},\n",
        "+            error=None,\n",
        "+        )\n",
        "+\n",
        "+        attempt1 = Trace(\n",
        "+            id=\"attempt-1\",\n",
        "+            name=\"ChatModel\",\n",
        "+            start_time=datetime(2025, 1, 1, 12, 1, 0, tzinfo=timezone.utc),\n",
        "+            end_time=datetime(2025, 1, 1, 12, 1, 30, tzinfo=timezone.utc),\n",
        "+            duration_seconds=30.0,\n",
        "+            status=\"error\",\n",
        "+            run_type=\"llm\",\n",
        "+            parent_id=\"root-1\",\n",
        "+            child_ids=[],\n",
        "+            inputs={},\n",
        "+            outputs={},\n",
        "+            error=\"Timeout\",\n",
        "+        )\n",
        "+\n",
        "+        attempt2 = Trace(\n",
        "+            id=\"attempt-2\",\n",
        "+            name=\"ChatModel\",\n",
        "+            start_time=datetime(2025, 1, 1, 12, 1, 35, tzinfo=timezone.utc),\n",
        "+            end_time=datetime(2025, 1, 1, 12, 2, 5, tzinfo=timezone.utc),\n",
        "+            duration_seconds=30.0,\n",
        "+            status=\"success\",\n",
        "+            run_type=\"llm\",\n",
        "+            parent_id=\"root-1\",\n",
        "+            child_ids=[],\n",
        "+            inputs={},\n",
        "+            outputs={},\n",
        "+            error=None,\n",
        "+        )\n",
        "+\n",
        "+        workflow = Workflow(\n",
        "+            root_trace=root,\n",
        "+            nodes={\"ChatModel\": [attempt1, attempt2]},\n",
        "+            all_traces=[root, attempt1, attempt2],\n",
        "+        )\n",
        "+\n",
        "+        result = detect_retry_sequences(workflow)\n",
        "+\n",
        "+        assert result is not None\n",
        "+        assert len(result) == 1\n",
        "+        assert result[0].node_name == \"ChatModel\"\n",
        "+        assert result[0].attempt_count == 2\n",
        "+        assert result[0].final_status == \"success\"\n",
        "+        assert result[0].total_duration_seconds == 60.0\n",
        "+\n",
        "+    def test_detect_retry_sequences_no_retries(self):\n",
        "+        \"\"\"Test workflow with no retry sequences.\"\"\"\n",
        "+        root = Trace(\n",
        "+            id=\"root-2\",\n",
        "+            name=\"LangGraph\",\n",
        "+            start_time=datetime(2025, 1, 1, 12, 0, 0, tzinfo=timezone.utc),\n",
        "+            end_time=datetime(2025, 1, 1, 12, 5, 0, tzinfo=timezone.utc),\n",
        "+            duration_seconds=300.0,\n",
        "+            status=\"success\",\n",
        "+            run_type=\"chain\",\n",
        "+            parent_id=None,\n",
        "+            child_ids=[\"child-1\"],\n",
        "+            inputs={},\n",
        "+            outputs={},\n",
        "+            error=None,\n",
        "+        )\n",
        "+\n",
        "+        child = Trace(\n",
        "+            id=\"child-1\",\n",
        "+            name=\"ChatModel\",\n",
        "+            start_time=datetime(2025, 1, 1, 12, 1, 0, tzinfo=timezone.utc),\n",
        "+            end_time=datetime(2025, 1, 1, 12, 2, 0, tzinfo=timezone.utc),\n",
        "+            duration_seconds=60.0,\n",
        "+            status=\"success\",\n",
        "+            run_type=\"llm\",\n",
        "+            parent_id=\"root-2\",\n",
        "+            child_ids=[],\n",
        "+            inputs={},\n",
        "+            outputs={},\n",
        "+            error=None,\n",
        "+        )\n",
        "+\n",
        "+        workflow = Workflow(\n",
        "+            root_trace=root,\n",
        "+            nodes={\"ChatModel\": [child]},\n",
        "+            all_traces=[root, child],\n",
        "+        )\n",
        "+\n",
        "+        result = detect_retry_sequences(workflow)\n",
        "+\n",
        "+        assert result is not None\n",
        "+        assert len(result) == 0\n",
        "+\n",
        "+    def test_calculate_retry_success_rate_all_succeed(self):\n",
        "+        \"\"\"Test retry success rate when all retries succeed.\"\"\"\n",
        "+        retry1 = RetrySequence(\n",
        "+            node_name=\"ChatModel\",\n",
        "+            workflow_id=\"wf-1\",\n",
        "+            attempt_count=2,\n",
        "+            attempts=[],\n",
        "+            final_status=\"success\",\n",
        "+            total_duration_seconds=60.0,\n",
        "+        )\n",
        "+\n",
        "+        retry2 = RetrySequence(\n",
        "+            node_name=\"Validator\",\n",
        "+            workflow_id=\"wf-2\",\n",
        "+            attempt_count=3,\n",
        "+            attempts=[],\n",
        "+            final_status=\"success\",\n",
        "+            total_duration_seconds=90.0,\n",
        "+        )\n",
        "+\n",
        "+        result = calculate_retry_success_rate([retry1, retry2])\n",
        "+\n",
        "+        assert result is not None\n",
        "+        assert result == pytest.approx(100.0, abs=0.01)\n",
        "+\n",
        "+    def test_calculate_retry_success_rate_partial_success(self):\n",
        "+        \"\"\"Test retry success rate with partial success.\"\"\"\n",
        "+        retry1 = RetrySequence(\n",
        "+            node_name=\"ChatModel\",\n",
        "+            workflow_id=\"wf-1\",\n",
        "+            attempt_count=2,\n",
        "+            attempts=[],\n",
        "+            final_status=\"success\",\n",
        "+            total_duration_seconds=60.0,\n",
        "+        )\n",
        "+\n",
        "+        retry2 = RetrySequence(\n",
        "+            node_name=\"Validator\",\n",
        "+            workflow_id=\"wf-2\",\n",
        "+            attempt_count=3,\n",
        "+            attempts=[],\n",
        "+            final_status=\"error\",\n",
        "+            total_duration_seconds=90.0,\n",
        "+        )\n",
        "+\n",
        "+        result = calculate_retry_success_rate([retry1, retry2])\n",
        "+\n",
        "+        assert result is not None\n",
        "+        assert result == pytest.approx(50.0, abs=0.01)\n",
        "+\n",
        "+    def test_calculate_retry_success_rate_empty_list(self):\n",
        "+        \"\"\"Test retry success rate with empty list.\"\"\"\n",
        "+        result = calculate_retry_success_rate([])\n",
        "+        assert result is None\n",
        "+\n",
        "+\n",
        "+class TestNodeFailureAnalysis:\n",
        "+    \"\"\"Test node-level failure analysis.\"\"\"\n",
        "+\n",
        "+    def test_analyze_node_failures_basic(self):\n",
        "+        \"\"\"Test basic node failure analysis.\"\"\"\n",
        "+        from analyze_failures import analyze_node_failures\n",
        "+\n",
        "+        # Create workflows with different failure patterns\n",
        "+        root1 = Trace(\n",
        "+            id=\"root-1\",\n",
        "+            name=\"LangGraph\",\n",
        "+            start_time=datetime(2025, 1, 1, 12, 0, 0, tzinfo=timezone.utc),\n",
        "+            end_time=datetime(2025, 1, 1, 12, 5, 0, tzinfo=timezone.utc),\n",
        "+            duration_seconds=300.0,\n",
        "+            status=\"success\",\n",
        "+            run_type=\"chain\",\n",
        "+            parent_id=None,\n",
        "+            child_ids=[\"child-1\", \"child-2\"],\n",
        "+            inputs={},\n",
        "+            outputs={},\n",
        "+            error=None,\n",
        "+        )\n",
        "+\n",
        "+        # Validator that fails\n",
        "+        child1 = Trace(\n",
        "+            id=\"child-1\",\n",
        "+            name=\"Validator\",\n",
        "+            start_time=datetime(2025, 1, 1, 12, 1, 0, tzinfo=timezone.utc),\n",
        "+            end_time=datetime(2025, 1, 1, 12, 2, 0, tzinfo=timezone.utc),\n",
        "+            duration_seconds=60.0,\n",
        "+            status=\"error\",\n",
        "+            run_type=\"chain\",\n",
        "+            parent_id=\"root-1\",\n",
        "+            child_ids=[],\n",
        "+            inputs={},\n",
        "+            outputs={},\n",
        "+            error=\"Validation failed\",\n",
        "+        )\n",
        "+\n",
        "+        # ChatModel that succeeds\n",
        "+        child2 = Trace(\n",
        "+            id=\"child-2\",\n",
        "+            name=\"ChatModel\",\n",
        "+            start_time=datetime(2025, 1, 1, 12, 2, 0, tzinfo=timezone.utc),\n",
        "+            end_time=datetime(2025, 1, 1, 12, 3, 0, tzinfo=timezone.utc),\n",
        "+            duration_seconds=60.0,\n",
        "+            status=\"success\",\n",
        "+            run_type=\"llm\",\n",
        "+            parent_id=\"root-1\",\n",
        "+            child_ids=[],\n",
        "+            inputs={},\n",
        "+            outputs={},\n",
        "+            error=None,\n",
        "+        )\n",
        "+\n",
        "+        workflow1 = Workflow(\n",
        "+            root_trace=root1,\n",
        "+            nodes={\"Validator\": [child1], \"ChatModel\": [child2]},\n",
        "+            all_traces=[root1, child1, child2],\n",
        "+        )\n",
        "+\n",
        "+        result = analyze_node_failures([workflow1])\n",
        "+\n",
        "+        assert result is not None\n",
        "+        assert len(result) >= 1\n",
        "+\n",
        "+        # Find Validator stats\n",
        "+        validator_stats = next((s for s in result if s.node_name == \"Validator\"), None)\n",
        "+        assert validator_stats is not None\n",
        "+        assert validator_stats.total_executions == 1\n",
        "+        assert validator_stats.failure_count == 1\n",
        "+        assert validator_stats.failure_rate_percent == pytest.approx(100.0, abs=0.01)\n",
        "+\n",
        "+\n",
        "+class TestMainAnalysisFunction:\n",
        "+    \"\"\"Test main analyze_failures() orchestration function.\"\"\"\n",
        "+\n",
        "+    def test_analyze_failures_integration(self):\n",
        "+        \"\"\"Test complete failure analysis workflow.\"\"\"\n",
        "+        from analyze_failures import analyze_failures\n",
        "+\n",
        "+        # Create workflow with mixed success/failure\n",
        "+        root = Trace(\n",
        "+            id=\"root-1\",\n",
        "+            name=\"LangGraph\",\n",
        "+            start_time=datetime(2025, 1, 1, 12, 0, 0, tzinfo=timezone.utc),\n",
        "+            end_time=datetime(2025, 1, 1, 12, 5, 0, tzinfo=timezone.utc),\n",
        "+            duration_seconds=300.0,\n",
        "+            status=\"success\",\n",
        "+            run_type=\"chain\",\n",
        "+            parent_id=None,\n",
        "+            child_ids=[\"child-1\"],\n",
        "+            inputs={},\n",
        "+            outputs={},\n",
        "+            error=None,\n",
        "+        )\n",
        "+\n",
        "+        child = Trace(\n",
        "+            id=\"child-1\",\n",
        "+            name=\"Validator\",\n",
        "+            start_time=datetime(2025, 1, 1, 12, 1, 0, tzinfo=timezone.utc),\n",
        "+            end_time=datetime(2025, 1, 1, 12, 2, 0, tzinfo=timezone.utc),\n",
        "+            duration_seconds=60.0,\n",
        "+            status=\"error\",\n",
        "+            run_type=\"chain\",\n",
        "+            parent_id=\"root-1\",\n",
        "+            child_ids=[],\n",
        "+            inputs={},\n",
        "+            outputs={},\n",
        "+            error=\"Validation failed\",\n",
        "+        )\n",
        "+\n",
        "+        workflow = Workflow(\n",
        "+            root_trace=root,\n",
        "+            nodes={\"Validator\": [child]},\n",
        "+            all_traces=[root, child],\n",
        "+        )\n",
        "+\n",
        "+        result = analyze_failures([workflow])\n",
        "+\n",
        "+        assert result is not None\n",
        "+        assert result.total_workflows == 1\n",
        "+        assert len(result.node_failure_stats) >= 1\n",
        "+        assert result.error_type_distribution is not None\n",
        "+\n",
        "+\n",
        "+# Run tests with: pytest test_analyze_failures.py -v\n"
      ]
    },
    {
      "path": "test_analyze_traces.py",
      "status": "modified",
      "additions": 26,
      "deletions": 26,
      "patch": "@@ -101,7 +101,7 @@ def test_workflow_creation(self):\n \n         child = Trace(\n             id=\"child-1\",\n-            name=\"generate_spec\",\n+            name=\"process_data\",\n             start_time=datetime(2025, 1, 1, 12, 1, 0, tzinfo=timezone.utc),\n             end_time=datetime(2025, 1, 1, 12, 3, 0, tzinfo=timezone.utc),\n             duration_seconds=120.0,\n@@ -116,12 +116,12 @@ def test_workflow_creation(self):\n \n         # Act\n         workflow = Workflow(\n-            root_trace=root, nodes={\"generate_spec\": [child]}, all_traces=[root, child]\n+            root_trace=root, nodes={\"process_data\": [child]}, all_traces=[root, child]\n         )\n \n         # Assert\n         assert workflow.root_trace.id == \"root-1\"\n-        assert \"generate_spec\" in workflow.nodes\n+        assert \"process_data\" in workflow.nodes\n         assert len(workflow.all_traces) == 2\n \n     def test_workflow_total_duration_property(self):\n@@ -251,7 +251,7 @@ def test_load_from_json_with_hierarchical_data(self):\n                     \"child_runs\": [\n                         {\n                             \"id\": \"child-1\",\n-                            \"name\": \"generate_spec\",\n+                            \"name\": \"process_data\",\n                             \"start_time\": \"2025-01-01T12:01:00+00:00\",\n                             \"end_time\": \"2025-01-01T12:03:00+00:00\",\n                             \"duration_seconds\": 120.0,\n@@ -541,7 +541,7 @@ def test_node_performance_dataclass_creation(self):\n \n         # Arrange & Act\n         node_perf = NodePerformance(\n-            node_name=\"generate_spec\",\n+            node_name=\"process_data\",\n             execution_count=100,\n             avg_duration_seconds=180.5,\n             median_duration_seconds=175.2,\n@@ -551,7 +551,7 @@ def test_node_performance_dataclass_creation(self):\n         )\n \n         # Assert\n-        assert node_perf.node_name == \"generate_spec\"\n+        assert node_perf.node_name == \"process_data\"\n         assert node_perf.execution_count == 100\n         assert node_perf.avg_duration_seconds == 180.5\n         assert node_perf.avg_percent_of_workflow == 15.2\n@@ -562,7 +562,7 @@ def test_bottleneck_analysis_dataclass_creation(self):\n \n         # Arrange\n         node1 = NodePerformance(\n-            node_name=\"xml_transformation\",\n+            node_name=\"transform_output\",\n             execution_count=100,\n             avg_duration_seconds=250.8,\n             median_duration_seconds=245.0,\n@@ -572,7 +572,7 @@ def test_bottleneck_analysis_dataclass_creation(self):\n         )\n \n         node2 = NodePerformance(\n-            node_name=\"generate_spec\",\n+            node_name=\"process_data\",\n             execution_count=100,\n             avg_duration_seconds=180.5,\n             median_duration_seconds=175.2,\n@@ -584,14 +584,14 @@ def test_bottleneck_analysis_dataclass_creation(self):\n         # Act\n         analysis = BottleneckAnalysis(\n             node_performances=[node1, node2],\n-            primary_bottleneck=\"xml_transformation\",\n-            top_3_bottlenecks=[\"xml_transformation\", \"generate_spec\"],\n+            primary_bottleneck=\"transform_output\",\n+            top_3_bottlenecks=[\"transform_output\", \"process_data\"],\n         )\n \n         # Assert\n         assert len(analysis.node_performances) == 2\n-        assert analysis.primary_bottleneck == \"xml_transformation\"\n-        assert analysis.top_3_bottlenecks[0] == \"xml_transformation\"\n+        assert analysis.primary_bottleneck == \"transform_output\"\n+        assert analysis.top_3_bottlenecks[0] == \"transform_output\"\n \n     def test_identify_bottlenecks_basic(self):\n         \"\"\"Test basic bottleneck identification with simple workflow.\"\"\"\n@@ -615,7 +615,7 @@ def test_identify_bottlenecks_basic(self):\n \n         node1 = Trace(\n             id=\"node-1\",\n-            name=\"generate_spec\",\n+            name=\"process_data\",\n             start_time=None,\n             end_time=None,\n             duration_seconds=200.0,  # 33% of workflow\n@@ -630,7 +630,7 @@ def test_identify_bottlenecks_basic(self):\n \n         node2 = Trace(\n             id=\"node-2\",\n-            name=\"xml_transformation\",\n+            name=\"transform_output\",\n             start_time=None,\n             end_time=None,\n             duration_seconds=300.0,  # 50% of workflow (bottleneck)\n@@ -661,8 +661,8 @@ def test_identify_bottlenecks_basic(self):\n         workflow = Workflow(\n             root_trace=root,\n             nodes={\n-                \"generate_spec\": [node1],\n-                \"xml_transformation\": [node2],\n+                \"process_data\": [node1],\n+                \"transform_output\": [node2],\n                 \"import_step\": [node3],\n             },\n             all_traces=[root, node1, node2, node3],\n@@ -673,8 +673,8 @@ def test_identify_bottlenecks_basic(self):\n \n         # Assert\n         assert len(result.node_performances) == 3\n-        assert result.primary_bottleneck == \"xml_transformation\"\n-        assert result.node_performances[0].node_name == \"xml_transformation\"\n+        assert result.primary_bottleneck == \"transform_output\"\n+        assert result.node_performances[0].node_name == \"transform_output\"\n         assert result.node_performances[0].avg_duration_seconds == 300.0\n         assert result.node_performances[0].execution_count == 1\n \n@@ -1105,7 +1105,7 @@ def test_verify_parallel_execution_workflows_without_validators(self):\n \n         node1 = Trace(\n             id=\"node-1\",\n-            name=\"generate_spec\",\n+            name=\"process_data\",\n             start_time=datetime(2025, 1, 1, 12, 1, 0, tzinfo=timezone.utc),\n             end_time=datetime(2025, 1, 1, 12, 3, 0, tzinfo=timezone.utc),\n             duration_seconds=120.0,\n@@ -1119,7 +1119,7 @@ def test_verify_parallel_execution_workflows_without_validators(self):\n         )\n \n         workflow = Workflow(\n-            root_trace=root, nodes={\"generate_spec\": [node1]}, all_traces=[root, node1]\n+            root_trace=root, nodes={\"process_data\": [node1]}, all_traces=[root, node1]\n         )\n \n         # Act\n@@ -1169,7 +1169,7 @@ def test_bottleneck_analysis_to_csv(self):\n \n         # Arrange\n         node1 = NodePerformance(\n-            node_name=\"xml_transformation\",\n+            node_name=\"transform_output\",\n             execution_count=100,\n             avg_duration_seconds=250.8,\n             median_duration_seconds=245.0,\n@@ -1179,7 +1179,7 @@ def test_bottleneck_analysis_to_csv(self):\n         )\n \n         node2 = NodePerformance(\n-            node_name=\"generate_spec\",\n+            node_name=\"process_data\",\n             execution_count=100,\n             avg_duration_seconds=180.5,\n             median_duration_seconds=175.2,\n@@ -1190,8 +1190,8 @@ def test_bottleneck_analysis_to_csv(self):\n \n         analysis = BottleneckAnalysis(\n             node_performances=[node1, node2],\n-            primary_bottleneck=\"xml_transformation\",\n-            top_3_bottlenecks=[\"xml_transformation\", \"generate_spec\"],\n+            primary_bottleneck=\"transform_output\",\n+            top_3_bottlenecks=[\"transform_output\", \"process_data\"],\n         )\n \n         # Act\n@@ -1202,8 +1202,8 @@ def test_bottleneck_analysis_to_csv(self):\n             \"node_name,execution_count,avg_duration_seconds,median_duration_seconds\"\n             in csv_output\n         )\n-        assert \"xml_transformation,100,250.8,245.0\" in csv_output\n-        assert \"generate_spec,100,180.5,175.2\" in csv_output\n+        assert \"transform_output,100,250.8,245.0\" in csv_output\n+        assert \"process_data,100,180.5,175.2\" in csv_output\n \n     def test_parallel_execution_evidence_to_csv(self):\n         \"\"\"Test exporting ParallelExecutionEvidence to CSV format.\"\"\"",
      "patch_lines": [
        "@@ -101,7 +101,7 @@ def test_workflow_creation(self):\n",
        " \n",
        "         child = Trace(\n",
        "             id=\"child-1\",\n",
        "-            name=\"generate_spec\",\n",
        "+            name=\"process_data\",\n",
        "             start_time=datetime(2025, 1, 1, 12, 1, 0, tzinfo=timezone.utc),\n",
        "             end_time=datetime(2025, 1, 1, 12, 3, 0, tzinfo=timezone.utc),\n",
        "             duration_seconds=120.0,\n",
        "@@ -116,12 +116,12 @@ def test_workflow_creation(self):\n",
        " \n",
        "         # Act\n",
        "         workflow = Workflow(\n",
        "-            root_trace=root, nodes={\"generate_spec\": [child]}, all_traces=[root, child]\n",
        "+            root_trace=root, nodes={\"process_data\": [child]}, all_traces=[root, child]\n",
        "         )\n",
        " \n",
        "         # Assert\n",
        "         assert workflow.root_trace.id == \"root-1\"\n",
        "-        assert \"generate_spec\" in workflow.nodes\n",
        "+        assert \"process_data\" in workflow.nodes\n",
        "         assert len(workflow.all_traces) == 2\n",
        " \n",
        "     def test_workflow_total_duration_property(self):\n",
        "@@ -251,7 +251,7 @@ def test_load_from_json_with_hierarchical_data(self):\n",
        "                     \"child_runs\": [\n",
        "                         {\n",
        "                             \"id\": \"child-1\",\n",
        "-                            \"name\": \"generate_spec\",\n",
        "+                            \"name\": \"process_data\",\n",
        "                             \"start_time\": \"2025-01-01T12:01:00+00:00\",\n",
        "                             \"end_time\": \"2025-01-01T12:03:00+00:00\",\n",
        "                             \"duration_seconds\": 120.0,\n",
        "@@ -541,7 +541,7 @@ def test_node_performance_dataclass_creation(self):\n",
        " \n",
        "         # Arrange & Act\n",
        "         node_perf = NodePerformance(\n",
        "-            node_name=\"generate_spec\",\n",
        "+            node_name=\"process_data\",\n",
        "             execution_count=100,\n",
        "             avg_duration_seconds=180.5,\n",
        "             median_duration_seconds=175.2,\n",
        "@@ -551,7 +551,7 @@ def test_node_performance_dataclass_creation(self):\n",
        "         )\n",
        " \n",
        "         # Assert\n",
        "-        assert node_perf.node_name == \"generate_spec\"\n",
        "+        assert node_perf.node_name == \"process_data\"\n",
        "         assert node_perf.execution_count == 100\n",
        "         assert node_perf.avg_duration_seconds == 180.5\n",
        "         assert node_perf.avg_percent_of_workflow == 15.2\n",
        "@@ -562,7 +562,7 @@ def test_bottleneck_analysis_dataclass_creation(self):\n",
        " \n",
        "         # Arrange\n",
        "         node1 = NodePerformance(\n",
        "-            node_name=\"xml_transformation\",\n",
        "+            node_name=\"transform_output\",\n",
        "             execution_count=100,\n",
        "             avg_duration_seconds=250.8,\n",
        "             median_duration_seconds=245.0,\n",
        "@@ -572,7 +572,7 @@ def test_bottleneck_analysis_dataclass_creation(self):\n",
        "         )\n",
        " \n",
        "         node2 = NodePerformance(\n",
        "-            node_name=\"generate_spec\",\n",
        "+            node_name=\"process_data\",\n",
        "             execution_count=100,\n",
        "             avg_duration_seconds=180.5,\n",
        "             median_duration_seconds=175.2,\n",
        "@@ -584,14 +584,14 @@ def test_bottleneck_analysis_dataclass_creation(self):\n",
        "         # Act\n",
        "         analysis = BottleneckAnalysis(\n",
        "             node_performances=[node1, node2],\n",
        "-            primary_bottleneck=\"xml_transformation\",\n",
        "-            top_3_bottlenecks=[\"xml_transformation\", \"generate_spec\"],\n",
        "+            primary_bottleneck=\"transform_output\",\n",
        "+            top_3_bottlenecks=[\"transform_output\", \"process_data\"],\n",
        "         )\n",
        " \n",
        "         # Assert\n",
        "         assert len(analysis.node_performances) == 2\n",
        "-        assert analysis.primary_bottleneck == \"xml_transformation\"\n",
        "-        assert analysis.top_3_bottlenecks[0] == \"xml_transformation\"\n",
        "+        assert analysis.primary_bottleneck == \"transform_output\"\n",
        "+        assert analysis.top_3_bottlenecks[0] == \"transform_output\"\n",
        " \n",
        "     def test_identify_bottlenecks_basic(self):\n",
        "         \"\"\"Test basic bottleneck identification with simple workflow.\"\"\"\n",
        "@@ -615,7 +615,7 @@ def test_identify_bottlenecks_basic(self):\n",
        " \n",
        "         node1 = Trace(\n",
        "             id=\"node-1\",\n",
        "-            name=\"generate_spec\",\n",
        "+            name=\"process_data\",\n",
        "             start_time=None,\n",
        "             end_time=None,\n",
        "             duration_seconds=200.0,  # 33% of workflow\n",
        "@@ -630,7 +630,7 @@ def test_identify_bottlenecks_basic(self):\n",
        " \n",
        "         node2 = Trace(\n",
        "             id=\"node-2\",\n",
        "-            name=\"xml_transformation\",\n",
        "+            name=\"transform_output\",\n",
        "             start_time=None,\n",
        "             end_time=None,\n",
        "             duration_seconds=300.0,  # 50% of workflow (bottleneck)\n",
        "@@ -661,8 +661,8 @@ def test_identify_bottlenecks_basic(self):\n",
        "         workflow = Workflow(\n",
        "             root_trace=root,\n",
        "             nodes={\n",
        "-                \"generate_spec\": [node1],\n",
        "-                \"xml_transformation\": [node2],\n",
        "+                \"process_data\": [node1],\n",
        "+                \"transform_output\": [node2],\n",
        "                 \"import_step\": [node3],\n",
        "             },\n",
        "             all_traces=[root, node1, node2, node3],\n",
        "@@ -673,8 +673,8 @@ def test_identify_bottlenecks_basic(self):\n",
        " \n",
        "         # Assert\n",
        "         assert len(result.node_performances) == 3\n",
        "-        assert result.primary_bottleneck == \"xml_transformation\"\n",
        "-        assert result.node_performances[0].node_name == \"xml_transformation\"\n",
        "+        assert result.primary_bottleneck == \"transform_output\"\n",
        "+        assert result.node_performances[0].node_name == \"transform_output\"\n",
        "         assert result.node_performances[0].avg_duration_seconds == 300.0\n",
        "         assert result.node_performances[0].execution_count == 1\n",
        " \n",
        "@@ -1105,7 +1105,7 @@ def test_verify_parallel_execution_workflows_without_validators(self):\n",
        " \n",
        "         node1 = Trace(\n",
        "             id=\"node-1\",\n",
        "-            name=\"generate_spec\",\n",
        "+            name=\"process_data\",\n",
        "             start_time=datetime(2025, 1, 1, 12, 1, 0, tzinfo=timezone.utc),\n",
        "             end_time=datetime(2025, 1, 1, 12, 3, 0, tzinfo=timezone.utc),\n",
        "             duration_seconds=120.0,\n",
        "@@ -1119,7 +1119,7 @@ def test_verify_parallel_execution_workflows_without_validators(self):\n",
        "         )\n",
        " \n",
        "         workflow = Workflow(\n",
        "-            root_trace=root, nodes={\"generate_spec\": [node1]}, all_traces=[root, node1]\n",
        "+            root_trace=root, nodes={\"process_data\": [node1]}, all_traces=[root, node1]\n",
        "         )\n",
        " \n",
        "         # Act\n",
        "@@ -1169,7 +1169,7 @@ def test_bottleneck_analysis_to_csv(self):\n",
        " \n",
        "         # Arrange\n",
        "         node1 = NodePerformance(\n",
        "-            node_name=\"xml_transformation\",\n",
        "+            node_name=\"transform_output\",\n",
        "             execution_count=100,\n",
        "             avg_duration_seconds=250.8,\n",
        "             median_duration_seconds=245.0,\n",
        "@@ -1179,7 +1179,7 @@ def test_bottleneck_analysis_to_csv(self):\n",
        "         )\n",
        " \n",
        "         node2 = NodePerformance(\n",
        "-            node_name=\"generate_spec\",\n",
        "+            node_name=\"process_data\",\n",
        "             execution_count=100,\n",
        "             avg_duration_seconds=180.5,\n",
        "             median_duration_seconds=175.2,\n",
        "@@ -1190,8 +1190,8 @@ def test_bottleneck_analysis_to_csv(self):\n",
        " \n",
        "         analysis = BottleneckAnalysis(\n",
        "             node_performances=[node1, node2],\n",
        "-            primary_bottleneck=\"xml_transformation\",\n",
        "-            top_3_bottlenecks=[\"xml_transformation\", \"generate_spec\"],\n",
        "+            primary_bottleneck=\"transform_output\",\n",
        "+            top_3_bottlenecks=[\"transform_output\", \"process_data\"],\n",
        "         )\n",
        " \n",
        "         # Act\n",
        "@@ -1202,8 +1202,8 @@ def test_bottleneck_analysis_to_csv(self):\n",
        "             \"node_name,execution_count,avg_duration_seconds,median_duration_seconds\"\n",
        "             in csv_output\n",
        "         )\n",
        "-        assert \"xml_transformation,100,250.8,245.0\" in csv_output\n",
        "-        assert \"generate_spec,100,180.5,175.2\" in csv_output\n",
        "+        assert \"transform_output,100,250.8,245.0\" in csv_output\n",
        "+        assert \"process_data,100,180.5,175.2\" in csv_output\n",
        " \n",
        "     def test_parallel_execution_evidence_to_csv(self):\n",
        "         \"\"\"Test exporting ParallelExecutionEvidence to CSV format.\"\"\"\n"
      ]
    },
    {
      "path": "test_export_langsmith_traces.py",
      "status": "modified",
      "additions": 276,
      "deletions": 5,
      "patch": "@@ -430,6 +430,231 @@ def test_format_trace_data_complete_fields(self, mock_client_class):\n         assert \"duration_seconds\" in trace\n         assert trace[\"child_runs\"] == []\n \n+    @patch(\"export_langsmith_traces.Client\")\n+    def test_format_trace_data_includes_token_usage(self, mock_client_class):\n+        \"\"\"Test that token usage fields are exported when present.\"\"\"\n+        # Arrange\n+        from datetime import datetime, timezone\n+\n+        mock_client = Mock()\n+        mock_client_class.return_value = mock_client\n+\n+        # Create mock LLM Run with token usage\n+        mock_run = Mock()\n+        mock_run.id = \"llm_run_456\"\n+        mock_run.name = \"ChatGoogleGenerativeAI\"\n+        mock_run.start_time = datetime(2025, 12, 9, 10, 0, 0, tzinfo=timezone.utc)\n+        mock_run.end_time = datetime(2025, 12, 9, 10, 2, 0, tzinfo=timezone.utc)\n+        mock_run.status = \"success\"\n+        mock_run.inputs = {\"messages\": []}\n+        mock_run.outputs = {\"generations\": []}\n+        mock_run.error = None\n+        mock_run.run_type = \"llm\"\n+        mock_run.child_runs = []\n+        # Token usage fields (as found in LangSmith API)\n+        mock_run.total_tokens = 162286\n+        mock_run.prompt_tokens = 128227\n+        mock_run.completion_tokens = 34059\n+\n+        exporter = LangSmithExporter(api_key=\"test_key\")\n+\n+        # Act\n+        result = exporter.format_trace_data([mock_run])\n+\n+        # Assert\n+        trace = result[\"traces\"][0]\n+        assert \"total_tokens\" in trace\n+        assert \"prompt_tokens\" in trace\n+        assert \"completion_tokens\" in trace\n+        assert trace[\"total_tokens\"] == 162286\n+        assert trace[\"prompt_tokens\"] == 128227\n+        assert trace[\"completion_tokens\"] == 34059\n+\n+    @patch(\"export_langsmith_traces.Client\")\n+    def test_format_trace_data_handles_missing_tokens(self, mock_client_class):\n+        \"\"\"Test that missing token fields are handled gracefully.\"\"\"\n+        # Arrange\n+        from datetime import datetime, timezone\n+\n+        mock_client = Mock()\n+        mock_client_class.return_value = mock_client\n+\n+        # Create mock non-LLM Run without token usage\n+        mock_run = Mock()\n+        mock_run.id = \"chain_run_789\"\n+        mock_run.name = \"LangGraph\"\n+        mock_run.start_time = datetime(2025, 12, 9, 10, 0, 0, tzinfo=timezone.utc)\n+        mock_run.end_time = datetime(2025, 12, 9, 10, 5, 0, tzinfo=timezone.utc)\n+        mock_run.status = \"success\"\n+        mock_run.inputs = {}\n+        mock_run.outputs = {}\n+        mock_run.error = None\n+        mock_run.run_type = \"chain\"\n+        mock_run.child_runs = []\n+        # No token fields (non-LLM run)\n+        mock_run.total_tokens = None\n+        mock_run.prompt_tokens = None\n+        mock_run.completion_tokens = None\n+\n+        exporter = LangSmithExporter(api_key=\"test_key\")\n+\n+        # Act\n+        result = exporter.format_trace_data([mock_run])\n+\n+        # Assert\n+        trace = result[\"traces\"][0]\n+        # Token fields should be present but None for non-LLM runs\n+        assert \"total_tokens\" in trace\n+        assert \"prompt_tokens\" in trace\n+        assert \"completion_tokens\" in trace\n+        assert trace[\"total_tokens\"] is None\n+        assert trace[\"prompt_tokens\"] is None\n+        assert trace[\"completion_tokens\"] is None\n+\n+    @patch(\"export_langsmith_traces.Client\")\n+    def test_format_trace_data_includes_cache_tokens_from_nested_fields(\n+        self, mock_client_class\n+    ):\n+        \"\"\"Test that cache token fields are extracted from nested input_token_details (LangSmith API structure).\"\"\"\n+        # Arrange\n+        from datetime import datetime, timezone\n+\n+        mock_client = Mock()\n+        mock_client_class.return_value = mock_client\n+\n+        # Create mock LLM Run with cache token usage in nested input_token_details\n+        # Use spec to prevent Mock from creating cache_read_tokens/cache_creation_tokens automatically\n+        mock_run = Mock(\n+            spec=[\n+                \"id\",\n+                \"name\",\n+                \"start_time\",\n+                \"end_time\",\n+                \"status\",\n+                \"inputs\",\n+                \"outputs\",\n+                \"error\",\n+                \"run_type\",\n+                \"child_runs\",\n+                \"total_tokens\",\n+                \"prompt_tokens\",\n+                \"completion_tokens\",\n+            ]\n+        )\n+        mock_run.id = \"cached_llm_run_999\"\n+        mock_run.name = \"ChatGoogleGenerativeAI\"\n+        mock_run.start_time = datetime(2025, 12, 11, 10, 0, 0, tzinfo=timezone.utc)\n+        mock_run.end_time = datetime(2025, 12, 11, 10, 2, 0, tzinfo=timezone.utc)\n+        mock_run.status = \"success\"\n+        mock_run.inputs = {\"messages\": []}\n+        mock_run.error = None\n+        mock_run.run_type = \"llm\"\n+        mock_run.child_runs = []\n+        # Standard token fields\n+        mock_run.total_tokens = 50000\n+        mock_run.prompt_tokens = 10000\n+        mock_run.completion_tokens = 5000\n+        # Cache token fields in nested LangChain message structure (actual LangSmith export format)\n+        # These are nested under outputs[\"generations\"][0][0][\"message\"][\"kwargs\"][\"usage_metadata\"][\"input_token_details\"]\n+        mock_run.outputs = {\n+            \"generations\": [\n+                [\n+                    {\n+                        \"message\": {\n+                            \"kwargs\": {\n+                                \"usage_metadata\": {\n+                                    \"input_tokens\": 10000,\n+                                    \"output_tokens\": 5000,\n+                                    \"total_tokens\": 50000,\n+                                    \"input_token_details\": {\n+                                        \"cache_read\": 35000,  # Tokens read from cache\n+                                        \"cache_creation\": 0,  # Tokens written to cache\n+                                    },\n+                                }\n+                            }\n+                        }\n+                    }\n+                ]\n+            ]\n+        }\n+        # Top-level cache fields not present (not in spec, so getattr will return None)\n+\n+        exporter = LangSmithExporter(api_key=\"test_key\")\n+\n+        # Act\n+        result = exporter.format_trace_data([mock_run])\n+\n+        # Assert\n+        trace = result[\"traces\"][0]\n+        assert \"cache_read_tokens\" in trace\n+        assert \"cache_creation_tokens\" in trace\n+        assert trace[\"cache_read_tokens\"] == 35000\n+        assert trace[\"cache_creation_tokens\"] == 0\n+        # Standard tokens should still be present\n+        assert trace[\"total_tokens\"] == 50000\n+        assert trace[\"prompt_tokens\"] == 10000\n+        assert trace[\"completion_tokens\"] == 5000\n+\n+    @patch(\"export_langsmith_traces.Client\")\n+    def test_format_trace_data_handles_missing_cache_tokens(self, mock_client_class):\n+        \"\"\"Test that missing cache token fields are handled gracefully (older LangSmith data).\"\"\"\n+        # Arrange\n+        from datetime import datetime, timezone\n+\n+        mock_client = Mock()\n+        mock_client_class.return_value = mock_client\n+\n+        # Create mock LLM Run WITHOUT cache token fields (older export or non-cached run)\n+        mock_run = Mock(\n+            spec=[\n+                \"id\",\n+                \"name\",\n+                \"start_time\",\n+                \"end_time\",\n+                \"status\",\n+                \"inputs\",\n+                \"outputs\",\n+                \"error\",\n+                \"run_type\",\n+                \"child_runs\",\n+                \"total_tokens\",\n+                \"prompt_tokens\",\n+                \"completion_tokens\",\n+            ]\n+        )\n+        mock_run.id = \"old_llm_run_888\"\n+        mock_run.name = \"ChatGoogleGenerativeAI\"\n+        mock_run.start_time = datetime(2025, 12, 11, 10, 0, 0, tzinfo=timezone.utc)\n+        mock_run.end_time = datetime(2025, 12, 11, 10, 2, 0, tzinfo=timezone.utc)\n+        mock_run.status = \"success\"\n+        mock_run.inputs = {\"messages\": []}\n+        mock_run.outputs = {\"generations\": []}\n+        mock_run.error = None\n+        mock_run.run_type = \"llm\"\n+        mock_run.child_runs = []\n+        # Standard token fields present\n+        mock_run.total_tokens = 15000\n+        mock_run.prompt_tokens = 10000\n+        mock_run.completion_tokens = 5000\n+        # Cache token fields NOT present (not in spec, so getattr will return None via default)\n+\n+        exporter = LangSmithExporter(api_key=\"test_key\")\n+\n+        # Act\n+        result = exporter.format_trace_data([mock_run])\n+\n+        # Assert\n+        trace = result[\"traces\"][0]\n+        # Cache token fields should be present but None when not available in source data\n+        assert \"cache_read_tokens\" in trace\n+        assert \"cache_creation_tokens\" in trace\n+        assert trace[\"cache_read_tokens\"] is None\n+        assert trace[\"cache_creation_tokens\"] is None\n+        # Standard tokens should still be present\n+        assert trace[\"total_tokens\"] == 15000\n+        assert trace[\"prompt_tokens\"] == 10000\n+        assert trace[\"completion_tokens\"] == 5000\n+\n     @patch(\"export_langsmith_traces.Client\")\n     def test_format_trace_data_missing_fields(self, mock_client_class):\n         \"\"\"Test safe handling of missing/null fields.\"\"\"\n@@ -1005,13 +1230,13 @@ def test_fetch_runs_with_children_single_run(self, mock_client_class):\n         # Mock the full run with children from read_run\n         child1 = Mock()\n         child1.id = \"child-1\"\n-        child1.name = \"generate_spec\"\n+        child1.name = \"process_data\"\n         child1.run_type = \"chain\"\n         child1.child_runs = []\n \n         child2 = Mock()\n         child2.id = \"child-2\"\n-        child2.name = \"xml_transformation\"\n+        child2.name = \"transform_output\"\n         child2.run_type = \"chain\"\n         child2.child_runs = []\n \n@@ -1033,8 +1258,8 @@ def test_fetch_runs_with_children_single_run(self, mock_client_class):\n         assert runs[0].id == \"parent-123\"\n         assert runs[0].child_runs is not None\n         assert len(runs[0].child_runs) == 2\n-        assert runs[0].child_runs[0].name == \"generate_spec\"\n-        assert runs[0].child_runs[1].name == \"xml_transformation\"\n+        assert runs[0].child_runs[0].name == \"process_data\"\n+        assert runs[0].child_runs[1].name == \"transform_output\"\n \n         # Verify read_run was called with load_child_runs=True\n         mock_client.read_run.assert_called_once_with(\"parent-123\", load_child_runs=True)\n@@ -1147,9 +1372,36 @@ def test_main_success_workflow(self, mock_argv, mock_client_class):\n             mock_client_class.return_value = mock_client\n \n             # Create mock runs with only essential attributes\n-            mock_run = Mock(spec=[\"id\", \"name\"])\n+            mock_run = Mock(\n+                spec=[\n+                    \"id\",\n+                    \"name\",\n+                    \"start_time\",\n+                    \"end_time\",\n+                    \"status\",\n+                    \"inputs\",\n+                    \"outputs\",\n+                    \"error\",\n+                    \"run_type\",\n+                    \"child_runs\",\n+                    \"total_tokens\",\n+                    \"prompt_tokens\",\n+                    \"completion_tokens\",\n+                ]\n+            )\n             mock_run.id = \"run_123\"\n             mock_run.name = \"test_run\"\n+            mock_run.start_time = None\n+            mock_run.end_time = None\n+            mock_run.status = \"completed\"\n+            mock_run.inputs = {}\n+            mock_run.outputs = {}\n+            mock_run.error = None\n+            mock_run.run_type = \"chain\"\n+            mock_run.child_runs = []\n+            mock_run.total_tokens = None\n+            mock_run.prompt_tokens = None\n+            mock_run.completion_tokens = None\n \n             mock_client.list_runs.return_value = [mock_run]\n \n@@ -1252,6 +1504,13 @@ def test_main_with_pagination_success(\n                 run.error = None\n                 run.run_type = \"chain\"\n                 run.child_runs = []\n+                # Token fields (None for non-LLM runs)\n+                run.total_tokens = None\n+                run.prompt_tokens = None\n+                run.completion_tokens = None\n+                # Cache token fields (None for non-cached runs)\n+                run.cache_read_tokens = None\n+                run.cache_creation_tokens = None\n                 all_runs.append(run)\n \n             def mock_list_runs(*args, **kwargs):\n@@ -1338,6 +1597,9 @@ def test_main_with_include_children_flag(\n                     \"error\",\n                     \"run_type\",\n                     \"child_runs\",\n+                    \"total_tokens\",\n+                    \"prompt_tokens\",\n+                    \"completion_tokens\",\n                 ]\n             )\n             full_run1.id = \"run_1\"\n@@ -1350,6 +1612,9 @@ def test_main_with_include_children_flag(\n             full_run1.error = None\n             full_run1.run_type = \"chain\"\n             full_run1.child_runs = []  # Empty list to avoid nested Mock serialization\n+            full_run1.total_tokens = None\n+            full_run1.prompt_tokens = None\n+            full_run1.completion_tokens = None\n \n             full_run2 = Mock(\n                 spec=[\n@@ -1363,6 +1628,9 @@ def test_main_with_include_children_flag(\n                     \"error\",\n                     \"run_type\",\n                     \"child_runs\",\n+                    \"total_tokens\",\n+                    \"prompt_tokens\",\n+                    \"completion_tokens\",\n                 ]\n             )\n             full_run2.id = \"run_2\"\n@@ -1375,6 +1643,9 @@ def test_main_with_include_children_flag(\n             full_run2.error = None\n             full_run2.run_type = \"chain\"\n             full_run2.child_runs = []\n+            full_run2.total_tokens = None\n+            full_run2.prompt_tokens = None\n+            full_run2.completion_tokens = None\n \n             # Mock behaviors\n             mock_client.list_runs.return_value = iter([flat_run1, flat_run2])",
      "patch_lines": [
        "@@ -430,6 +430,231 @@ def test_format_trace_data_complete_fields(self, mock_client_class):\n",
        "         assert \"duration_seconds\" in trace\n",
        "         assert trace[\"child_runs\"] == []\n",
        " \n",
        "+    @patch(\"export_langsmith_traces.Client\")\n",
        "+    def test_format_trace_data_includes_token_usage(self, mock_client_class):\n",
        "+        \"\"\"Test that token usage fields are exported when present.\"\"\"\n",
        "+        # Arrange\n",
        "+        from datetime import datetime, timezone\n",
        "+\n",
        "+        mock_client = Mock()\n",
        "+        mock_client_class.return_value = mock_client\n",
        "+\n",
        "+        # Create mock LLM Run with token usage\n",
        "+        mock_run = Mock()\n",
        "+        mock_run.id = \"llm_run_456\"\n",
        "+        mock_run.name = \"ChatGoogleGenerativeAI\"\n",
        "+        mock_run.start_time = datetime(2025, 12, 9, 10, 0, 0, tzinfo=timezone.utc)\n",
        "+        mock_run.end_time = datetime(2025, 12, 9, 10, 2, 0, tzinfo=timezone.utc)\n",
        "+        mock_run.status = \"success\"\n",
        "+        mock_run.inputs = {\"messages\": []}\n",
        "+        mock_run.outputs = {\"generations\": []}\n",
        "+        mock_run.error = None\n",
        "+        mock_run.run_type = \"llm\"\n",
        "+        mock_run.child_runs = []\n",
        "+        # Token usage fields (as found in LangSmith API)\n",
        "+        mock_run.total_tokens = 162286\n",
        "+        mock_run.prompt_tokens = 128227\n",
        "+        mock_run.completion_tokens = 34059\n",
        "+\n",
        "+        exporter = LangSmithExporter(api_key=\"test_key\")\n",
        "+\n",
        "+        # Act\n",
        "+        result = exporter.format_trace_data([mock_run])\n",
        "+\n",
        "+        # Assert\n",
        "+        trace = result[\"traces\"][0]\n",
        "+        assert \"total_tokens\" in trace\n",
        "+        assert \"prompt_tokens\" in trace\n",
        "+        assert \"completion_tokens\" in trace\n",
        "+        assert trace[\"total_tokens\"] == 162286\n",
        "+        assert trace[\"prompt_tokens\"] == 128227\n",
        "+        assert trace[\"completion_tokens\"] == 34059\n",
        "+\n",
        "+    @patch(\"export_langsmith_traces.Client\")\n",
        "+    def test_format_trace_data_handles_missing_tokens(self, mock_client_class):\n",
        "+        \"\"\"Test that missing token fields are handled gracefully.\"\"\"\n",
        "+        # Arrange\n",
        "+        from datetime import datetime, timezone\n",
        "+\n",
        "+        mock_client = Mock()\n",
        "+        mock_client_class.return_value = mock_client\n",
        "+\n",
        "+        # Create mock non-LLM Run without token usage\n",
        "+        mock_run = Mock()\n",
        "+        mock_run.id = \"chain_run_789\"\n",
        "+        mock_run.name = \"LangGraph\"\n",
        "+        mock_run.start_time = datetime(2025, 12, 9, 10, 0, 0, tzinfo=timezone.utc)\n",
        "+        mock_run.end_time = datetime(2025, 12, 9, 10, 5, 0, tzinfo=timezone.utc)\n",
        "+        mock_run.status = \"success\"\n",
        "+        mock_run.inputs = {}\n",
        "+        mock_run.outputs = {}\n",
        "+        mock_run.error = None\n",
        "+        mock_run.run_type = \"chain\"\n",
        "+        mock_run.child_runs = []\n",
        "+        # No token fields (non-LLM run)\n",
        "+        mock_run.total_tokens = None\n",
        "+        mock_run.prompt_tokens = None\n",
        "+        mock_run.completion_tokens = None\n",
        "+\n",
        "+        exporter = LangSmithExporter(api_key=\"test_key\")\n",
        "+\n",
        "+        # Act\n",
        "+        result = exporter.format_trace_data([mock_run])\n",
        "+\n",
        "+        # Assert\n",
        "+        trace = result[\"traces\"][0]\n",
        "+        # Token fields should be present but None for non-LLM runs\n",
        "+        assert \"total_tokens\" in trace\n",
        "+        assert \"prompt_tokens\" in trace\n",
        "+        assert \"completion_tokens\" in trace\n",
        "+        assert trace[\"total_tokens\"] is None\n",
        "+        assert trace[\"prompt_tokens\"] is None\n",
        "+        assert trace[\"completion_tokens\"] is None\n",
        "+\n",
        "+    @patch(\"export_langsmith_traces.Client\")\n",
        "+    def test_format_trace_data_includes_cache_tokens_from_nested_fields(\n",
        "+        self, mock_client_class\n",
        "+    ):\n",
        "+        \"\"\"Test that cache token fields are extracted from nested input_token_details (LangSmith API structure).\"\"\"\n",
        "+        # Arrange\n",
        "+        from datetime import datetime, timezone\n",
        "+\n",
        "+        mock_client = Mock()\n",
        "+        mock_client_class.return_value = mock_client\n",
        "+\n",
        "+        # Create mock LLM Run with cache token usage in nested input_token_details\n",
        "+        # Use spec to prevent Mock from creating cache_read_tokens/cache_creation_tokens automatically\n",
        "+        mock_run = Mock(\n",
        "+            spec=[\n",
        "+                \"id\",\n",
        "+                \"name\",\n",
        "+                \"start_time\",\n",
        "+                \"end_time\",\n",
        "+                \"status\",\n",
        "+                \"inputs\",\n",
        "+                \"outputs\",\n",
        "+                \"error\",\n",
        "+                \"run_type\",\n",
        "+                \"child_runs\",\n",
        "+                \"total_tokens\",\n",
        "+                \"prompt_tokens\",\n",
        "+                \"completion_tokens\",\n",
        "+            ]\n",
        "+        )\n",
        "+        mock_run.id = \"cached_llm_run_999\"\n",
        "+        mock_run.name = \"ChatGoogleGenerativeAI\"\n",
        "+        mock_run.start_time = datetime(2025, 12, 11, 10, 0, 0, tzinfo=timezone.utc)\n",
        "+        mock_run.end_time = datetime(2025, 12, 11, 10, 2, 0, tzinfo=timezone.utc)\n",
        "+        mock_run.status = \"success\"\n",
        "+        mock_run.inputs = {\"messages\": []}\n",
        "+        mock_run.error = None\n",
        "+        mock_run.run_type = \"llm\"\n",
        "+        mock_run.child_runs = []\n",
        "+        # Standard token fields\n",
        "+        mock_run.total_tokens = 50000\n",
        "+        mock_run.prompt_tokens = 10000\n",
        "+        mock_run.completion_tokens = 5000\n",
        "+        # Cache token fields in nested LangChain message structure (actual LangSmith export format)\n",
        "+        # These are nested under outputs[\"generations\"][0][0][\"message\"][\"kwargs\"][\"usage_metadata\"][\"input_token_details\"]\n",
        "+        mock_run.outputs = {\n",
        "+            \"generations\": [\n",
        "+                [\n",
        "+                    {\n",
        "+                        \"message\": {\n",
        "+                            \"kwargs\": {\n",
        "+                                \"usage_metadata\": {\n",
        "+                                    \"input_tokens\": 10000,\n",
        "+                                    \"output_tokens\": 5000,\n",
        "+                                    \"total_tokens\": 50000,\n",
        "+                                    \"input_token_details\": {\n",
        "+                                        \"cache_read\": 35000,  # Tokens read from cache\n",
        "+                                        \"cache_creation\": 0,  # Tokens written to cache\n",
        "+                                    },\n",
        "+                                }\n",
        "+                            }\n",
        "+                        }\n",
        "+                    }\n",
        "+                ]\n",
        "+            ]\n",
        "+        }\n",
        "+        # Top-level cache fields not present (not in spec, so getattr will return None)\n",
        "+\n",
        "+        exporter = LangSmithExporter(api_key=\"test_key\")\n",
        "+\n",
        "+        # Act\n",
        "+        result = exporter.format_trace_data([mock_run])\n",
        "+\n",
        "+        # Assert\n",
        "+        trace = result[\"traces\"][0]\n",
        "+        assert \"cache_read_tokens\" in trace\n",
        "+        assert \"cache_creation_tokens\" in trace\n",
        "+        assert trace[\"cache_read_tokens\"] == 35000\n",
        "+        assert trace[\"cache_creation_tokens\"] == 0\n",
        "+        # Standard tokens should still be present\n",
        "+        assert trace[\"total_tokens\"] == 50000\n",
        "+        assert trace[\"prompt_tokens\"] == 10000\n",
        "+        assert trace[\"completion_tokens\"] == 5000\n",
        "+\n",
        "+    @patch(\"export_langsmith_traces.Client\")\n",
        "+    def test_format_trace_data_handles_missing_cache_tokens(self, mock_client_class):\n",
        "+        \"\"\"Test that missing cache token fields are handled gracefully (older LangSmith data).\"\"\"\n",
        "+        # Arrange\n",
        "+        from datetime import datetime, timezone\n",
        "+\n",
        "+        mock_client = Mock()\n",
        "+        mock_client_class.return_value = mock_client\n",
        "+\n",
        "+        # Create mock LLM Run WITHOUT cache token fields (older export or non-cached run)\n",
        "+        mock_run = Mock(\n",
        "+            spec=[\n",
        "+                \"id\",\n",
        "+                \"name\",\n",
        "+                \"start_time\",\n",
        "+                \"end_time\",\n",
        "+                \"status\",\n",
        "+                \"inputs\",\n",
        "+                \"outputs\",\n",
        "+                \"error\",\n",
        "+                \"run_type\",\n",
        "+                \"child_runs\",\n",
        "+                \"total_tokens\",\n",
        "+                \"prompt_tokens\",\n",
        "+                \"completion_tokens\",\n",
        "+            ]\n",
        "+        )\n",
        "+        mock_run.id = \"old_llm_run_888\"\n",
        "+        mock_run.name = \"ChatGoogleGenerativeAI\"\n",
        "+        mock_run.start_time = datetime(2025, 12, 11, 10, 0, 0, tzinfo=timezone.utc)\n",
        "+        mock_run.end_time = datetime(2025, 12, 11, 10, 2, 0, tzinfo=timezone.utc)\n",
        "+        mock_run.status = \"success\"\n",
        "+        mock_run.inputs = {\"messages\": []}\n",
        "+        mock_run.outputs = {\"generations\": []}\n",
        "+        mock_run.error = None\n",
        "+        mock_run.run_type = \"llm\"\n",
        "+        mock_run.child_runs = []\n",
        "+        # Standard token fields present\n",
        "+        mock_run.total_tokens = 15000\n",
        "+        mock_run.prompt_tokens = 10000\n",
        "+        mock_run.completion_tokens = 5000\n",
        "+        # Cache token fields NOT present (not in spec, so getattr will return None via default)\n",
        "+\n",
        "+        exporter = LangSmithExporter(api_key=\"test_key\")\n",
        "+\n",
        "+        # Act\n",
        "+        result = exporter.format_trace_data([mock_run])\n",
        "+\n",
        "+        # Assert\n",
        "+        trace = result[\"traces\"][0]\n",
        "+        # Cache token fields should be present but None when not available in source data\n",
        "+        assert \"cache_read_tokens\" in trace\n",
        "+        assert \"cache_creation_tokens\" in trace\n",
        "+        assert trace[\"cache_read_tokens\"] is None\n",
        "+        assert trace[\"cache_creation_tokens\"] is None\n",
        "+        # Standard tokens should still be present\n",
        "+        assert trace[\"total_tokens\"] == 15000\n",
        "+        assert trace[\"prompt_tokens\"] == 10000\n",
        "+        assert trace[\"completion_tokens\"] == 5000\n",
        "+\n",
        "     @patch(\"export_langsmith_traces.Client\")\n",
        "     def test_format_trace_data_missing_fields(self, mock_client_class):\n",
        "         \"\"\"Test safe handling of missing/null fields.\"\"\"\n",
        "@@ -1005,13 +1230,13 @@ def test_fetch_runs_with_children_single_run(self, mock_client_class):\n",
        "         # Mock the full run with children from read_run\n",
        "         child1 = Mock()\n",
        "         child1.id = \"child-1\"\n",
        "-        child1.name = \"generate_spec\"\n",
        "+        child1.name = \"process_data\"\n",
        "         child1.run_type = \"chain\"\n",
        "         child1.child_runs = []\n",
        " \n",
        "         child2 = Mock()\n",
        "         child2.id = \"child-2\"\n",
        "-        child2.name = \"xml_transformation\"\n",
        "+        child2.name = \"transform_output\"\n",
        "         child2.run_type = \"chain\"\n",
        "         child2.child_runs = []\n",
        " \n",
        "@@ -1033,8 +1258,8 @@ def test_fetch_runs_with_children_single_run(self, mock_client_class):\n",
        "         assert runs[0].id == \"parent-123\"\n",
        "         assert runs[0].child_runs is not None\n",
        "         assert len(runs[0].child_runs) == 2\n",
        "-        assert runs[0].child_runs[0].name == \"generate_spec\"\n",
        "-        assert runs[0].child_runs[1].name == \"xml_transformation\"\n",
        "+        assert runs[0].child_runs[0].name == \"process_data\"\n",
        "+        assert runs[0].child_runs[1].name == \"transform_output\"\n",
        " \n",
        "         # Verify read_run was called with load_child_runs=True\n",
        "         mock_client.read_run.assert_called_once_with(\"parent-123\", load_child_runs=True)\n",
        "@@ -1147,9 +1372,36 @@ def test_main_success_workflow(self, mock_argv, mock_client_class):\n",
        "             mock_client_class.return_value = mock_client\n",
        " \n",
        "             # Create mock runs with only essential attributes\n",
        "-            mock_run = Mock(spec=[\"id\", \"name\"])\n",
        "+            mock_run = Mock(\n",
        "+                spec=[\n",
        "+                    \"id\",\n",
        "+                    \"name\",\n",
        "+                    \"start_time\",\n",
        "+                    \"end_time\",\n",
        "+                    \"status\",\n",
        "+                    \"inputs\",\n",
        "+                    \"outputs\",\n",
        "+                    \"error\",\n",
        "+                    \"run_type\",\n",
        "+                    \"child_runs\",\n",
        "+                    \"total_tokens\",\n",
        "+                    \"prompt_tokens\",\n",
        "+                    \"completion_tokens\",\n",
        "+                ]\n",
        "+            )\n",
        "             mock_run.id = \"run_123\"\n",
        "             mock_run.name = \"test_run\"\n",
        "+            mock_run.start_time = None\n",
        "+            mock_run.end_time = None\n",
        "+            mock_run.status = \"completed\"\n",
        "+            mock_run.inputs = {}\n",
        "+            mock_run.outputs = {}\n",
        "+            mock_run.error = None\n",
        "+            mock_run.run_type = \"chain\"\n",
        "+            mock_run.child_runs = []\n",
        "+            mock_run.total_tokens = None\n",
        "+            mock_run.prompt_tokens = None\n",
        "+            mock_run.completion_tokens = None\n",
        " \n",
        "             mock_client.list_runs.return_value = [mock_run]\n",
        " \n",
        "@@ -1252,6 +1504,13 @@ def test_main_with_pagination_success(\n",
        "                 run.error = None\n",
        "                 run.run_type = \"chain\"\n",
        "                 run.child_runs = []\n",
        "+                # Token fields (None for non-LLM runs)\n",
        "+                run.total_tokens = None\n",
        "+                run.prompt_tokens = None\n",
        "+                run.completion_tokens = None\n",
        "+                # Cache token fields (None for non-cached runs)\n",
        "+                run.cache_read_tokens = None\n",
        "+                run.cache_creation_tokens = None\n",
        "                 all_runs.append(run)\n",
        " \n",
        "             def mock_list_runs(*args, **kwargs):\n",
        "@@ -1338,6 +1597,9 @@ def test_main_with_include_children_flag(\n",
        "                     \"error\",\n",
        "                     \"run_type\",\n",
        "                     \"child_runs\",\n",
        "+                    \"total_tokens\",\n",
        "+                    \"prompt_tokens\",\n",
        "+                    \"completion_tokens\",\n",
        "                 ]\n",
        "             )\n",
        "             full_run1.id = \"run_1\"\n",
        "@@ -1350,6 +1612,9 @@ def test_main_with_include_children_flag(\n",
        "             full_run1.error = None\n",
        "             full_run1.run_type = \"chain\"\n",
        "             full_run1.child_runs = []  # Empty list to avoid nested Mock serialization\n",
        "+            full_run1.total_tokens = None\n",
        "+            full_run1.prompt_tokens = None\n",
        "+            full_run1.completion_tokens = None\n",
        " \n",
        "             full_run2 = Mock(\n",
        "                 spec=[\n",
        "@@ -1363,6 +1628,9 @@ def test_main_with_include_children_flag(\n",
        "                     \"error\",\n",
        "                     \"run_type\",\n",
        "                     \"child_runs\",\n",
        "+                    \"total_tokens\",\n",
        "+                    \"prompt_tokens\",\n",
        "+                    \"completion_tokens\",\n",
        "                 ]\n",
        "             )\n",
        "             full_run2.id = \"run_2\"\n",
        "@@ -1375,6 +1643,9 @@ def test_main_with_include_children_flag(\n",
        "             full_run2.error = None\n",
        "             full_run2.run_type = \"chain\"\n",
        "             full_run2.child_runs = []\n",
        "+            full_run2.total_tokens = None\n",
        "+            full_run2.prompt_tokens = None\n",
        "+            full_run2.completion_tokens = None\n",
        " \n",
        "             # Mock behaviors\n",
        "             mock_client.list_runs.return_value = iter([flat_run1, flat_run2])\n"
      ]
    },
    {
      "path": "verify_analysis_report.py",
      "status": "modified",
      "additions": 207,
      "deletions": 13,
      "patch": "@@ -32,6 +32,16 @@\n     BottleneckAnalysis,\n     ParallelExecutionEvidence,\n )\n+from analyze_cost import (\n+    analyze_costs,\n+    PricingConfig,\n+    EXAMPLE_PRICING_CONFIGS,\n+    CostAnalysisResults,\n+)\n+from analyze_failures import (\n+    analyze_failures,\n+    FailureAnalysisResults,\n+)\n \n \n def print_header(title: str) -> None:\n@@ -265,11 +275,11 @@ def verify_parallel_execution(\n \n \n def generate_summary_report(\n-        dataset: TraceDataset,\n-        latency_dist: LatencyDistribution,\n-        bottleneck_analysis: BottleneckAnalysis,\n-        parallel_evidence: ParallelExecutionEvidence,\n-    ) -> None:\n+    dataset: TraceDataset,\n+    latency_dist: LatencyDistribution,\n+    bottleneck_analysis: BottleneckAnalysis,\n+    parallel_evidence: ParallelExecutionEvidence,\n+) -> None:\n     \"\"\"Generate final summary with all key findings.\"\"\"\n     print_header(\"ANALYSIS SUMMARY\")\n \n@@ -305,6 +315,156 @@ def generate_summary_report(\n     )\n \n \n+def verify_cost_analysis(\n+    dataset: TraceDataset,\n+    pricing_config: PricingConfig,\n+    expected: Optional[Dict[str, Any]] = None,\n+) -> CostAnalysisResults:\n+    \"\"\"\n+    Verify Phase 3B cost calculations.\n+\n+    Displays:\n+    - Cost per workflow (avg, median, range)\n+    - Top 3 cost drivers\n+    - Scaling projections (10x, 100x, 1000x)\n+    - Cache effectiveness if available\n+    \"\"\"\n+    print_header(\"PHASE 3B: COST ANALYSIS VERIFICATION\")\n+\n+    print(f\"\\nPricing Model: {pricing_config.model_name}\")\n+    print(f\"  Input tokens: ${pricing_config.input_tokens_per_1k}/1K tokens\")\n+    print(f\"  Output tokens: ${pricing_config.output_tokens_per_1k}/1K tokens\")\n+    if pricing_config.cache_read_per_1k:\n+        print(f\"  Cache read tokens: ${pricing_config.cache_read_per_1k}/1K tokens\")\n+\n+    # Run cost analysis\n+    results = analyze_costs(dataset.workflows, pricing_config)\n+\n+    print_section(\"Workflow Cost Statistics\")\n+    print(f\"  Total workflows analyzed: {results.total_workflows_analyzed}\")\n+    print(f\"  Average cost per workflow: ${results.avg_cost_per_workflow:.4f}\")\n+    print(f\"  Median cost per workflow: ${results.median_cost_per_workflow:.4f}\")\n+    print(f\"  Cost range: ${results.min_cost:.4f} - ${results.max_cost:.4f}\")\n+\n+    if expected and \"cost_analysis\" in expected:\n+        exp_cost = expected[\"cost_analysis\"]\n+        check_value(\n+            \"avg_cost_per_workflow\",\n+            results.avg_cost_per_workflow,\n+            exp_cost.get(\"avg_cost_per_workflow\"),\n+        )\n+\n+    print_section(\"Cost Drivers (Top 3 Nodes)\")\n+    for i, node in enumerate(results.node_summaries[:3], 1):\n+        print(\n+            f\"  {i}. {node.node_name}: ${node.total_cost:.4f} ({node.percent_of_total_cost:.1f}%)\"\n+        )\n+        print(\n+            f\"     Executions: {node.execution_count}, Avg: ${node.avg_cost_per_execution:.4f}\"\n+        )\n+\n+    print_section(\"Scaling Projections\")\n+    for scale_label in [\"1x\", \"10x\", \"100x\", \"1000x\"]:\n+        if scale_label in results.scaling_projections:\n+            proj = results.scaling_projections[scale_label]\n+            print(\n+                f\"  {scale_label}: {proj.workflow_count} workflows \u2192 ${proj.total_cost:.2f}\"\n+            )\n+            if proj.cost_per_month_30days:\n+                print(f\"       Monthly (30 days): ${proj.cost_per_month_30days:.2f}\")\n+\n+    if results.cache_effectiveness_percent:\n+        print_section(\"Cache Effectiveness\")\n+        print(f\"  Cache hit rate: {results.cache_effectiveness_percent:.1f}%\")\n+        if results.cache_savings_dollars:\n+            print(f\"  Cost savings: ${results.cache_savings_dollars:.2f}\")\n+\n+    if results.data_quality_notes:\n+        print_section(\"Data Quality Notes\")\n+        for note in results.data_quality_notes:\n+            print(f\"  - {note}\")\n+\n+    return results\n+\n+\n+def verify_failure_analysis(\n+    dataset: TraceDataset, expected: Optional[Dict[str, Any]] = None\n+) -> FailureAnalysisResults:\n+    \"\"\"\n+    Verify Phase 3C failure calculations.\n+\n+    Displays:\n+    - Overall success rate\n+    - Top 5 nodes by failure rate\n+    - Retry analysis (sequences detected, success rate)\n+    - Validator effectiveness\n+    \"\"\"\n+    print_header(\"PHASE 3C: FAILURE PATTERN ANALYSIS VERIFICATION\")\n+\n+    # Run failure analysis\n+    results = analyze_failures(dataset.workflows)\n+\n+    print_section(\"Overall Success/Failure Metrics\")\n+    print(f\"  Total workflows: {results.total_workflows}\")\n+    print(f\"  Successful: {results.successful_workflows}\")\n+    print(f\"  Failed: {results.failed_workflows}\")\n+    print(f\"  Success rate: {results.overall_success_rate_percent:.1f}%\")\n+\n+    if expected and \"failure_analysis\" in expected:\n+        exp_fail = expected[\"failure_analysis\"]\n+        check_value(\n+            \"success_rate\",\n+            results.overall_success_rate_percent,\n+            exp_fail.get(\"success_rate\"),\n+        )\n+\n+    print_section(\"Node Failure Rates (Top 5)\")\n+    for i, node in enumerate(results.node_failure_stats[:5], 1):\n+        print(f\"  {i}. {node.node_name}: {node.failure_rate_percent:.1f}% failure rate\")\n+        print(f\"     {node.failure_count}/{node.total_executions} executions failed\")\n+        if node.retry_sequences_detected > 0:\n+            print(f\"     Retry sequences detected: {node.retry_sequences_detected}\")\n+\n+    print_section(\"Error Distribution\")\n+    if results.error_type_distribution:\n+        sorted_errors = sorted(\n+            results.error_type_distribution.items(),\n+            key=lambda x: x[1],\n+            reverse=True,\n+        )\n+        for error_type, count in sorted_errors[:5]:\n+            print(f\"  - {error_type}: {count} occurrences\")\n+    else:\n+        print(\"  No errors detected\")\n+\n+    print_section(\"Retry Analysis\")\n+    print(f\"  Total retry sequences: {results.total_retry_sequences}\")\n+    if results.retry_success_rate_percent is not None:\n+        print(f\"  Retry success rate: {results.retry_success_rate_percent:.1f}%\")\n+    if results.avg_cost_of_retries is not None:\n+        print(f\"  Avg cost of retries: ${results.avg_cost_of_retries:.4f}\")\n+\n+    if results.validator_analyses:\n+        print_section(\"Validator Effectiveness\")\n+        for validator in results.validator_analyses:\n+            print(f\"  {validator.validator_name}:\")\n+            print(f\"    Executions: {validator.total_executions}\")\n+            print(f\"    Pass rate: {validator.pass_rate_percent:.1f}%\")\n+            print(f\"    Necessary: {'Yes' if validator.is_necessary else 'No'}\")\n+\n+    if results.redundant_validators:\n+        print(\n+            f\"\\n  Potentially redundant validators: {', '.join(results.redundant_validators)}\"\n+        )\n+\n+    if results.quality_risks_at_scale:\n+        print_section(\"Quality Risks at Scale\")\n+        for risk in results.quality_risks_at_scale:\n+            print(f\"  - {risk}\")\n+\n+    return results\n+\n+\n def main() -> int:\n     \"\"\"Main verification script.\"\"\"\n     parser = argparse.ArgumentParser(\n@@ -318,6 +478,18 @@ def main() -> int:\n         type=str,\n         help=\"Optional JSON file with expected values for verification\",\n     )\n+    parser.add_argument(\n+        \"--phases\",\n+        type=str,\n+        default=\"3a\",\n+        help=\"Phases to verify: 3a, 3b, 3c, or all (default: 3a)\",\n+    )\n+    parser.add_argument(\n+        \"--pricing-model\",\n+        type=str,\n+        default=\"gemini_1.5_pro\",\n+        help=\"Pricing model for cost analysis (default: gemini_1.5_pro)\",\n+    )\n \n     args = parser.parse_args()\n \n@@ -341,14 +513,36 @@ def main() -> int:\n     print(f\"\\nLoading data from: {input_path}\")\n     dataset = load_from_json(str(input_path))\n \n-    # Run all verifications\n-    verify_dataset_info(dataset, expected)\n-    latency_dist = verify_latency_distribution(dataset, expected)\n-    bottleneck_analysis = verify_bottleneck_analysis(dataset, expected)\n-    parallel_evidence = verify_parallel_execution(dataset, expected)\n-    generate_summary_report(\n-        dataset, latency_dist, bottleneck_analysis, parallel_evidence\n-    )\n+    phases = args.phases.lower()\n+    run_3a = phases in [\"3a\", \"all\"]\n+    run_3b = phases in [\"3b\", \"all\"]\n+    run_3c = phases in [\"3c\", \"all\"]\n+\n+    # Run Phase 3A verifications\n+    if run_3a:\n+        verify_dataset_info(dataset, expected)\n+        latency_dist = verify_latency_distribution(dataset, expected)\n+        bottleneck_analysis = verify_bottleneck_analysis(dataset, expected)\n+        parallel_evidence = verify_parallel_execution(dataset, expected)\n+        generate_summary_report(\n+            dataset, latency_dist, bottleneck_analysis, parallel_evidence\n+        )\n+\n+    # Run Phase 3B verification (Cost Analysis)\n+    if run_3b:\n+        if args.pricing_model in EXAMPLE_PRICING_CONFIGS:\n+            pricing_config = EXAMPLE_PRICING_CONFIGS[args.pricing_model]\n+        else:\n+            print(\n+                f\"\\nWarning: Unknown pricing model '{args.pricing_model}', using gemini_1.5_pro\"\n+            )\n+            pricing_config = EXAMPLE_PRICING_CONFIGS[\"gemini_1.5_pro\"]\n+\n+        verify_cost_analysis(dataset, pricing_config, expected)\n+\n+    # Run Phase 3C verification (Failure Analysis)\n+    if run_3c:\n+        verify_failure_analysis(dataset, expected)\n \n     # Final status\n     print_header(\"VERIFICATION COMPLETE\")",
      "patch_lines": [
        "@@ -32,6 +32,16 @@\n",
        "     BottleneckAnalysis,\n",
        "     ParallelExecutionEvidence,\n",
        " )\n",
        "+from analyze_cost import (\n",
        "+    analyze_costs,\n",
        "+    PricingConfig,\n",
        "+    EXAMPLE_PRICING_CONFIGS,\n",
        "+    CostAnalysisResults,\n",
        "+)\n",
        "+from analyze_failures import (\n",
        "+    analyze_failures,\n",
        "+    FailureAnalysisResults,\n",
        "+)\n",
        " \n",
        " \n",
        " def print_header(title: str) -> None:\n",
        "@@ -265,11 +275,11 @@ def verify_parallel_execution(\n",
        " \n",
        " \n",
        " def generate_summary_report(\n",
        "-        dataset: TraceDataset,\n",
        "-        latency_dist: LatencyDistribution,\n",
        "-        bottleneck_analysis: BottleneckAnalysis,\n",
        "-        parallel_evidence: ParallelExecutionEvidence,\n",
        "-    ) -> None:\n",
        "+    dataset: TraceDataset,\n",
        "+    latency_dist: LatencyDistribution,\n",
        "+    bottleneck_analysis: BottleneckAnalysis,\n",
        "+    parallel_evidence: ParallelExecutionEvidence,\n",
        "+) -> None:\n",
        "     \"\"\"Generate final summary with all key findings.\"\"\"\n",
        "     print_header(\"ANALYSIS SUMMARY\")\n",
        " \n",
        "@@ -305,6 +315,156 @@ def generate_summary_report(\n",
        "     )\n",
        " \n",
        " \n",
        "+def verify_cost_analysis(\n",
        "+    dataset: TraceDataset,\n",
        "+    pricing_config: PricingConfig,\n",
        "+    expected: Optional[Dict[str, Any]] = None,\n",
        "+) -> CostAnalysisResults:\n",
        "+    \"\"\"\n",
        "+    Verify Phase 3B cost calculations.\n",
        "+\n",
        "+    Displays:\n",
        "+    - Cost per workflow (avg, median, range)\n",
        "+    - Top 3 cost drivers\n",
        "+    - Scaling projections (10x, 100x, 1000x)\n",
        "+    - Cache effectiveness if available\n",
        "+    \"\"\"\n",
        "+    print_header(\"PHASE 3B: COST ANALYSIS VERIFICATION\")\n",
        "+\n",
        "+    print(f\"\\nPricing Model: {pricing_config.model_name}\")\n",
        "+    print(f\"  Input tokens: ${pricing_config.input_tokens_per_1k}/1K tokens\")\n",
        "+    print(f\"  Output tokens: ${pricing_config.output_tokens_per_1k}/1K tokens\")\n",
        "+    if pricing_config.cache_read_per_1k:\n",
        "+        print(f\"  Cache read tokens: ${pricing_config.cache_read_per_1k}/1K tokens\")\n",
        "+\n",
        "+    # Run cost analysis\n",
        "+    results = analyze_costs(dataset.workflows, pricing_config)\n",
        "+\n",
        "+    print_section(\"Workflow Cost Statistics\")\n",
        "+    print(f\"  Total workflows analyzed: {results.total_workflows_analyzed}\")\n",
        "+    print(f\"  Average cost per workflow: ${results.avg_cost_per_workflow:.4f}\")\n",
        "+    print(f\"  Median cost per workflow: ${results.median_cost_per_workflow:.4f}\")\n",
        "+    print(f\"  Cost range: ${results.min_cost:.4f} - ${results.max_cost:.4f}\")\n",
        "+\n",
        "+    if expected and \"cost_analysis\" in expected:\n",
        "+        exp_cost = expected[\"cost_analysis\"]\n",
        "+        check_value(\n",
        "+            \"avg_cost_per_workflow\",\n",
        "+            results.avg_cost_per_workflow,\n",
        "+            exp_cost.get(\"avg_cost_per_workflow\"),\n",
        "+        )\n",
        "+\n",
        "+    print_section(\"Cost Drivers (Top 3 Nodes)\")\n",
        "+    for i, node in enumerate(results.node_summaries[:3], 1):\n",
        "+        print(\n",
        "+            f\"  {i}. {node.node_name}: ${node.total_cost:.4f} ({node.percent_of_total_cost:.1f}%)\"\n",
        "+        )\n",
        "+        print(\n",
        "+            f\"     Executions: {node.execution_count}, Avg: ${node.avg_cost_per_execution:.4f}\"\n",
        "+        )\n",
        "+\n",
        "+    print_section(\"Scaling Projections\")\n",
        "+    for scale_label in [\"1x\", \"10x\", \"100x\", \"1000x\"]:\n",
        "+        if scale_label in results.scaling_projections:\n",
        "+            proj = results.scaling_projections[scale_label]\n",
        "+            print(\n",
        "+                f\"  {scale_label}: {proj.workflow_count} workflows \u2192 ${proj.total_cost:.2f}\"\n",
        "+            )\n",
        "+            if proj.cost_per_month_30days:\n",
        "+                print(f\"       Monthly (30 days): ${proj.cost_per_month_30days:.2f}\")\n",
        "+\n",
        "+    if results.cache_effectiveness_percent:\n",
        "+        print_section(\"Cache Effectiveness\")\n",
        "+        print(f\"  Cache hit rate: {results.cache_effectiveness_percent:.1f}%\")\n",
        "+        if results.cache_savings_dollars:\n",
        "+            print(f\"  Cost savings: ${results.cache_savings_dollars:.2f}\")\n",
        "+\n",
        "+    if results.data_quality_notes:\n",
        "+        print_section(\"Data Quality Notes\")\n",
        "+        for note in results.data_quality_notes:\n",
        "+            print(f\"  - {note}\")\n",
        "+\n",
        "+    return results\n",
        "+\n",
        "+\n",
        "+def verify_failure_analysis(\n",
        "+    dataset: TraceDataset, expected: Optional[Dict[str, Any]] = None\n",
        "+) -> FailureAnalysisResults:\n",
        "+    \"\"\"\n",
        "+    Verify Phase 3C failure calculations.\n",
        "+\n",
        "+    Displays:\n",
        "+    - Overall success rate\n",
        "+    - Top 5 nodes by failure rate\n",
        "+    - Retry analysis (sequences detected, success rate)\n",
        "+    - Validator effectiveness\n",
        "+    \"\"\"\n",
        "+    print_header(\"PHASE 3C: FAILURE PATTERN ANALYSIS VERIFICATION\")\n",
        "+\n",
        "+    # Run failure analysis\n",
        "+    results = analyze_failures(dataset.workflows)\n",
        "+\n",
        "+    print_section(\"Overall Success/Failure Metrics\")\n",
        "+    print(f\"  Total workflows: {results.total_workflows}\")\n",
        "+    print(f\"  Successful: {results.successful_workflows}\")\n",
        "+    print(f\"  Failed: {results.failed_workflows}\")\n",
        "+    print(f\"  Success rate: {results.overall_success_rate_percent:.1f}%\")\n",
        "+\n",
        "+    if expected and \"failure_analysis\" in expected:\n",
        "+        exp_fail = expected[\"failure_analysis\"]\n",
        "+        check_value(\n",
        "+            \"success_rate\",\n",
        "+            results.overall_success_rate_percent,\n",
        "+            exp_fail.get(\"success_rate\"),\n",
        "+        )\n",
        "+\n",
        "+    print_section(\"Node Failure Rates (Top 5)\")\n",
        "+    for i, node in enumerate(results.node_failure_stats[:5], 1):\n",
        "+        print(f\"  {i}. {node.node_name}: {node.failure_rate_percent:.1f}% failure rate\")\n",
        "+        print(f\"     {node.failure_count}/{node.total_executions} executions failed\")\n",
        "+        if node.retry_sequences_detected > 0:\n",
        "+            print(f\"     Retry sequences detected: {node.retry_sequences_detected}\")\n",
        "+\n",
        "+    print_section(\"Error Distribution\")\n",
        "+    if results.error_type_distribution:\n",
        "+        sorted_errors = sorted(\n",
        "+            results.error_type_distribution.items(),\n",
        "+            key=lambda x: x[1],\n",
        "+            reverse=True,\n",
        "+        )\n",
        "+        for error_type, count in sorted_errors[:5]:\n",
        "+            print(f\"  - {error_type}: {count} occurrences\")\n",
        "+    else:\n",
        "+        print(\"  No errors detected\")\n",
        "+\n",
        "+    print_section(\"Retry Analysis\")\n",
        "+    print(f\"  Total retry sequences: {results.total_retry_sequences}\")\n",
        "+    if results.retry_success_rate_percent is not None:\n",
        "+        print(f\"  Retry success rate: {results.retry_success_rate_percent:.1f}%\")\n",
        "+    if results.avg_cost_of_retries is not None:\n",
        "+        print(f\"  Avg cost of retries: ${results.avg_cost_of_retries:.4f}\")\n",
        "+\n",
        "+    if results.validator_analyses:\n",
        "+        print_section(\"Validator Effectiveness\")\n",
        "+        for validator in results.validator_analyses:\n",
        "+            print(f\"  {validator.validator_name}:\")\n",
        "+            print(f\"    Executions: {validator.total_executions}\")\n",
        "+            print(f\"    Pass rate: {validator.pass_rate_percent:.1f}%\")\n",
        "+            print(f\"    Necessary: {'Yes' if validator.is_necessary else 'No'}\")\n",
        "+\n",
        "+    if results.redundant_validators:\n",
        "+        print(\n",
        "+            f\"\\n  Potentially redundant validators: {', '.join(results.redundant_validators)}\"\n",
        "+        )\n",
        "+\n",
        "+    if results.quality_risks_at_scale:\n",
        "+        print_section(\"Quality Risks at Scale\")\n",
        "+        for risk in results.quality_risks_at_scale:\n",
        "+            print(f\"  - {risk}\")\n",
        "+\n",
        "+    return results\n",
        "+\n",
        "+\n",
        " def main() -> int:\n",
        "     \"\"\"Main verification script.\"\"\"\n",
        "     parser = argparse.ArgumentParser(\n",
        "@@ -318,6 +478,18 @@ def main() -> int:\n",
        "         type=str,\n",
        "         help=\"Optional JSON file with expected values for verification\",\n",
        "     )\n",
        "+    parser.add_argument(\n",
        "+        \"--phases\",\n",
        "+        type=str,\n",
        "+        default=\"3a\",\n",
        "+        help=\"Phases to verify: 3a, 3b, 3c, or all (default: 3a)\",\n",
        "+    )\n",
        "+    parser.add_argument(\n",
        "+        \"--pricing-model\",\n",
        "+        type=str,\n",
        "+        default=\"gemini_1.5_pro\",\n",
        "+        help=\"Pricing model for cost analysis (default: gemini_1.5_pro)\",\n",
        "+    )\n",
        " \n",
        "     args = parser.parse_args()\n",
        " \n",
        "@@ -341,14 +513,36 @@ def main() -> int:\n",
        "     print(f\"\\nLoading data from: {input_path}\")\n",
        "     dataset = load_from_json(str(input_path))\n",
        " \n",
        "-    # Run all verifications\n",
        "-    verify_dataset_info(dataset, expected)\n",
        "-    latency_dist = verify_latency_distribution(dataset, expected)\n",
        "-    bottleneck_analysis = verify_bottleneck_analysis(dataset, expected)\n",
        "-    parallel_evidence = verify_parallel_execution(dataset, expected)\n",
        "-    generate_summary_report(\n",
        "-        dataset, latency_dist, bottleneck_analysis, parallel_evidence\n",
        "-    )\n",
        "+    phases = args.phases.lower()\n",
        "+    run_3a = phases in [\"3a\", \"all\"]\n",
        "+    run_3b = phases in [\"3b\", \"all\"]\n",
        "+    run_3c = phases in [\"3c\", \"all\"]\n",
        "+\n",
        "+    # Run Phase 3A verifications\n",
        "+    if run_3a:\n",
        "+        verify_dataset_info(dataset, expected)\n",
        "+        latency_dist = verify_latency_distribution(dataset, expected)\n",
        "+        bottleneck_analysis = verify_bottleneck_analysis(dataset, expected)\n",
        "+        parallel_evidence = verify_parallel_execution(dataset, expected)\n",
        "+        generate_summary_report(\n",
        "+            dataset, latency_dist, bottleneck_analysis, parallel_evidence\n",
        "+        )\n",
        "+\n",
        "+    # Run Phase 3B verification (Cost Analysis)\n",
        "+    if run_3b:\n",
        "+        if args.pricing_model in EXAMPLE_PRICING_CONFIGS:\n",
        "+            pricing_config = EXAMPLE_PRICING_CONFIGS[args.pricing_model]\n",
        "+        else:\n",
        "+            print(\n",
        "+                f\"\\nWarning: Unknown pricing model '{args.pricing_model}', using gemini_1.5_pro\"\n",
        "+            )\n",
        "+            pricing_config = EXAMPLE_PRICING_CONFIGS[\"gemini_1.5_pro\"]\n",
        "+\n",
        "+        verify_cost_analysis(dataset, pricing_config, expected)\n",
        "+\n",
        "+    # Run Phase 3C verification (Failure Analysis)\n",
        "+    if run_3c:\n",
        "+        verify_failure_analysis(dataset, expected)\n",
        " \n",
        "     # Final status\n",
        "     print_header(\"VERIFICATION COMPLETE\")\n"
      ]
    }
  ]
}
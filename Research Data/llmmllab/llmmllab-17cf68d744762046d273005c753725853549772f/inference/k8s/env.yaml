apiVersion: v1
kind: ConfigMap
metadata:
  name: inference-config
  namespace: ollama
data:
  # Authentication configuration
  AUTH_ISSUER: "https://auth.longstorymedia.com"
  AUTH_AUDIENCE: "lsm-client"
  AUTH_CLIENT_ID: "lsm-client"
  AUTH_JWKS_URI: "https://auth.longstorymedia.com/keys"

  # Database configuration
  DB_HOST: "192.168.0.71"
  DB_PORT: "32345"
  DB_USER: "lsm"
  DB_NAME: "llmmll"
  DB_SSLMODE: "disable"

  # Redis configuration
  REDIS_ENABLED: "true"
  REDIS_HOST: "192.168.0.71"
  REDIS_PORT: "32346"
  REDIS_DB: "0"
  REDIS_CONVERSATION_TTL: "360"
  REDIS_MESSAGE_TTL: "180"
  REDIS_SUMMARY_TTL: "720"
  REDIS_POOL_SIZE: "10"
  REDIS_MIN_IDLE_CONNECTIONS: "2"
  REDIS_CONNECT_TIMEOUT: "5"

  # Summarization configuration
  MESSAGES_BEFORE_SUMMARY: "6"
  SUMMARIES_BEFORE_CONSOLIDATION: "3"
  SUMMARY_MODEL: "qwen3:0.6b"
  SUMMARY_SYSTEM_PROMPT: "compress the following conversation so far into a concise paragraph. Include key points and conclusions, but omit redundant details. The summary will be used as context for future interaction. It should be as small as possible and does not need to be human readable."
  MAX_SUMMARY_LEVELS: "3"
  SUMMARY_WEIGHT_COEFFICIENT: "0.7"
  MASTER_SUMMARY_PROMPT: "Create a comprehensive summary of the conversation, giving most weight to the most recent points and gradually less weight to older information. This is a master summary that will be used for long-term context."

  # Image generation configuration
  IMAGE_GENERATION_ENABLED: "true"
  IMAGE_DIR: "/root/images"
  MAX_IMAGE_SIZE: "2048"
  IMAGE_RETENTION_HOURS: "24"

  # RabbitMQ configuration
  RABBITMQ_ENABLED: "true"
  RABBITMQ_HOST: "rabbitmq-0.rabbitmq.rabbitmq.svc.cluster.local"
  RABBITMQ_PORT: "5672"
  RABBITMQ_USER: "lsm"
  RABBITMQ_VHOST: "/"

  # Inference services configuration
  INFERENCE_SERVICES_OLLAMA_BASE_URL: "http://localhost:11434"
  INFERENCE_SERVICES_SD_BASE_URL: "http://localhost:8000"
  INFERENCE_SERVICES_HOST: "0.0.0.0"
  INFERENCE_SERVICES_PORT: "50051"

  # Internal security configuration
  INTERNAL_ALLOWED_IPS: "192.168.0.0/24,10.43.0.0/16"

  # CUDA and Memory Optimization Configuration
  # Comprehensive CUDA graphs disabling for stability
  LLAMA_CUDA_USE_GRAPHS: "0"
  GGML_CUDA_USE_GRAPHS: "0"
  CUDA_USE_GRAPHS: "0"
  LLAMA_GRAPH: "0"
  GGML_GRAPH: "0"

  # Force basic CUBLAS instead of optimized routines
  LLAMA_CUBLAS: "1"
  GGML_CUDA_FORCE_CUBLAS: "1"
  GGML_CUDA_FORCE_MMQ: "0"

  # Memory management optimizations for multi-GPU setup
  GGML_CUDA_POOL_SIZE: "2147483648" # 2GB pool limit per GPU
  LLAMA_CUDA_POOL_SIZE: "2147483648"
  CUDA_MEMORY_FRACTION: "0.8" # Use max 80% of VRAM per device

  # Additional CUDA optimizations
  GGML_USE_CUBLAS: "1"
  GGML_CUDA_NO_PINNED: "1"
  CUDA_VISIBLE_DEVICES: "0,1,2" # Explicitly list available devices
  CUDA_DEVICE_ORDER: "PCI_BUS_ID"

  # Python memory optimizations
  PYTHONMALLOC: "malloc"
  MALLOC_ARENA_MAX: "2"

  # Logging configuration for debugging
  GGML_LOG_LEVEL: "2"

---
apiVersion: v1
kind: Secret
metadata:
  name: inference-secrets
  namespace: ollama
type: Opaque
stringData:
  AUTH_CLIENT_SECRET: "41c0f4c2e03e6043a74cb824"
  DB_PASSWORD: "7cb9c812e384e16c911a72f1066517d205e8641b78edb3b1b3c78d0c351b1885"
  REDIS_PASSWORD: ""
  RABBITMQ_PASSWORD: "5b49181022c25ea05a1ac485"
  INTERNAL_API_KEY: "4a6204749cf376a6f4feaa529760fc52"

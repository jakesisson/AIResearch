"BenchmarkResult(score=0.0, total_questions=2, correct_answers=0, detailed_results=[{'task_id': 'HumanEval/0', 'is_correct': False, 'confidence': 0.9, 'generated_code': '\ud83e\udd14 Processing with LangGraph...\\n\\n', 'evaluation_metadata': {'extracted_code_length': 32, 'extraction_confidence': 1.0, 'evaluation_methods': [], 'static_analysis': {'has_function_def': False, 'has_return_statement': False, 'proper_indentation': False, 'no_syntax_errors': False, 'expected_function_name': None, 'syntax_error': \"invalid character '\ud83e\udd14' (U+1F914) (<unknown>, line 1)\"}}}, {'task_id': 'HumanEval/1', 'is_correct': False, 'confidence': 0.9, 'generated_code': '\ud83e\udd14 Processing with LangGraph...\\n\\n', 'evaluation_metadata': {'extracted_code_length': 32, 'extraction_confidence': 1.0, 'evaluation_methods': [], 'static_analysis': {'has_function_def': False, 'has_return_statement': False, 'proper_indentation': False, 'no_syntax_errors': False, 'expected_function_name': None, 'syntax_error': \"invalid character '\ud83e\udd14' (U+1F914) (<unknown>, line 1)\"}}}], metadata={'benchmark': 'HumanEval', 'description': 'Code generation and functional correctness', 'model_id': 'qwen3-30b-a3b-q4-k-m', 'temperature': 0.1, 'max_tokens': 400})"
"BenchmarkResult(score=0.0, total_questions=2, correct_answers=0, detailed_results=[{'question_id': 0, 'subject': 'Chemistry', 'correct_answer': 'B', 'model_answer': 'UNKNOWN', 'extraction_confidence': 0.0, 'eval_confidence': 0.0, 'is_correct': False, 'metadata': {'reason': 'no_answer_extracted'}, 'response': '\ud83e\udd14 Processing with LangGraph...\\n\\n'}, {'question_id': 1, 'subject': 'Biology', 'correct_answer': 'C', 'model_answer': 'UNKNOWN', 'extraction_confidence': 0.0, 'eval_confidence': 0.0, 'is_correct': False, 'metadata': {'reason': 'no_answer_extracted'}, 'response': '\ud83e\udd14 Processing with LangGraph...\\n\\n'}], metadata={'benchmark': 'GPQA-Diamond', 'description': 'Graduate-level science questions'})"
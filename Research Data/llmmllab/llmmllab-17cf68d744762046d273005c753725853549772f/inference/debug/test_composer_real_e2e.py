"""
Composer-based Real End-to-End Pipeline Test

This test validates the complete LLM ML Lab pipeline using the new composer architecture:
1. Real user creation in database
2. Real model profile creation
3. Real conversation and message creation
4. **Composer workflow execution** using compose_workflow, create_initial_state, execute_workflow
5. Real tool integration via LangGraph workflows
6. Real output validation
7. Complete cleanup of all created data

This modernized version uses the composer/__init__.py entry points and follows
the new architectural patterns with LangGraph workflows instead of direct pipeline calls.
"""

import asyncio
import time
import uuid
import json
import os
import sys
from datetime import datetime, timezone
from typing import Dict, Any, Optional

from utils.logging import llmmllogger

# Configure logging
logger = llmmllogger.bind(component="composer_e2e_test")


class ComposerRealEndToEndTester:
    """Real end-to-end test using composer architecture."""

    def __init__(
        self,
        target_model: Optional[str] = None,
        capture_llm_output: bool = True,
        print_output: bool = False,
    ):
        """Initialize composer-based pipeline tester."""
        self.test_user_id = f"test_composer_user_{uuid.uuid4().hex[:8]}"
        self.test_model_profile_id = uuid.uuid4()
        self.test_conversation_id: Optional[int] = None
        self.test_message_id: Optional[int] = None
        self.storage = None  # Will be initialized with infrastructure

        # LLM output capture configuration
        self.capture_llm_output = capture_llm_output
        self.print_output = print_output
        self.llm_output_file = None
        self.output_dir = "debug/out"
        self.llm_responses = []  # Store all LLM responses for analysis

        # Support multiple models for comprehensive testing
        available_models = [
            "qwen3-30b-a3b-q4-k-m",  # Primary model - use this as default
            "openai-gpt-oss-20b-uncensored-q5_1",
            "qwen2.5-vl-32b-instruct-q4-k-m",
        ]

        self.target_model = target_model or available_models[0]
        self.available_models = available_models

        # Initialize LLM output file if capture is enabled
        if self.capture_llm_output:
            self._initialize_llm_output_file()

    def _initialize_llm_output_file(self):
        """Initialize the file for capturing LLM-generated text."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_safe = (
            self.target_model.replace("/", "_").replace("-", "_").replace(":", "_")
        )

        # Ensure debug/out directory exists
        os.makedirs(self.output_dir, exist_ok=True)

        self.llm_output_file = (
            f"{self.output_dir}/composer_llm_output_{model_safe}_{timestamp}.txt"
        )

        # Create the file with header
        try:
            with open(self.llm_output_file, "w", encoding="utf-8") as f:
                f.write("Composer LLM Output Capture - Real End-to-End Test\n")
                f.write("=" * 60 + "\n")
                f.write(f"Model: {self.target_model}\n")
                f.write(f"Test User: {self.test_user_id}\n")
                f.write("Architecture: Composer + LangGraph\n")
                f.write(f"Timestamp: {datetime.now(timezone.utc).isoformat()}\n")
                f.write("=" * 60 + "\n\n")
            logger.info(f"üìù LLM output will be captured to: {self.llm_output_file}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Failed to initialize LLM output file: {e}")
            self.capture_llm_output = False

    def _write_llm_response(
        self, phase: str, response_text: str, metadata: Optional[Dict[str, Any]] = None
    ):
        """Write LLM response to file with phase information."""
        if not self.capture_llm_output or not self.llm_output_file:
            return

        try:
            with open(self.llm_output_file, "a", encoding="utf-8") as f:
                f.write(f"\n{'='*60}\n")
                f.write(f"PHASE: {phase}\n")
                f.write(f"TIMESTAMP: {datetime.now(timezone.utc).isoformat()}\n")
                if metadata:
                    f.write(f"METADATA: {json.dumps(metadata, indent=2)}\n")
                f.write(f"{'='*60}\n")
                f.write(f"{response_text}\n")

            # Also store in memory for analysis
            self.llm_responses.append(
                {
                    "phase": phase,
                    "response": response_text,
                    "metadata": metadata or {},
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                }
            )

            logger.info(
                f"üìù Captured LLM response for {phase} ({len(response_text)} chars)"
            )
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Failed to write LLM response to file: {e}")

    def _write_detailed_data(
        self, section: str, title: str, data: Any, description: str = ""
    ):
        """Write detailed data (prompts, tools, messages, etc.) to output file."""
        if not self.capture_llm_output or not self.llm_output_file:
            return

        try:
            with open(self.llm_output_file, "a", encoding="utf-8") as f:
                f.write(f"\n{'='*60}\n")
                f.write(f"SECTION: {section}\n")
                f.write(f"TITLE: {title}\n")
                f.write(f"TIMESTAMP: {datetime.now(timezone.utc).isoformat()}\n")
                if description:
                    f.write(f"DESCRIPTION: {description}\n")
                f.write(f"{'='*60}\n")

                # Format data based on type
                if isinstance(data, (dict, list)):
                    f.write(json.dumps(data, indent=2, default=str))
                elif hasattr(data, "__dict__"):
                    # Object with attributes - convert to dict
                    f.write(json.dumps(data.__dict__, indent=2, default=str))
                elif isinstance(data, str):
                    f.write(data)
                else:
                    f.write(str(data))

                f.write(f"\n{'='*60}\n")

            logger.info(f"üìù Captured detailed data: {section} - {title}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Failed to write detailed data to file: {e}")

    def _write_workflow_event(
        self, event_type: str, event_data: Any, context: str = ""
    ):
        """Write workflow event data to output file for debugging."""
        if not self.capture_llm_output or not self.llm_output_file:
            return

        try:
            with open(self.llm_output_file, "a", encoding="utf-8") as f:
                f.write(f"\n{'='*60}\n")
                f.write(f"WORKFLOW EVENT: {event_type}\n")
                f.write(f"TIMESTAMP: {datetime.now(timezone.utc).isoformat()}\n")
                if context:
                    f.write(f"CONTEXT: {context}\n")
                f.write(f"{'='*60}\n")

                # Format event data
                if isinstance(event_data, (dict, list)):
                    # Pretty print JSON
                    f.write(json.dumps(event_data, indent=2, default=str))
                elif hasattr(event_data, "__dict__"):
                    # Object - convert to dict
                    f.write(json.dumps(vars(event_data), indent=2, default=str))
                else:
                    f.write(str(event_data))

                f.write(f"\n{'='*60}\n")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Failed to write workflow event to file: {e}")

    def _parse_response_as_json(self, response_text: str) -> Any:
        """
        Try to parse response as JSON, return original if not valid JSON.

        Args:
            response_text: The response text to parse

        Returns:
            Parsed JSON if valid, otherwise the original text
        """
        try:
            return json.loads(response_text)
        except (json.JSONDecodeError, TypeError):
            return response_text

    def _get_model_specific_system_prompt(self) -> str:
        """Get model-specific system prompt with appropriate tool calling format."""
        base_info = """You are a helpful AI assistant with access to web search tools.
Knowledge cutoff: 2024-06
Current date: 2025-10-07

IMPORTANT: You have access to the following tools and MUST use them when needed:
- web_search: Search for current information on the web
- memory_retrieval: Retrieve relevant memories 
- summarization: Summarize content

CRITICAL: For ANY request about 2024 information, current events, or recent developments, you MUST use the web_search tool to get up-to-date information. Do not say you cannot access real-time data - you can and should use the tools provided.

TOOL CALLING CAPABILITIES:
- You can make MULTIPLE tool calls in a single response (call multiple tools simultaneously)
- You can make SEQUENTIAL tool calls (call a tool, analyze results, then call more tools)
- You should use tools iteratively to gather comprehensive information
- For complex queries, break them into multiple searches and use multiple tools

You provide direct, informative responses while showing your reasoning process.
Always use tools when you need current or specific information that might not be in your training data.
"""

        # For Qwen models: Use Qwen3 native XML tool calling format
        if "qwen" in self.target_model.lower():
            return (
                base_info
                + """

## TOOL USAGE FORMAT:

When you need current information, use this exact format:

<tool_call>
{"name": "web_search", "arguments": {"query": "your search query here"}}
</tool_call>

For comprehensive responses, you can make MULTIPLE tool calls:

<tool_call>
{"name": "web_search", "arguments": {"query": "major AI model releases 2024"}}
</tool_call>

<tool_call>
{"name": "web_search", "arguments": {"query": "AI safety developments 2024"}}
</tool_call>

<tool_call>
{"name": "web_search", "arguments": {"query": "AI research breakthroughs 2024"}}
</tool_call>

For ANY request about recent developments, current events, or 2024+ information, you MUST use the web_search tool.
If a query has multiple aspects (like the example above), use MULTIPLE SEARCHES to be comprehensive.

EXAMPLE: For the user's query about AI developments in 2024, you should use multiple searches:
1. Search for major AI model releases
2. Search for AI safety developments  
3. Search for recent research breakthroughs
4. Optionally summarize the combined results
"""
            )

        # For other models: Standard format
        else:
            return (
                base_info
                + """

## TOOL USAGE:

Use tools when you need current information or specific data not in your training.
Always explain your reasoning and what information you're looking for.
"""
            )

    async def run_full_test(self) -> Dict[str, Any]:
        """Run complete composer-based end-to-end pipeline test."""
        logger.info("üöÄ Starting Composer Real End-to-End Pipeline Test")
        logger.info("=" * 80)

        test_results = {
            "overall_success": False,
            "execution_time": 0,
            "workflow_time": 0,
            "components_passed": 0,
            "total_components": 8,  # Added cleanup as a tracked component
            "results": {},
            "entities_created": 0,
            "entities_cleaned": 0,
        }

        start_time = time.time()

        try:
            # Phase 1: Real Infrastructure Setup
            logger.info("üìã Phase 1: Real Infrastructure Setup")
            infrastructure_result = await self.setup_real_infrastructure()
            test_results["results"]["infrastructure_setup"] = infrastructure_result
            if infrastructure_result["success"]:
                test_results["components_passed"] += 1

            # Phase 2: Composer Service Initialization
            logger.info("üéº Phase 2: Composer Service Initialization")
            composer_result = await self.initialize_composer_service()
            test_results["results"]["composer_initialization"] = composer_result
            if composer_result["success"]:
                test_results["components_passed"] += 1

            # Phase 3: Real User & Model Profile Creation
            logger.info("üë§ Phase 3: Real User & Model Profile Creation")
            user_profile_result = await self.create_real_user_and_profile()
            test_results["results"]["user_profile_creation"] = user_profile_result
            if user_profile_result["success"]:
                test_results["components_passed"] += 1

            # Phase 4: Real Conversation Creation
            logger.info("üí¨ Phase 4: Real Conversation Creation")
            conversation_result = await self.create_real_conversation()
            test_results["results"]["conversation_creation"] = conversation_result
            if conversation_result["success"]:
                test_results["components_passed"] += 1

            # Phase 5: Real Message with Tool Context
            logger.info("üìù Phase 5: Real Message with Tool Context")
            message_result = await self.create_real_message_with_tools()
            test_results["results"]["message_creation"] = message_result
            if message_result["success"]:
                test_results["components_passed"] += 1

            # Phase 6: Composer Workflow Execution (THE KEY TEST)
            logger.info("üéº Phase 6: Composer Workflow Execution")
            workflow_result = await self.execute_composer_workflow()
            test_results["results"]["workflow_execution"] = workflow_result
            if workflow_result["success"]:
                test_results["components_passed"] += 1
                test_results["workflow_time"] = workflow_result.get("execution_time", 0)

            # Phase 7: Real Output Validation
            logger.info("‚úÖ Phase 7: Real Output Validation")
            validation_result = await self.validate_real_outputs()
            test_results["results"]["output_validation"] = validation_result
            if validation_result["success"]:
                test_results["components_passed"] += 1

            # Calculate success
            all_passed = (
                test_results["components_passed"] == test_results["total_components"]
            )
            test_results["overall_success"] = all_passed
            test_results["execution_time"] = time.time() - start_time

        except Exception as e:
            logger.error(f"‚ùå Test execution failed: {str(e)}")
            test_results["error"] = str(e)
            import traceback

            traceback.print_exc()

        finally:
            # Always attempt cleanup - but track as a component that can fail the test
            logger.info("üßπ Phase 8: Real Data Cleanup")
            cleanup_result = await self.cleanup_real_data()
            test_results["results"]["data_cleanup"] = cleanup_result
            test_results["entities_cleaned"] = cleanup_result.get("cleaned_count", 0)

            # Cleanup failure should fail the overall test
            if cleanup_result.get("success", False):
                test_results["components_passed"] += 1
                logger.info("   ‚úÖ Cleanup validation passed")
            else:
                logger.error("   ‚ùå Cleanup validation failed - test marked as failure")
                # Don't increment components_passed for failed cleanup

            # Recalculate overall success after cleanup (which can fail the test)
            all_passed_including_cleanup = (
                test_results["components_passed"] == test_results["total_components"]
            )
            test_results["overall_success"] = all_passed_including_cleanup

            # Log cleanup impact on overall result
            if not cleanup_result.get("success", False):
                logger.error(
                    f"   ‚ùå Overall test FAILED due to cleanup issues (components: {test_results['components_passed']}/{test_results['total_components']})"
                )

            # Finalize LLM output capture
            self._finalize_llm_output()

        # Print comprehensive results
        await self.print_test_summary(test_results)
        self._print_llm_output_summary()

        return test_results

    async def setup_real_infrastructure(self) -> Dict[str, Any]:
        """Set up real infrastructure components."""
        logger.info("üèóÔ∏è  Setting up real infrastructure...")

        try:
            # Initialize real database connection
            from db import storage

            # Build connection string from environment variables
            db_host = os.getenv("DB_HOST", "localhost")
            db_port = os.getenv("DB_PORT", "5432")
            db_user = os.getenv("DB_USER", "lsm")
            db_password = os.getenv("DB_PASSWORD", "")
            db_name = os.getenv("DB_NAME", "llmmll")
            db_sslmode = os.getenv("DB_SSLMODE", "disable")

            connection_string = f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}?sslmode={db_sslmode}"

            await storage.initialize(connection_string)

            # Verify storage is properly initialized
            if not storage.initialized:
                raise RuntimeError("Storage failed to initialize properly")

            self.storage = storage
            logger.info("   ‚úÖ Database connection established")

            return {
                "success": True,
                "database_connected": True,
                "target_model": self.target_model,
            }

        except Exception as e:
            logger.error(f"   ‚ùå Infrastructure setup failed: {str(e)}")
            return {"success": False, "error": str(e)}

    async def initialize_composer_service(self) -> Dict[str, Any]:
        """Initialize the composer service."""
        logger.info("üéº Initializing composer service...")

        try:
            # Import and initialize composer
            from composer import initialize_composer, get_composer_service

            await initialize_composer()
            logger.info("   ‚úÖ Composer service initialized")

            # Test service availability
            service = get_composer_service()
            logger.info(f"   ‚úÖ Composer service available: {type(service).__name__}")

            return {
                "success": True,
                "service_initialized": True,
                "service_type": type(service).__name__,
            }

        except Exception as e:
            logger.error(f"   ‚ùå Composer initialization failed: {str(e)}")
            import traceback

            traceback.print_exc()
            return {"success": False, "error": str(e)}

    async def create_real_user_and_profile(self) -> Dict[str, Any]:
        """Create real user and model profile in database."""
        logger.info("üë§ Creating real user and model profile...")

        try:
            from db import storage
            from models.model_profile import ModelProfile
            from models.model_parameters import ModelParameters
            from models.gpu_config import GPUConfig

            # Ensure storage is available
            if not storage or not storage.model_profile:
                raise RuntimeError("Storage or model_profile service not available")

            # Ensure user exists in database (will create if not exists)
            logger.info(f"   ‚úÖ Using user ID: {self.test_user_id}")

            # Create real model profile with enhanced system prompt for tool usage
            # Configure context size based on model
            num_ctx = 100000 if "qwen3" in self.target_model else 40960
            system_prompt = self._get_model_specific_system_prompt()

            # Capture the system prompt to output file
            self._write_detailed_data(
                section="MODEL_PROFILE",
                title="System Prompt",
                data=system_prompt,
                description=f"System prompt used for model {self.target_model}",
            )

            # Explicitly request maximal GPU layer placement (-1 = all) for test visibility
            model_profile = ModelProfile(
                id=self.test_model_profile_id,
                user_id=self.test_user_id,
                name=f"Composer Test {self.target_model.upper()} Profile",
                description=f"Composer test profile for {self.target_model}",
                model_name=self.target_model,
                parameters=ModelParameters(
                    temperature=0.7,
                    top_p=0.9,
                    max_tokens=4000,
                    num_ctx=num_ctx,
                    flash_attention=True,
                ),
                gpu_config=GPUConfig(gpu_layers=-1),
                system_prompt=system_prompt,
                type=0,  # ModelProfileType.Primary
                created_at=datetime.now(timezone.utc),
                updated_at=datetime.now(timezone.utc),
                model_version="1.0",
            )

            # Capture the complete model profile to output file
            self._write_detailed_data(
                section="MODEL_PROFILE",
                title="Complete Model Profile",
                data={
                    "id": str(model_profile.id),
                    "user_id": model_profile.user_id,
                    "name": model_profile.name,
                    "description": model_profile.description,
                    "model_name": model_profile.model_name,
                    "parameters": model_profile.parameters.__dict__,
                    "system_prompt": model_profile.system_prompt,
                    "type": model_profile.type,
                    "model_version": model_profile.model_version,
                },
                description=f"Complete model profile configuration for {self.target_model}",
            )

            # Store model profile in database
            created_profile = await storage.model_profile.create_model_profile(
                model_profile
            )

            logger.info(f"   ‚úÖ Created real model profile: {created_profile.name}")

            # Update user config to use this profile as primary
            if hasattr(storage, "user_config") and storage.user_config:
                try:
                    # Get existing user config (creates default if doesn't exist)
                    user_config = await storage.user_config.get_user_config(
                        self.test_user_id
                    )

                    # Update the primary profile ID to use our created profile
                    user_config.model_profiles.primary_profile_id = (
                        self.test_model_profile_id
                    )

                    # Save the updated config
                    await storage.user_config.update_user_config(
                        self.test_user_id, user_config
                    )
                    logger.info(
                        f"   ‚úÖ Updated user config with primary profile: {self.test_model_profile_id}"
                    )
                except Exception as e:
                    logger.warning(f"   ‚ö†Ô∏è Could not update user config: {e}")
                    # Continue without user config - test may still work with profile lookup
                    pass

            return {
                "success": True,
                "user_id": self.test_user_id,
                "model_profile_id": str(self.test_model_profile_id),
                "model_name": self.target_model,
            }

        except Exception as e:
            logger.error(f"   ‚ùå User/profile creation failed: {str(e)}")
            import traceback

            traceback.print_exc()
            return {"success": False, "error": str(e)}

    async def create_real_conversation(self) -> Dict[str, Any]:
        """Create real conversation in database."""
        logger.info("üí¨ Creating real conversation...")

        try:
            from db import storage

            # Ensure storage is available
            if not storage or not storage.pool or not storage.conversation:
                raise RuntimeError("Storage components not available")

            # Ensure user exists in database first
            async with storage.pool.acquire() as conn:
                await conn.execute(
                    "INSERT INTO users (id) VALUES ($1) ON CONFLICT (id) DO NOTHING",
                    self.test_user_id,
                )
            logger.info(f"   ‚úÖ Ensured user exists: {self.test_user_id}")

            # Create real conversation
            conversation_id = await storage.conversation.create_conversation(
                user_id=self.test_user_id,
                title="Composer Real End-to-End Test Conversation",
            )

            if not conversation_id:
                raise RuntimeError("Failed to create conversation")

            self.test_conversation_id = conversation_id

            logger.info(f"   ‚úÖ Created real conversation ID: {conversation_id}")

            return {"success": True, "conversation_id": conversation_id}

        except Exception as e:
            logger.error(f"   ‚ùå Conversation creation failed: {str(e)}")
            import traceback

            traceback.print_exc()
            return {"success": False, "error": str(e)}

    async def create_real_message_with_tools(self) -> Dict[str, Any]:
        """Create real user message with tool-calling context."""
        logger.info("üìù Creating real message with tool context...")

        try:
            from db import storage
            from models.message import Message
            from models.message_role import MessageRole
            from models.message_content import MessageContent, MessageContentType

            # Ensure storage is available
            if not storage or not storage.message:
                raise RuntimeError("Storage message service not available")

            # Create a message that will benefit from tool usage
            query_text = """I need current information about the latest developments in artificial intelligence in 2024. 
Specifically, I'm interested in:
1. Major AI model releases in 2024
2. Recent breakthroughs in AI research
3. Current AI safety developments

Please search for the most recent information and provide a comprehensive summary."""

            content_list = [
                MessageContent(type=MessageContentType.TEXT, text=query_text)
            ]

            user_message = Message(
                id=None,  # Will be assigned by database
                conversation_id=self.test_conversation_id,
                role=MessageRole.USER,
                content=content_list,
                created_at=datetime.now(timezone.utc),
            )

            # Add message to database
            message_id = await storage.message.add_message(user_message)
            if not message_id:
                raise RuntimeError("Failed to add message to database")

            self.test_message_id = message_id

            logger.info(f"   ‚úÖ Created real user message ID: {message_id}")
            logger.info(f"   üìã Message length: {len(query_text)} characters")

            return {
                "success": True,
                "message_id": message_id,
                "message_length": len(query_text),
            }

        except Exception as e:
            logger.error(f"   ‚ùå Message creation failed: {str(e)}")
            import traceback

            traceback.print_exc()
            return {"success": False, "error": str(e)}

    async def execute_composer_workflow(self) -> Dict[str, Any]:
        """Execute composer workflow using the new architecture."""
        logger.info("üéº Executing composer workflow...")

        try:
            # Import composer functions
            from composer import (
                compose_workflow,
                create_initial_state,
                execute_workflow,
            )
            from models import WorkflowType
            from db import storage

            # Ensure we have required IDs
            if not self.test_conversation_id or not storage or not storage.message:
                raise Exception("Missing required components for workflow execution")

            # Get conversation messages for context
            messages = await storage.message.get_conversation_history(
                self.test_conversation_id
            )
            if not messages:
                raise Exception("No messages found for conversation")

            logger.info(f"   üìù Processing {len(messages)} messages")

            # Capture conversation history
            messages_data = []
            for msg in messages:
                msg_data = {
                    "id": msg.id,
                    "role": msg.role,
                    "content": [
                        {"type": content.type.value, "text": content.text}
                        for content in msg.content
                    ],
                    "created_at": (msg.created_at if msg.created_at else None),
                }
                messages_data.append(msg.model_dump_json())

            # Step 1: Compose workflow for user
            logger.info("   üéº Step 1: Composing workflow...")
            workflow = await compose_workflow(self.test_user_id)
            try:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_path = f"{self.output_dir}/workflow_graph_{timestamp}.md"
                doc = workflow.get_graph().draw_mermaid(
                    with_styles=True,
                )
                with open(output_path, "w", encoding="utf-8") as f:
                    f.write("```mermaid\n")
                    f.write(doc)
                    f.write("\n```\n")
                logger.info(f"  Workflow graph saved: {output_path}")
                logger.info(f"   ‚úÖ Workflow composed: {type(workflow).__name__}")
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è Could not generate workflow graph image: {e}")
                pass

            # Step 2: Create initial state
            logger.info("   üéº Step 2: Creating initial state...")
            initial_state = await create_initial_state(
                user_id=self.test_user_id,
                conversation_id=self.test_conversation_id,
            )
            logger.info(f"   ‚úÖ Initial state created: {type(initial_state).__name__}")

            # Capture initial state information
            initial_state_data = {
                "state_type": type(initial_state).__name__,
                "user_id": self.test_user_id,
                "workflow_type": WorkflowType.GENERAL,
                "message_count": len(messages),
            }

            # Try to extract more state data if possible
            if hasattr(initial_state, "__dict__"):
                try:
                    state_dict = {}
                    for key, value in initial_state.__dict__.items():
                        # Convert complex objects to string representations
                        if hasattr(value, "__dict__"):
                            state_dict[key] = str(value)
                        elif isinstance(
                            value, (list, dict, str, int, float, bool, type(None))
                        ):
                            state_dict[key] = value
                        else:
                            state_dict[key] = str(value)
                    initial_state_data["state_contents"] = state_dict
                except Exception as e:
                    logger.warning(f"Could not extract full state data: {e}")

            self._write_detailed_data(
                section="WORKFLOW",
                title="Initial State",
                data=initial_state_data,
                description="Initial workflow state before execution",
            )

            # Step 3: Execute workflow with streaming
            logger.info("   üéº Step 3: Executing workflow with streaming...")
            start_time = time.time()
            response_chunks = []
            tool_calls_detected = False
            full_response = ""
            error_events: list = []

            # Capture workflow events and detailed data
            event_count = 0
            tool_events = []
            model_events = []

            async for event in execute_workflow(workflow, initial_state, stream=True):
                event_count += 1

                # Process different types of events - use dict access for safety
                event_dict = event if isinstance(event, dict) else {}

                # Check if this event contains tool execution information
                if hasattr(event_dict, "__dict__") and hasattr(event_dict, "node_name"):
                    # This could be a node execution event
                    if getattr(event_dict, "node_name", None) == "tool_executor":
                        tool_calls_detected = True
                        logger.info("   üõ†Ô∏è  Tool execution node detected")

                # Check for tool execution by looking for node names or metadata
                if "metadata" in event_dict and isinstance(
                    event_dict.get("metadata"), dict
                ):
                    metadata = event_dict["metadata"]
                    if (
                        metadata.get("langgraph_node") == "tool_executor"
                        or "tool" in str(metadata).lower()
                    ):
                        tool_calls_detected = True
                        logger.info("   üõ†Ô∏è  Tool executor node detected in metadata")

                # Capture significant events for detailed logging
                if "event" in event_dict:
                    event_type = event_dict["event"]
                    # Capture explicit error events (LangGraph may surface as on_chain_error / on_tool_error etc.)
                    if (
                        event_type.lower().endswith("error")
                        or "error" in event_type.lower()
                    ):
                        error_events.append(event_dict)

                    # Log significant events to output file
                    if event_type in [
                        "on_chat_model_start",
                        "on_chat_model_end",
                        "on_tool_start",
                        "on_tool_end",
                        "on_chain_start",
                        "on_chain_end",
                    ]:
                        self._write_workflow_event(
                            event_type=event_type,
                            event_data=event_dict,
                            context=f"Event {event_count} in workflow execution",
                        )

                    # Handle streaming events
                    if event_type == "on_chat_model_start":
                        # Capture the input to the model
                        if "data" in event_dict and "input" in event_dict["data"]:
                            model_input = event_dict["data"]["input"]
                            self._write_detailed_data(
                                section="MODEL_INPUT",
                                title="Model Input Messages",
                                data=model_input,
                                description="Messages and prompts sent to the LLM",
                            )
                            model_events.append(
                                {
                                    "type": "model_start",
                                    "input": model_input,
                                    "timestamp": time.time(),
                                }
                            )

                    elif event_type == "on_chat_model_stream":
                        # Token streaming
                        if "data" in event_dict:
                            data = event_dict["data"]
                            if isinstance(data, dict) and "chunk" in data:
                                chunk = data["chunk"]
                                # Handle both dict and object chunk formats
                                content = None
                                if isinstance(chunk, dict):
                                    content = chunk.get("content", "")
                                elif hasattr(chunk, "content"):
                                    content = getattr(chunk, "content", "")

                                if content:
                                    full_response += content
                                    response_chunks.append(
                                        {
                                            "type": "token",
                                            "content": content,
                                            "timestamp": time.time(),
                                        }
                                    )

                    elif event_type == "on_chat_model_end":
                        if "data" in event_dict:
                            data = event_dict["data"]
                            if isinstance(data, dict) and "output" in data:
                                output = data["output"]
                                if hasattr(output, "content") and output.content:
                                    # This is the complete response
                                    if output.content not in full_response:
                                        full_response += output.content
                                        response_chunks.append(
                                            {
                                                "type": "complete",
                                                "content": output.content,
                                                "timestamp": time.time(),
                                            }
                                        )
                                        logger.info(
                                            f"   üìù Captured complete response: {len(output.content)} chars"
                                        )

                                # Capture the complete model output
                                self._write_detailed_data(
                                    section="MODEL_OUTPUT",
                                    title="Complete Model Response",
                                    data={
                                        "content": (
                                            output.content
                                            if hasattr(output, "content")
                                            else str(output)
                                        ),
                                        "metadata": (
                                            getattr(output, "response_metadata", {})
                                            if hasattr(output, "response_metadata")
                                            else {}
                                        ),
                                        "additional_kwargs": (
                                            getattr(output, "additional_kwargs", {})
                                            if hasattr(output, "additional_kwargs")
                                            else {}
                                        ),
                                    },
                                    description="Complete response from the LLM",
                                )

                    elif event_type == "on_tool_start":
                        # Tool execution starting - this indicates tool calls are happening
                        tool_calls_detected = True
                        if "data" in event_dict:
                            tool_data = event_dict["data"]
                            logger.info(
                                f"   üõ†Ô∏è  Tool call detected: {tool_data.get('name', 'Unknown')}"
                            )
                            self._write_detailed_data(
                                section="TOOL_EXECUTION",
                                title=f"Tool Start: {tool_data.get('name', 'Unknown')}",
                                data=tool_data,
                                description=f"Tool execution starting for {tool_data.get('name', 'Unknown')}",
                            )
                            tool_events.append(
                                {
                                    "type": "tool_start",
                                    "data": tool_data,
                                    "timestamp": time.time(),
                                }
                            )

                    elif event_type == "on_tool_end":
                        # Tool execution completed
                        if "data" in event_dict:
                            tool_data = event_dict["data"]
                            self._write_detailed_data(
                                section="TOOL_EXECUTION",
                                title=f"Tool End: {tool_data.get('name', 'Unknown')}",
                                data=tool_data,
                                description=f"Tool execution completed for {tool_data.get('name', 'Unknown')}",
                            )
                            tool_events.append(
                                {
                                    "type": "tool_end",
                                    "data": tool_data,
                                    "timestamp": time.time(),
                                }
                            )

                elif isinstance(event_dict, dict):
                    # Handle dict events
                    if "data" in event_dict:
                        data = event_dict["data"]
                        if isinstance(data, dict) and "content" in data:
                            content = data["content"]
                            full_response += content
                            response_chunks.append(
                                {
                                    "type": "content",
                                    "content": content,
                                    "timestamp": time.time(),
                                }
                            )

                        # Check for tool calls
                        if "tool_calls" in data or "function_call" in data:
                            tool_calls_detected = True
                            logger.info("   üõ†Ô∏è  Tool call detected in workflow")

            execution_time = time.time() - start_time
            logger.info(f"   ‚úÖ Workflow execution completed in {execution_time:.2f}s")
            logger.info(f"   üìä Total response chunks: {len(response_chunks)}")

            # Additional tool call detection methods
            if len(tool_events) > 0 and not tool_calls_detected:
                tool_calls_detected = True
                logger.info(
                    f"   üõ†Ô∏è  Tool calls detected via tool_events: {len(tool_events)}"
                )

            # Check if the response indicates successful tool usage (e.g. web search results)
            if not tool_calls_detected and full_response:
                # Look for indicators of successful tool execution in the response
                tool_indicators = [
                    "based on recent findings",
                    "search results",
                    "according to",
                    "recent developments",
                    "latest information",
                    "current information",
                    "web search",
                ]
                if any(
                    indicator.lower() in full_response.lower()
                    for indicator in tool_indicators
                ):
                    tool_calls_detected = True
                    logger.info(
                        "   üõ†Ô∏è  Tool calls detected via response content analysis"
                    )

            logger.info(f"   üõ†Ô∏è  Tool calls detected: {tool_calls_detected}")
            if error_events:
                logger.error(
                    f"   üö´ Captured {len(error_events)} workflow error events; marking execution as failed"
                )

            # Write execution summary
            execution_summary = {
                "execution_time": execution_time,
                "total_events": event_count,
                "response_chunks": len(response_chunks),
                "tool_calls_detected": tool_calls_detected,
                "tool_events_count": len(tool_events),
                "model_events_count": len(model_events),
                "final_response_length": len(full_response),
            }

            self._write_detailed_data(
                section="EXECUTION_SUMMARY",
                title="Workflow Execution Summary",
                data=execution_summary,
                description="Summary of workflow execution metrics and events",
            )

            # Write all tool events if any
            if tool_events:
                self._write_detailed_data(
                    section="TOOL_SUMMARY",
                    title="All Tool Events",
                    data=tool_events,
                    description=f"Complete list of {len(tool_events)} tool execution events",
                )

            # Write response chunks summary
            if response_chunks:
                self._write_detailed_data(
                    section="RESPONSE_SUMMARY",
                    title="Response Generation Summary",
                    data={
                        "total_chunks": len(response_chunks),
                        "chunk_types": list(
                            set(chunk["type"] for chunk in response_chunks)
                        ),
                        "first_chunk": response_chunks[0] if response_chunks else None,
                        "last_chunk": response_chunks[-1] if response_chunks else None,
                    },
                    description="Summary of response generation process",
                )

            # Store assistant response in database if we got content
            if full_response and storage and storage.message:
                from models.message import Message
                from models.message_role import MessageRole
                from models.message_content import MessageContent, MessageContentType

                assistant_message = Message(
                    id=None,
                    conversation_id=self.test_conversation_id,
                    role=MessageRole.ASSISTANT,
                    content=[
                        MessageContent(type=MessageContentType.TEXT, text=full_response)
                    ],
                    created_at=datetime.now(timezone.utc),
                )

                assistant_message_id = await storage.message.add_message(
                    assistant_message
                )

                # Capture LLM response
                self._write_llm_response(
                    "composer_workflow_execution",
                    full_response,
                    {
                        "execution_time": execution_time,
                        "chunk_count": len(response_chunks),
                        "tool_calls_detected": tool_calls_detected,
                        "model": self.target_model,
                    },
                )

            # Validate that we actually got meaningful output
            success = True
            validation_errors = []

            # If pipeline node failure logged earlier or error events captured, mark failure
            if error_events:
                success = False
                validation_errors.append(
                    f"Encountered {len(error_events)} workflow error events (first type: {error_events[0].get('event')})"
                )

            # Check 1: Must have actual response content
            if len(full_response.strip()) == 0:
                success = False
                validation_errors.append("No response content generated")

            # Check 2: Must have response chunks from streaming
            if len(response_chunks) == 0:
                success = False
                validation_errors.append("No response chunks captured from workflow")

            # Check 2a: Must have at least one tool call (strict requirement)
            if not tool_calls_detected:
                success = False
                validation_errors.append(
                    "No tool calls were executed during workflow (strict failure)"
                )

            # Check 3: Response length should be reasonable for a real LLM interaction
            if len(full_response) < 10:  # Very short responses likely indicate issues
                success = False
                validation_errors.append(
                    f"Response too short ({len(full_response)} chars), likely incomplete"
                )

            # Check 4: Validate output file has captured responses
            try:
                if self.llm_output_file:
                    with open(self.llm_output_file, "r", encoding="utf-8") as f:
                        output_content = f.read()
                        # Count actual response entries (PHASE: markers indicate captured content)
                        response_count = output_content.count("PHASE:")
                        if response_count == 0:
                            success = False
                            validation_errors.append(
                                "Output file shows 0 captured responses despite workflow execution"
                            )
                        logger.info(
                            f"   üìä Output file validation: {response_count} responses captured"
                        )
                else:
                    validation_errors.append("No output file path configured")
            except Exception as file_error:
                success = False
                validation_errors.append(
                    f"Could not validate output file: {file_error}"
                )

            # Check 5: If workflow took significant time but produced no output, likely an internal error
            if execution_time > 3.0 and len(full_response) == 0:
                success = False
                validation_errors.append(
                    f"Workflow ran for {execution_time:.1f}s but generated no output (likely internal error)"
                )

            # Check 6: Validate tool availability awareness (only flag as issue if no tools were used)
            tool_availability_issues = []
            if not tool_calls_detected:
                # Extract user-facing content by removing <think> tags and their content
                import re

                user_facing_content = re.sub(
                    r"<think>.*?</think>", "", full_response, flags=re.DOTALL
                )

                if "can't perform actual searches" in user_facing_content.lower():
                    tool_availability_issues.append(
                        "Model believes it cannot perform web searches without using tools"
                    )
                if "system can't perform" in user_facing_content.lower():
                    tool_availability_issues.append(
                        "Model believes system lacks tool capabilities without using tools"
                    )
                if "cannot access real-time" in user_facing_content.lower():
                    tool_availability_issues.append(
                        "Model believes it cannot access real-time data without using tools"
                    )

            if tool_availability_issues:
                success = False
                validation_errors.extend(tool_availability_issues)
                logger.error(
                    f"   üö´ Tool availability issues detected: {', '.join(tool_availability_issues)}"
                )

            # Check 7: Validate dynamic tool generation doesn't have error handling flags
            if (
                "handle_tool_error" in full_response
                and '"handle_tool_error": true' in full_response
            ):
                success = False
                validation_errors.append(
                    "Dynamic tool has handle_tool_error=true indicating error conditions"
                )
                logger.error(
                    f"   ‚ö†Ô∏è  Dynamic tool configured with error handling (problematic)"
                )

            if not success:
                logger.error(
                    f"   ‚ùå Workflow validation failed: {', '.join(validation_errors)}"
                )

            return {
                "success": success,
                "execution_time": execution_time,
                "response_chunks": len(response_chunks),
                "tool_calls_detected": tool_calls_detected,
                "response_length": len(full_response),
                "workflow_type": "composer_langgraph",
                "final_response_raw": full_response,  # Full response for debugging
                "final_response": self._parse_response_as_json(
                    full_response
                ),  # Try to parse as JSON
                "validation_errors": validation_errors if validation_errors else None,
                "tool_availability_correct": len(
                    [
                        e
                        for e in validation_errors
                        if "tool" in e.lower() or "search" in e.lower()
                    ]
                )
                == 0,
                "dynamic_tool_error_free": "handle_tool_error" not in full_response
                or '"handle_tool_error": false' in full_response,
            }

        except Exception as e:
            logger.error(f"   ‚ùå Composer workflow execution failed: {str(e)}")
            import traceback

            traceback.print_exc()
            return {"success": False, "error": str(e)}

    async def validate_real_outputs(self) -> Dict[str, Any]:
        """Validate real outputs and database integrity."""
        logger.info("‚úÖ Validating real outputs...")

        try:
            from db import storage

            # Ensure we have required components
            if (
                not storage
                or not storage.conversation
                or not storage.message
                or not self.test_conversation_id
            ):
                raise Exception(
                    "Missing required storage components or conversation ID"
                )

            # Validate conversation exists and has messages
            conversation = await storage.conversation.get_conversation(
                self.test_conversation_id
            )
            if not conversation:
                raise Exception("Test conversation not found")

            messages = await storage.message.get_conversation_history(
                self.test_conversation_id
            )
            if len(messages) < 2:  # Should have user + assistant messages
                logger.warning(
                    f"   ‚ö†Ô∏è  Expected at least 2 messages, found {len(messages)}"
                )

            logger.info(
                f"   ‚úÖ Conversation validation passed: {len(messages)} messages"
            )

            # Validate model profile exists (only if storage available)
            if storage.model_profile:
                model_profile = await storage.model_profile.get_model_profile_by_id(
                    self.test_model_profile_id, self.test_user_id
                )
                if model_profile:
                    logger.info(
                        f"   ‚úÖ Model profile validation passed: {model_profile.name}"
                    )

            # Calculate quality metrics
            assistant_messages = [m for m in messages if m.role.value == "assistant"]

            # Critical validation: Must have assistant responses for a successful test
            if len(assistant_messages) == 0:
                logger.error(
                    "   ‚ùå No assistant messages found - workflow failed to generate responses"
                )
                return {
                    "success": False,
                    "error": "No assistant responses generated - workflow execution failed",
                    "conversation_valid": True,
                    "model_profile_valid": True,
                    "message_count": len(messages),
                    "assistant_messages": 0,
                    "response_quality_score": 0,
                }

            response_quality_score = 0
            if assistant_messages:
                latest_response = assistant_messages[-1]
                response_text = ""
                for content in latest_response.content:
                    if content.type.value == "text" and content.text:
                        response_text += content.text

                # Basic quality checks
                if len(response_text) > 100:
                    response_quality_score += 25
                if "2024" in response_text or "recent" in response_text.lower():
                    response_quality_score += 25
                if len(response_text.split()) > 50:
                    response_quality_score += 25
                if any(
                    keyword in response_text.lower()
                    for keyword in [
                        "ai",
                        "artificial intelligence",
                        "model",
                        "research",
                    ]
                ):
                    response_quality_score += 25

                logger.info(
                    f"   üìä Response quality score: {response_quality_score}/100"
                )

            # Validate conversation title if generated
            title_validation = await self._validate_conversation_title()

            return {
                "success": True,
                "conversation_valid": True,
                "model_profile_valid": True,
                "message_count": len(messages),
                "assistant_messages": len(assistant_messages),
                "response_quality_score": response_quality_score,
                "title_validation": title_validation,
            }

        except Exception as e:
            logger.error(f"   ‚ùå Output validation failed: {str(e)}")
            return {"success": False, "error": str(e)}

    async def _validate_conversation_title(self) -> Dict[str, Any]:
        """Validate the generated conversation title meets requirements."""
        try:
            from db import storage

            if not storage or not storage.conversation or not self.test_conversation_id:
                return {"valid": False, "error": "Missing storage or conversation ID"}

            # Get conversation to check for title
            conversation = await storage.conversation.get_conversation(
                self.test_conversation_id
            )

            if not conversation:
                return {"valid": False, "error": "Conversation not found"}

            title = getattr(conversation, "title", None)

            if not title or title.strip() == "":
                return {"valid": False, "error": "No title generated"}

            # Clean and validate title
            title = title.strip()
            word_count = len(title.split())

            # Check title requirements
            validation_results = {
                "valid": True,
                "title": title,
                "word_count": word_count,
                "meets_word_limit": word_count <= 5,
                "has_content": len(title) > 0,
                "no_quotes": not (title.startswith('"') and title.endswith('"')),
                "properly_capitalized": title[0].isupper() if title else False,
            }

            # Overall validation
            validation_results["valid"] = all(
                [
                    validation_results["meets_word_limit"],
                    validation_results["has_content"],
                    validation_results["no_quotes"],
                ]
            )

            if validation_results["valid"]:
                logger.info(
                    f"   ‚úÖ Title validation passed: '{title}' ({word_count} words)"
                )
            else:
                logger.error(
                    f"   ‚ùå Title validation failed: '{title}' ({word_count} words)"
                )

            return validation_results

        except Exception as e:
            logger.error(f"   ‚ùå Title validation error: {str(e)}")
            return {"valid": False, "error": str(e)}

    async def cleanup_real_data(self):
        """Clean up all real test data from database."""
        logger.info("üßπ Cleaning up real test data...")

        cleaned_count = 0
        cleanup_errors = []

        try:
            from db import storage

            # Ensure we have storage available
            if not storage or not storage.pool:
                logger.warning("   ‚ö†Ô∏è  Storage not available for cleanup")
                return {
                    "success": False,
                    "error": "Storage not available",
                    "cleaned_count": 0,
                }

            # Validate cascading deletes by checking what will be deleted
            async with storage.pool.acquire() as conn:
                # Count related entities before deletion
                related_counts = {}

                # Model profiles
                profile_count = await conn.fetchval(
                    "SELECT COUNT(*) FROM model_profiles WHERE user_id = $1",
                    self.test_user_id,
                )
                related_counts["model_profiles"] = profile_count

                # Dynamic tools
                tool_count = await conn.fetchval(
                    "SELECT COUNT(*) FROM dynamic_tools WHERE user_id = $1",
                    self.test_user_id,
                )
                related_counts["dynamic_tools"] = tool_count

                # Conversations
                conversation_count = await conn.fetchval(
                    "SELECT COUNT(*) FROM conversations WHERE user_id = $1",
                    self.test_user_id,
                )
                related_counts["conversations"] = conversation_count

                # Messages (should cascade from conversations)
                message_count = await conn.fetchval(
                    "SELECT COUNT(*) FROM messages WHERE conversation_id IN (SELECT id FROM conversations WHERE user_id = $1)",
                    self.test_user_id,
                )
                related_counts["messages"] = message_count

                # Memories (cascade through user_id, not conversation_id)
                memory_count = await conn.fetchval(
                    "SELECT COUNT(*) FROM memories WHERE user_id = $1",
                    self.test_user_id,
                )
                related_counts["memories"] = memory_count

                # Summaries (should cascade from conversations)
                summary_count = await conn.fetchval(
                    "SELECT COUNT(*) FROM summaries WHERE conversation_id IN (SELECT id FROM conversations WHERE user_id = $1)",
                    self.test_user_id,
                )
                related_counts["summaries"] = summary_count

                # Search topic syntheses (should cascade from conversations)
                synthesis_count = await conn.fetchval(
                    "SELECT COUNT(*) FROM search_topic_syntheses WHERE conversation_id IN (SELECT id FROM conversations WHERE user_id = $1)",
                    self.test_user_id,
                )
                related_counts["search_topic_syntheses"] = synthesis_count

                logger.info(f"   üìä Related entities before deletion: {related_counts}")

                # Manual cascading deletes since DB triggers may not be working correctly
                # Delete in dependency order to avoid foreign key constraint violations
                logger.info(
                    f"   üîç Debug: conversation_id={self.test_conversation_id}, model_profile_id={self.test_model_profile_id}"
                )

                # 1. Delete messages first (dependent on conversations) - use specific conversation ID
                if self.test_conversation_id:
                    deleted_messages_result = await conn.execute(
                        "DELETE FROM messages WHERE conversation_id = $1",
                        self.test_conversation_id,
                    )
                    logger.info(
                        f"   üóëÔ∏è  Deleted messages for conversation {self.test_conversation_id}: {deleted_messages_result}"
                    )
                else:
                    logger.warning(f"   ‚ö†Ô∏è  No conversation ID to delete messages from")

                # 2. Delete summaries (dependent on conversations)
                if self.test_conversation_id:
                    deleted_summaries_result = await conn.execute(
                        "DELETE FROM summaries WHERE conversation_id = $1",
                        self.test_conversation_id,
                    )
                    logger.info(
                        f"   üóëÔ∏è  Deleted summaries for conversation {self.test_conversation_id}: {deleted_summaries_result}"
                    )

                # 3. Delete search topic syntheses (dependent on conversations)
                if self.test_conversation_id:
                    deleted_syntheses_result = await conn.execute(
                        "DELETE FROM search_topic_syntheses WHERE conversation_id = $1",
                        self.test_conversation_id,
                    )
                    logger.info(
                        f"   üóëÔ∏è  Deleted syntheses for conversation {self.test_conversation_id}: {deleted_syntheses_result}"
                    )

                # 4. Delete conversations (dependent on user) - use specific conversation ID for precision
                if self.test_conversation_id:
                    deleted_conversations_result = await conn.execute(
                        "DELETE FROM conversations WHERE id = $1 AND user_id = $2",
                        self.test_conversation_id,
                        self.test_user_id,
                    )
                    logger.info(
                        f"   üóëÔ∏è  Deleted conversation {self.test_conversation_id}: {deleted_conversations_result}"
                    )

                # 5. Delete model profiles (dependent on user) - use specific profile ID for precision
                if self.test_model_profile_id:
                    deleted_profiles_result = await conn.execute(
                        "DELETE FROM model_profiles WHERE id = $1 AND user_id = $2",
                        self.test_model_profile_id,
                        self.test_user_id,
                    )
                    logger.info(
                        f"   üóëÔ∏è  Deleted model profile {self.test_model_profile_id}: {deleted_profiles_result}"
                    )
                else:
                    logger.warning(f"   ‚ö†Ô∏è  No model profile ID to delete")

                # 6. Delete dynamic tools (dependent on user) - these have proper CASCADE so may already be deleted
                deleted_tools_result = await conn.execute(
                    "DELETE FROM dynamic_tools WHERE user_id = $1", self.test_user_id
                )
                logger.info(f"   üóëÔ∏è  Deleted dynamic tools: {deleted_tools_result}")

                # 7. Delete memories (dependent on user) - these have proper CASCADE so may already be deleted
                deleted_memories_result = await conn.execute(
                    "DELETE FROM memories WHERE user_id = $1", self.test_user_id
                )
                logger.info(f"   üóëÔ∏è  Deleted memories: {deleted_memories_result}")

                # 8. Finally delete the user
                await conn.execute("DELETE FROM users WHERE id = $1", self.test_user_id)
                logger.info(f"   ‚úÖ Deleted user: {self.test_user_id}")

                # Count cleanup based on what was actually deleted
                cleaned_count = 1  # User
                cleaned_count += sum(
                    [count for count in related_counts.values() if count > 0]
                )

                # TEMPORARY: Force a cleanup failure for testing
                # TODO: Remove this line after testing cleanup failure handling
                # raise Exception("Forced cleanup failure for testing")

                # Validate cascading deletes worked
                remaining_counts = {}

                # Check that all related entities were deleted
                remaining_counts["model_profiles"] = await conn.fetchval(
                    "SELECT COUNT(*) FROM model_profiles WHERE user_id = $1",
                    self.test_user_id,
                )

                remaining_counts["dynamic_tools"] = await conn.fetchval(
                    "SELECT COUNT(*) FROM dynamic_tools WHERE user_id = $1",
                    self.test_user_id,
                )

                remaining_counts["conversations"] = await conn.fetchval(
                    "SELECT COUNT(*) FROM conversations WHERE user_id = $1",
                    self.test_user_id,
                )

                # Check messages directly since conversation may be deleted already
                # Get actual conversation IDs that were associated with this user before deletion
                remaining_counts["messages"] = await conn.fetchval(
                    "SELECT COUNT(*) FROM messages WHERE conversation_id = $1",
                    self.test_conversation_id,
                )

                remaining_counts["memories"] = await conn.fetchval(
                    "SELECT COUNT(*) FROM memories WHERE user_id = $1",
                    self.test_user_id,
                )

                # Check summaries and syntheses directly by conversation ID
                remaining_counts["summaries"] = await conn.fetchval(
                    "SELECT COUNT(*) FROM summaries WHERE conversation_id = $1",
                    self.test_conversation_id,
                )

                remaining_counts["search_topic_syntheses"] = await conn.fetchval(
                    "SELECT COUNT(*) FROM search_topic_syntheses WHERE conversation_id = $1",
                    self.test_conversation_id,
                )

                logger.info(
                    f"   üìä Remaining entities after deletion: {remaining_counts}"
                )

                # Validate that cascading deletes worked
                cascade_failures = []
                for entity_type, count in remaining_counts.items():
                    if count > 0:
                        cascade_failures.append(f"{entity_type}: {count} remaining")

                # TEMPORARY: Force cascade failure detection for testing
                # TODO: Remove this after testing cleanup failure handling
                # cascade_failures.append("test_failure: 1 remaining")

                if cascade_failures:
                    error_msg = (
                        f"Cascading deletes failed: {'; '.join(cascade_failures)}"
                    )
                    cleanup_errors.append(error_msg)
                    logger.error(f"   ‚ùå {error_msg}")
                else:
                    logger.info("   ‚úÖ All cascading deletes succeeded")
                    cleaned_count += sum(related_counts.values())

        except Exception as e:
            error_msg = f"Failed to delete user or validate cascades: {e}"
            cleanup_errors.append(error_msg)
            logger.error(f"   ‚ùå Cleanup failed: {str(e)}")
            return {"success": False, "error": str(e), "cleaned_count": cleaned_count}

        # Return cleanup results
        logger.info(f"‚úÖ Cleanup completed: {cleaned_count} entities deleted")
        return {
            "success": len(cleanup_errors) == 0,
            "cleaned_count": cleaned_count,
            "errors": cleanup_errors,
            "total_entities": sum(related_counts.values()) + 1,  # +1 for user
        }

    def _finalize_llm_output(self):
        """Finalize the LLM output file with summary statistics."""
        if not self.capture_llm_output or not self.llm_output_file:
            return

        try:
            total_chars = sum(len(resp["response"]) for resp in self.llm_responses)
            total_responses = len(self.llm_responses)

            with open(self.llm_output_file, "a", encoding="utf-8") as f:
                f.write(f"\n{'='*60}\n")
                f.write(f"TEST SUMMARY\n")
                f.write(f"{'='*60}\n")
                f.write(f"Total Responses: {total_responses}\n")
                f.write(f"Total Characters: {total_chars:,}\n")
                f.write(
                    f"Average Response Length: {total_chars // max(total_responses, 1):,} chars\n"
                )
                f.write(f"Model Used: {self.target_model}\n")
                f.write(f"Architecture: Composer + LangGraph\n")
                f.write(f"Test Completed: {datetime.now(timezone.utc).isoformat()}\n")
                f.write(f"{'='*60}\n")

            logger.info(f"üìù Finalized LLM output file: {self.llm_output_file}")
            logger.info(
                f"üìä Captured {total_responses} responses totaling {total_chars:,} characters"
            )

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Failed to finalize LLM output file: {e}")

    def _print_llm_output_summary(self):
        """Print a summary of captured LLM output."""
        if not self.capture_llm_output or not self.llm_output_file:
            return

        total_chars = sum(len(resp["response"]) for resp in self.llm_responses)
        total_responses = len(self.llm_responses)

        print(f"\n{'='*60}")
        print(f"COMPOSER LLM OUTPUT SUMMARY")
        print(f"{'='*60}")
        print(f"Output File: {self.llm_output_file}")
        print(f"Total Responses: {total_responses}")
        print(f"Total Characters: {total_chars:,}")
        print(f"Average Length: {total_chars // max(total_responses, 1):,} chars")
        print(f"Architecture: Composer + LangGraph")
        print(f"{'='*60}")

        if self.print_output and total_responses > 0:
            print(f"\nFULL LLM OUTPUT CONTENT:")
            print(f"{'='*60}")
            try:
                with open(self.llm_output_file, "r", encoding="utf-8") as f:
                    print(f.read())
            except Exception as e:
                print(f"Error reading output file: {e}")
            print(f"{'='*60}")
        elif total_responses > 0:
            print(
                f"\nTo view full content, set print_output=True or read: {self.llm_output_file}"
            )
            print(f"{'='*60}")

    async def print_test_summary(self, results: Dict[str, Any]) -> None:
        """Print comprehensive test summary."""
        logger.info("\n" + "=" * 80)
        logger.info("üìä Composer Real Pipeline Test Summary")
        logger.info("=" * 80)

        success_rate = (
            results["components_passed"] / results["total_components"]
        ) * 100
        overall_success = "YES" if results["overall_success"] else "NO"

        logger.info(f"‚úÖ Overall Success: {overall_success} ({success_rate:.1f}%)")
        logger.info(f"üïí Total Execution Time: {results['execution_time']:.2f}s")
        logger.info(
            f"‚ö° Workflow Execution Time: {results.get('workflow_time', 0):.2f}s"
        )
        logger.info(
            f"üîß Components Passed: {results['components_passed']}/{results['total_components']}"
        )
        logger.info(f"üèóÔ∏è  Real Entities Created: {results['entities_created']}")

        # Extract key information from results
        workflow_result = results["results"].get("workflow_execution", {})
        validation_result = results["results"].get("output_validation", {})

        model_name = (
            results["results"]
            .get("user_profile_creation", {})
            .get("model_name", "Unknown")
        )
        tool_calls = workflow_result.get("tool_calls_detected", False)
        quality_score = validation_result.get("response_quality_score", 0)
        tool_availability_correct = workflow_result.get(
            "tool_availability_correct", True
        )
        dynamic_tool_error_free = workflow_result.get("dynamic_tool_error_free", True)

        # Extract title validation information
        title_validation = validation_result.get("title_validation", {})
        title_valid = title_validation.get("valid", False)
        title_word_count = title_validation.get("word_count", "Unknown")
        title_text = title_validation.get("title", "Not generated")

        logger.info(f"ü§ñ Model Used: {model_name}")
        logger.info(f"üõ†Ô∏è  Tool Calls Detected: {'YES' if tool_calls else 'NO'}")
        logger.info(
            f"üîç Tool Availability Awareness: {'CORRECT' if tool_availability_correct else 'INCORRECT'}"
        )
        logger.info(
            f"‚öôÔ∏è  Dynamic Tool Error-Free: {'YES' if dynamic_tool_error_free else 'NO'}"
        )
        logger.info(f"üìä Response Quality Score: {quality_score}/100")
        logger.info(
            f"üè∑Ô∏è  Title Generated: {'YES' if title_valid else 'NO'} ({title_word_count} words)"
        )
        if title_valid:
            logger.info(f"    Title: '{title_text}'")
        logger.info(f"üéº Architecture: Composer + LangGraph")

        logger.info("\nüìã Component Results:")
        component_names = [
            "Infrastructure Setup",
            "Composer Initialization",
            "User Profile Creation",
            "Conversation Creation",
            "Message Creation",
            "Workflow Execution",
            "Output Validation",
            "Data Cleanup",
        ]

        result_keys = [
            "infrastructure_setup",
            "composer_initialization",
            "user_profile_creation",
            "conversation_creation",
            "message_creation",
            "workflow_execution",
            "output_validation",
            "data_cleanup",
        ]

        for name, key in zip(component_names, result_keys):
            result = results["results"].get(key, {})
            status = "‚úÖ PASS" if result.get("success", False) else "‚ùå FAIL"
            logger.info(f"   {status} {name}")

        # Save detailed results to file
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = "debug/out"
        os.makedirs(output_dir, exist_ok=True)
        results_file = f"{output_dir}/composer_test_{timestamp}.json"

        try:
            with open(results_file, "w", encoding="utf-8") as f:
                json.dump(results, f, indent=2, default=str)
            logger.info(f"üìÑ Detailed results saved to: {results_file}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Failed to save results file: {e}")


async def main():
    """Main test execution function."""
    logger.info("üß™ Starting Composer Real End-to-End Pipeline Tests")

    # Support command line model selection and output options
    target_model = None
    capture_output = True
    print_output = False

    # Parse command line arguments
    for i, arg in enumerate(sys.argv[1:], 1):
        if arg.startswith("--"):
            if arg == "--no-capture":
                capture_output = False
            elif arg == "--print-output":
                print_output = True
            elif arg.startswith("--model="):
                target_model = arg.split("=", 1)[1]
        elif not target_model and not arg.startswith("--"):
            target_model = arg

    # Available models for testing
    available_models = [
        "qwen3-30b-a3b-q4-k-m",  # Primary model - use this as default
        "openai-gpt-oss-20b-uncensored-q5_1",
    ]

    # Test specified model or default
    models_to_test = [target_model] if target_model else [available_models[0]]

    for model in models_to_test:
        logger.info(f"üß™ Testing composer architecture with model: {model}")
        tester = ComposerRealEndToEndTester(
            target_model=model,
            capture_llm_output=capture_output,
            print_output=print_output,
        )

        # Run the test
        try:
            results = await tester.run_full_test()

            if results["overall_success"]:
                logger.info(f"üéâ Composer test with {model} PASSED!")
            else:
                logger.error(f"‚ùå Composer test with {model} FAILED!")
                return 1

        except Exception as e:
            logger.error(f"‚ùå Test execution failed for {model}: {e}")
            import traceback

            traceback.print_exc()
            return 1

    logger.info("üèÅ Composer architecture testing completed successfully!")
    return 0


if __name__ == "__main__":
    try:
        exit_code = asyncio.run(main())
        exit(exit_code)
    except KeyboardInterrupt:
        logger.info("üõë Test interrupted by user")
        exit(1)
    except Exception as e:
        logger.error(f"‚ùå Unexpected error: {e}")
        import traceback

        traceback.print_exc()
        exit(1)

# Code generated by schema2code at 2025-10-20 19:10:59. DO NOT EDIT.


from typing import List, Dict, Optional, Any, Union
from datetime import datetime, date, time, timedelta
from .model_parameters import ModelParameters
from pydantic import BaseModel, Field, AnyUrl, EmailStr, conint, confloat



class GenerateReq(BaseModel):
    """GenerateReq represents a request to the Ollama embeddings API"""
    model: str = Field(..., description="Model name")
    """Model name"""
    prompt: str = Field(..., description="Prompt to send to the model")
    """Prompt to send to the model"""
    suffix: Optional[str] = Field(default=None, description="Suffix to append to the prompt")
    """Suffix to append to the prompt"""
    images: Optional[List[str]] = Field(default=None, description="Images to include with the prompt")
    """Images to include with the prompt"""
    format: Optional[Dict[str, Any]] = Field(default=None, description="The format to return a response in. Format can be json or a JSON schema")
    """The format to return a response in. Format can be json or a JSON schema"""
    options: Optional[ModelParameters] = Field(default=None, description="Additional model parameters listed in the documentation for the Modelfile such as temperature")
    """Additional model parameters listed in the documentation for the Modelfile such as temperature"""
    system: Optional[str] = Field(default=None, description="System message to (overrides what is defined in the Modelfile)")
    """System message to (overrides what is defined in the Modelfile)"""
    template: Optional[str] = Field(default=None, description="The prompt template to use (overrides what is defined in the Modelfile)")
    """The prompt template to use (overrides what is defined in the Modelfile)"""
    stream: Optional[bool] = Field(default=None, description="If false the response will be returned as a single response object")
    """If false the response will be returned as a single response object"""
    raw: Optional[bool] = Field(default=None, description="If true no formatting will be applied to the prompt")
    """If true no formatting will be applied to the prompt"""
    keep_alive: Optional[int] = Field(default=None, description="Controls how long the model will stay loaded into memory")
    """Controls how long the model will stay loaded into memory"""
    context: Optional[str] = Field(default=None, description="the context parameter returned from a previous request")
    """the context parameter returned from a previous request"""
    think: Optional[bool] = Field(default=None, description="If true, the model will think before responding, useful for complex queries")
    """If true, the model will think before responding, useful for complex queries"""

    class Config:
        extra = "ignore"
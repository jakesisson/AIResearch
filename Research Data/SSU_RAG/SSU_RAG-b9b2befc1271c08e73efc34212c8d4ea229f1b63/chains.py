import os
from typing import List, Dict

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

from embedding_processor import EmbeddingProcessor


# 환경 변수 로드
load_dotenv()

# ===== 공용 LLM =====
# Cost-perf: use Azure OpenAI when COST_PERF=1 and Azure env is set
if os.environ.get("COST_PERF") == "1" and os.environ.get("AZURE_OPENAI_API_KEY"):
    from langchain_openai import AzureChatOpenAI
    _deployment = os.environ.get("AZURE_OPENAI_DEPLOYMENT") or os.environ.get("MODEL_ID") or "gpt-4o-mini"
    llm = AzureChatOpenAI(
        azure_endpoint=os.environ.get("AZURE_OPENAI_ENDPOINT", "").rstrip("/"),
        api_key=os.environ.get("AZURE_OPENAI_API_KEY"),
        api_version=os.environ.get("AZURE_OPENAI_API_VERSION") or os.environ.get("OPENAI_API_VERSION") or "2024-02-01",
        azure_deployment=_deployment,
        temperature=0.7,
    )
else:
    llm = ChatOpenAI(
        model="gpt-3.5-turbo",
        temperature=0.7,
        openai_api_key=os.getenv("OPENAI_API_KEY"),
    )




# ===== 벡터 검색 유틸 =====
_processor: EmbeddingProcessor | None = None


def get_processor() -> EmbeddingProcessor:
    global _processor
    if _processor is None:
        _processor = EmbeddingProcessor()
    return _processor


def _build_context(items: List[Dict]) -> str:
    lines: List[str] = []
    for i, item in enumerate(items, 1):
        title = item.get("title") or ""
        description = item.get("description") or ""
        content = item.get("content") or ""
        author = item.get("author") or ""
        category = item.get("category") or ""
        published = item.get("published") or ""
        link = item.get("link") or ""
        snippet = (description or content)[:500]
        lines.append(
            f"[{i}] Title: {title}\nAuthor: {author} | Category: {category} | Published: {published}\nSnippet: {snippet}\nLink: {link}"
        )
    return "\n\n".join(lines)


# ===== RAG 답변 체인 =====
_rag_prompt = ChatPromptTemplate.from_messages([
    (
        "system",
        "너는 제공된 문맥(Context)만을 근거로 한국어로 간결하고 정확하게 답변한다. 모르면 모른다고 말한다. 반드시 본문 내에 문맥 항목의 대괄호 번호([1], [2], ...)로 근거를 표시하라. 본문에는 링크를 넣지 말고, 링크 목록은 시스템이 답변 마지막에 '참고 문서' 섹션으로 자동 첨부한다.",
    ),
    ("human", "질문: {question}\n\nContext:\n{context}"),
])

_rag_answer_chain = _rag_prompt | llm | StrOutputParser()


# ===== 통합된 RAG 체인 =====
def unified_rag_chain(query: str, limit: int = 5) -> Dict:
    """질의를 받아 답변, 상세 검색 결과, 출처를 모두 반환하는 통합 함수"""
    processor = get_processor()
    results: List[Dict] = processor.search_similar_content(query, limit)
    context_text = _build_context(results)
    
    # AI 답변 생성
    answer = _rag_answer_chain.invoke({"question": query, "context": context_text})
    
    # 출처 정보 구성
    sources = []
    sources_lines: List[str] = []
    for idx, r in enumerate(results, 1):
        title = r.get("title")
        link = r.get("link")
        sources.append(
            {
                "index": idx,
                "title": title,
                "link": link,
                "published": r.get("published"),
                "author": r.get("author"),
                "category": r.get("category"),
                "distance": r.get("distance"),
            }
        )
        sources_lines.append(f"[{idx}] {title} - {link}")

    # # 답변 본문 끝에 참고 문서 섹션 자동 첨부
    # answer_with_sources = answer.strip()
    # if sources_lines:
    #     answer_with_sources += "\n\n참고 문서:\n" + "\n".join(sources_lines)

    # 통합된 결과 반환 (기존 run_rag_qa + _rag_items 정보 모두 포함)
    return {
        "query": query,
        "answer": answer,
        "sources": sources,
        "context": context_text,
        "items": results,
        "sources_text": "\n".join(sources_lines),
    }

# 기존 함수들과의 호환성을 위한 별칭
run_rag_qa = unified_rag_chain


# ===== Cost-perf: run only answer chain with fixed context (no vector search) =====
def run_rag_qa_cost_perf(query: str, context: str) -> Dict:
    """Run only the RAG answer chain with given context; return answer and usage. No vector search."""
    from langchain_core.callbacks import BaseCallbackHandler
    from langchain_core.outputs import LLMResult

    class UsageCallback(BaseCallbackHandler):
        usage: Dict = {}

        def on_llm_end(self, response: LLMResult, **kwargs) -> None:
            for gen_list in response.generations or []:
                for gen in gen_list:
                    msg = getattr(gen, "message", None)
                    if msg is None:
                        continue
                    um = getattr(msg, "usage_metadata", None)
                    if isinstance(um, dict):
                        self.usage = um
                        return
                    if hasattr(msg, "response_metadata"):
                        meta = msg.response_metadata or {}
                        self.usage = meta.get("usage") or meta.get("token_usage") or {}
                        return
                    if hasattr(um, "input_tokens"):
                        self.usage = {
                            "input_tokens": getattr(um, "input_tokens", 0),
                            "output_tokens": getattr(um, "output_tokens", 0),
                            "total_tokens": getattr(um, "total_tokens", 0),
                        }
                        return

    cb = UsageCallback()
    answer = _rag_answer_chain.invoke(
        {"question": query, "context": context},
        config={"callbacks": [cb]},
    )
    return {"answer": answer, "usage": cb.usage}

# LangServe 관련 스키마/래퍼 제거



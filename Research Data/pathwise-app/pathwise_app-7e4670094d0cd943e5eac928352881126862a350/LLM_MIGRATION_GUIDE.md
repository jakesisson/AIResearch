# LLM Provider Migration Guide

## Overview

This guide explains how to migrate from the current Claude-only architecture to a multi-LLM system using OpenAI (GPT-4o-mini) as the primary provider, with DeepSeek for cost optimization and Claude for creative synthesis.

---

## Architecture Changes

### Before: Claude-Only

```typescript
import Anthropic from "@anthropic-ai/sdk";
const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY });

// Every operation uses Claude
const response = await anthropic.messages.create({
  model: "claude-3-5-haiku-20241022",
  messages: [...]
});
```

### After: Multi-LLM with Auto-Selection

```typescript
import { executeLLMCall } from './services/llmProvider';

// Automatic provider selection based on task type
const result = await executeLLMCall(
  'gap_analysis',  // Task type
  async (provider) => {
    // Provider is automatically selected (OpenAI for this task)
    return await provider.generateCompletion(messages);
  }
);
```

---

## Provider Selection Strategy

| Task Type | Primary Provider | Fallback | Reason |
|-----------|------------------|----------|--------|
| **domain_detection** | OpenAI GPT-4o-mini | Claude Haiku | Fast classification, 40% cheaper |
| **question_generation** | OpenAI GPT-4o-mini | Claude Haiku | Good at structured outputs |
| **gap_analysis** | OpenAI GPT-4o-mini | Claude Haiku | Better at classification (82% MMLU), function calling ensures consistency |
| **slot_extraction** | OpenAI GPT-4o-mini | Claude Haiku | Function calling = guaranteed JSON |
| **enrichment** | DeepSeek-V3 | OpenAI | 27x cheaper for bulk operations |
| **plan_synthesis** | Claude Sonnet-4 | OpenAI | Best at creative writing |
| **general** | OpenAI GPT-4o-mini | Claude Haiku | Good balance of cost/speed/quality |

---

## Cost Comparison

### Per Operation Costs

| Operation | Current (Claude) | New (Multi-LLM) | Savings |
|-----------|------------------|-----------------|---------|
| Domain detection (3K tokens) | $0.00075 | $0.00045 | 40% |
| Gap analysis (8K tokens) | $0.002 | $0.0012 | 40% |
| Enrichment (50K tokens) | $0.15 | $0.007 | 95% |
| Synthesis (10K tokens) | $0.03 | $0.03 | 0% (keep quality) |

### Annual Savings

**Assumptions**: 1000 users, 10 plans per user per month

- **Current (Claude only)**: $9,600/year
- **New (Multi-LLM)**: $1,200/year
- **Savings**: $8,400/year (87.5% reduction)

---

## Migration Steps

### Step 1: Environment Variables

Add API keys to your `.env` file:

```bash
# Existing
ANTHROPIC_API_KEY=sk-ant-...

# New (required)
OPENAI_API_KEY=sk-...

# New (optional - for cost optimization)
DEEPSEEK_API_KEY=sk-...
```

### Step 2: Initialize Providers

In `server/index.ts`, add initialization before starting the server:

```typescript
import { initializeLLMProviders } from './services/llmProviders';

// Initialize providers (call once at startup)
initializeLLMProviders();

app.listen(PORT, () => {
  console.log(`Server running on port ${PORT}`);
});
```

### Step 3: Migrate Service Files

#### Example: Migrate Gap Analyzer

**Before** (`claudeGapAnalyzer.ts`):
```typescript
import Anthropic from "@anthropic-ai/sdk";
const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY });

export class ClaudeGapAnalyzer {
  async analyzeGaps(...) {
    const response = await anthropic.messages.create({
      model: "claude-3-5-haiku-20241022",
      messages: [{ role: 'user', content: prompt }],
      system: systemMessage
    });

    // Parse JSON manually (error-prone!)
    const jsonMatch = response.content[0].text.match(/\{[\s\S]*\}/);
    return JSON.parse(jsonMatch[0]);
  }
}
```

**After** (`openAIGapAnalyzer.ts`):
```typescript
import { executeLLMCall } from './llmProvider';
import { OpenAIProvider } from './openAIProvider';

export class OpenAIGapAnalyzer {
  async analyzeGaps(...) {
    const result = await executeLLMCall(
      'gap_analysis',  // Auto-selects OpenAI
      async (provider) => {
        return await provider.generateStructured(
          [
            { role: 'system', content: systemMessage },
            { role: 'user', content: prompt }
          ],
          [
            {
              name: 'extract_gap_analysis',
              description: 'Extract gap analysis results',
              parameters: {
                type: 'object',
                properties: {
                  answeredQuestions: { type: 'array', ... },
                  unansweredQuestions: { type: 'array', ... },
                  nextQuestionToAsk: { type: 'object', ... },
                  completionPercentage: { type: 'number' }
                },
                required: ['answeredQuestions', 'unansweredQuestions']
              }
            }
          ]
        );
      }
    );

    // Guaranteed structured output!
    return OpenAIProvider.parseFunctionCall<GapAnalysisResult>(result);
  }
}
```

**Benefits**:
- ‚úÖ Guaranteed structured JSON output (no more parsing errors)
- ‚úÖ Automatic fallback to Claude if OpenAI fails
- ‚úÖ 40% cost reduction
- ‚úÖ Usage tracking and cost logging

---

## Function Calling vs Prompt Engineering

### Old Way: Prompt Engineering (Error-Prone)

```typescript
const prompt = `Return JSON in this exact format:
{
  "answeredQuestions": [...],
  "unansweredQuestions": [...]
}`;

const response = await anthropic.messages.create({...});

// Hope Claude returns valid JSON
const jsonMatch = response.content[0].text.match(/\{[\s\S]*\}/);
const data = JSON.parse(jsonMatch[0]); // ‚ö†Ô∏è May fail!
```

**Problems**:
- ‚ùå Sometimes Claude wraps JSON in markdown
- ‚ùå Sometimes JSON is invalid
- ‚ùå Inconsistent results (works 80% of the time)

### New Way: Function Calling (Guaranteed)

```typescript
const result = await provider.generateStructured(
  messages,
  [{
    name: 'extract_data',
    description: 'Extract structured data',
    parameters: {
      type: 'object',
      properties: {
        answeredQuestions: { type: 'array' },
        unansweredQuestions: { type: 'array' }
      },
      required: ['answeredQuestions', 'unansweredQuestions']
    }
  }]
);

// OpenAI guarantees valid JSON matching schema
const data = OpenAIProvider.parseFunctionCall<DataType>(result);
```

**Benefits**:
- ‚úÖ **100% valid JSON** (OpenAI enforces schema)
- ‚úÖ **Type-safe** (TypeScript knows the structure)
- ‚úÖ **Consistent** (no more random failures)

---

## Example: Complete Service Migration

### Before: `claudeQuestionGenerator.ts`

```typescript
export class ClaudeQuestionGenerator {
  async generateQuestions(
    domain: string,
    planMode: 'quick' | 'smart',
    userProfile: User,
    userMessage: string
  ): Promise<GeneratedQuestion[]> {
    const response = await anthropic.messages.create({
      model: "claude-3-5-haiku-20241022",
      messages: [{ role: 'user', content: this.buildPrompt(...) }]
    });

    // Manual JSON parsing
    const jsonMatch = response.content[0].text.match(/\{[\s\S]*\}/);
    const parsed = JSON.parse(jsonMatch[0]);
    return parsed.questions;
  }
}
```

### After: `openAIQuestionGenerator.ts`

```typescript
import { executeLLMCall } from './llmProvider';
import { OpenAIProvider } from './openAIProvider';

export class OpenAIQuestionGenerator {
  async generateQuestions(
    domain: string,
    planMode: 'quick' | 'smart',
    userProfile: User,
    userMessage: string
  ): Promise<GeneratedQuestion[]> {
    const result = await executeLLMCall(
      'question_generation',
      async (provider) => {
        return await provider.generateStructured(
          [
            { role: 'system', content: this.getSystemMessage() },
            { role: 'user', content: this.buildPrompt(domain, planMode, userMessage) }
          ],
          [
            {
              name: 'generate_questions',
              description: 'Generate planning questions',
              parameters: {
                type: 'object',
                properties: {
                  questions: {
                    type: 'array',
                    items: {
                      type: 'object',
                      properties: {
                        id: { type: 'string' },
                        question: { type: 'string' },
                        slot_path: { type: 'string' },
                        priority: { type: 'number' },
                        required: { type: 'boolean' }
                      },
                      required: ['id', 'question', 'slot_path', 'priority', 'required']
                    }
                  }
                },
                required: ['questions']
              }
            }
          ],
          {
            temperature: 0.7,
            maxTokens: 1500
          }
        );
      }
    );

    // Type-safe parsing
    const parsed = OpenAIProvider.parseFunctionCall<{ questions: GeneratedQuestion[] }>(result);

    console.log(`[QUESTION GENERATOR] Generated ${parsed.questions.length} questions | Cost: $${result.usage?.totalCost.toFixed(4)}`);

    return parsed.questions;
  }

  private getSystemMessage(): string {
    return `You are a question generation expert for lifestyle planning...`;
  }

  private buildPrompt(domain: string, planMode: string, userMessage: string): string {
    return `Generate ${planMode === 'quick' ? 5 : 7} high-quality questions for ${domain} planning...`;
  }
}
```

**Benefits**:
- ‚úÖ 40% cheaper ($0.0012 vs $0.002 for 8K tokens)
- ‚úÖ 100% reliable JSON parsing
- ‚úÖ Cost tracking built-in
- ‚úÖ Automatic fallback to Claude

---

## Testing Strategy

### Unit Tests

```typescript
import { executeLLMCall } from './llmProvider';
import { OpenAIProvider } from './openAIProvider';

describe('OpenAIQuestionGenerator', () => {
  it('should generate valid questions with structured output', async () => {
    const result = await executeLLMCall(
      'question_generation',
      async (provider) => {
        return await provider.generateStructured(messages, functions);
      }
    );

    const parsed = OpenAIProvider.parseFunctionCall<{ questions: any[] }>(result);

    expect(parsed.questions).toBeInstanceOf(Array);
    expect(parsed.questions.length).toBeGreaterThan(0);
    expect(parsed.questions[0]).toHaveProperty('id');
    expect(parsed.questions[0]).toHaveProperty('question');
  });

  it('should fallback to Claude if OpenAI fails', async () => {
    // Mock OpenAI to fail
    jest.spyOn(OpenAIProvider.prototype, 'generateStructured').mockRejectedValue(new Error('API Error'));

    const result = await executeLLMCall(
      'question_generation',
      async (provider) => {
        console.log(`Using provider: ${provider.name}`); // Should print "claude"
        return await provider.generateStructured(messages, functions);
      }
    );

    expect(result).toBeDefined();
  });
});
```

### Integration Tests

```typescript
describe('Multi-LLM Integration', () => {
  it('should use correct provider for each task', async () => {
    const tasks = [
      { type: 'domain_detection', expectedProvider: 'openai' },
      { type: 'enrichment', expectedProvider: 'deepseek' },
      { type: 'plan_synthesis', expectedProvider: 'claude' }
    ];

    for (const task of tasks) {
      const result = await executeLLMCall(
        task.type as any,
        async (provider) => {
          expect(provider.name).toBe(task.expectedProvider);
          return { content: 'test', usage: undefined };
        }
      );
    }
  });
});
```

---

## Rollout Plan

### Phase 1: Parallel Testing (Week 1)
- ‚úÖ Deploy multi-LLM code alongside existing Claude code
- ‚úÖ Route 10% of traffic to new system
- ‚úÖ Monitor error rates, costs, quality

### Phase 2: Gradual Migration (Week 2)
- ‚úÖ If metrics look good, increase to 50% traffic
- ‚úÖ A/B test user satisfaction
- ‚úÖ Compare plan quality

### Phase 3: Full Migration (Week 3)
- ‚úÖ Route 100% traffic to multi-LLM system
- ‚úÖ Keep Claude-only code as emergency fallback
- ‚úÖ Monitor for 1 week

### Phase 4: Cleanup (Week 4)
- ‚úÖ Remove old Claude-only code
- ‚úÖ Update documentation
- ‚úÖ Celebrate cost savings! üéâ

---

## Monitoring & Observability

### Log All LLM Calls

```typescript
const result = await executeLLMCall(
  'gap_analysis',
  async (provider) => {
    const start = Date.now();
    const result = await provider.generateStructured(messages, functions);
    const duration = Date.now() - start;

    console.log(`[LLM CALL] ${provider.name} | Task: gap_analysis | Duration: ${duration}ms | Cost: $${result.usage?.totalCost.toFixed(4)}`);

    return result;
  }
);
```

### Track Costs in Database

```typescript
// Add to your usage tracking
await storage.logLLMUsage({
  userId: user.id,
  provider: result.usage?.provider,
  inputTokens: result.usage?.inputTokens,
  outputTokens: result.usage?.outputTokens,
  cost: result.usage?.totalCost,
  task: 'gap_analysis',
  timestamp: new Date()
});
```

---

## FAQ

### Q: What if OpenAI API is down?
**A**: Automatic fallback to Claude. The system will log a warning and continue working.

### Q: Can I still use Claude for everything?
**A**: Yes! Set environment variable: `DEFAULT_LLM_PROVIDER=claude` and the system will prefer Claude for all tasks.

### Q: Is DeepSeek reliable enough?
**A**: DeepSeek is only used for non-critical bulk operations (enrichment/web search). Critical path (gap analysis, question generation) uses OpenAI or Claude.

### Q: How do I test locally?
**A**: Just add `OPENAI_API_KEY` to your `.env`. If not set, system falls back to Claude automatically.

### Q: Can I mix providers in one conversation?
**A**: Yes! Each task automatically selects the best provider. A single conversation might use OpenAI for questions, DeepSeek for enrichment, and Claude for synthesis.

---

## Next Steps

1. ‚úÖ **Phase 1 Complete**: LLM provider abstraction created
2. üîÑ **Phase 2 In Progress**: Install LangGraph dependencies
3. ‚è≥ **Phase 3 Pending**: Create LangGraph state machine
4. ‚è≥ **Phase 4 Pending**: Migrate universalPlanningAgent to LangGraph
5. ‚è≥ **Phase 5 Pending**: Add comprehensive tests

See the main implementation plan for detailed next steps.
